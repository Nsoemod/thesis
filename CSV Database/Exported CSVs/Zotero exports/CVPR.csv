"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"WLIXSJPZ","conferencePaper","2024","Xiong, Zhangyang; Li, Chenghong; Liu, Kenkun; Liao, Hongjie; Hu, Jianqiao; Zhu, Junyi; Ning, Shuliang; Qiu, Lingteng; Wang, Chongjie; Wang, Shijie; Cui, Shuguang; Han, Xiaoguang","MVHumanNet: A Large-Scale Dataset of Multi-View Daily Dressing Human Captures","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01872","https://ieeexplore.ieee.org/document/10656111/","","2024-06-16","2025-03-30 16:16:26","2025-03-30 16:16:26","2025-03-30 16:16:26","19801-19811","","","","","","MVHumanNet","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MW744ZPQ/Xiong et al. - 2024 - MVHumanNet A Large-Scale Dataset of Multi-View Daily Dressing Human Captures.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"6K8EAF8P","conferencePaper","2024","Liu, Haolin; Ye, Chongjie; Nie, Yinyu; He, Yingfan; Han, Xiaoguang","LASA: Instance Reconstruction from Real Scans using A Large-scale Aligned Shape Annotation Dataset","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01933","https://ieeexplore.ieee.org/document/10657169/","","2024-06-16","2025-03-30 16:16:29","2025-03-30 16:16:29","2025-03-30 16:16:29","20454-20464","","","","","","LASA","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/29SA2FK2/Liu et al. - 2024 - LASA Instance Reconstruction from Real Scans using A Large-scale Aligned Shape Annotation Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"9C8KUSIR","conferencePaper","2024","Sun, Peng; Shi, Bei; Yu, Daiwei; Lin, Tao","On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00897","https://ieeexplore.ieee.org/document/10657238/","Contemporary machine learning, which involves training large neural networks on massive datasets, faces significant computational challenges. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggles with large-scale and high-resolution datasets, hindering its practicality and feasibility. Thus, we re-examine existing methods and identify three properties essential for real-world applications: realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationallyefficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various model architectures and datasets demonstrate the advancement of RDED: we can distill a dataset to 10 images per class from full ImageNet1K [6] within 7 minutes, achieving a notable 42% accuracy with ResNet-18 [14] on a single RTX-4090 GPU (while the SOTA only achieves 21% but requires 6 hours). Code: https://github.com/LINs-lab/RDED.","2024-06-16","2025-03-30 16:16:32","2025-03-30 16:16:32","2025-03-30 16:16:32","9390-9399","","","","","","On the Diversity and Realism of Distilled Dataset","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TYEDFUDC/Sun et al. - 2024 - On the Diversity and Realism of Distilled Dataset An Efficient Dataset Distillation Paradigm.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"NUMF9YYQ","conferencePaper","2024","Zhang, Zhanwei; Chen, Minghao; Xiao, Shuai; Peng, Liang; Li, Hengjia; Lin, Binbin; Li, Ping; Wang, Wenxiao; Wu, Boxi; Cai, Deng","Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-Dataset 3D Object Detection","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01448","https://ieeexplore.ieee.org/document/10655619/","Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA). These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain. However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background. Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process. To resolve this problem, in this paper, we propose a novel pseudo label refinery framework. Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy. This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box. Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process. We alleviate this issue by generating additional proposals and aligning RoI features across different domains. Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks. Code will be available at https://github.com/Zhanwei-Z/PERE.","2024-06-16","2025-03-30 16:16:36","2025-03-30 16:16:36","2025-03-30 16:16:36","15291-15300","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/L5XLE273/Zhang et al. - 2024 - Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-Dataset 3D Object Detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"EPA2424Y","journalArticle","","Verma, Aayush Atul; Chakravarthi, Bharatesh; Vaghela, Arpitsinh; Wei, Hua; Yang, Yezhou","eTraM: Event-based Traffic Monitoring Dataset","","","","","","Event cameras, with their high temporal and dynamic range and minimal memory usage, have found applications in various fields. However, their potential in static traffic monitoring remains largely unexplored. To facilitate this exploration, we present eTraM - a first-of-itskind, fully event-based traffic monitoring dataset. eTraM offers 10 hr of data from different traffic scenarios in various lighting and weather conditions, providing a comprehensive overview of real-world situations. Providing 2M bounding box annotations, it covers eight distinct classes of traffic participants, ranging from vehicles to pedestrians and micro-mobility. eTraM’s utility has been assessed using state-of-the-art methods for traffic participant detection, including RVT, RED, and YOLOv8. We quantitatively evaluate the ability of event-based models to generalize on nighttime and unseen scenes. Our findings substantiate the compelling potential of leveraging event cameras for traffic monitoring, opening new avenues for research and application. eTraM is available at https: //eventbasedvision.github.io/eTraM .","","2025-03-30 16:16:38","2025-03-30 16:16:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MNAM585C/Verma et al. - eTraM Event-based Traffic Monitoring Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KCXVKPD6","journalArticle","","Gu, Jianyang; Vahidian, Saeed; Kungurtsev, Vyacheslav; Wang, Haonan; Jiang, Wei; You, Yang; Chen, Yiran","Efficient Dataset Distillation via Minimax Diffusion","","","","","","Dataset distillation reduces the storage and computational consumption of training a network by generating a small surrogate dataset that encapsulates rich information of the original large-scale one. However, previous distillation methods heavily rely on the sample-wise iterative optimization scheme. As the images-per-class (IPC) setting or image resolution grows larger, the necessary computation will demand overwhelming time and resources. In this work, we intend to incorporate generative diffusion techniques for computing the surrogate dataset. Observing that key factors for constructing an effective surrogate dataset are representativeness and diversity, we design additional minimax criteria in the generative training to enhance these facets for the generated images of diffusion models. We present a theoretical model of the process as hierarchical diffusion control demonstrating the flexibility of the diffusion process to target these criteria without jeopardizing the faithfulness of the sample to the desired distribution. The proposed method achieves state-of-the-art validation performance while demanding much less computational resources. Under the 100-IPC setting on ImageWoof, our method requires less than one-twentieth the distillation time of previous methods, yet yields even better performance. Source code and generated data are available in https://github.com/vimar-gu/MinimaxDiffusion.","","2025-03-30 16:16:40","2025-03-30 16:16:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Y7MPIHR2/Gu et al. - Efficient Dataset Distillation via Minimax Diffusion.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQZEQAT3","preprint","2024","Wu, Xiaoyang; Tian, Zhuotao; Wen, Xin; Peng, Bohao; Liu, Xihui; Yu, Kaicheng; Zhao, Hengshuang","Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training","","","","10.48550/arXiv.2308.09718","http://arxiv.org/abs/2308.09718","The rapid advancement of deep learning models often attributes to their ability to leverage massive training data. In contrast, such privilege has not yet fully benefited 3D deep learning, mainly due to the limited availability of large-scale 3D datasets. Merging multiple available data sources and letting them collaboratively train a single model is a potential solution. However, due to the large domain gap between 3D point cloud datasets, such mixed supervision could adversely affect the model's performance and lead to degenerated performance (i.e., negative transfer) compared to single-dataset training. In view of this challenge, we introduce Point Prompt Training (PPT), a novel framework for multi-dataset synergistic learning in the context of 3D representation learning that supports multiple pre-training paradigms. Based on this framework, we propose Prompt-driven Normalization, which adapts the model to different datasets with domain-specific prompts and Language-guided Categorical Alignment that decently unifies the multiple-dataset label spaces by leveraging the relationship between label text. Extensive experiments verify that PPT can overcome the negative transfer associated with synergistic learning and produce generalizable representations. Notably, it achieves state-of-the-art performance on each dataset using a single weight-shared model with supervised multi-dataset training. Moreover, when served as a pre-training framework, it outperforms other pre-training approaches regarding representation quality and attains remarkable state-of-the-art performance across over ten diverse downstream tasks spanning both indoor and outdoor 3D scenarios.","2024-07-21","2025-03-30 16:16:43","2025-03-30 16:16:43","2025-03-30 16:16:43","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2308.09718 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/PM8ISDUA/Wu et al. - 2024 - Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2308.09718","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JSR7GZ77","conferencePaper","2024","Geada, Rob; Towers, David; Forshaw, Matthew; Atapour-Abarghouei, Amir; McGough, A. Stephen","Insights from the Use of Previously Unseen Neural Architecture Search Datasets","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02127","https://ieeexplore.ieee.org/document/10657037/","The boundless possibility of neural networks which can be used to solve a problem – each with different performance – leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning methods as well as the best results from challenge participants.","2024-06-16","2025-03-30 16:16:45","2025-03-30 16:16:45","2025-03-30 16:16:45","22541-22550","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/W87INSAH/Geada et al. - 2024 - Insights from the Use of Previously Unseen Neural Architecture Search Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"EAS85N62","conferencePaper","2024","Khanna, Mukul; Mao, Yongsen; Jiang, Hanxiao; Haresh, Sanjay; Shacklett, Brennan; Batra, Dhruv; Clegg, Alexander; Undersander, Eric; Chang, Angel X.; Savva, Manolis","Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01550","https://ieeexplore.ieee.org/document/10657917/","We contribute the Habitat Synthetic Scenes Dataset (HSSD-200), a dataset of 211 high-quality 3D scenes, and use it to test navigation agent generalization to realistic 3D environments. Our dataset represents real interiors and contains a diverse set of 18,656 models of real-world objects. We investigate the impact of synthetic 3D scene dataset scale and realism on the task of training embodied agents to find and navigate to objects (ObjectGoal navigation). By comparing to synthetic 3D scene datasets from prior work, we find that scale helps in generalization, but the benefits quickly saturate, making visual fidelity and correlation to real-world scenes more important. Our experiments show that agents trained on our smaller-scale dataset can outperform agents trained on much larger datasets. Surprisingly, we observe that agents trained on just 122 scenes from our dataset outperform agents trained on 10,000 scenes from the ProcTHOR-10K dataset in terms of zero-shot generalization in real-world scanned environments.","2024-06-16","2025-03-30 16:16:48","2025-03-30 16:16:48","2025-03-30 16:16:48","16384-16393","","","","","","Habitat Synthetic Scenes Dataset (HSSD-200)","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NGMNPNU9/Khanna et al. - 2024 - Habitat Synthetic Scenes Dataset (HSSD-200) An Analysis of 3D Scene Scale and Realism Tradeoffs for.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"LNR73Q6S","conferencePaper","2024","Zhang, Xin; Du, Jiawei; Li, Yunsong; Xie, Weiying; Zhou, Joey Tianyi","Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02477","https://ieeexplore.ieee.org/document/10656153/","Dataset pruning aims to construct a coreset capable of achieving performance comparable to the original, full dataset. Most existing dataset pruning methods rely on snapshot-based criteria to identify representative samples, often resulting in poor generalization across various pruning and cross-architecture scenarios. Recent studies have addressed this issue by expanding the scope of training dynamics considered, including factors such as forgetting event and probability change, typically using an averaging approach. However, these works struggle to integrate a broader range of training dynamics without overlooking well-generalized samples, which may not be sufficiently highlighted in an averaging manner. In this study, we propose a novel dataset pruning method termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS utilizes a dual-depth strategy to achieve a balance between incorporating extensive training dynamics and identifying representative samples for dataset pruning. In the first depth, we estimate the series of each sample’s individual contributions spanning the training progress, ensuring comprehensive integration of training dynamics. In the second depth, we focus on the variability of the sample-wise contributions identified in the first depth to highlight wellgeneralized samples. Extensive experiments conducted on CIFAR and ImageNet datasets verify the superiority of TDDS over previous SOTA methods. Specifically on CIFAR100, our method achieves 54.51% accuracy with only 10% training data, surpassing baselines methods by more than 12.69%. Our codes are available at https://github. com/zhangxin-xd/Dataset-Pruning-TDDS.","2024-06-16","2025-03-30 16:16:51","2025-03-30 16:16:51","2025-03-30 16:16:51","26213-26222","","","","","","Spanning Training Progress","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5TP6UG93/Zhang et al. - 2024 - Spanning Training Progress Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"IKX6QP43","conferencePaper","2024","Le, Duy Tho; Gou, Chenhui; Datta, Stavya; Shi, Hengcan; Reid, Ian; Cai, Jianfei; Rezatofighi, Hamid","JRDB-PanoTrack: An Open-World Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02107","https://ieeexplore.ieee.org/document/10655429/","","2024-06-16","2025-03-30 16:16:56","2025-03-30 16:16:56","2025-03-30 16:16:56","22325-22334","","","","","","JRDB-PanoTrack","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Y7AIJA7E/Le et al. - 2024 - JRDB-PanoTrack An Open-World Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human En.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"BUXSFHWX","conferencePaper","2024","Zhan, Xinyu; Yang, Lixin; Zhao, Yifei; Mao, Kangrui; Xu, Hanlin; Lin, Zenan; Li, Kailin; Lu, Cewu","OakInk2 : A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00050","https://ieeexplore.ieee.org/document/10657079/","","2024-06-16","2025-03-30 16:17:00","2025-03-30 16:17:00","2025-03-30 16:17:00","445-456","","","","","","OakInk2","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3JV466ZU/Zhan et al. - 2024 - OakInk2  A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"L4288P6F","conferencePaper","2024","Feng, Chengjian; Zhong, Yujie; Jie, Zequn; Xie, Weidi; Ma, Lin","InstaGen: Enhancing Object Detection by Training on Synthetic Dataset","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01339","https://ieeexplore.ieee.org/document/10657847/","In this paper, we present a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pretrained, generative diffusion model, to augment it with the ability of localising instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. We conduct thorough experiments to show that, this enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer, to enhance object detectors by training on its generated samples, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 ∼ 5.2 AP) scenarios.","2024-06-16","2025-03-30 16:17:03","2025-03-30 16:17:03","2025-03-30 16:17:03","14121-14130","","","","","","InstaGen","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GQQ4B87J/Feng et al. - 2024 - InstaGen Enhancing Object Detection by Training on Synthetic Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"W7UI4ZIJ","preprint","2024","Zimmer, Walter; Wardana, Gerhard Arya; Sritharan, Suren; Zhou, Xingcheng; Song, Rui; Knoll, Alois C.","TUMTraf V2X Cooperative Perception Dataset","","","","10.48550/arXiv.2403.01316","http://arxiv.org/abs/2403.01316","Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, nearmiss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website.","2024-03-02","2025-03-30 16:17:05","2025-03-30 16:17:05","2025-03-30 16:17:05","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.01316 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/TM8T5JCZ/Zimmer et al. - 2024 - TUMTraf V2X Cooperative Perception Dataset.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2403.01316","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XPGXZWMQ","preprint","2025","Huang, Yifei; Chen, Guo; Xu, Jilan; Zhang, Mingfang; Yang, Lijin; Pei, Baoqi; Zhang, Hongjie; Dong, Lu; Wang, Yali; Wang, Limin; Qiao, Yu","EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World","","","","10.48550/arXiv.2403.16182","http://arxiv.org/abs/2403.16182","Being able to map the activities of others into one’s own point of view is a fundamental human skill even from a very early age. Taking a step toward understanding this human ability, we introduce EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by exocentric-view demonstration videos. Focusing on the potential applications in daily assistance and professional support, EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories. Along with the videos we record high-quality gaze data and provide detailed multimodal annotations, formulating a playground for modeling the human ability to bridge asynchronous procedural actions from different viewpoints. To this end, we present benchmarks such as crossview association, cross-view action planning, and crossview referenced skill assessment, along with detailed analysis. We expect EgoExoLearn can serve as an important resource for bridging the actions across views, thus paving the way for creating AI agents capable of seamlessly learning by observing humans in the real world. The dataset and benchmark codes are available at https: //github.com/OpenGVLab/EgoExoLearn.","2025-03-06","2025-03-30 16:17:07","2025-03-30 16:17:07","2025-03-30 16:17:07","","","","","","","EgoExoLearn","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.16182 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/R6HH9WKR/Huang et al. - 2025 - EgoExoLearn A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2403.16182","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WLH426M5","conferencePaper","2024","Flepp, Roman; Ignatov, Andrey; Timofte, Radu; Van Gool, Luc","Real-World Mobile Image Denoising Dataset with Efficient Baselines","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02111","https://ieeexplore.ieee.org/document/10658253/","The recently increased role of mobile photography has raised the standards of on-device photo processing tremendously. Despite the latest advancements in camera hardware, the mobile camera sensor area cannot be increased significantly due to physical constraints, leading to a pixel size of 0.6–2.0 µm, which results in strong image noise even in moderate lighting conditions. In the era of deep learning, one can train a CNN model to perform robust image denoising. However, there is still a lack of a substantially diverse dataset for this task. To address this problem, we introduce a novel Mobile Image Denoising Dataset (MIDD) comprising over 400,000 noisy / noise-free image pairs captured under various conditions by 20 different mobile camera sensors. Additionally, we propose a new DPreview test set consisting of data from 294 different cameras for precise model evaluation. Furthermore, we present the efficient baseline model SplitterNet for the considered mobile image denoising task that achieves high numerical and visual results, while being able to process 8MP photos directly on smartphone GPUs in under one second. Thereby outperforming models with similar runtimes. This model is also compatible with recent mobile NPUs, demonstrating an even higher speed when deployed on them. The conducted experiments demonstrate high robustness of the proposed solution when applied to images from previously unseen sensors, showing its high generalizability. The datasets, code and models can be found on the official project website1,2.","2024-06-16","2025-03-30 16:17:09","2025-03-30 16:17:09","2025-03-30 16:17:09","22368-22377","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9CB965R8/Flepp et al. - 2024 - Real-World Mobile Image Denoising Dataset with Efficient Baselines.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"87TVMP6C","journalArticle","","Ling, Lu; Sheng, Yichen; Tu, Zhi; Zhao, Wentian; Xin, Cheng; Wan, Kun; Yu, Lantao; Guo, Qianyu; Yu, Zixun; Lu, Yawen; Li, Xuanmao; Sun, Xingpeng; Ashok, Rohan; Mukherjee, Aniruddha; Kang, Hao; Kong, Xiangrui; Hua, Gang; Zhang, Tianyi; Benes, Bedrich; Bera, Aniket","DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision","","","","","","","","2025-03-30 16:17:11","2025-03-30 16:17:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EJ54P5IX/Ling et al. - DL3DV-10K A Large-Scale Scene Dataset for Deep Learning-based 3D Vision.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGVPSQ9G","conferencePaper","2024","Chen, Xiaoyang; Zheng, Hao; Li, Yuemeng; Ma, Yuncong; Ma, Liang; Li, Hongming; Fan, Yong","Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguation","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01116","https://ieeexplore.ieee.org/document/10656515/","A versatile medical image segmentation model applicable to images acquired with diverse equipment and protocols can facilitate model deployment and maintenance. However, building such a model typically demands a large, diverse, and fully annotated dataset, which is challenging to obtain due to the labor-intensive nature of data curation. To address this challenge, we propose a cost-effective alternative that harnesses multi-source data with only partial or sparse segmentation labels for training, substantially reducing the cost of developing a versatile model. We devise strategies for model self-disambiguation, prior knowledge incorporation, and imbalance mitigation to tackle challenges associated with inconsistently labeled multi-source data, including label ambiguity and modality, dataset, and class imbalances. Experimental results on a multi-modal dataset compiled from eight different sources for abdominal structure segmentation have demonstrated the effectiveness and superior performance of our method compared to state-of-the-art alternative approaches. We anticipate that its cost-saving features, which optimize the utilization of existing annotated data and reduce annotation efforts for new data, will have a signiﬁcant impact in the ﬁeld.","2024-06-16","2025-03-30 16:17:13","2025-03-30 16:17:13","2025-03-30 16:17:13","11747-11756","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TVUUBUTA/Chen et al. - 2024 - Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguatio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"BXWUPI3L","conferencePaper","2024","Qu, Chenfan; Zhong, Yiwu; Liu, Chongyu; Xu, Guitao; Peng, Dezhi; Guo, Fengjun; Jin, Lianwen","Towards Modern Image Manipulation Localization: A Large-Scale Dataset and Novel Methods","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01025","https://ieeexplore.ieee.org/document/10656008/","In recent years, image manipulation localization has attracted increasing attention due to its pivotal role in guaranteeing social media security. However, how to accurately identify the forged regions remains an open challenge. One of the main bottlenecks lies in the severe scarcity of highquality data, due to its costly creation process. To address this limitation, we propose a novel paradigm, termed as CAAA, to automatically and precisely annotate the numerous manually forged images from the web at the pixel level. We further propose a novel metric QES to facilitate the automatic filtering of unreliable annotations. With CAAA and QES, we construct a large-scale, diverse, and high-quality dataset comprising 123,150 manually forged images with mask annotations. Besides, we develop a new model APSCNet for accurate image manipulation localization. According to extensive experiments, our dataset significantly improves the performance of various models on the widelyused benchmarks and such improvements are attributed to our proposed effective methods. The dataset and code are publicly available at https://github.com/qcf-568/MIML.","2024-06-16","2025-03-30 16:17:15","2025-03-30 16:17:16","2025-03-30 16:17:15","10781-10790","","","","","","Towards Modern Image Manipulation Localization","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JJKDJFTF/Qu et al. - 2024 - Towards Modern Image Manipulation Localization A Large-Scale Dataset and Novel Methods.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"6B9NMZML","conferencePaper","2024","Xie, Yiming; Wei, Henglu; Liu, Zhenyi; Wang, Xiaoyu; Ji, Xiangyang","SynFog: A Photorealistic Synthetic Fog Dataset Based on End-to-End Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02056","https://ieeexplore.ieee.org/document/10656511/","To advance research in learning-based defogging algorithms, various synthetic fog datasets have been developed. However, existing datasets created using the Atmospheric Scattering Model (ASM) or real-time rendering engines often struggle to produce photo-realistic foggy images that accurately mimic the actual imaging process. This limitation hinders the effective generalization of models from synthetic to real data. In this paper, we introduce an end-to-end simulation pipeline designed to generate photorealistic foggy images. This pipeline comprehensively considers the entire physically-based foggy scene imaging process, closely aligning with real-world image capture methods. Based on this pipeline, we present a new synthetic fog dataset named SynFog, which features both sky light and active lighting conditions, as well as three levels of fog density. Experimental results demonstrate that models trained on SynFog exhibit superior performance in visual perception and detection accuracy compared to others when applied to real-world foggy images.","2024-06-16","2025-03-30 16:17:19","2025-03-30 16:17:19","2025-03-30 16:17:19","21763-21772","","","","","","SynFog","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/97IHQBC6/Xie et al. - 2024 - SynFog A Photorealistic Synthetic Fog Dataset Based on End-to-End Imaging Simulation for Advancing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"UWAVR42Z","preprint","2024","Liang, Guoqiang; Chen, Kanghao; Li, Hangyu; Lu, Yunfan; Wang, Lin","Towards Robust Event-guided Low-Light Image Enhancement: A Large-Scale Real-World Event-Image Dataset and Novel Approach","","","","10.48550/arXiv.2404.00834","http://arxiv.org/abs/2404.00834","Event camera has recently received much attention for low-light image enhancement (LIE) thanks to their distinct advantages, such as high dynamic range. However, current research is prohibitively restricted by the lack of largescale, real-world, and spatial-temporally aligned eventimage datasets. To this end, we propose a real-world (indoor and outdoor) dataset comprising over 30K pairs of images and events under both low and normal illumination conditions. To achieve this, we utilize a robotic arm that traces a consistent non-linear trajectory to curate the dataset with spatial alignment precision under 0.03mm. We then introduce a matching alignment strategy, rendering 90% of our dataset with errors less than 0.01s. Based on the dataset, we propose a novel event-guided LIE approach, called EvLight, towards robust performance in real-world low-light scenes. Specifically, we first design the multiscale holistic fusion branch to extract holistic structural and textural information from both events and images. To ensure robustness against variations in the regional illumination and noise, we then introduce a Signal-to-Noise-Ratio (SNR)-guided regional feature selection to selectively fuse features of images from regions with high SNR and enhance those with low SNR by extracting regional structure information from events. Extensive experiments on our dataset and the synthetic SDSD dataset demonstrate our EvLight significantly surpasses the frame-based methods, e.g., [4] by 1.14 dB and 2.62 dB, respectively.","2024-04-01","2025-03-30 16:17:21","2025-03-30 16:17:21","2025-03-30 16:17:21","","","","","","","Towards Robust Event-guided Low-Light Image Enhancement","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.00834 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/XEUTFAYH/Liang et al. - 2024 - Towards Robust Event-guided Low-Light Image Enhancement A Large-Scale Real-World Event-Image Datase.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2404.00834","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8KTWLYP","preprint","2024","Wang, Wenbo; Ho, Hsuan-I.; Guo, Chen; Rong, Boxiang; Grigorev, Artur; Song, Jie; Zarate, Juan Jose; Hilliges, Otmar","4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations","","","","10.48550/arXiv.2404.18630","http://arxiv.org/abs/2404.18630","The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.","2024-04-29","2025-03-30 16:17:23","2025-03-30 16:17:23","2025-03-30 16:17:23","","","","","","","4D-DRESS","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.18630 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/5D3YISP4/Wang et al. - 2024 - 4D-DRESS A 4D Dataset of Real-world Human Clothing with Semantic Annotations.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2404.18630","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTZL8FIW","preprint","2024","Ge, Yunhao; Tang, Yihe; Xu, Jiashu; Gokmen, Cem; Li, Chengshu; Ai, Wensi; Martinez, Benjamin Jose; Aydin, Arman; Anvari, Mona; Chakravarthy, Ayush K.; Yu, Hong-Xing; Wong, Josiah; Srivastava, Sanjana; Lee, Sharon; Zha, Shengxin; Itti, Laurent; Li, Yunzhu; Martín-Martín, Roberto; Liu, Miao; Zhang, Pengchuan; Zhang, Ruohan; Fei-Fei, Li; Wu, Jiajun","BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation","","","","10.48550/arXiv.2405.09546","http://arxiv.org/abs/2405.09546","The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as ""filled"" and ""folded""), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction. Project website: https://behavior-vision-suite.github.io/","2024-05-15","2025-03-30 16:17:26","2025-03-30 16:17:26","2025-03-30 16:17:26","","","","","","","BEHAVIOR Vision Suite","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.09546 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/VA26KPI7/Ge et al. - 2024 - BEHAVIOR Vision Suite Customizable Dataset Generation via Simulation.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2405.09546","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QXXHIN4Y","conferencePaper","2024","Hao, Ruiyang; Fan, Siqi; Dai, Yingru; Zhang, Zhenlin; Li, Chenxi; Wang, Yuntian; Yu, Haibao; Yang, Wenxian; Yuan, Jirui; Nie, Zaiqing","RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02109","https://ieeexplore.ieee.org/document/10656910/","The value of roadside perception, which could extend the boundaries of autonomous driving and trafﬁc management, has gradually become more prominent and acknowledged in recent years. However, existing roadside perception approaches only focus on the single-infrastructure sensor system, which cannot realize a comprehensive understanding of a trafﬁc area because of the limited sensing range and blind spots. Orienting high-quality roadside perception, we need Roadside Cooperative Perception (RCooper) to achieve practical area-coverage roadside perception for restricted trafﬁc areas. Rcooper has its own domain-speciﬁc challenges, but further exploration is hindered due to the lack of datasets. We hence release the ﬁrst real-world, large-scale RCooper dataset to bloom the research on practical roadside cooperative perception, including detection and tracking. The manually annotated dataset comprises 50k images and 30k point clouds, including two representative trafﬁc scenes (i.e., intersection and corridor). The constructed benchmarks prove the effectiveness of roadside cooperation perception and demonstrate the direction of further research. Codes and dataset can be accessed at: https://github.com/AIR-THU/DAIR-RCooper.","2024-06-16","2025-03-30 16:17:28","2025-03-30 16:17:29","2025-03-30 16:17:28","22347-22357","","","","","","RCooper","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YPDP3GDI/Hao et al. - 2024 - RCooper A Real-world Large-scale Dataset for Roadside Cooperative Perception.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"FSL3VID7","conferencePaper","2024","Jiang, Peng-Tao; Yang, Yuqi; Cao, Yang; Hou, Qibin; Cheng, Ming-Ming; Shen, Chunhua","Traffic Scene Parsing Through the TSP6K Dataset","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02066","https://ieeexplore.ieee.org/document/10657907/","Traffic scene perception in computer vision is a critically important task to achieve intelligent cities. To date, most existing datasets focus on autonomous driving scenes. We observe that the models trained on those driving datasets often yield unsatisfactory results on traffic monitoring scenes. However, little effort has been put into improving the traffic monitoring scene understanding, mainly due to the lack of specific datasets. To fill this gap, we introduce a specialized traffic monitoring dataset, termed TSP6K, containing images from the traffic monitoring scenario, with high-quality pixellevel and instance-level annotations. The TSP6K dataset captures more crowded traffic scenes with several times more traffic participants than the existing driving scenes. We perform a detailed analysis of the dataset and comprehensively evaluate previous popular scene parsing methods, instance segmentation methods and unsupervised domain adaption methods. Furthermore, considering the vast difference in instance sizes, we propose a detail refining decoder for scene parsing, which recovers the details of different semantic regions in traffic scenes owing to the proposed TSP6K dataset. Experiments show its effectiveness in parsing the traffic monitoring scenes. Code and dataset are available at https://github.com/PengtaoJiang/TSP6K .","2024-06-16","2025-03-30 16:17:32","2025-03-30 16:17:32","2025-03-30 16:17:32","21874-21885","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XUHW2X78/Jiang et al. - 2024 - Traffic Scene Parsing Through the TSP6K Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"8SEJBQC4","conferencePaper","2024","Li, Yiming; Li, Zhiheng; Chen, Nuo; Gong, Moonjun; Lyu, Zonglin; Wang, Zehong; Jiang, Peili; Feng, Chen","Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02081","https://ieeexplore.ieee.org/document/10658087/","","2024-06-16","2025-03-30 16:17:34","2025-03-30 16:17:34","2025-03-30 16:17:34","22041-22051","","","","","","Multiagent Multitraversal Multimodal Self-Driving","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4HLEF8PM/Li et al. - 2024 - Multiagent Multitraversal Multimodal Self-Driving Open MARS Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"95GZ66HS","conferencePaper","2024","Duan, Yuxing","LED: A Large-scale Real-world Paired Dataset for Event Camera Denoising","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02422","https://ieeexplore.ieee.org/document/10655931/","Event camera has significant advantages in capturing dynamic scene information while being prone to noise interference, particularly in challenging conditions like low threshold and low illumination. However, most existing research focuses on gentle situations, hindering event camera applications in realistic complex scenarios. To tackle this limitation and advance the field, we construct a new paired real-world event denoising dataset (LED), including 3K sequences with 18K seconds of high-resolution (1200*680) event streams and showing three notable distinctions compared to others: diverse noise levels and scenes, largerscale with high-resolution, and high-quality GT. Specifically, it contains stepped parameters and varying illumination with diverse scenarios. Moreover, based on the property of noise events inconsistency and signal events consistency, we propose a novel effective denoising framework(DED) using homogeneous dual events to generate the GT with better separating noise from the raw. Furthermore, we design a bio-inspired baseline leveraging LeakyIntegrate-and-Fire (LIF) neurons with dynamic thresholds to realize accurate denoising. The experimental results demonstrate that the remarkable performance of the proposed approach on different datasets.The dataset and code are at https://github.com/Yee-Sing/led.","2024-06-16","2025-03-30 16:17:38","2025-03-30 16:17:38","2025-03-30 16:17:38","25637-25647","","","","","","LED","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EBFTBWIV/Duan - 2024 - LED A Large-scale Real-world Paired Dataset for Event Camera Denoising.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"QR72N4F7","conferencePaper","2024","Vecchio, Giuseppe; Deschaintre, Valentin","MatSynth: A Modern PBR Materials Dataset","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","10.1109/CVPR52733.2024.02087","http://arxiv.org/abs/2401.06056","We introduce MatSynth, a dataset of 4, 000+ CC0 ultrahigh resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation, and acquisition. However, in the past 6 years, most research in material acquisition or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset, we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset for material acquisition and generation applications. The complete data further contains metadata with each material’s origin, license, category, tags, creation method, and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www. gvecchio.com/matsynth.","2024-06-16","2025-03-30 16:17:41","2025-03-30 16:17:41","2025-03-30 16:17:41","22109-22118","","","","","","MatSynth","","","","","","","en","","","","","arXiv.org","","arXiv:2401.06056 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/4XPUTMTU/Vecchio and Deschaintre - 2024 - MatSynth A Modern PBR Materials Dataset.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Graphics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6VY3PM9C","preprint","2024","Lilja, Adam; Fu, Junsheng; Stenborg, Erik; Hammarstrand, Lars","Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It","","","","10.48550/arXiv.2312.06420","http://arxiv.org/abs/2312.06420","The task of online mapping is to predict a local map using current sensor observations, e.g. from lidar and camera, without relying on a pre-built map. State-of-the-art methods are based on supervised learning and are trained predominantly using two datasets: nuScenes and Argoverse 2. However, these datasets revisit the same geographic locations across training, validation, and test sets. Specifically, over $80$% of nuScenes and $40$% of Argoverse 2 validation and test samples are less than $5$ m from a training sample. At test time, the methods are thus evaluated more on how well they localize within a memorized implicit map built from the training data than on extrapolating to unseen locations. Naturally, this data leakage causes inflated performance numbers and we propose geographically disjoint data splits to reveal the true performance in unseen environments. Experimental results show that methods perform considerably worse, some dropping more than $45$ mAP, when trained and evaluated on proper data splits. Additionally, a reassessment of prior design choices reveals diverging conclusions from those based on the original split. Notably, the impact of lifting methods and the support from auxiliary tasks (e.g., depth supervision) on performance appears less substantial or follows a different trajectory than previously perceived. Splits can be found at https://github.com/LiljaAdam/geographical-splits","2024-04-05","2025-03-30 16:17:43","2025-03-30 16:17:43","2025-03-30 16:17:43","","","","","","","Localization Is All You Evaluate","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2312.06420 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/GT6TUABJ/Lilja et al. - 2024 - Localization Is All You Evaluate Data Leakage in Online Mapping Datasets and How to Fix It.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2312.06420","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IM6GKE7Z","conferencePaper","2024","Burgert, Ryan D.; Price, Brian L.; Kuen, Jason; Li, Yijun; Ryoo, Michael S.","MAGICK: A Large-Scale Captioned Dataset from Matting Generated Images Using Chroma Keying","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02132","https://ieeexplore.ieee.org/document/10656389/","We introduce MAGICK, a large-scale dataset of generated objects with high-quality alpha mattes. While image generation methods have produced segmentations, they cannot generate alpha mattes with accurate details in hair, fur, and transparencies. This is likely due to the small size of current alpha matting datasets and the difficulty in obtaining ground-truth alpha. We propose a scalable method for synthesizing images of objects with high-quality alpha that can be used as a ground-truth dataset. A key idea is to generate objects on a single-colored background so chroma keying approaches can be used to extract the alpha. However, this faces several challenges, including that current text-to-image generation methods cannot create images that can be easily chroma keyed and that chroma keying is an underconstrained problem that generally requires manual intervention for high-quality results. We address this using a combination of generation and alpha extraction methods. Using our method, we generate a dataset of 150,000 objects with alpha. We show the utility of our dataset by training an alpha-to-rgb generation method that outperforms baselines. Please see our project website at https://ryanndagreat.github.io/MAGICK/.","2024-06-16","2025-03-30 16:17:46","2025-03-30 16:17:46","2025-03-30 16:17:46","22595-22604","","","","","","MAGICK","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YFP92E5D/Burgert et al. - 2024 - MAGICK A Large-Scale Captioned Dataset from Matting Generated Images Using Chroma Keying.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"P6SGFNE6","conferencePaper","2024","Yan, Ming; Zhang, Yan; Cai, Shuqiang; Fan, Shuqi; Lin, Xincheng; Dai, Yudi; Shen, Siqi; Wen, Chenglu; Xu, Lan; Ma, Yuexin; Wang, Cheng","RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00219","https://ieeexplore.ieee.org/document/10655546/","","2024-06-16","2025-03-30 16:17:48","2025-03-30 16:17:48","2025-03-30 16:17:48","2250-2262","","","","","","RELI11D","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6W4KCDUV/Yan et al. - 2024 - RELI11D A Comprehensive Multimodal Human Motion Dataset and Method.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"WMYZPHVK","preprint","2024","Slyman, Eric; Lee, Stefan; Cohen, Scott; Kafle, Kushal","FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication","","","","10.48550/arXiv.2404.16123","http://arxiv.org/abs/2404.16123","Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used imagecaption datasets collected from the web – datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-toimplement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.","2024-04-24","2025-03-30 16:17:50","2025-03-30 16:17:50","2025-03-30 16:17:50","","","","","","","FairDeDup","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2404.16123 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/4B9XJTTJ/Slyman et al. - 2024 - FairDeDup Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Dedupli.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2404.16123","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"863Q7SJB","conferencePaper","2024","Wu, Tao; He, Runyu; Wu, Gangshan; Wang, Limin","SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01754","https://ieeexplore.ieee.org/document/10656134/","Video-based visual relation detection tasks, such as video scene graph generation, play important roles in finegrained video understanding. However, current video visual relation detection datasets have two main limitations that hinder the progress of research in this area. First, they do not explore complex human-human interactions in multi-person scenarios. Second, the relation types of existing datasets have relatively low-level semantics and can be often recognized by appearance or simple prior information, without the need for detailed spatio-temporal context reasoning. Nevertheless, comprehending high-level interactions between humans is crucial for understanding complex multi-person videos, such as sports and surveillance videos. To address this issue, we propose a new video visual relation detection task: video human-human interaction detection, and build a dataset named SportsHHI for it. SportsHHI contains 34 high-level interaction classes from basketball and volleyball sports. 118,075 human bounding boxes and 50,649 interaction instances are annotated on 11,398 keyframes. To benchmark this, we propose a twostage baseline method and conduct extensive experiments to reveal the key factors for a successful human-human interaction detector. We hope that SportsHHI can stimulate research on human interaction understanding in videos and promote the development of spatio-temporal context modeling techniques in video visual relation detection.","2024-06-16","2025-03-30 16:17:54","2025-03-30 16:17:54","2025-03-30 16:17:54","18537-18546","","","","","","SportsHHI","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QDI6P8LV/Wu et al. - 2024 - SportsHHI A Dataset for Human-Human Interaction Detection in Sports Videos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"KNJIGCQN","conferencePaper","2024","Yuan, Tongtong; Zhang, Xuange; Liu, Kun; Liu, Bo; Chen, Chen; Jin, Jian; Jiao, Zhenzhen","Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02082","https://ieeexplore.ieee.org/document/10656129/","Surveillance videos are important for public security. However, current surveillance video tasks mainly focus on classifying and localizing anomalous events. Existing methods are limited to detecting and classifying the predefined events with unsatisfactory semantic understanding, although they have obtained considerable performance. To address this issue, we propose a new research direction of surveillance video-and-language understanding (VALU), and construct the first multimodal surveillance video dataset. We manually annotate the real-world surveillance dataset UCFCrime with fine-grained event content and timing. Our newly annotated dataset, UCA (UCF-Crime Annotation)1, contains 23,542 sentences, with an average length of 20 words, and its annotated videos are as long as 110.7 hours. Furthermore, we benchmark SOTA models for four multimodal tasks on this newly created dataset, which serve as new baselines for surveillance VALU. Through experiments, we find that mainstream models used in previously public datasets perform poorly on surveillance video, demonstrating new challenges in surveillance VALU. We also conducted experiments on multimodal anomaly detection. These results demonstrate that our multimodal surveillance learning can improve the performance of anomaly detection. All the experiments highlight the necessity of constructing this dataset to advance surveillance AI.","2024-06-16","2025-03-30 16:17:56","2025-03-30 16:17:56","2025-03-30 16:17:56","22052-22061","","","","","","Towards Surveillance Video-and-Language Understanding","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KDYNXIEY/Yuan et al. - 2024 - Towards Surveillance Video-and-Language Understanding New Dataset, Baselines, and Challenges.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"EEGSQ3HM","conferencePaper","2024","Deng, Wenxiao; Li, Wenbin; Ding, Tianyu; Wang, Lei; Zhang, Hongguang; Huang, Kuihua; Huo, Jing; Gao, Yang","Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01614","https://ieeexplore.ieee.org/document/10656653/","Dataset distillation has emerged as a promising approach in deep learning, enabling efﬁcient training with small synthetic datasets derived from larger real ones. Particularly, distribution matching-based distillation methods attract attention thanks to its effectiveness and low computational cost. However, these methods face two primary limitations: the dispersed feature distribution within the same class in synthetic datasets, reducing class discrimination, and an exclusive focus on mean feature consistency, lacking precision and comprehensiveness. To address these challenges, we introduce two novel constraints: a class centralization constraint and a covariance matching constraint. The class centralization constraint aims to enhance class discrimination by more closely clustering samples within classes. The covariance matching constraint seeks to achieve more accurate feature distribution matching between real and synthetic datasets through local feature covariance matrices, particularly beneﬁcial when sample sizes are much smaller than the number of features. Experiments demonstrate notable improvements with these constraints, yielding performance boosts of up to 6.6% on CIFAR10, 2.9% on SVHN, 2.5% on CIFAR100, and 2.5% on TinyImageNet, compared to the state-of-the-art relevant methods. In addition, our method maintains robust performance in cross-architecture settings, with a maximum performance drop of 1.7% on four architectures. Code is available at https://github.com/VincenDen/IID.","2024-06-16","2025-03-30 16:17:59","2025-03-30 16:17:59","2025-03-30 16:17:59","17057-17066","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AGPQ3MPF/Deng et al. - 2024 - Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"62YNJGNW","conferencePaper","2024","Xu, Jinglin; Zhao, Guohao; Yin, Sibo; Zhou, Wenhao; Peng, Yuxin","FineSports: A Multi-Person Hierarchical Sports Video Dataset for Fine-Grained Action Understanding","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02057","https://ieeexplore.ieee.org/document/10657871/","","2024-06-16","2025-03-30 16:18:01","2025-03-30 16:18:01","2025-03-30 16:18:01","21773-21782","","","","","","FineSports","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MREL4YHA/Xu et al. - 2024 - FineSports A Multi-Person Hierarchical Sports Video Dataset for Fine-Grained Action Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"HUHYD3IE","conferencePaper","2024","Guo, Heng; Ren, Jieji; Wang, Feishi; Shi, Boxin; Ren, Mingjun; Matsushita, Yasuyuki","DiLiGenRT: A Photometric Stereo Dataset with Quantified Roughness and Translucency","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01122","https://ieeexplore.ieee.org/document/10655452/","Photometric stereo faces challenges from non-Lambertian reflectance in real-world scenarios. Systematically measuring the reliability of photometric stereo methods in handling such complex reflectance necessitates a real-world dataset with quantitatively controlled reflectances. This paper introduces DiLiGenRT, the first real-world dataset for evaluating photometric stereo methods under quantified reflectances by manufacturing 54 hemispheres with varying degrees of two reflectance properties: Roughness and Transluency. Unlike qualitative and semantic labels, such as “diffuse” and “specular,” that have been used in previous datasets, our quantified dataset allows comprehensive and systematic benchmark evaluations. In addition, it facilitates selecting best-fit photometric stereo methods based on the quantitative reflectance properties. Our dataset and benchmark results are available at https://photometricstereo. github.io/diligentrt.html.","2024-06-16","2025-03-30 16:18:05","2025-03-30 16:18:05","2025-03-30 16:18:05","11810-11820","","","","","","DiLiGenRT","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/G2ZQFPXC/Guo et al. - 2024 - DiLiGenRT A Photometric Stereo Dataset with Quantified Roughness and Translucency.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"J2TQ2TVC","conferencePaper","2024","Nguyen, Thien-Minh; Yuan, Shenghai; Nguyen, Thien Hoang; Yin, Pengyu; Cao, Haozhi; Xie, Lihua; Wozniak, Maciej; Jensfelt, Patric; Thiel, Marko; Ziegenbein, Justin; Blunder, Noel","MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02105","https://ieeexplore.ieee.org/document/10656126/","","2024-06-16","2025-03-30 16:18:07","2025-03-30 16:18:07","2025-03-30 16:18:07","22304-22313","","","","","","MCD","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8RNNDLIU/Nguyen et al. - 2024 - MCD Diverse Large-Scale Multi-Campus Dataset for Robot Perception.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"K4ISHFHM","conferencePaper","2024","Shu, Yong; Shen, Liquan; Hu, Xiangyu; Li, Mengyao; Zhou, Zihao","Towards Real-World HDR Video Reconstruction: A Large-Scale Benchmark Dataset and A Two-Stage Alignment Network","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00278","https://ieeexplore.ieee.org/document/10658278/","As an important and practical way to obtain high dynamic range (HDR) video, HDR video reconstruction from sequences with alternating exposures is still less explored, mainly due to the lack of large-scale real-world datasets. Existing methods are mostly trained on synthetic datasets, which perform poorly in real scenes. In this work, to facilitate the development of real-world HDR video reconstruction, we present Real-HDRV, a large-scale real-world benchmark dataset for HDR video reconstruction, featuring various scenes, diverse motion patterns, and high-quality labels. Specifically, our dataset contains 500 LDRs-HDRs video pairs, comprising about 28,000 LDR frames and 4,000 HDR labels, covering daytime, nighttime, indoor, and outdoor scenes. To our best knowledge, our dataset is the largest real-world HDR video reconstruction dataset. Correspondingly, we propose an end-to-end network for HDR video reconstruction, where a novel two-stage strategy is designed to perform alignment sequentially. Specifically, the first stage performs global alignment with the adaptively estimated global offsets, reducing the difficulty of subsequent alignment. The second stage implicitly performs local alignment in a coarse-to-fine manner at the feature level using the adaptive separable convolution. Extensive experiments demonstrate that: (1) models trained on our dataset can achieve better performance on real scenes than those trained on synthetic datasets; (2) our method outperforms previous state-of-the-art methods. Our dataset is available at https://github.com/yungsyu99/Real-HDRV.","2024-06-16","2025-03-30 16:18:09","2025-03-30 16:18:09","2025-03-30 16:18:09","2879-2888","","","","","","Towards Real-World HDR Video Reconstruction","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RP2JT2XX/Shu et al. - 2024 - Towards Real-World HDR Video Reconstruction A Large-Scale Benchmark Dataset and A Two-Stage Alignme.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"MVITSKC5","conferencePaper","2024","Kent, Daniel; Alyaqoub, Mohammed; Lu, Xiaohu; Khatounabadi, Hamed; Sung, Kookjin; Scheller, Cole; Dalat, Alexander; Guo, Xinwei; Bin Thabit, Asma; Whitley, Roberto; Radha, Hayder","MSU-4S - The Michigan State University Four Seasons Dataset","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02138","https://ieeexplore.ieee.org/document/10656488/","Public datasets, such as KITTI, nuScenes, and Waymo, have played a key role in the research and development of autonomous vehicles and advanced driver assistance systems. However, many of these datasets fail to incorporate a full range of driving conditions; some datasets only contain clear-weather conditions, underrepresenting or entirely missing colder weather conditions such as snow or autumn scenes with bright colorful foliage. In this paper, we present the Michigan State University Four Seasons (MSU4S) Dataset, which contains real-world collections of autonomous vehicle data from varied types of driving scenarios. These scenarios were recorded throughout a full range of seasons, and capture clear, rainy, snowy, and fall weather conditions, at varying times of day. MSU-4S contains more than 100,000 two- and three-dimensional frames for camera, lidar, and radar data, as well as Global Navigation Satellite System (GNSS), wheel speed, and steering data, all annotated with weather, time-of-day, and time-of-year. Our data includes cluttered scenes that have large numbers of vehicles and pedestrians; and it also captures industrial scenes, busy traffic thoroughfare with traffic lights and numerous signs, and scenes with dense foliage. While providing a diverse set of scenes, our data incorporate an important feature: virtually every scene and its corresponding lidar, camera, and radar frames were captured in four different seasons, enabling unparalleled object detection analysis and testing of the domain shift problem across weather conditions. In that context, we present detailed analyses for 3D and 2D object detection showing a strong domain shift effect among MSU-4S data segments collected across different conditions. MSU-4S will also enable advanced multimodal fusion research including different combinations of camera-lidar-radar fusion, which continues to be of strong interest for the computer vision, autonomous driving and ADAS development communities. The MSU-4S dataset is available online at https://egr.msu.edu/waves/msu4s.","2024-06-16","2025-03-30 16:18:11","2025-03-30 16:18:11","2025-03-30 16:18:11","22658-22667","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CRZW65X6/Kent et al. - 2024 - MSU-4S - The Michigan State University Four Seasons Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"5U3P8TBK","preprint","2023","Jung, HyunJun; Zhai, Guangyao; Wu, Shun-Cheng; Ruhkamp, Patrick; Schieber, Hannah; Rizzoli, Giulia; Wang, Pengyuan; Zhao, Hongcheng; Garattoni, Lorenzo; Meier, Sven; Roth, Daniel; Navab, Nassir; Busam, Benjamin","HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Perception Dataset with Household Objects in Realistic Scenarios","","","","10.48550/arXiv.2212.10428","http://arxiv.org/abs/2212.10428","Estimating 6D object poses is a major challenge in 3D computer vision. Building on successful instance-level approaches, research is shifting towards category-level pose estimation for practical applications. Current category-level datasets, however, fall short in annotation quality and pose variety. Addressing this, we introduce HouseCat6D, a new category-level 6D pose dataset. It features 1) multi-modality with Polarimetric RGB and Depth (RGBD+P), 2) encompasses 194 diverse objects across 10 household categories, including two photometrically challenging ones, and 3) provides high-quality pose annotations with an error range of only 1.35 mm to 1.74 mm. The dataset also includes 4) 41 large-scale scenes with comprehensive viewpoint and occlusion coverage, 5) a checkerboard-free environment, and 6) dense 6D parallel-jaw robotic grasp annotations. Additionally, we present benchmark results for leading category-level pose estimation networks.","2023-12-01","2025-03-30 16:18:14","2025-03-30 16:18:14","2025-03-30 16:18:14","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2212.10428 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/HNRA8VNS/Jung et al. - 2023 - HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Perception Dataset with Household O.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2212.10428","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3V8YX3G","journalArticle","","Tudosiu, Petru-Daniel; Yang, Yongxin; Zhang, Shifeng; Chen, Fei; McDonagh, Steven; Lampouras, Gerasimos; Iacobacci, Ignacio; Parisot, Sarah","MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation","","","","","","","","2025-03-30 16:18:16","2025-03-30 16:18:16","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JB8Q3DCG/Tudosiu et al. - MULAN A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8YUT66N","conferencePaper","2024","Sun, Yanjun; Qiu, Yue; Khan, Mariia; Matsuzawa, Fumiya; Iwata, Kenji","The STVchrono Dataset: Towards Continuous Change Recognition in Time","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01338","https://ieeexplore.ieee.org/document/10657948/","Recognizing continuous changes offers valuable insights into past historical events, supports current trend analysis, and facilitates future planning. This knowledge is crucial for a variety of fields, such as meteorology and agriculture, environmental science, urban planning and construction, tourism, and cultural preservation. Currently available datasets in the field of scene change understanding primarily concentrate on two main tasks: the detection of changed regions within a scene and the linguistic description of the change content. Existing datasets focus on recognizing discrete changes, such as adding or deleting an object from two images, and largely rely on artificially generated images. Consequently, the existing change understanding methods primarily focus on identifying distinct object differences, overlooking the importance of continuous, gradual changes occurring over extended time intervals.","2024-06-16","2025-03-30 16:18:19","2025-03-30 16:18:19","2025-03-30 16:18:19","14111-14120","","","","","","The STVchrono Dataset","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7R6YXQWR/Sun et al. - 2024 - The STVchrono Dataset Towards Continuous Change Recognition in Time.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"HKCD7W58","journalArticle","","Jahangard, Simindokht; Cai, Zhixi; Wen, Shiki; Rezatofighi, Hamid","JRDB-Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Interactions Within Social Groups","","","","","","Understanding human social behaviour is crucial in computer vision and robotics. Micro-level observations like individual actions fall short, necessitating a comprehensive approach that considers individual behaviour, intra-group dynamics, and social group levels for a thorough understanding. To address dataset limitations, this paper introduces JRDB-Social, an extension of JRDB [2]. Designed to fill gaps in human understanding across diverse indoor and outdoor social contexts, JRDB-Social provides annotations at three levels: individual attributes, intra-group interactions, and social group context. This dataset aims to enhance our grasp of human social dynamics for robotic applications. Utilizing the recent cutting-edge multi-modal large language models, we evaluated our benchmark to explore their capacity to decipher social human behaviour.","","2025-03-30 16:18:21","2025-03-30 16:18:21","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NF75HDPL/Jahangard et al. - JRDB-Social A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Inter.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2C8LHBZW","conferencePaper","2024","Su, Duo; Hou, Junjie; Gao, Weizhi; Tian, Yingjie; Tang, Bowen","D<sup>4</sup> M: Dataset Distillation via Disentangled Diffusion Model","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00555","https://ieeexplore.ieee.org/document/10657869/","Dataset distillation offers a lightweight synthetic dataset for fast network training with promising test accuracy. To imitate the performance of the original dataset, most approaches employ bi-level optimization and the distillation space relies on the matching architecture. Nevertheless, these approaches either suffer significant computational costs on large-scale datasets or experience performance decline on cross-architectures. We advocate for designing an economical dataset distillation framework that is independent of the matching architectures. With empirical observations, we argue that constraining the consistency of the real and synthetic image spaces will enhance the cross-architecture generalization. Motivated by this, we introduce Dataset Distillation via Disentangled Diffusion Model (D4M), an efficient framework for dataset distillation. Compared to architecture-dependent methods, D4M employs latent diffusion model to guarantee consistency and incorporates label information into category prototypes. The distilled datasets are versatile, eliminating the need for repeated generation of distinct datasets for various architectures. Through comprehensive experiments, D4M demonstrates superior performance and robust generalization, surpassing the SOTA methods across most aspects.","2024-06-16","2025-03-30 16:18:24","2025-03-30 16:18:24","2025-03-30 16:18:24","5809-5818","","","","","","D<sup>4</sup> M","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WJTBYCV2/Su et al. - 2024 - D4 M Dataset Distillation via Disentangled Diffusion Model.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"36F9K7SF","conferencePaper","2024","Lu, Cheng-You; Zhou, Peisen; Xing, Angela; Pokhariya, Chandradeep; Dey, Arnab; Shah, Ishaan Nikhil; Mavidipalli, Rugved; Hu, Dylan; Comport, Andrew I.; Chen, Kefan; Sridhar, Srinath","DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02120","https://ieeexplore.ieee.org/document/10657232/","Advances in neural fields are enabling high-fidelity capture of the shape and appearance of dynamic 3D scenes. However, their capabilities lag behind those offered by conventional representations such as 2D videos because of algorithmic challenges and the lack of large-scale multi-view real-world datasets. We address the dataset limitation with DiVa-360, a real-world 360◦ dynamic visual dataset that contains synchronized high-resolution and long-duration multi-view video sequences of table-scale scenes captured using a customized low-cost system with 53 cameras. It contains 21 object-centric sequences categorized by different motion types, 25 intricate hand-object interaction sequences, and 8 long-duration sequences for a total of 17.4 M image frames. In addition, we provide foreground-background segmentation masks, synchronized audio, and text descriptions. We benchmark the state-of-the-art dynamic neural field methods on DiVa-360 and provide insights about existing methods and future challenges on long-duration neural field capture.","2024-06-16","2025-03-30 16:18:27","2025-03-30 16:18:27","2025-03-30 16:18:27","22466-22476","","","","","","DiVa-360","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3UNEMUFK/Lu et al. - 2024 - DiVa-360 The Dynamic Visual Dataset for Immersive Neural Fields.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"2HC7VPMH","conferencePaper","2024","Rangwani, Harsh; Mondal, Pradipto; Mondal, Pradipto; Mishra, Mayank; Asokan, Ashish Ramayee; Babu, R. Venkatesh","DeiT-LT: Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02208","https://ieeexplore.ieee.org/document/10658499/","Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self-attention blocks. However, unlike Convolutional Neural Network (CNN), ViT’s simple architecture has no informative inductive bias (e.g., locality, etc.). Due to this, ViT requires a large amount of data for pre-training. Various data-efﬁcient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on longtailed datasets. In DeiT-LT, we introduce an efﬁcient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. This leads to the learning of local CNN-like features in early ViT blocks, improving generalization for tail classes. Further, to mitigate overﬁtting, we propose distilling from a ﬂat CNN teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks. With the proposed DeiT-LT scheme, the distillation DIST token becomes an expert on the tail classes, and the classiﬁer CLS token becomes an expert on the head classes. The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture. We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from smallscale CIFAR-10 LT to large-scale iNaturalist-2018. Project Page: https://rangwani-harsh.github.io/DeiT-LT.","2024-06-16","2025-03-30 16:18:30","2025-03-30 16:18:30","2025-03-30 16:18:30","23396-23406","","","","","","DeiT-LT","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/75GDUDBQ/Rangwani et al. - 2024 - DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"LFA9BEYJ","conferencePaper","2024","Shum, Ka Chun; Kim, Jaeyeon; Hua, Binh-Son; Nguyen, Duc Thanh; Yeung, Sai-Kit","Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00495","https://ieeexplore.ieee.org/document/10656426/","Neural radiance field (NeRF) is an emerging technique for 3D scene reconstruction and modeling. However, current NeRF-based methods are limited in the capabilities of adding or removing objects. This paper fills the aforementioned gap by proposing a new language-driven method for object manipulation in NeRFs through dataset updates. Specifically, to insert an object represented by a set of multi-view images into a background NeRF, we use a text-to-image diffusion model to blend the object into the given background across views. The generated images are then used to update the NeRF so that we can render view-consistent images of the object within the background. To ensure view consistency, we propose a dataset update strategy that prioritizes the radiance field training based on camera poses in a poseordered manner. We validate our method in two case studies: object insertion and object removal. Experimental results show that our method can generate photo-realistic results and achieves state-of-the-art performance in NeRF editing.","2024-06-16","2025-03-30 16:18:33","2025-03-30 16:18:33","2025-03-30 16:18:33","5176-5187","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZM7T5DBJ/Shum et al. - 2024 - Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"6LHBYJ9W","preprint","2024","Dao, Trung Tuan; Vu, Duc Hong; Pham, Cuong; Tran, Anh","EFHQ: Multi-purpose ExtremePose-Face-HQ dataset","","","","10.48550/arXiv.2312.17205","http://arxiv.org/abs/2312.17205","The existing facial datasets, while having plentiful images at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with profile or pitched faces. This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k high-quality images of faces at extreme poses. To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings. Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation, and face reenactment. Specifically, training with EFHQ helps models generalize well across diverse poses, significantly improving performance in scenarios involving extreme views, confirmed by extensive experiments. Additionally, we utilize EFHQ to define a challenging cross-view face verification benchmark, in which the performance of SOTA face recognition models drops 5-37% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild.","2024-04-11","2025-03-30 16:18:34","2025-03-30 16:18:34","2025-03-30 16:18:34","","","","","","","EFHQ","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2312.17205 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/TMYZ9TEW/Dao et al. - 2024 - EFHQ Multi-purpose ExtremePose-Face-HQ dataset.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2312.17205","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MXVPKXP","conferencePaper","2024","Nguyen, Hoang-Quan; Truong, Thanh-Dat; Nguyen, Xuan Bac; Dowling, Ashley; Li, Xin; Luu, Khoa","Insect-Foundation: A Foundation Model and Large-Scale 1M Dataset for Visual Insect Understanding","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02072","https://ieeexplore.ieee.org/document/10655889/","In precision agriculture, the detection and recognition of insects play an essential role in the ability of crops to grow healthy and produce a high-quality yield. The current machine vision model requires a large volume of data to achieve high performance. However, there are approximately 5.5 million different insect species in the world. None of the existing insect datasets can cover even a fraction of them due to varying geographic locations and acquisition costs. In this paper, we introduce a novel “Insect1M” dataset, a game-changing resource poised to revolutionize insect-related foundation model training. Covering a vast spectrum of insect species, our dataset, including 1 million images with dense identiﬁcation labels of taxonomy hierarchy and insect descriptions, offers a panoramic view of entomology, enabling foundation models to comprehend visual and semantic information about insects like never before. Then, to efﬁciently establish an Insect Foundation Model, we develop a micro-feature self-supervised learning method with a Patch-wise Relevant Attention mechanism capable of discerning the subtle differences among insect images. In addition, we introduce Description Consistency loss to improve micro-feature modeling via insect descriptions. Through our experiments, we illustrate the effectiveness of our proposed approach in insect modeling and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks. Our Insect Foundation Model and Dataset promise to empower the next generation of insect-related vision models, bringing them closer to the ultimate goal of precision agriculture.","2024-06-16","2025-03-30 16:18:37","2025-03-30 16:18:37","2025-03-30 16:18:37","21945-21955","","","","","","Insect-Foundation","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XK482I74/Nguyen et al. - 2024 - Insect-Foundation A Foundation Model and Large-Scale 1M Dataset for Visual Insect Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"W6Q2E3RU","conferencePaper","2024","Liao, Jiaqi; Luo, Chuanchen; Du, Yinuo; Wang, Yuxi; Yin, Xucheng; Zhang, Man; Zhang, Zhaoxiang; Peng, Junran","HardMo: A Large-Scale Hardcase Dataset for Motion Capture","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00161","https://ieeexplore.ieee.org/document/10657414/","","2024-06-16","2025-03-30 16:18:40","2025-03-30 16:18:40","2025-03-30 16:18:40","1629-1638","","","","","","HardMo","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/67ZQ5D8P/Liao et al. - 2024 - HardMo A Large-Scale Hardcase Dataset for Motion Capture.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"EBDY4X3Y","conferencePaper","2024","Zhang, Jing; Fang, Irving; Wu, Hao; Kaushik, Akshat; Rodriguez, Alice; Zhao, Hanwen; Zhang, Juexiao; Zheng, Zhuo; Iovita, Radu; Feng, Chen","LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02129","https://ieeexplore.ieee.org/document/10657240/","","2024-06-16","2025-03-30 16:18:42","2025-03-30 16:18:42","2025-03-30 16:18:42","22563-22573","","","","","","LUWA Dataset","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z74VK4GC/Zhang et al. - 2024 - LUWA Dataset Learning Lithic Use-Wear Analysis on Microscopic Images.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"9EXZP5US","conferencePaper","2024","Guo, Yanwen; Li, Yuanqi; Ren, Dayong; Zhang, Xiaohong; Li, Jiawei; Pu, Liang; Ma, Changfeng; Zhan, Xiaoyu; Guo, Jie; Wei, Mingqiang; Zhang, Yan; Yu, Piaopiao; Yang, Shuangyu; Ji, Donghao; Ye, Huisheng; Sun, Hao; Liu, Yansong; Chen, Yinuo; Zhu, Jiaqi; Liu, Hongyu","LiDAR-Net: A Real-Scanned 3D Point Cloud Dataset for Indoor Scenes","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02076","https://ieeexplore.ieee.org/document/10655599/","In this paper, we present LiDAR-Net, a new real-scanned indoor point cloud dataset, containing nearly 3.6 billion precisely point-level annotated points, covering an expansive area of 30,000m2. It encompasses three prevalent daily environments, including learning scenes, working scenes, and living scenes. LiDAR-Net is characterized by its nonuniform point distribution, e.g., scanning holes and scanning lines. Additionally, it meticulously records and annotates scanning anomalies, including reflection noise and ghost. These anomalies stem from specular reflections on glass or metal, as well as distortions due to moving persons. LiDAR-Net’s realistic representation of non-uniform distribution and anomalies significantly enhances the training of deep learning models, leading to improved generalization in practical applications. We thoroughly evaluate the performance of state-of-the-art algorithms on LiDARNet and provide a detailed analysis of the results. Crucially, our research identifies several fundamental challenges in understanding indoor point clouds, contributing essential insights to future explorations in this field. Our dataset can be found online: http://lidar-net.njumeta.com.","2024-06-16","2025-03-30 16:18:45","2025-03-30 16:18:46","2025-03-30 16:18:45","21989-21999","","","","","","LiDAR-Net","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/M4Y8ISTS/Guo et al. - 2024 - LiDAR-Net A Real-Scanned 3D Point Cloud Dataset for Indoor Scenes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"U5J5VKSW","journalArticle","","Qiu, Jielin; Zhu, Jiacheng; Han, William; Kumar, Aditesh; Mittal, Karthik; Jin, Claire; Yang, Zhengyuan; Li, Linjie; Wang, Jianfeng; Zhao, Ding; Li, Bo; Wang, Lijuan","MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos","","","","","","Multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction. Nonetheless, numerous limitations exist within existing public MSMO datasets, including insufficient maintenance, data inaccessibility, limited size, and the absence of proper categorization, which pose significant challenges. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated the MMSum dataset. Our new dataset features (1) Humanvalidated summaries for both video and textual content, providing superior human instruction and labels for multimodal learning. (2) Comprehensively and meticulously arranged categorization, spanning 17 principal categories and 170 subcategories to encapsulate a diverse array of real-world scenarios. (3) Benchmark tests performed on the proposed dataset to assess various tasks and methods, including video summarization, text summarization, and multimodal summarization. To champion accessibility and collaboration, we released the MMSum dataset and the data collection tool as fully open-source resources, fostering transparency and accelerating future developments, at https://mmsum-dataset.github.io/.","","2025-03-30 16:18:47","2025-03-30 16:18:47","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5HGRHR54/Qiu et al. - MMSum A Dataset for Multimodal Summarization and Thumbnail Generation of Videos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YZEX2CG","conferencePaper","2024","Mahmoud, Anas; Elhoushi, Mostafa; Abbas, Amro; Yang, Yu; Ardalani, Newsha; Leather, Hugh; Morcos, Ari S.","Sieve: Multimodal Dataset Pruning Using Image Captioning Models","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02116","https://ieeexplore.ieee.org/document/10655641/","Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy web-crawled datasets. This underscores the critical need for dataset pruning, as the quality of these datasets is strongly correlated with the performance of VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train models using highlyaligned samples is one of the most successful methods for pruning. We argue that this approach suffers from multiple limitations including: false positives and negatives due to CLIP’s pretraining on noisy labels. We propose a pruning signal, Sieve, that employs synthetic captions generated by image-captioning models pretrained on small, diverse, and well-aligned image-text pairs to evaluate the alignment of noisy image-text pairs. To bridge the gap between the limited diversity of generated captions and the high diversity of alternative text (alt-text), we estimate the semantic textual similarity in the embedding space of a language model pretrained on unlabeled text corpus. Using DataComp, a multimodal dataset ﬁltering benchmark, when evaluating on 38 downstream tasks, our pruning approach, surpasses CLIPScore by 2.6% and 1.7% on medium and large scale respectively. In addition, on retrieval tasks, Sieve leads to a signiﬁcant improvement of 2.7% and 4.5% on medium and large scale respectively.","2024-06-16","2025-03-30 16:18:49","2025-03-30 16:18:49","2025-03-30 16:18:49","22423-22432","","","","","","Sieve","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7FG87TH5/Mahmoud et al. - 2024 - Sieve Multimodal Dataset Pruning Using Image Captioning Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"PZXYBNEF","conferencePaper","2024","Jeon, Yujin; Choi, Eunsue; Kim, Youngchan; Moon, Yunseong; Omer, Khalid; Heide, Felix; Baek, Seung-Hwan","Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02086","https://ieeexplore.ieee.org/document/10656737/","Image datasets are essential not only in validating existing methods in computer vision but also in developing new methods. Many image datasets exist, consisting of trichromatic intensity images taken with RGB cameras, which are designed to replicate human vision. However, polarization and spectrum, the wave properties of light that animals in harsh environments and with limited brain capacity often rely on, remain underrepresented in existing datasets. Although there are previous spectro-polarimetric datasets, they have insufﬁcient object diversity, limited illumination conditions, linear-only polarization data, and inadequate image count. Here, we introduce two spectro-polarimetric datasets, consisting of trichromatic Stokes images and hyperspectral Stokes images. These datasets encompass both linear and circular polarization; they introduce multiple spectral channels; and they feature a broad selection of real-world scenes. With our dataset in hand, we analyze the spectro-polarimetric image statistics, develop efﬁcient representations of such high-dimensional data, and evaluate spectral dependency of shape-from-polarization methods. As such, the proposed dataset promises a foundation for data-driven spectro-polarimetric imaging and vision research.","2024-06-16","2025-03-30 16:18:52","2025-03-30 16:18:52","2025-03-30 16:18:52","22098-22108","","","","","","Spectral and Polarization Vision","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XUKW2UEU/Jeon et al. - 2024 - Spectral and Polarization Vision Spectro-polarimetric Real-world Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"VXE54RTQ","conferencePaper","2024","Wang, Yinong Oliver; Chung, Younjoon; Wu, Chen Henry; De La Torre, Fernando","Domain Gap Embeddings for Generative Dataset Augmentation","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02710","https://ieeexplore.ieee.org/document/10657264/","The performance of deep learning models is intrinsically tied to the quality, volume, and relevance of their training data. Gathering ample data for production scenarios often demands signiﬁcant time and resources. Among various strategies, data augmentation circumvents exhaustive data collection by generating new data points from existing ones. However, traditional augmentation techniques can be less effective amidst a shift in training and testing distributions. This paper explores the potential of synthetic data by leveraging large pre-trained models for data augmentation, especially when confronted with distribution shifts. Although recent advancements in generative models have enabled several prior works in cross-distribution data generation, they require model ﬁne-tuning and a complex setup. To bypass these shortcomings, we introduce Domain Gap Embeddings (DoGE), a plug-and-play semantic data augmentation framework in a cross-distribution few-shot setting. Our method extracts disparities between source and desired data distributions in a latent form, and subsequently steers a generative process to supplement the training set with endless diverse synthetic samples. Our evaluations, conducted on a subpopulation shift and three domain adaptation scenarios under a few-shot paradigm, reveal that our versatile method improves performance across tasks without needing hands-on intervention or intricate ﬁne-tuning. DoGE paves the way to effortlessly generate realistic, controllable synthetic datasets following the test distributions, bolstering real-world efﬁcacy for downstream task models.","2024-06-16","2025-03-30 16:18:57","2025-03-30 16:18:58","2025-03-30 16:18:57","28684-28694","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/USENWT79/Wang et al. - 2024 - Domain Gap Embeddings for Generative Dataset Augmentation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"6FZI9XKV","preprint","2024","Zhao, Shibo; Gao, Yuanjun; Wu, Tianhao; Singh, Damanpreet; Jiang, Rushan; Sun, Haoxiang; Sarawata, Mansi; Qiu, Yuheng; Whittaker, Warren; Higgins, Ian; Du, Yi; Su, Shaoshu; Xu, Can; Keller, John; Karhade, Jay; Nogueira, Lucas; Saha, Sourojit; Zhang, Ji; Wang, Wenshan; Wang, Chen; Scherer, Sebastian","SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments","","","","10.48550/arXiv.2307.07607","http://arxiv.org/abs/2307.07607","Simultaneous localization and mapping (SLAM) is a fundamental task for numerous applications such as autonomous navigation and exploration. Despite many SLAM datasets have been released, current SLAM solutions still struggle to have sustained and resilient performance. One major issue is the absence of high-quality datasets including diverse all-weather conditions and a reliable metric for assessing robustness. This limitation significantly restricts the scalability and generalizability of SLAM technologies, impacting their development, validation, and deployment. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push SLAM towards all-weather environments to pursue the most robust SLAM performance. It contains multi-degraded environments including over 30 diverse scenes such as structureless corridors, varying lighting conditions, and perceptual obscurants like smoke and dust; multimodal sensors such as LiDAR, fisheye camera, IMU, and thermal camera; and multiple locomotions like aerial, legged, and wheeled robots. We developed accuracy and robustness evaluation tracks for SLAM and introduced novel robustness metrics. Comprehensive studies are performed, revealing new observations, challenges, and opportunities for future research.","2024-05-30","2025-03-30 16:19:01","2025-03-30 16:19:02","2025-03-30 16:19:01","","","","","","","SubT-MRS Dataset","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2307.07607 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/BNH56XWQ/Zhao et al. - 2024 - SubT-MRS Dataset Pushing SLAM Towards All-weather Environments.pdf","","","Computer Science - Robotics","","","","","","","","","","","","","","","","","","","arXiv:2307.07607","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HH5WVXHI","preprint","2024","Zhang, He; Ren, Shenghao; Yuan, Haolei; Zhao, Jianhui; Li, Fan; Sun, Shuangpeng; Liang, Zhenghao; Yu, Tao; Shen, Qiu; Cao, Xun","MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors","","","","10.48550/arXiv.2403.17610","http://arxiv.org/abs/2403.17610","Foot contact is an important cue for human motion capture, understanding, and generation. Existing datasets tend to annotate dense foot contact using visual matching with thresholding or incorporating pressure signals. However, these approaches either suffer from low accuracy or are only designed for small-range and slow motion. There is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/.","2024-03-30","2025-03-30 16:19:05","2025-03-30 16:19:05","2025-03-30 16:19:04","","","","","","","MMVP","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.17610 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/RN5TEFIR/Zhang et al. - 2024 - MMVP A Multimodal MoCap Dataset with Vision and Pressure Sensors.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2403.17610","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K3LMU29Y","conferencePaper","2024","Chen, Hao; Hou, Yuqi; Qu, Chenyuan; Testini, Irene; Hong, Xiaohan; Jiao, Jianbo","$360+x$: A Panoptic Multi-modal Scene Understanding Dataset","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01833","https://ieeexplore.ieee.org/document/10657996/","Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 different scene understanding tasks on the proposed 360+x dataset to evaluate the impact and benefit of each data modality and perspective in panoptic scene understanding. We hope this unique dataset could broaden the scope of comprehensive scene understanding and encourage the community to approach these problems from more diverse perspectives.","2024-06-16","2025-03-30 16:19:09","2025-03-30 16:19:10","2025-03-30 16:19:09","19373-19382","","","","","","$360+x$","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QC88TAVU/Chen et al. - 2024 - $360+x$ A Panoptic Multi-modal Scene Understanding Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"TVSHGSED","conferencePaper","2024","Wang, Xiao; Wang, Shiao; Tang, Chuanming; Zhu, Lin; Jiang, Bo; Tian, Yonghong; Tang, Jin","Event Stream-Based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01821","https://ieeexplore.ieee.org/document/10655615/","","2024-06-16","2025-03-30 16:19:12","2025-03-30 16:19:12","2025-03-30 16:19:12","19248-19257","","","","","","Event Stream-Based Visual Object Tracking","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3C4YUKMH/Wang et al. - 2024 - Event Stream-Based Visual Object Tracking A High-Resolution Benchmark Dataset and A Novel Baseline.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"PWZS32K7","preprint","2023","Chen, Jiaben; Jiang, Huaizu","SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation","","","","10.48550/arXiv.2308.16876","http://arxiv.org/abs/2308.16876","Human-centric video frame interpolation has great potential for enhancing entertainment experiences and finding commercial applications in the sports analysis industry, e.g., synthesizing slow-motion videos. Although there are multiple benchmark datasets available for video frame interpolation in the community, none of them is dedicated to human-centric scenarios. To bridge this gap, we introduce SportsSloMo, a benchmark featuring over 130K highresolution (≥720p) slow-motion sports video clips, totaling over 1M video frames, sourced from YouTube. We retrain several state-of-the-art methods on our benchmark, and we observed a noticeable decrease in their accuracy compared to other datasets. This highlights the difficulty of our benchmark and suggests that it poses significant challenges even for the best-performing methods, as human bodies are highly deformable and occlusions are frequent in sports videos. To tackle these challenges, we propose human-aware loss terms, where we add auxiliary supervision for human segmentation in panoptic settings and keypoints detection. These loss terms are model-agnostic and can be easily plugged into any video frame interpolation approach. Experimental results validate the effectiveness of our proposed human-aware loss terms, leading to consistent performance improvement over existing models. The dataset and code can be found at: https://neuvi.github.io/SportsSlomo/.","2023-12-12","2025-03-30 16:19:18","2025-03-30 16:19:18","2025-03-30 16:19:18","","","","","","","SportsSloMo","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2308.16876 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/NF6LCZGT/Chen and Jiang - 2023 - SportsSloMo A New Benchmark and Baselines for Human-centric Video Frame Interpolation.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2308.16876","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"799BHHHL","conferencePaper","2024","Huang, Huajian; Liu, Changkun; Zhu, Yipeng; Cheng, Hui; Braud, Tristan; Yeung, Sai-Kit","360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-Device Queries","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02106","https://ieeexplore.ieee.org/document/10655932/","Portable 360◦ cameras are becoming a cheap and efficient tool to establish large visual databases. By capturing omnidirectional views of a scene, these cameras could expedite building environment models that are essential for visual localization. However, such an advantage is often overlooked due to the lack of valuable datasets. This paper introduces a new benchmark dataset, 360Loc, composed of 360◦ images with ground truth poses for visual localization. We present a practical implementation of 360◦ mapping combining 360◦ images with lidar data to generate the ground truth 6DoF poses. 360Loc is the first dataset and benchmark that explores the challenge of crossdevice visual positioning, involving 360◦ reference frames, and query frames from pinhole, ultra-wide FoV fisheye, and 360◦ cameras. We propose a virtual camera approach to generate lower-FoV query frames from 360◦ images, which ensures a fair comparison of performance among different query types in visual localization tasks. We also extend this virtual camera approach to feature matchingbased and pose regression-based methods to alleviate the performance loss caused by the cross-device domain gap, and evaluate its effectiveness against state-of-the-art baselines. We demonstrate that omnidirectional visual localization is more robust in challenging large-scale scenes with symmetries and repetitive structures. These results provide new insights into 360-camera mapping and omnidirectional visual localization with cross-device queries. Project Page and dataset: https://huajianup.github. io/research/360Loc/.","2024-06-16","2025-03-30 16:19:22","2025-03-30 16:19:22","2025-03-30 16:19:22","22314-22324","","","","","","360Loc","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8WI6R6A3/Huang et al. - 2024 - 360Loc A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-Device Queries.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"7MEC7S4F","conferencePaper","2024","Wang, Jiong; Yang, Fengyu; Li, Bingliang; Gou, Wenbo; Yan, Danqi; Zeng, Ailing; Gao, Yijun; Wang, Junle; Jing, Yanqing; Zhang, Ruimao","FreeMan: Towards Benchmarking 3D Human Pose Estimation Under Real-World Conditions","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02075","https://ieeexplore.ieee.org/document/10655984/","Estimating the 3D structure of the human body from natural scenes is a fundamental aspect of visual perception. 3D human pose estimation is a vital step in advancing fields like AIGC and human-robot interaction, serving as a crucial technique for understanding and interacting with human actions in real-world settings. However, the current datasets, often collected under single laboratory conditions using complex motion capture equipment and unvarying backgrounds, are insufficient. The absence of datasets on variable conditions is stalling the progress of this crucial task. To facilitate the development of 3D pose estimation, we present FreeMan, the first large-scale, multi-view dataset collected under the real-world conditions. FreeMan was captured by synchronizing 8 smartphones across diverse scenarios. It comprises 11M frames from 8000 sequences, viewed from different perspectives. These sequences cover 40 subjects across 10 different scenarios, each with varying lighting conditions.","2024-06-16","2025-03-30 16:19:29","2025-03-30 16:19:29","2025-03-30 16:19:29","21978-21988","","","","","","FreeMan","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/G2JHV6MB/Wang et al. - 2024 - FreeMan Towards Benchmarking 3D Human Pose Estimation Under Real-World Conditions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"RAHZR8CB","conferencePaper","2024","Rosasco, Andrea; Berti, Stefano; Pasquale, Giulia; Malafronte, Damiano; Sato, Shogo; Segawa, Hiroyuki; Inada, Tetsugo; Natale, Lorenzo","ConCon-Chi: Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02099","https://ieeexplore.ieee.org/document/10655903/","","2024-06-16","2025-03-30 16:19:31","2025-03-30 16:19:31","2025-03-30 16:19:31","22239-22248","","","","","","ConCon-Chi","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3DZMNGF9/Rosasco et al. - 2024 - ConCon-Chi Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"LAPTQBE9","conferencePaper","2024","Khanna, Mukul; Ramrakhya, Ram; Chhablani, Gunjan; Yenamandra, Sriram; Gervet, Theophile; Chang, Matthew; Kira, Zsolt; Chaplot, Devendra Singh; Batra, Dhruv; Mottaghi, Roozbeh","GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01549","https://ieeexplore.ieee.org/document/10655426/","","2024-06-16","2025-03-30 16:19:35","2025-03-30 16:19:35","2025-03-30 16:19:35","16373-16383","","","","","","GOAT-Bench","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z4V2EYG8/Khanna et al. - 2024 - GOAT-Bench A Benchmark for Multi-Modal Lifelong Navigation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"PE5MZ8XG","conferencePaper","2024","Ma, Cong; Qiao, Lei; Zhu, Chengkai; Liu, Kai; Kong, Zelong; Li, Qing; Zhou, Xueqi; Kan, Yuheng; Wu, Wei","HoloVic:Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02089","https://ieeexplore.ieee.org/document/10658281/","Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicleinfrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.","2024-06-16","2025-03-30 16:19:37","2025-03-30 16:19:37","2025-03-30 16:19:37","22129-22138","","","","","","HoloVic","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/US8LB7XC/Ma et al. - 2024 - HoloVicLarge-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"XXV7UPJS","conferencePaper","2024","Zhao, Xiaoqi; Pang, Youwei; Chen, Zhenyu; Yu, Qian; Zhang, Lihe; Liu, Hanqi; Zuo, Jiaming; Lu, Huchuan","Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02079","https://ieeexplore.ieee.org/document/10655788/","We conduct a comprehensive study on a new task named power battery detection (PBD), which aims to localize the dense cathode and anode plates endpoints from X-ray images to evaluate the quality of power batteries. Existing manufacturers usually rely on human eye observation to complete PBD, which makes it difficult to balance the accuracy and efficiency of detection. To address this issue and drive more attention into this meaningful task, we first elaborately collect a dataset, called X-ray PBD, which has 1, 500 diverse X-ray images selected from thousands of power batteries of 5 manufacturers, with 7 different visual interference. Then, we propose a novel segmentation-based solution for PBD, termed multi-dimensional collaborative network (MDCNet). With the help of line and counting predictors, the representation of the point segmentation branch can be improved at both semantic and detail aspects. Besides, we design an effective distance-adaptive mask generation strategy, which can alleviate the visual challenge caused by the inconsistent distribution density of plates to provide MDCNet with stable supervision. Without any bells and whistles, our segmentation-based MDCNet consistently outperforms various other corner detection, crowd counting and general/tiny object detection-based solutions, making it a strong baseline that can help facilitate future research in PBD. Finally, we share some potential difficulties and works for future researches. The source code and datasets will be publicly available at X-ray PBD.","2024-06-16","2025-03-30 16:19:39","2025-03-30 16:19:39","2025-03-30 16:19:39","22020-22029","","","","","","Towards Automatic Power Battery Detection","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SRR6JYVI/Zhao et al. - 2024 - Towards Automatic Power Battery Detection New Challenge, Benchmark Dataset and Baseline.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"LTJDHBUP","conferencePaper","2024","Liu, Yaofang; Cun, Xiaodong; Liu, Xuebo; Wang, Xintao; Zhang, Yong; Chen, Haoxin; Liu, Yang; Zeng, Tieyong; Chan, Raymond; Shan, Ying","EvalCrafter: Benchmarking and Evaluating Large Video Generation Models","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02090","https://ieeexplore.ieee.org/document/10657882/","The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services have been developed to generate high-quality videos. However, these methods often use a few metrics, e.g., FVD [56] or IS [45], to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a novel framework and pipeline for exhaustively evaluating the performance of the generated videos. Our approach involves generating a diverse and comprehensive list of 700 prompts for text-to-video generation, which is based on an analysis of real-world user data and generated with the assistance of a large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmark, in terms of visual qualities, content qualities, motion qualities, and text-video alignment with 17 well-selected objective metrics. To obtain the final leaderboard of the models, we further fit a series of coefficients to align the objective metrics to the users’ opinions. Based on the proposed human alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.","2024-06-16","2025-03-30 16:19:42","2025-03-30 16:19:42","2025-03-30 16:19:42","22139-22149","","","","","","EvalCrafter","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Q973T2F4/Liu et al. - 2024 - EvalCrafter Benchmarking and Evaluating Large Video Generation Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"2EIM2GB7","conferencePaper","2024","Yang, Karren D.; Ranjan, Anurag; Chang, Jen-Hao Rick; Vemulapalli, Raviteja; Tuzel, Oncel","Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02577","https://ieeexplore.ieee.org/document/10657034/","We consider the task of animating 3D facial geometry from speech signal. Existing works are primarily deterministic, focusing on learning a one-to-one mapping from speech signal to 3D face meshes on small datasets with limited speakers. While these models can achieve high-quality lip articulation for speakers in the training set, they are unable to capture the full and diverse distribution of 3D facial motions that accompany speech in the real world. Importantly, the relationship between speech and facial motion is one-to-many, containing both inter-speaker and intraspeaker variations and necessitating a probabilistic approach. In this paper, we identify and address key challenges that have so far limited the development of probabilistic models: lack of datasets and metrics that are suitable for training and evaluating them, as well as the difficulty of designing a model that generates diverse results while remaining faithful to a strong conditioning signal as speech. We first propose large-scale benchmark datasets and metrics suitable for probabilistic modeling. Then, we demonstrate a probabilistic model that achieves both diversity and fidelity to speech, outperforming other methods across the proposed benchmarks. Finally, we showcase useful applications of probabilistic models trained on these large-scale datasets: we can generate diverse speechdriven 3D facial motion that matches unseen speaker styles extracted from reference clips; and our synthetic meshes can be used to improve the performance of downstream audio-visual models.","2024-06-16","2025-03-30 16:19:45","2025-03-30 16:19:45","2025-03-30 16:19:45","27284-27293","","","","","","Probabilistic Speech-Driven 3D Facial Motion Synthesis","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/W6SK53IF/Yang et al. - 2024 - Probabilistic Speech-Driven 3D Facial Motion Synthesis New Benchmarks, Methods, and Applications.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"AZFVUPKP","conferencePaper","2024","Paplhám, Jakub; Franc, Vojtêch","A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00120","https://ieeexplore.ieee.org/document/10656298/","Comparing different age estimation methods poses a challenge due to the unreliability of published results stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. This paper identifies two trivial, yet persistent issues with the currently used evaluation protocol and describes how to resolve them. We offer an extensive comparative analysis for state-of-the-art facial age estimation methods. Surprisingly, we find that the performance differences between the methods are negligible compared to the effect of other factors, such as facial alignment, facial coverage, image resolution, model architecture, or the amount of data used for pretraining. We use the gained insights to propose using FaRL as the backbone model and demonstrate its effectiveness on all public datasets. We make the source code and exact data splits public on GitHub and in the supplementary material.","2024-06-16","2025-03-30 16:19:48","2025-03-30 16:19:48","2025-03-30 16:19:48","1196-1205","","","","","","A Call to Reflect on Evaluation Practices for Age Estimation","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/E7NUV27T/Paplhám and Franc - 2024 - A Call to Reflect on Evaluation Practices for Age Estimation Comparative Analysis of the State-of-t.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"A9NE2VBL","conferencePaper","2024","Hua, Tonayan; Wang, Lin","Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02017","https://ieeexplore.ieee.org/document/10656090/","Implicit neural representation (INR), in combination with geometric rendering, has recently been employed in real-time dense RGB-D SLAM. Despite active research endeavors being made, there lacks a unified protocol for fair evaluation, impeding the evolution of this area. In this work, we establish, to our knowledge, the first open-source benchmark framework to evaluate the performance of a wide spectrum of commonly used INRs and rendering functions for mapping and localization. The goal of our benchmark is to 1) gain an intuition of how different INRs and rendering functions impact mapping and localization and 2) establish a unified evaluation protocol w.r.t. the design choices that may impact the mapping and localization. With the framework, we conduct a large suite of experiments, offering various insights in choosing the INRs and geometric rendering functions: for example, the dense feature grid outperforms other INRs (e.g. tri-plane and hash grid), even when geometric and color features are jointly encoded for memory efficiency. To extend the findings into the practical scenario, a hybrid encoding strategy is proposed to bring the best of the accuracy and completion from the gridbased and decomposition-based INRs. We further propose explicit hybrid encoding for high-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts the premise on robustness and computation efficiency.","2024-06-16","2025-03-30 16:19:51","2025-03-30 16:19:51","2025-03-30 16:19:51","21346-21356","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AKMCTMNV/Hua and Wang - 2024 - Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"GXW5I4YC","preprint","2024","Chen, Yufan; Zhang, Jiaming; Peng, Kunyu; Zheng, Junwei; Liu, Ruiping; Torr, Philip; Stiefelhagen, Rainer","RoDLA: Benchmarking the Robustness of Document Layout Analysis Models","","","","10.48550/arXiv.2403.14442","http://arxiv.org/abs/2403.14442","Before developing a Document Layout Analysis (DLA) model in real-world applications, conducting comprehensive robustness testing is essential. However, the robustness of DLA models remains underexplored in the literature. To address this, we are the first to introduce a robustness benchmark for DLA models, which includes 450K document images of three datasets. To cover realistic corruptions, we propose a perturbation taxonomy with 36 common document perturbations inspired by real-world document processing. Additionally, to better understand document perturbation impacts, we propose two metrics, Mean Perturbation Effect (mPE) for perturbation assessment and Mean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we introduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA), which improves attention mechanisms to boost extraction of robust features. Experiments on the proposed benchmarks (PubLayNet-P, DocLayNet-P, and M$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of 115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA achieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.","2024-03-21","2025-03-30 16:19:53","2025-03-30 16:19:53","2025-03-30 16:19:53","","","","","","","RoDLA","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.14442 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/V6VYC7QP/Chen et al. - 2024 - RoDLA Benchmarking the Robustness of Document Layout Analysis Models.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2403.14442","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H3RCDR5R","preprint","2023","Huang, Ziqi; He, Yinan; Yu, Jiashuo; Zhang, Fan; Si, Chenyang; Jiang, Yuming; Zhang, Yuanhan; Wu, Tianxing; Jin, Qingyang; Chanpaisit, Nattapol; Wang, Yaohui; Chen, Xinyuan; Wang, Limin; Lin, Dahua; Qiao, Yu; Liu, Ziwei","VBench: Comprehensive Benchmark Suite for Video Generative Models","","","","10.48550/arXiv.2311.17982","http://arxiv.org/abs/2311.17982","Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects ""video generation quality"" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has three appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). The evaluation metrics with fine-grained levels reveal individual models' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models' ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. We will open-source VBench, including all prompts, evaluation methods, generated videos, and human preference annotations, and also include more video generation models in VBench to drive forward the field of video generation.","2023-11-29","2025-03-30 16:19:55","2025-03-30 16:19:55","2025-03-30 16:19:55","","","","","","","VBench","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.17982 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/2HUWE237/Huang et al. - 2023 - VBench Comprehensive Benchmark Suite for Video Generative Models.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2311.17982","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P7IVMYAH","conferencePaper","2024","Chen, Ziyang; Gebru, Israel D.; Richardt, Christian; Kumar, Anurag; Laney, William; Owens, Andrew; Richard, Alexander","Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02067","https://ieeexplore.ieee.org/document/10656013/","We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities. The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms. We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data. In our evaluation, we thoroughly assessed existing audio and audio-visual models against multiple criteria and proposed settings to enhance their performance on real-world data. We also conducted experiments to investigate the impact of incorporating visual data (i.e., images and depth) into neural acoustic field models. Additionally, we demonstrated the effectiveness of a simple sim2real approach, where a model is pre-trained with simulated data and fine-tuned with sparse real-world data, resulting in significant improvements in the few-shot learning approach. RAF is the first dataset to provide densely captured room acoustic data, making it an ideal resource for researchers working on audio and audiovisual neural acoustic field modeling techniques. Demos and datasets are available on our project page.","2024-06-16","2025-03-30 16:19:58","2025-03-30 16:19:58","2025-03-30 16:19:57","21886-21896","","","","","","Real Acoustic Fields","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/V9JRUTKL/Chen et al. - 2024 - Real Acoustic Fields An Audio-Visual Room Acoustics Dataset and Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"AW99UX2Y","conferencePaper","2024","Li, Wenqiao; Xu, Xiaohao; Gu, Yao; Zheng, Bozhong; Gaol, Shenghua; Wu, Yingna","Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02096","https://ieeexplore.ieee.org/document/10658462/","Recently, 3D anomaly detection, a crucial problem involving fine-grained geometry discrimination, is getting more attention. However, the lack of abundant real 3D anomaly data limits the scalability of current models. To enable scalable anomaly data collection, we propose a 3D anomaly synthesis pipeline to adapt existing large-scale 3D models for 3D anomaly detection. Specifically, we construct a synthetic dataset, i.e., Anomaly-ShapeNet, based on ShapeNet. Anomaly-ShapeNet consists of 1600 point cloud samples under 40 categories, which provides a rich and varied collection of data, enabling efficient training and enhancing adaptability to industrial scenarios. Meanwhile, to enable scalable representation learning for 3D anomaly localization, we propose a self-supervised method, i.e., Iterative Mask Reconstruction Network (IMRNet). During training, we propose a geometry-aware sample module to preserve potentially anomalous local regions during point cloud down-sampling. Then, we randomly mask out point patches and sent the visible patches to a transformer for reconstruction-based self-supervision. During testing, the point cloud repeatedly goes through the Mask Reconstruction Network, with each iteration’s output becoming the next input. By merging and contrasting the final reconstructed point cloud with the initial input, our method successfully locates anomalies. Experiments show that IMRNet outperforms previous state-of-the-art methods, achieving 66.1% in I-AUC on our Anomaly-ShapeNet dataset and 72.5% in I-AUC on Real3D-AD dataset. Our benchmark will be released at https://github.com/Chopper233/Anomaly-ShapeNet.","2024-06-16","2025-03-30 16:20:00","2025-03-30 16:20:00","2025-03-30 16:20:00","22207-22216","","","","","","Towards Scalable 3D Anomaly Detection and Localization","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4FG42CSW/Li et al. - 2024 - Towards Scalable 3D Anomaly Detection and Localization A Benchmark via 3D Anomaly Synthesis and A S.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"3YR82PLT","journalArticle","","Liu, Yun; Yang, Haolin; Si, Xu; Liu, Ling; Li, Zipeng; Zhang, Yuxiang; Liu, Yebin; Yi, Li","TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding","","","","","","","","2025-03-30 16:20:01","2025-03-30 16:20:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PNGDVUKF/Liu et al. - TACO Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UE33QNUG","conferencePaper","2024","Wang, Chengjie; Zhu, Wenbing; Gao, Bin-Bin; Gan, Zhenye; Zhang, Jiangning; Gu, Zhihao; Qian, Shuguang; Chen, Mingang; Ma, Lizhuang","Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02159","https://ieeexplore.ieee.org/document/10658200/","Industrial anomaly detection (IAD) has garnered significant attention and experienced rapid development. However, the recent development of IAD approach has encountered certain difficulties due to dataset limitations. On the one hand, most of the state-of-the-art methods have achieved saturation (over 99% in AUROC) on mainstream datasets such as MVTec, and the differences of methods cannot be well distinguished, leading to a significant gap between public datasets and actual application scenarios. On the other hand, the research on various new practical anomaly detection settings is limited by the scale of the dataset, posing a risk of overfitting in evaluation results. Therefore, we propose a large-scale, Real-world, and multi-view Industrial Anomaly Detection dataset, named Real-IAD, which contains 150K high-resolution images of 30 different objects, an order of magnitude larger than existing datasets. It has a larger range of defect area and ratio proportions, making it more challenging than previous datasets. To make the dataset closer to real application scenarios, we adopted a multi-view shooting method and proposed sample-level evaluation metrics. In addition, beyond the general unsupervised anomaly detection setting, we propose a new setting for Fully Unsupervised Industrial Anomaly Detection (FUIAD) based on the observation that the yield rate in industrial production is usually greater than 60%, which has more practical application value. Finally, we report the results of popular IAD methods on the Real-IAD dataset, providing a highly challenging benchmark to promote the development of the IAD field.","2024-06-16","2025-03-30 16:20:04","2025-03-30 16:20:04","2025-03-30 16:20:04","22883-22892","","","","","","Real-IAD","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AFEJWN9K/Wang et al. - 2024 - Real-IAD A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"4TRBIH4V","conferencePaper","2024","Pu, Bin; Wang, Liwen; Yang, Jiewen; He, Guannan; Dong, Xingbo; Li, Shengli; Tan, Ying; Chen, Ming; Jin, Zhe; Li, Kenli; Li, Xiaomeng","M<sup>3</sup> -UDA: A New Benchmark for Unsupervised Domain Adaptive Fetal Cardiac Structure Detection","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01104","https://ieeexplore.ieee.org/document/10656881/","","2024-06-16","2025-03-30 16:20:07","2025-03-30 16:20:07","2025-03-30 16:20:07","11621-11630","","","","","","M<sup>3</sup> -UDA","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/G7QQWFVD/Pu et al. - 2024 - M3 -UDA A New Benchmark for Unsupervised Domain Adaptive Fetal Cardiac Structure Detecti.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"LAVHNET7","conferencePaper","2024","Liu, Chen; Li, Peike Patrick; Yu, Qingtao; Sheng, Hongwei; Wang, Dadong; Li, Lincheng; Yu, Xin","Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02143","https://ieeexplore.ieee.org/document/10655596/","","2024-06-16","2025-03-30 16:20:11","2025-03-30 16:20:11","2025-03-30 16:20:11","22712-22722","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/E69TMFJH/Liu et al. - 2024 - Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"JCZ9PEWJ","journalArticle","","Hu, Yutao; Li, Tianbin; Lu, Quanfeng; Shao, Wenqi; He, Junjun; Qiao, Yu; Luo, Ping","OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM","","","","","","Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks. However, their potential in the medical domain remains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications. To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical Visual Question Answering (VQA) benchmark. This benchmark is collected from 73 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions. Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs. Through our extensive experiments, we have found that existing LVLMs struggle to address these medical VQA problems effectively. Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field. The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset’s significance. Our code with dataset are available at https://github.com/OpenGVLab/ Multi-Modality-Arena.","","2025-03-30 16:20:12","2025-03-30 16:20:12","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4I57CSB4/Hu et al. - OmniMedVQA A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PNFGPH84","conferencePaper","2024","Papa, Samuele; Valperga, Riccardo; Knigge, David; Kofinas, Miltiadis; Lippe, Phillip; Sonke, Jan-Jakob; Gavves, Efstratios","How to Train Neural Field Representations: A Comprehensive Study and Benchmark","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02134","https://ieeexplore.ieee.org/document/10657405/","Neural fields (NeFs) have recently emerged as a versatile method for modeling signals of various modalities, including images, shapes, and scenes. Subsequently, a number of works have explored the use of NeFs as representations for downstream tasks, e.g. classifying an image based on the parameters of a NeF that has been fit to it. However, the impact of the NeF hyperparameters on their quality as downstream representation is scarcely understood and remains largely unexplored. This is in part caused by the large amount of time required to fit datasets of neural fields.","2024-06-16","2025-03-30 16:20:16","2025-03-30 16:20:16","2025-03-30 16:20:16","22616-22625","","","","","","How to Train Neural Field Representations","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6T5MLTDE/Papa et al. - 2024 - How to Train Neural Field Representations A Comprehensive Study and Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"JS6HNCJH","preprint","2024","Zhang, Chenshuang; Pan, Fei; Kim, Junmo; Kweon, In So; Mao, Chengzhi","ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object","","","","10.48550/arXiv.2403.18775","http://arxiv.org/abs/2403.18775","We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models’ robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at https://github.com/chenshuangzhang/imagenet_d.","2024-03-27","2025-03-30 16:20:18","2025-03-30 16:20:18","2025-03-30 16:20:18","","","","","","","ImageNet-D","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.18775 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/SE77FBLE/Zhang et al. - 2024 - ImageNet-D Benchmarking Neural Network Robustness on Diffusion Synthetic Object.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2403.18775","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9FNQGQPT","preprint","2024","Li, Kunchang; Wang, Yali; He, Yinan; Li, Yizhuo; Wang, Yi; Liu, Yi; Wang, Zun; Xu, Jilan; Chen, Guo; Luo, Ping; Wang, Limin; Qiao, Yu","MVBench: A Comprehensive Multi-modal Video Understanding Benchmark","","","","10.48550/arXiv.2311.17005","http://arxiv.org/abs/2311.17005","With the rapid development of Multi-modal Large Language Models (MLLMs), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models. However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks. To alleviate this issue, we introduce a comprehensive Multi-modal Video understanding Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot be effectively solved with a single frame. Specifically, we first introduce a novel staticto-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition. Then, guided by the task definition, we automatically convert public video annotations into multiplechoice QA to evaluate each task. On one hand, such a distinct paradigm allows us to build MVBench efficiently, without much manual intervention. On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of LLMs. Moreover, we further develop a robust video MLLM baseline, i.e., VideoChat2, by progressive multi-modal training with diverse instruction-tuning data. The extensive results on our MVBench reveal that, the existing MLLMs are far from satisfactory in temporal understanding, while our VideoChat2 largely surpasses these leading models by over 15% on MVBench. All models and data are available at https: //github.com/OpenGVLab/Ask-Anything.","2024-05-23","2025-03-30 16:20:20","2025-03-30 16:20:20","2025-03-30 16:20:20","","","","","","","MVBench","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.17005 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/WRFP84EU/Li et al. - 2024 - MVBench A Comprehensive Multi-modal Video Understanding Benchmark.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2311.17005","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JI5FHBYN","conferencePaper","2024","Cao, Xu; Zhou, Tong; Ma, Yunsheng; Ye, Wenqian; Cui, Can; Tang, Kun; Cao, Zhipeng; Liang, Kaizhao; Wang, Ziran; Rehg, James M.; Zheng, Chao","MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02061","https://ieeexplore.ieee.org/document/10657418/","Vision-language generative AI has demonstrated remarkable promise for empowering cross-modal scene understanding of autonomous driving and high-definition (HD) map systems. However, current benchmark datasets lack multi-modal point cloud, image, and language data pairs. Recent approaches utilize visual instruction learning and cross-modal prompt engineering to expand visionlanguage models into this domain. In this paper, we propose a new vision-language benchmark that can be used to finetune traffic and HD map domain-specific foundation models. Specifically, we annotate and leverage large-scale, broad-coverage traffic and map data extracted from huge HD map annotations, and use CLIP and LLaMA-2 / Vicuna to finetune a baseline model with instruction-following data. Our experimental results across various algorithms reveal that while visual instruction-tuning large language models (LLMs) can effectively learn meaningful representations from MAPLM-QA, there remains significant room for further advancements. To facilitate applying LLMs and multi-modal data into self-driving research, we will release our visual-language QA data, and the baseline models at GitHub.com/LLVM-AD/MAPLM.","2024-06-16","2025-03-30 16:20:22","2025-03-30 16:20:22","2025-03-30 16:20:22","21819-21830","","","","","","MAPLM","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6E2C8ZQ3/Cao et al. - 2024 - MAPLM A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"434HJQGC","conferencePaper","2024","Li, Bohao; Ge, Yuying; Ge, Yixiao; Wang, Guangzhi; Wang, Rui; Zhang, Ruimao; Shan, Ying","SEED-Bench: Benchmarking Multimodal Large Language Models","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01263","https://ieeexplore.ieee.org/document/10658180/","","2024-06-16","2025-03-30 16:20:25","2025-03-30 16:20:25","2025-03-30 16:20:25","13299-13308","","","","","","SEED-Bench","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4H4KTLUL/Li et al. - 2024 - SEED-Bench Benchmarking Multimodal Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"YVMW8CMR","conferencePaper","2024","Yin, Zijin; Liang, Kongming; Li, Bing; Ma, Zhanyu; Guo, Jun","Benchmarking Segmentation Models with Mask-Preserved Attribute Editing","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02124","https://ieeexplore.ieee.org/document/10657997/","","2024-06-16","2025-03-30 16:20:29","2025-03-30 16:20:29","2025-03-30 16:20:29","22509-22519","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZST5KCVX/Yin et al. - 2024 - Benchmarking Segmentation Models with Mask-Preserved Attribute Editing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"WN5QRLW9","conferencePaper","2024","Zeng, Runhao; Chen, Xiaoyong; Liang, Jiaming; Wu, Huisi; Cao, Guangzhong; Guo, Yong","Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01729","https://ieeexplore.ieee.org/document/10655650/","Temporal action detection (TAD) aims to locate action positions and recognize action categories in long-term untrimmed videos. Although many methods have achieved promising results, their robustness has not been thoroughly studied. In practice, we observe that temporal information in videos can be occasionally corrupted, such as missing or blurred frames. Interestingly, existing methods often incur a significant performance drop even if only one frame is affected. To formally evaluate the robustness, we establish two temporal corruption robustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper, we extensively analyze the robustness of seven leading TAD methods and obtain some interesting findings: 1) Existing methods are particularly vulnerable to temporal corruptions, and end-to-end methods are often more susceptible than those with a pre-trained feature extractor; 2) Vulnerability mainly comes from localization error rather than classification error; 3) When corruptions occur in the middle of an action instance, TAD models tend to yield the largest performance drop. Besides building a benchmark, we further develop a simple but effective robust training method to defend against temporal corruptions, through the FrameDrop augmentation and Temporal-Robust Consistency loss. Remarkably, our approach not only improves robustness but also yields promising improvements on clean data. We believe that this study will serve as a benchmark for future research in robust video analysis. Source code and models are available at https://github.com/AlvinZeng/temporal-robustness-benchmark.","2024-06-16","2025-03-30 16:20:31","2025-03-30 16:20:31","2025-03-30 16:20:31","18263-18274","","","","","","","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/46JIIS5R/Zeng et al. - 2024 - Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"YWPN9CJM","conferencePaper","2024","Deng, Bowen; Song, Siyang; French, Andrew P.; Schluppeck, Denis; Pound, Michael P.","Advancing Saliency Ranking with Human Fixations: Dataset, Models and Benchmarks","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02678","https://ieeexplore.ieee.org/document/10656059/","Saliency ranking detection (SRD) has emerged as a challenging task in computer vision, aiming not only to identify salient objects within images but also to rank them based on their degree of saliency. Existing SRD datasets have been created primarily using mouse-trajectory data, which inadequately captures the intricacies of human visual perception. Addressing this gap, this paper introduces the first large-scale SRD dataset, SIFR, constructed using genuine human fixation data, thereby aligning more closely with real visual perceptual processes. To establish a baseline for this dataset, we propose QAGNet, a novel model that leverages salient instance query features from a transformer detector within a tri-tiered nested graph. Through extensive experiments, we demonstrate that our approach outperforms existing state-of-the-art methods across two widely used SRD datasets and our newly proposed dataset. Code and dataset are available at https://github.com/ EricDengbowen/QAGNet.","2024-06-16","2025-03-30 16:20:34","2025-03-30 16:20:34","2025-03-30 16:20:34","28348-28357","","","","","","Advancing Saliency Ranking with Human Fixations","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DXUUDS2E/Deng et al. - 2024 - Advancing Saliency Ranking with Human Fixations Dataset, Models and Benchmarks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"UM6U2I9H","conferencePaper","2024","Tao, M.; Bai, Bing; Lin, Haozhe; Wang, Heyuan; Wang, Yu; Luo, Lin; Fang, Lu","When Visual Grounding Meets Gigapixel-Level Large-Scale Scenes: Benchmark and Approach","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02088","https://ieeexplore.ieee.org/document/10656407/","Visual grounding refers to the process of associating natural language expressions with corresponding regions within an image. Existing benchmarks for visual grounding primarily operate within small-scale scenes with a few objects. Nevertheless, recent advances in imaging technology have enabled the acquisition of gigapixel-level images, providing high-resolution details in large-scale scenes containing numerous objects. To bridge this gap between imaging and computer vision benchmarks and make grounding more practically valuable, we introduce a novel dataset, named GigaGrounding, designed to challenge visual grounding models in gigapixel-level large-scale scenes. We extensively analyze and compare the dataset with existing benchmarks, demonstrating that GigaGrounding presents unique challenges such as large-scale scene understanding, gigapixellevel resolution, significant variations in object scales, and the “multi-hop expressions”. Furthermore, we introduced a simple yet effective grounding approach, which employs a “glance-to-zoom-in” paradigm and exhibits enhanced capabilities for addressing the GigaGrounding task. The dataset is available at www.gigavision.ai.","2024-06-16","2025-03-30 16:20:38","2025-03-30 16:20:38","2025-03-30 16:20:38","22119-22128","","","","","","When Visual Grounding Meets Gigapixel-Level Large-Scale Scenes","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/32IHAKAM/Tao et al. - 2024 - When Visual Grounding Meets Gigapixel-Level Large-Scale Scenes Benchmark and Approach.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"WMKTCMNP","preprint","2024","Yuan, Haocheng; Xu, Jing; Pan, Hao; Bousseau, Adrien; Mitra, Niloy J.; Li, Changjian","CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs","","","","10.48550/arXiv.2311.16703","http://arxiv.org/abs/2311.16703","CAD programs are a popular way to compactly encode shapes as a sequence of operations that are easy to parametrically modify. However, without sufficient semantic comments and structure, such programs can be challenging to understand, let alone modify. We introduce the problem of semantic commenting CAD programs, wherein the goal is to segment the input program into code blocks corresponding to semantically meaningful shape parts and assign a semantic label to each block. We solve the problem by combining program parsing with visual-semantic analysis afforded by recent advances in foundational language and vision models. Specifically, by executing the input programs, we create shapes, which we use to generate conditional photorealistic images to make use of semantic annotators for such images. We then distill the information across the images and link back to the original programs to semantically comment on them. Additionally, we collected and annotated a benchmark dataset, CADTalk, consisting of 5,288 machine-made programs and 45 human-made programs with ground truth semantic comments. We extensively evaluated our approach, compared it to a GPT-based baseline, and an open-set shape segmentation baseline, and reported an 83.24% accuracy on the new CADTalk dataset. Code and data: https://enigma-li.github.io/CADTalk/.","2024-03-25","2025-03-30 16:20:40","2025-03-30 16:20:40","2025-03-30 16:20:40","","","","","","","CADTalk","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.16703 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/7XXGCIDA/Yuan et al. - 2024 - CADTalk An Algorithm and Benchmark for Semantic Commenting of CAD Programs.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Graphics","","","","","","","","","","","","","","","","","","","arXiv:2311.16703","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZDGB5ZF","preprint","2024","Zheng, Xiaoyun; Liao, Liwei; Li, Xufeng; Jiao, Jianbo; Wang, Rongjie; Gao, Feng; Wang, Shiqi; Wang, Ronggang","PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling","","","","10.48550/arXiv.2403.16080","http://arxiv.org/abs/2403.16080","High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data.","2024-04-02","2025-03-30 16:20:41","2025-03-30 16:20:41","2025-03-30 16:20:41","","","","","","","PKU-DyMVHumans","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2403.16080 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/J5FIE964/Zheng et al. - 2024 - PKU-DyMVHumans A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2403.16080","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CM99RLG","preprint","2023","Balasingam, Arjun; Chandler, Joseph; Li, Chenning; Zhang, Zhoutong; Balakrishnan, Hari","DriveTrack: A Benchmark for Long-Range Point Tracking in Real-World Videos","","","","10.48550/arXiv.2312.09523","http://arxiv.org/abs/2312.09523","This paper presents DriveTrack, a new benchmark and data generation framework for long-range keypoint tracking in real-world videos. DriveTrack is motivated by the observation that the accuracy of state-of-the-art trackers depends strongly on visual attributes around the selected keypoints, such as texture and lighting. The problem is that these artifacts are especially pronounced in real-world videos, but these trackers are unable to train on such scenes due to a dearth of annotations. DriveTrack bridges this gap by building a framework to automatically annotate point tracks on autonomous driving datasets. We release a dataset consisting of 1 billion point tracks across 24 hours of video, which is seven orders of magnitude greater than prior real-world benchmarks and on par with the scale of synthetic benchmarks. DriveTrack unlocks new use cases for point tracking in real-world videos. First, we show that finetuning keypoint trackers on DriveTrack improves accuracy on real-world scenes by up to 7%. Second, we analyze the sensitivity of trackers to visual artifacts in real scenes and motivate the idea of running assistive keypoint selectors alongside trackers.","2023-12-15","2025-03-30 16:20:43","2025-03-30 16:20:43","2025-03-30 16:20:43","","","","","","","DriveTrack","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2312.09523 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/CCRIXL9T/Balasingam et al. - 2023 - DriveTrack A Benchmark for Long-Range Point Tracking in Real-World Videos.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2312.09523","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JWGAGQCG","conferencePaper","2024","Xie, Yaofeng; Kong, Lingwei; Chen, Kai; Zheng, Ziqiang; Yu, Xiao; Yu, Zhibin; Zheng, Bing","UVEB: A Large-scale Benchmark and Baseline Towards Real-World Underwater Video Enhancement","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02110","https://ieeexplore.ieee.org/document/10656519/","Learning-based underwater image enhancement (UIE) methods have made great progress. However, the lack of large-scale and high-quality paired training samples has become the main bottleneck hindering the development of UIE. The inter-frame information in underwater videos can accelerate or optimize the UIE process. Thus, we constructed the first large-scale high-resolution underwater video enhancement benchmark (UVEB) to promote the development of underwater vision. It contains 1,308 pairs of video sequences and more than 453,000 high-resolution with 38% Ultra-High-Definition (UHD) 4K frame pairs. UVEB comes from multiple countries, containing various scenes and video degradation types to adapt to diverse and complex underwater environments. We also propose the first supervised underwater video enhancement method, UVENet. UVE-Net converts the current frame information into convolutional kernels and passes them to adjacent frames for efficient inter-frame information exchange. By fully utilizing the redundant degraded information of underwater videos, UVE-Net completes video enhancement better. Experiments show the effective network design and good performance of UVE-Net.","2024-06-16","2025-03-30 16:20:45","2025-03-30 16:20:45","2025-03-30 16:20:45","22358-22367","","","","","","UVEB","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9UH4UKIW/Xie et al. - 2024 - UVEB A Large-scale Benchmark and Baseline Towards Real-World Underwater Video Enhancement.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"2KVTGARI","conferencePaper","2024","Woo, Sanghyun; Park, Kwanyong; Shin, Inkyu; Kim, Myungchul; Kweon, In So","MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02108","https://ieeexplore.ieee.org/document/10655925/","","2024-06-16","2025-03-30 16:20:50","2025-03-30 16:20:50","2025-03-30 16:20:50","22335-22346","","","","","","MTMMC","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4ZTGTB7R/Woo et al. - 2024 - MTMMC A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"HX9XTY8Z","conferencePaper","2024","Fu, Huiyuan; Peng, Fei; Li, Xianwei; Li, Yejun; Wang, Xin; Ma, Huadong","Continuous Optical Zooming: A Benchmark for Arbitrary-Scale Image Super-Resolution in Real World","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.00293","https://ieeexplore.ieee.org/document/10655459/","Most current arbitrary-scale image super-resolution (SR) methods has commonly relied on simulated data generated by simple synthetic degradation models (e.g., bicubic downsampling) at continuous various scales, thereby falling short in capturing the complex degradation of real-world images. This limitation hinders the visual quality of these methods when applied to real-world images. To address this issue, we propose the Continuous Optical Zooming dataset (COZ), by constructing an automatic imaging system to collect images at fine-grained various focal lengths within a specific range and providing strict image pair alignment. The COZ dataset serves as a benchmark to provide real-world data for training and testing arbitrary-scale SR models. To enhance the model’s robustness against real-world image degradation, we propose a Local Mix Implicit network (LMI) based on the MLP-mixer architecture and meta-learning, which directly learns the local texture information by simultaneously mixing features and coordinates of multiple independent points. The extensive experiments demonstrate the superior performance of the arbitrary-scale SR models trained on the COZ dataset compared to models trained on simulated data. Our LMI model exhibits the superior effectiveness compared to other models. This study is of great significance in developing more efficient algorithms and improving the performance of arbitrary-scale image SR methods in practical applications. Our dataset and codes are available at https://github.com/pf0607/COZ.","2024-06-16","2025-03-30 16:20:52","2025-03-30 16:20:52","2025-03-30 16:20:52","3035-3044","","","","","","Continuous Optical Zooming","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/P5QIH8EP/Fu et al. - 2024 - Continuous Optical Zooming A Benchmark for Arbitrary-Scale Image Super-Resolution in Real World.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"CST8WHRX","conferencePaper","2024","Kaul, Prannay; Li, Zhizhong; Yang, Hao; Dukler, Yonatan; Swaminathan, Ashwin; Taylor, C. J.; Soatto, Stefano","THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02571","https://ieeexplore.ieee.org/document/10657848/","Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term “Type I hallucinations”. Instead, they focus on hallucinations responding to very specific question formats—typically a multiple-choice response regarding a particular object or attribute—which we term “Type II hallucinations”. Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.","2024-06-16","2025-03-30 16:20:56","2025-03-30 16:20:56","2025-03-30 16:20:56","27218-27228","","","","","","THRONE","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YA7TW4FC/Kaul et al. - 2024 - THRONE An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Langua.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"2LQQKSLV","conferencePaper","2024","Mais, Lisa; Hirsch, Peter; Managan, Claire; Kandarpa, Ramya; Rumberger, Josef Lorenz; Reinke, Annika; Maier-Hein, Lena; Ihrke, Gudrun; Kainmueller, Dagmar","FISBe: A Real-World Benchmark Dataset for Instance Segmentation of Long-Range thin Filamentous Structures","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.02100","https://ieeexplore.ieee.org/document/10656899/","","2024-06-16","2025-03-30 16:21:01","2025-03-30 16:21:01","2025-03-30 16:21:01","22249-22259","","","","","","FISBe","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FNS4ZS2G/Mais et al. - 2024 - FISBe A Real-World Benchmark Dataset for Instance Segmentation of Long-Range thin Filamentous Struc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"CNAGWNS5","conferencePaper","2024","Ma, Yunsheng; Cui, Can; Cao, Xu; Ye, Wenqian; Liu, Peiran; Lu, Juanwu; Abdelraouf, Amr; Gupta, Rohit; Han, Kyungtae; Bera, Aniket; Rehg, James M.; Wang, Ziran","LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01434","https://ieeexplore.ieee.org/document/10657936/","","2024-06-16","2025-03-30 16:21:04","2025-03-30 16:21:04","2025-03-30 16:21:04","15141-15151","","","","","","LaMPilot","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5964MG7R/Ma et al. - 2024 - LaMPilot An Open Benchmark Dataset for Autonomous Driving with Language Model Programs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"2UKF2U35","journalArticle","","Du, Hang; Zhang, Sicheng; Xie, Binzhu; Nan, Guoshun; Zhang, Jiayang; Xu, Junrui; Liu, Hangyu; Leng, Sicong; Liu, Jiangming; Fan, Hehe; Huang, Dajiu; Feng, Jing; Chen, Linli; Zhang, Can; Li, Xuhuan; Zhang, Hao; Chen, Jianhang; Cui, Qimei; Tao, Xiaofeng","Uncovering What Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly","","","","","","","","2025-03-30 16:21:05","2025-03-30 16:21:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/P3MCRVLX/Du et al. - Uncovering What Why and How A Comprehensive Benchmark for Causation Understanding of Video Anomaly.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKD6587F","preprint","2024","Wang, Andong; Wu, Bo; Chen, Sunli; Chen, Zhenfang; Guan, Haotian; Lee, Wei-Ning; Li, Li Erran; Gan, Chuang","SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge","","","","10.48550/arXiv.2405.09713","http://arxiv.org/abs/2405.09713","Learning commonsense reasoning from visual contexts and scenes in real-world is a crucial step toward advanced artificial intelligence. However, existing video reasoning benchmarks are still inadequate since they were mainly designed for factual or situated reasoning and rarely involve broader knowledge in the real world. Our work aims to delve deeper into reasoning evaluations, specifically within dynamic, open-world, and structured context knowledge. We propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K situations with instance-level annotations depicted in the videos. The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving. To create such a dataset, we propose an automatic and scalable generation method to generate question-answer pairs, knowledge graphs, and rationales by instructing the combinations of LLMs and MLLMs. Concretely, we first extract observable situated entities, relations, and processes from videos for situated knowledge and then extend to openworld knowledge beyond the visible content. The task generation is facilitated through multiple dialogues as iterations and subsequently corrected and refined by our designed self-promptings and demonstrations. With a corpus of both explicit situated facts and implicit commonsense, we generate associated question-answer pairs and reasoning processes, finally followed by manual reviews for quality assurance. We evaluated recent mainstream large visionlanguage models on the benchmark and found several insightful conclusions. For more information, please refer to our benchmark at www.bobbywu.com/SOKBench.","2024-05-17","2025-03-30 16:21:07","2025-03-30 16:21:07","2025-03-30 16:21:07","","","","","","","SOK-Bench","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2405.09713 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/WVLY3TX5/Wang et al. - 2024 - SOK-Bench A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2405.09713","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSNU5R64","preprint","2023","Ma, Junyi; Chen, Xieyuanli; Huang, Jiawei; Xu, Jingyi; Luo, Zhen; Xu, Jintao; Gu, Weihao; Ai, Rui; Wang, Hesheng","Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications","","","","10.48550/arXiv.2311.17663","http://arxiv.org/abs/2311.17663","Understanding how the surrounding environment changes is crucial for performing downstream tasks safely and reliably in autonomous driving applications. Recent occupancy estimation techniques using only camera images as input can provide dense occupancy representations of large-scale scenes based on the current observation. However, they are mostly limited to representing the current 3D space and do not consider the future state of surrounding objects along the time axis. To extend camera-only occupancy estimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark for camera-only 4D occupancy forecasting, evaluating the surrounding scene changes in a near future. We build our benchmark based on multiple publicly available datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5, which provides sequential occupancy states of general movable and static objects, as well as their 3D backward centripetal flow. To establish this benchmark for future research with comprehensive comparisons, we introduce four baseline types from diverse camera-based perception and prediction implementations, including a staticworld occupancy model, voxelization of point cloud prediction, 2D-3D instance-based prediction, and our proposed novel endto-end 4D occupancy forecasting network. Furthermore, the standardized evaluation protocol for preset multiple tasks is also provided to compare the performance of all the proposed baselines on present and future occupancy estimation with respect to objects of interest in autonomous driving scenarios. The dataset and our implementation of all four baselines in the proposed Cam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.","2023-12-07","2025-03-30 16:21:10","2025-03-30 16:21:10","2025-03-30 16:21:10","","","","","","","Cam4DOcc","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.17663 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/357GSVRX/Ma et al. - 2023 - Cam4DOcc Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2311.17663","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNQSM5LP","preprint","2024","Yue, Xiang; Ni, Yuansheng; Zhang, Kai; Zheng, Tianyu; Liu, Ruoqi; Zhang, Ge; Stevens, Samuel; Jiang, Dongfu; Ren, Weiming; Sun, Yuxuan; Wei, Cong; Yu, Botao; Yuan, Ruibin; Sun, Renliang; Yin, Ming; Zheng, Boyuan; Yang, Zhenzhu; Liu, Yibo; Huang, Wenhao; Sun, Huan; Su, Yu; Chen, Wenhu","MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI","","","","10.48550/arXiv.2311.16502","http://arxiv.org/abs/2311.16502","We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.","2024-06-13","2025-03-30 16:21:12","2025-03-30 16:21:12","2025-03-30 16:21:12","","","","","","","MMMU","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.16502 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/9KWFYW73/Yue et al. - 2024 - MMMU A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2311.16502","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXQF9STX","conferencePaper","2024","Zhang, Junyuan; Zeng, Shuang; Zhang, Miao; Wang, Runxi; Wang, Feifei; Zhou, Yuyin; Liang, Paul Pu; Qu, Liangqiong","FLHetBench: Benchmarking Device and State Heterogeneity in Federated Learning","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9798350353006","","10.1109/CVPR52733.2024.01150","https://ieeexplore.ieee.org/document/10657899/","Federated learning (FL) is a powerful technology that enables collaborative training of machine learning models without sharing private data among clients. The fundamental challenge in FL lies in learning over extremely heterogeneous data distributions, device capacities, and device state availabilities, all of which adversely impact performance and communication efficiency. While data heterogeneity has been well-studied in the literature, this paper introduces FLHetBench, the first FL benchmark targeted toward understanding device and state heterogeneity. FLHetBench comprises two new sampling methods to generate real-world device and state databases with varying heterogeneity and new metrics for quantifying the success of FL methods under these real-world constraints. Using FLHetBench, we conduct a comprehensive evaluation of existing methods and find that they struggle under these settings, which inspires us to propose BiasPrompt+, a new method employing staleness-aware aggregation and fast weights to tackle these new heterogeneity challenges. Experiments on various FL tasks and datasets validate the effectiveness of our BiasPrompt+ method and highlight the value of FLHetBench in fostering the development of more efficient and robust FL solutions under real-world device and state constraints.","2024-06-16","2025-03-30 16:21:17","2025-03-30 16:21:17","2025-03-30 16:21:17","12098-12108","","","","","","FLHetBench","","","","","IEEE","Seattle, WA, USA","en","https://doi.org/10.15223/policy-029","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TFV9QNAW/Zhang et al. - 2024 - FLHetBench Benchmarking Device and State Heterogeneity in Federated Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"X3QJSSHG","preprint","2023","Chen, Huancheng; Vikalo, Haris","Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices","","","","10.48550/arXiv.2311.18129","http://arxiv.org/abs/2311.18129","While federated learning (FL) systems often utilize quantization to battle communication and computational bottlenecks, they have heretofore been limited to deploying fixedprecision quantization schemes. Meanwhile, the concept of mixed-precision quantization (MPQ), where different layers of a deep learning model are assigned varying bit-width, remains unexplored in the FL settings. We present a novel FL algorithm, FedMPQ, which introduces mixed-precision quantization to resource-heterogeneous FL systems. Specifically, local models, quantized so as to satisfy bit-width constraint, are trained by optimizing an objective function that includes a regularization term which promotes reduction of precision in some of the layers without significant performance degradation. The server collects local model updates, de-quantizes them into full-precision models, and then aggregates them into a global model. To initialize the next round of local training, the server relies on the information learned in the previous training round to customize bit-width assignments of the models delivered to different clients. In extensive benchmarking experiments on several model architectures and different datasets in both iid and non-iid settings, FedMPQ outperformed the baseline FL schemes that utilize fixed-precision quantization while incurring only a minor computational overhead on the participating devices.","2023-11-29","2025-03-30 16:21:19","2025-03-30 16:21:19","2025-03-30 16:21:19","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2311.18129 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/BHI5JUUZ/Chen and Vikalo - 2023 - Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices.pdf","","","Computer Science - Distributed, Parallel, and Cluster Computing; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2311.18129","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMEBI2NQ","preprint","2023","Patel, Maitreya; Kim, Changhoon; Cheng, Sheng; Baral, Chitta; Yang, Yezhou","ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations","","","","10.48550/arXiv.2312.04655","http://arxiv.org/abs/2312.04655","Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g., DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional T2I benchmarks, at the cost of significant computational resources. The unCLIP stack comprises T2I prior and diffusion image decoder. The T2I prior model alone adds a billion parameters compared to the Latent Diffusion Models, which increases the computational and high-quality data requirements. We introduce ECLIPSE, a novel contrastive learning method that is both parameter and data-efficient. ECLIPSE leverages pre-trained vision-language models (e.g., CLIP) to distill the knowledge into the prior model. We demonstrate that the ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere 2.8% of the data, surpasses the baseline T2I priors with an average of 71.6% preference score under resource-limited setting. It also attains performance on par with SOTA big models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions. Extensive experiments on two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE priors consistently deliver high performance while significantly reducing resource dependency.","2023-12-07","2025-03-30 16:21:20","2025-03-30 16:21:20","2025-03-30 16:21:20","","","","","","","ECLIPSE","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2312.04655 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/MSNURSF5/Patel et al. - 2023 - ECLIPSE A Resource-Efficient Text-to-Image Prior for Image Generations.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2312.04655","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QEWGFYA9","journalArticle","","Ilhan, Fatih; Su, Gong; Tekin, Selim Furkan; Huang, Tiansheng; Hu, Sihao; Liu, Ling","Resource-Efficient Transformer Pruning for Finetuning of Large Models","","","","","","With the recent advances in vision transformers and large language models (LLMs), ﬁnetuning costly large models on downstream learning tasks poses signiﬁcant challenges under limited computational resources. This paper presents a REsource and ComputAtion-efﬁcient Pruning framework (RECAP) for the ﬁnetuning of transformerbased large models. RECAP by design bridges the gap between efﬁciency and performance through an iterative process cycling between pruning, ﬁnetuning, and updating stages to explore different chunks of the given largescale model. At each iteration, we ﬁrst prune the model with Taylor-approximation-based importance estimation and then only update a subset of the pruned model weights based on the Fisher-information criterion. In this way, RECAP achieves two synergistic and yet conﬂicting goals: reducing the GPU memory footprint while maintaining model performance, unlike most existing pruning methods that require the model to be ﬁnetuned beforehand for better preservation of model performance. We perform extensive experiments with a wide range of large transformer-based architectures on various computer vision and natural language understanding tasks. Compared to recent pruning techniques, we demonstrate that RECAP offers signiﬁcant improvements in GPU memory efﬁciency, capable of reducing the footprint by up to 65%.","","2025-03-30 16:21:22","2025-03-30 16:21:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2TWV4CIC/Ilhan et al. - Resource-Efficient Transformer Pruning for Finetuning of Large Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EP3NEYEB","preprint","2024","Zhang, Yunhua; Doughty, Hazel; Snoek, Cees G. M.","Low-Resource Vision Challenges for Foundation Models","","","","10.48550/arXiv.2401.04716","http://arxiv.org/abs/2401.04716","Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for deep learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we address this gap and explore the challenges of low-resource image tasks with vision foundation models. We first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share three challenges: data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on our three low-resource tasks demonstrate our proposals already provide a better baseline than transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project page: https://xiaobai1217.github.io/ Low-Resource-Vision/.","2024-04-11","2025-03-30 16:21:23","2025-03-30 16:21:23","2025-03-30 16:21:23","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2401.04716 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/S8BGY6RF/Zhang et al. - 2024 - Low-Resource Vision Challenges for Foundation Models.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2401.04716","","","","","","","","","","","","","","","","","","","","","","","","","","",""