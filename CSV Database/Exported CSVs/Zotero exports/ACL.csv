"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"ZLSVM7UD","conferencePaper","2024","Li, Haoran; Guo, Dadi; Li, Donghao; Fan, Wei; Hu, Qi; Liu, Xin; Chan, Chunkit; Yao, Duanyi; Yao, Yuan; Song, Yangqiu","PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.4","https://aclanthology.org/2024.acl-long.4","","2024","2025-03-30 16:11:08","2025-03-30 16:11:08","2025-03-30 16:11:08","54-73","","","","","","PrivLM-Bench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/24R9657F/Li et al. - 2024 - PrivLM-Bench A Multi-level Privacy Evaluation Benchmark for Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"WBN4DR9E","preprint","2024","Hu, Yong; Meng, Fandong; Zhou, Jie","CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers","","","","10.48550/arXiv.2211.08788","http://arxiv.org/abs/2211.08788","In this paper, we present CSCD-NS, the first Chinese spelling check (CSC) dataset designed for native speakers, containing 40,000 samples from a Chinese social platform. Compared with existing CSC datasets aimed at Chinese learners, CSCD-NS is ten times larger in scale and exhibits a distinct error distribution, with a significantly higher proportion of word-level errors. To further enhance the data resource, we propose a novel method that simulates the input process through an input method, generating large-scale and high-quality pseudo data that closely resembles the actual error distribution and outperforms existing methods. Moreover, we investigate the performance of various models in this scenario, including large language models (LLMs), such as ChatGPT. The result indicates that generative models underperform BERT-like classification models due to strict length and pronunciation constraints. The high prevalence of word-level errors also makes CSC for native speakers challenging enough, leaving substantial room for improvement.","2024-05-23","2025-03-30 16:11:11","2025-03-30 16:11:12","2025-03-30 16:11:11","","","","","","","CSCD-NS","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2211.08788 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/PYKH4YL5/Hu et al. - 2024 - CSCD-NS a Chinese Spelling Check Dataset for Native Speakers.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2211.08788","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XF59PKK","conferencePaper","2024","Poesina, Eduard; Caragea, Cornelia; Ionescu, Radu","A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.15","https://aclanthology.org/2024.acl-long.15","Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classiﬁcation, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the ﬁrst Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available at https://github.com/Eduard6421/RONLI.","2024","2025-03-30 16:11:15","2025-03-30 16:11:16","2025-03-30 16:11:15","236-253","","","","","","A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TRS34YFG/Poesina et al. - 2024 - A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI The First Romanian Natural La.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"IVP948X3","conferencePaper","2024","Wang, Xiyao; Zhou, Yuhang; Liu, Xiaoyu; Lu, Hongjin; Xu, Yuancheng; He, Feihong; Yoon, Jaehong; Lu, Taixi; Liu, Fuxiao; Bertasius, Gedas; Bansal, Mohit; Yao, Huaxiu; Huang, Furong","Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.25","https://aclanthology.org/2024.acl-long.25","Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs’ sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs’ sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of cooccurring behaviors, and the compounding impact of behavioral hallucinations.","2024","2025-03-30 16:11:19","2025-03-30 16:11:19","2025-03-30 16:11:19","416-442","","","","","","Mementos","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Y2I3XJXB/Wang et al. - 2024 - Mementos A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequenc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"9FJT7KT3","conferencePaper","2024","Xia, Congying; Xing, Chen; Du, Jiangshu; Yang, Xinyi; Feng, Yihao; Xu, Ran; Yin, Wenpeng; Xiong, Caiming","FOFO: A Benchmark to Evaluate LLMs’ Format-Following Capability","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.40","https://aclanthology.org/2024.acl-long.40","","2024","2025-03-30 16:11:23","2025-03-30 16:11:23","2025-03-30 16:11:23","680-699","","","","","","FOFO","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CRMRDITF/Xia et al. - 2024 - FOFO A Benchmark to Evaluate LLMs’ Format-Following Capability.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"JLPZF3DP","conferencePaper","2024","Ghosh, Sreyan; Tyagi, Utkarsh; Kumar, Sonal; Evuru, Chandra Kiran; S, Ramaneswaran; Sakshi, S; Manocha, Dinesh","ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.43","https://aclanthology.org/2024.acl-long.43","","2024","2025-03-30 16:11:26","2025-03-30 16:11:26","2025-03-30 16:11:26","726-748","","","","","","ABEX","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MAKRT5YM/Ghosh et al. - 2024 - ABEX Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"AKN5AHRM","conferencePaper","2024","Bandarkar, Lucas; Liang, Davis; Muller, Benjamin; Artetxe, Mikel; Shukla, Satya Narayan; Husa, Donald; Goyal, Naman; Krishnan, Abhinandan; Zettlemoyer, Luke; Khabsa, Madian","The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.44","https://aclanthology.org/2024.acl-long.44","We present BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiplechoice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in Englishcentric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, BELEBELE opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.","2024","2025-03-30 16:11:28","2025-03-30 16:11:29","2025-03-30 16:11:28","749-775","","","","","","The Belebele Benchmark","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ISUBPGN2/Bandarkar et al. - 2024 - The Belebele Benchmark a Parallel Reading Comprehension Dataset in 122 Language Variants.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"9UH78XFY","conferencePaper","2024","Zhang, Zhihan; Cao, Yixin; Ye, Chenchen; Ma, Yunshan; Liao, Lizi; Chua, Tat-Seng","Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.87","https://aclanthology.org/2024.acl-long.87","The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.","2024","2025-03-30 16:11:31","2025-03-30 16:11:31","2025-03-30 16:11:31","1588-1606","","","","","","Analyzing Temporal Complex Events with Large Language Models?","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HRVZZXFH/Zhang et al. - 2024 - Analyzing Temporal Complex Events with Large Language Models A Benchmark towards Temporal, Long Con.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"22LY8PZZ","conferencePaper","2024","Jin, Chuhao; Ren, Kening; Kong, Lingzhen; Wang, Xiting; Song, Ruihua; Chen, Huan","Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.92","https://aclanthology.org/2024.acl-long.92","Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrievalbased models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of user’s utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io.","2024","2025-03-30 16:11:33","2025-03-30 16:11:33","2025-03-30 16:11:33","1678-1706","","","","","","Persuading across Diverse Domains","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AIMAEEIM/Jin et al. - 2024 - Persuading across Diverse Domains a Dataset and Persuasion Large Language Model.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"CYVHMULL","conferencePaper","2024","Yang, Qian; Xu, Jin; Liu, Wenrui; Chu, Yunfei; Jiang, Ziyue; Zhou, Xiaohuan; Leng, Yichong; Lv, Yuanjun; Zhao, Zhou; Zhou, Chang; Zhou, Jingren","AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.109","https://aclanthology.org/2024.acl-long.109","Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as automatic speech recognition, and lack an assessment of the openended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (Audio InstRuction Benchmark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIRBench encompasses two dimensions: foundation and chat benchmarks. The former consists of 19 tasks with approximately 19k singlechoice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended questionand-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.","2024","2025-03-30 16:11:36","2025-03-30 16:11:36","2025-03-30 16:11:36","1979-1998","","","","","","AIR-Bench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UWCF4LDX/Yang et al. - 2024 - AIR-Bench Benchmarking Large Audio-Language Models via Generative Comprehension.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"MNNB7VUZ","conferencePaper","2024","Yin, Xunjian; Zhang, Xu; Ruan, Jie; Wan, Xiaojun","Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.124","https://aclanthology.org/2024.acl-long.124","In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and promptsensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose a projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.","2024","2025-03-30 16:11:39","2025-03-30 16:11:39","2025-03-30 16:11:39","2270-2286","","","","","","Benchmarking Knowledge Boundary for Large Language Models","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/P577JYTI/Yin et al. - 2024 - Benchmarking Knowledge Boundary for Large Language Models A Different Perspective on Model Evaluati.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"WWI9F5VX","conferencePaper","2024","Sadat, Mobashir; Caragea, Cornelia","Co-training for Low Resource Scientific Natural Language Inference","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.139","https://aclanthology.org/2024.acl-long.139","","2024","2025-03-30 16:11:41","2025-03-30 16:11:41","2025-03-30 16:11:41","2538-2550","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KM3X4Z7P/Sadat and Caragea - 2024 - Co-training for Low Resource Scientific Natural Language Inference.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"TTN9G3QF","conferencePaper","2024","Wei, Xiao; Xu, Qi; Yu, Hang; Liu, Qian; Cambria, Erik","Through the MUD: A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.158","https://aclanthology.org/2024.acl-long.158","The current charge prediction datasets mostly focus on single-defendant criminal cases. However, real-world criminal cases usually involve multiple defendants whose criminal facts are intertwined. In an early attempt to fill this gap, we introduce a new benchmark that encompasses legal cases involving multiple defendants, where each defendant is labeled with a charge and four types of crime elements, i.e., Object Element, Objective Element, Subject Element, and Subjective Element. Based on the dataset, we further develop an interpretable model called EJudge that incorporates crime elements and legal rules to infer charges. We observe that predicting crime charges while providing corresponding rationales benefits the interpretable AI system. Extensive experiments show that EJudge significantly surpasses state-of-the-art methods, which verify the importance of crime elements and legal rules in multi-defendant charge prediction. Source code and dataset available at https://github.com/welchxu/MCP.","2024","2025-03-30 16:11:45","2025-03-30 16:11:45","2025-03-30 16:11:45","2864-2878","","","","","","Through the MUD","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/D4S3JADH/Wei et al. - 2024 - Through the MUD A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"GJY3647G","conferencePaper","2024","Li, Qintong; Cui, Leyang; Zhao, Xueliang; Kong, Lingpeng; Bi, Wei","GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.163","https://aclanthology.org/2024.acl-long.163","Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs’ math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-PLUS) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.","2024","2025-03-30 16:11:48","2025-03-30 16:11:48","2025-03-30 16:11:48","2961-2984","","","","","","GSM-Plus","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ERJYG2UX/Li et al. - 2024 - GSM-Plus A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem So.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"A4PG2VSB","conferencePaper","2024","Bai, Yushi; Lv, Xin; Zhang, Jiajie; Lyu, Hongchang; Tang, Jiankai; Huang, Zhidian; Du, Zhengxiao; Liu, Xiao; Zeng, Aohan; Hou, Lei; Dong, Yuxiao; Tang, Jie; Li, Juanzi","LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.172","https://aclanthology.org/2024.acl-long.172","Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs’ long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including singledoc QA, multi-doc QA, summarization, fewshot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other opensourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.","2024","2025-03-30 16:11:50","2025-03-30 16:11:51","2025-03-30 16:11:50","3119-3137","","","","","","LongBench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TQ397QKX/Bai et al. - 2024 - LongBench A Bilingual, Multitask Benchmark for Long Context Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"DBRYFCLD","conferencePaper","2024","Chen, Yuyan; Wu, Chenwei; Yan, Songzhou; Liu, Panjun; Xiao, Yanghua","Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.173","https://aclanthology.org/2024.acl-long.173","Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs’ capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored. In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles. Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl’s taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs’ outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.","2024","2025-03-30 16:11:53","2025-03-30 16:11:53","2025-03-30 16:11:53","3138-3167","","","","","","Dr.Academy","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Q2BRH6FI/Chen et al. - 2024 - Dr.Academy A Benchmark for Evaluating Questioning Capability in Education for Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"HKCIF2M9","conferencePaper","2024","Pham, Trinh; Le, Khoi; Luu, Anh Tuan","UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.174","https://aclanthology.org/2024.acl-long.174","In this paper, we introduce UniBridge (CrossLingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.","2024","2025-03-30 16:11:55","2025-03-30 16:11:55","2025-03-30 16:11:55","3168-3184","","","","","","UniBridge","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/G5HGMDN6/Pham et al. - 2024 - UniBridge A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"98YFLJ9K","conferencePaper","2024","Park, Chanjun; Kim, Hyeonwoo; Kim, Dahyun; Cho, SeongHwan; Kim, Sanghoon; Lee, Sukyung; Kim, Yungi; Lee, Hwalsuk","Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.177","https://aclanthology.org/2024.acl-long.177","This paper introduces the Open Ko-LLM Leaderboard1 and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.","2024","2025-03-30 16:11:58","2025-03-30 16:11:58","2025-03-30 16:11:58","3220-3234","","","","","","Open Ko-LLM Leaderboard","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/C27XZ4LM/Park et al. - 2024 - Open Ko-LLM Leaderboard Evaluating Large Language Models in Korean with Ko-H5 Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"Z9U7G92S","conferencePaper","2024","Nguyen, Xuan-Phi; Aljunied, Mahani; Joty, Shafiq; Bing, Lidong","Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.192","https://aclanthology.org/2024.acl-long.192","Large language models (LLMs) are known to perform tasks by simply observing few exemplars. Moreover, competent generative capabilities of LLMs are observed mostly in highresource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs’ ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages. These prompts can directly induce generative capabilities in lowresource languages and serve as intra-lingual exemplars to even improve tasks in these languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 34 Indic and African languages, and surpasses supervised prompting in non-English tasks. The method also significantly improves low-resource performances in many other intra-lingual tasks like summarization (XLSum), question answering (XQUAD & TydiQA) and conversational instruction following (Sea-Bench).","2024","2025-03-30 16:12:03","2025-03-30 16:12:03","2025-03-30 16:12:02","3501-3516","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RGYB2Q2C/Nguyen et al. - 2024 - Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Li.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"T7TXIIYX","conferencePaper","2024","Tong, Xiaoyu; Choenni, Rochelle; Lewis, Martha; Shutova, Ekaterina","Metaphor Understanding Challenge Dataset for LLMs","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.193","https://aclanthology.org/2024.acl-long.193","Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs. The dataset is freely accessible at https://github.com/xiaoyuisrain/ metaphor-understanding-challenge.","2024","2025-03-30 16:12:05","2025-03-30 16:12:05","2025-03-30 16:12:05","3517-3536","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FLZTPVD7/Tong et al. - 2024 - Metaphor Understanding Challenge Dataset for LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"YBQNF4NU","conferencePaper","2024","Wang, Yuxin; Yang, Ivory; Hassanpour, Saeed; Vosoughi, Soroush","MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.206","https://aclanthology.org/2024.acl-long.206","Mental manipulation, a significant form of abuse in interpersonal conversations, presents a challenge to identify due to its contextdependent and often subtle nature. The detection of manipulative language is essential for protecting potential victims, yet the field of Natural Language Processing (NLP) currently faces a scarcity of resources and research on this topic. Our study addresses this gap by introducing a new dataset, named MENTALMANIP, which consists of 4, 000 annotated fictional dialogues. This dataset enables a comprehensive analysis of mental manipulation, pinpointing both the techniques utilized for manipulation and the vulnerabilities targeted in victims. Our research further explores the effectiveness of leading-edge models in recognizing manipulative dialogue and its components through a series of experiments with various configurations. The results demonstrate that these models inadequately identify and categorize manipulative content. Attempts to improve their performance by fine-tuning with existing datasets on mental health and toxicity have not overcome these limitations. We anticipate that MENTALMANIP will stimulate further research, leading to progress in both understanding and mitigating the impact of mental manipulation in conversations.","2024","2025-03-30 16:12:07","2025-03-30 16:12:07","2025-03-30 16:12:07","3747-3764","","","","","","MentalManip","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZCN6JIPV/Wang et al. - 2024 - MentalManip A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"5PCDI4MS","conferencePaper","2024","He, Chaoqun; Luo, Renjie; Bai, Yuzhuo; Hu, Shengding; Thai, Zhen; Shen, Junhao; Hu, Jinyi; Han, Xu; Huang, Yujie; Zhang, Yuxiang; Liu, Jie; Qi, Lei; Liu, Zhiyuan; Sun, Maosong","OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.211","https://aclanthology.org/2024.acl-long.211","","2024","2025-03-30 16:12:10","2025-03-30 16:12:10","2025-03-30 16:12:10","3828-3850","","","","","","OlympiadBench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/S9VQUWTC/He et al. - 2024 - OlympiadBench A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Sc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"U7A4CKDB","conferencePaper","2024","Wang, Yuxia; Mansurov, Jonibek; Ivanov, Petar; Su, Jinyan; Shelmanov, Artem; Tsvigun, Akim; Mohammed Afzal, Osama; Mahmoud, Tarek; Puccetti, Giovanni; Arnold, Thomas; Aji, Alham; Habash, Nizar; Gurevych, Iryna; Nakov, Preslav","M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.218","https://aclanthology.org/2024.acl-long.218","The advent of Large Language Models (LLMs) has brought an unprecedented surge in machinegenerated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multidomain, and multi-generator corpus of MGTs — M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed humanmachine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github. com/mbzuai-nlp/M4GT-Bench.","2024","2025-03-30 16:12:13","2025-03-30 16:12:13","2025-03-30 16:12:13","3964-3992","","","","","","M4GT-Bench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Y5BL27J8/Wang et al. - 2024 - M4GT-Bench Evaluation Benchmark for Black-Box Machine-Generated Text Detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"TNJ5W6TH","conferencePaper","2024","Wang, Xiaozhi; Peng, Hao; Guan, Yong; Zeng, Kaisheng; Chen, Jianhui; Hou, Lei; Han, Xu; Lin, Yankai; Liu, Zhiyuan; Xie, Ruobing; Zhou, Jie; Li, Juanzi","MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.224","https://aclanthology.org/2024.acl-long.224","Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a largescale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-ARG, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-ARG offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98, 591 events and 290, 613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVENARG is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVENARG and codes can be obtained from https: //github.com/THU-KEG/MAVEN-Argument.","2024","2025-03-30 16:12:15","2025-03-30 16:12:16","2025-03-30 16:12:15","4072-4091","","","","","","MAVEN-ARG","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z63AA3Y2/Wang et al. - 2024 - MAVEN-ARG Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annot.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"EN5IZDCU","conferencePaper","2024","Fan, Lizhou; Hua, Wenyue; Li, Lingyao; Ling, Haoyang; Zhang, Yongfeng","NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.225","https://aclanthology.org/2024.acl-long.225","Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting and memorization, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NPHard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting or memorizing the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https:// github.com/casmlab/NPHardEval.","2024","2025-03-30 16:12:17","2025-03-30 16:12:18","2025-03-30 16:12:17","4092-4114","","","","","","NPHardEval","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NECI2LT5/Fan et al. - 2024 - NPHardEval Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"K5PEQ2ID","conferencePaper","2024","Wang, Kexin; Reimers, Nils; Gurevych, Iryna","DAPR: A Benchmark on Document-Aware Passage Retrieval","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.236","https://aclanthology.org/2024.acl-long.236","The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. Wikipedia articles, research papers, etc. We propose and name this task Document-Aware Passage Retrieval (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextualized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries, but overall they also perform rather poorly. Our created benchmark enables future research on developing and comparing retrieval systems for the new task. The code and the data are available1.","2024","2025-03-30 16:12:21","2025-03-30 16:12:21","2025-03-30 16:12:21","4313-4330","","","","","","DAPR","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HAU98X2Z/Wang et al. - 2024 - DAPR A Benchmark on Document-Aware Passage Retrieval.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"HJD7GGA2","conferencePaper","2024","Jacovi, Alon; Bitton, Yonatan; Bohnet, Bernd; Herzig, Jonathan; Honovich, Or; Tseng, Michael; Collins, Michael; Aharoni, Roee; Geva, Mor","A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.254","https://aclanthology.org/2024.acl-long.254","Prompting language models to provide stepby-step answers (e.g., “Chain-of-Thought”) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model’s answer, across a wide variety of datasets and state-of-the-art language models. Available at reveal-dataset.github.io.","2024","2025-03-30 16:12:23","2025-03-30 16:12:23","2025-03-30 16:12:23","4615-4634","","","","","","A Chain-of-Thought Is as Strong as Its Weakest Link","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6AZV3EMF/Jacovi et al. - 2024 - A Chain-of-Thought Is as Strong as Its Weakest Link A Benchmark for Verifiers of Reasoning Chains.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"5VENFC44","conferencePaper","2024","Ruan, Qian; Kuznetsov, Ilia; Gurevych, Iryna","Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.255","https://aclanthology.org/2024.acl-long.255","","2024","2025-03-30 16:12:27","2025-03-30 16:12:27","2025-03-30 16:12:27","4635-4655","","","","","","Re3","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/R3SBPDV4/Ruan et al. - 2024 - Re3 A Holistic Framework and Dataset for Modeling Collaborative Document Revision.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"GPT4MEB7","conferencePaper","2024","Jiang, Yuxin; Wang, Yufei; Zeng, Xingshan; Zhong, Wanjun; Li, Liangyou; Mi, Fei; Shang, Lifeng; Jiang, Xin; Liu, Qun; Wang, Wei","FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.257","https://aclanthology.org/2024.acl-long.257","The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs’ outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and opensource popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github. com/YJiangcm/FollowBench.","2024","2025-03-30 16:12:30","2025-03-30 16:12:30","2025-03-30 16:12:30","4667-4688","","","","","","FollowBench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IA97BUYW/Jiang et al. - 2024 - FollowBench A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"E84KU33B","conferencePaper","2024","Guo, Shiguang; Deng, Ziliang; Lin, Hongyu; Lu, Yaojie; Han, Xianpei; Sun, Le","Open Grounded Planning: Challenges and Benchmark Construction","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.272","https://aclanthology.org/2024.acl-long.272","","2024","2025-03-30 16:12:33","2025-03-30 16:12:33","2025-03-30 16:12:33","4982-5003","","","","","","Open Grounded Planning","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DZARPXHR/Guo et al. - 2024 - Open Grounded Planning Challenges and Benchmark Construction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"XFTGQCSY","journalArticle","","Liang, Xun; Song, Shichao; Niu, Simin; Li, Zhiyu; Xiong, Feiyu; Tang, Bo; Wang, Yezhaohui; He, Dawei; Cheng, Peng; Wang, Zhonghao; Deng, Haiying","al UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation","","","","","","Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a wellestablished Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, containing hallucinations generated by LLMs with minimal restrictions1. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.","","2025-03-30 16:12:35","2025-03-30 16:12:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GTJATGRH/Liang et al. - al UHGEval Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Genera.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXPKPRBW","conferencePaper","2024","Jinadu, Uthman; Ding, Yi","Noise Correction on Subjective Datasets","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.294","https://aclanthology.org/2024.acl-long.294","Incorporating every annotator’s perspective is crucial for unbiased data modeling. Annotator fatigue and changing opinions over time can distort dataset annotations. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, this method provides a controllable way to encourage or discourage disagreement. We demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.","2024","2025-03-30 16:12:37","2025-03-30 16:12:37","2025-03-30 16:12:37","5385-5395","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/M9QH5B8S/Jinadu and Ding - 2024 - Noise Correction on Subjective Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"5XCTBWKY","conferencePaper","2024","Yan, Weixiang; Liu, Haitian; Wang, Yunkun; Li, Yunzhe; Chen, Qian; Wang, Wen; Lin, Tingyu; Zhao, Weishan; Zhu, Li; Sundaram, Hari; Deng, Shuiguang","CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.301","https://aclanthology.org/2024.acl-long.301","Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers 43 programming languages and eight coding tasks. It evaluates the coding performance of LLMs from three dimensions (perspectives): length, difficulty, and efficiency. To facilitate execution-based evaluations of code generation, we develop MultiCodeEngine, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/ WeixiangYAN/CodeScope.","2024","2025-03-30 16:12:43","2025-03-30 16:12:43","2025-03-30 16:12:43","5511-5558","","","","","","CodeScope","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/52AGL9GI/Yan et al. - 2024 - CodeScope An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"IY3LJ4WT","conferencePaper","2024","Zhang, Yuge; Jiang, Qiyang; XingyuHan, XingyuHan; Chen, Nan; Yang, Yuqing; Ren, Kan","Benchmarking Data Science Agents","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.308","https://aclanthology.org/2024.acl-long.308","","2024","2025-03-30 16:12:46","2025-03-30 16:12:46","2025-03-30 16:12:46","5677-5700","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CUF56RYW/Zhang et al. - 2024 - Benchmarking Data Science Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"6TJLDMSK","conferencePaper","2024","Chen, Jian; Zhou, Peilin; Hua, Yining; Xin, Loh; Chen, Kehui; Li, Ziyuan; Zhu, Bing; Liang, Junwei","FinTextQA: A Dataset for Long-form Financial Question Answering","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.328","https://aclanthology.org/2024.acl-long.328","Accurate evaluation of financial questionanswering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold. The dataset is publicly available 1.","2024","2025-03-30 16:12:48","2025-03-30 16:12:48","2025-03-30 16:12:48","6025-6047","","","","","","FinTextQA","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZB63DEU7/Chen et al. - 2024 - FinTextQA A Dataset for Long-form Financial Question Answering.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"LWC2Y2NJ","conferencePaper","2024","Zhong, Tianqi; Li, Zhaoyi; Wang, Quan; Song, Linqi; Wei, Ying; Lian, Defu; Mao, Zhendong","Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.351","https://aclanthology.org/2024.acl-long.351","Compositional generalization, representing the model’s ability to generate text with new attribute combinations obtained by recombining single attributes from the training data, is a crucial property for multi-aspect controllable text generation (MCTG) methods. Nonetheless, a comprehensive compositional generalization evaluation benchmark of MCTG is still lacking. We propose CompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets and a crafted three-dimensional evaluation protocol, to holistically evaluate the compositional generalization of MCTG approaches. We observe that existing MCTG works generally confront a noticeable performance drop in compositional testing. To mitigate this issue, we introduce Meta-MCTG, a training framework incorporating meta-learning, where we enable models to learn how to generalize by simulating compositional generalization scenarios in the training phase. We demonstrate the effectiveness of Meta-MCTG through achieving obvious improvement (by at most 3.64%) for compositional testing performance in 94.4% cases1.","2024","2025-03-30 16:12:51","2025-03-30 16:12:51","2025-03-30 16:12:50","6486-6517","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VWZPU5DK/Zhong et al. - 2024 - Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"TWAVSM8B","conferencePaper","2024","Khan, Mohammad Abdullah Matin; Bari, M Saiful; Long, Do; Wang, Weishi; Parvez, Md Rizwan; Joty, Shafiq","XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.367","https://aclanthology.org/2024.acl-long.367","Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce XCODEEVAL, the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. XCODEEVAL adopts an execution-based evaluation and offers a multilingual code execution engine, ExecEval that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI’s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate XCODEEVAL to be quite challenging as per the current advancements in language models.","2024","2025-03-30 16:12:54","2025-03-30 16:12:54","2025-03-30 16:12:54","6766-6805","","","","","","XCodeEval","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VJNDDG88/Khan et al. - 2024 - XCodeEval An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, G.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"8L4YG3Q7","conferencePaper","2024","D’Arcy, Mike; Ross, Alexis; Bransom, Erin; Kuehl, Bailey; Bragg, Jonathan; Hope, Tom; Downey, Doug","ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.377","https://aclanthology.org/2024.acl-long.377","We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment—especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4’s ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.","2024","2025-03-30 16:12:57","2025-03-30 16:12:57","2025-03-30 16:12:57","6985-7001","","","","","","ARIES","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/T5ADIYXI/D’Arcy et al. - 2024 - ARIES A Corpus of Scientific Paper Edits Made in Response to Peer Reviews.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"3JW5K3SZ","conferencePaper","2024","Bai, Ge; Liu, Jie; Bu, Xingyuan; He, Yancheng; Liu, Jiaheng; Zhou, Zhanhui; Lin, Zhuoran; Su, Wenbo; Ge, Tiezheng; Zheng, Bo; Ouyang, Wanli","MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.401","https://aclanthology.org/2024.acl-long.401","The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarsegrained and incomplete assessments of multiturn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench101, specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at https: //github.com/mtbench101/mt-bench-101.","2024","2025-03-30 16:13:00","2025-03-30 16:13:00","2025-03-30 16:13:00","7421-7454","","","","","","MT-Bench-101","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/X9HECUNI/Bai et al. - 2024 - MT-Bench-101 A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"82F892AX","conferencePaper","2024","Ai, Qihang; Li, Jiafan; Dai, Jincheng; Zhou, Jianwu; Liu, Lemao; Jiang, Haiyun; Shi, Shuming","Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.404","https://aclanthology.org/2024.acl-long.404","Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning. Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5%-15% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancement in graph data understanding.","2024","2025-03-30 16:13:02","2025-03-30 16:13:02","2025-03-30 16:13:02","7485-7501","","","","","","Advancement in Graph Understanding","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/S9QLFAVT/Ai et al. - 2024 - Advancement in Graph Understanding A Multimodal Benchmark and Fine-Tuning of Vision-Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"WHJ5WTSN","conferencePaper","2024","Das, Rocktim; Hristov, Simeon; Li, Haonan; Dimitrov, Dimitar; Koychev, Ivan; Nakov, Preslav","EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.420","https://aclanthology.org/2024.acl-long.420","","2024","2025-03-30 16:13:05","2025-03-30 16:13:05","2025-03-30 16:13:05","7768-7791","","","","","","EXAMS-V","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QY78SD7D/Das et al. - 2024 - EXAMS-V A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Mo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"8Y8R3IQM","conferencePaper","2024","Wu, Zongru; Zhang, Zhuosheng; Cheng, Pengzhou; Liu, Gongshen","Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.441","https://aclanthology.org/2024.acl-long.441","Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with lowrank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively highfrequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes are publicly available at https://github.com/ZrW00/MuScleLoRA.","2024","2025-03-30 16:13:08","2025-03-30 16:13:08","2025-03-30 16:13:08","8116-8134","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NR87HSM4/Wu et al. - 2024 - Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"ZCTAVVV2","conferencePaper","2024","Lv, Kai; Yang, Yuqing; Liu, Tengxiao; Guo, Qipeng; Qiu, Xipeng","Full Parameter Fine-tuning for Large Language Models with Limited Resources","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.445","https://aclanthology.org/2024.acl-long.445","","2024","2025-03-30 16:13:10","2025-03-30 16:13:10","2025-03-30 16:13:10","8187-8198","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8XB9FLUH/Lv et al. - 2024 - Full Parameter Fine-tuning for Large Language Models with Limited Resources.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"T5KTWLIQ","conferencePaper","2024","Chen, Qiguang; Qin, Libo; Zhang, Jin; Chen, Zhi; Xu, Xiao; Che, Wanxiang","M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.446","https://aclanthology.org/2024.acl-long.446","Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-bystep reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) Domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M3CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M3CoT and there remains a large gap between existing VLLMs and human performance in M3CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M3CoT can serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.","2024","2025-03-30 16:13:12","2025-03-30 16:13:12","2025-03-30 16:13:12","8199-8221","","","","","","M3CoT","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/848226ML/Chen et al. - 2024 - M3CoT A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"CZG4V3FY","conferencePaper","2024","Krumdick, Michael; Koncel-Kedziorski, Rik; Lai, Viet Dac; Reddy, Varshini; Lovering, Charles; Tanner, Chris","BizBench: A Quantitative Reasoning Benchmark for Business and Finance","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.452","https://aclanthology.org/2024.acl-long.452","Answering questions within business and finance requires reasoning, precision, and a widebreadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models’ ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on questionanswering (QA) over financial data via program synthesis. We include three financiallythemed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model’s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an indepth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs’ limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.","2024","2025-03-30 16:13:15","2025-03-30 16:13:15","2025-03-30 16:13:15","8309-8332","","","","","","BizBench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EPTCGHJ5/Krumdick et al. - 2024 - BizBench A Quantitative Reasoning Benchmark for Business and Finance.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"ARZQHE8N","journalArticle","","Xu, Hainiu; Zhao, Runcong; Zhu, Lixing; Du, Jinhua; He, Yulan","t OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models","","","","","","","","2025-03-30 16:13:17","2025-03-30 16:13:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QQB6JTP4/Xu et al. - t OpenToM A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large L.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYJ7EMZ3","conferencePaper","2024","Li, Yinghui; Xu, Zishan; Chen, Shaoshen; Huang, Haojing; Li, Yangning; Ma, Shirong; Jiang, Yong; Li, Zhongli; Zhou, Qingyu; Zheng, Hai-Tao; Shen, Ying","Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.469","https://aclanthology.org/2024.acl-long.469","Writing assistance aims to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. In the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters that can be represented by computer text encoding systems, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present Visual-C3, a human-annotated Visual Chinese Character Checking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C3 is the first real-world visual and the largest humancrafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C3. Extensive empirical results and analyses show that Visual-C3 is high-quality yet challenging. As the first study focusing on Chinese faked characters, the Visual-C3 dataset and the baseline methods are publicly available at https: //github.com/THUKElab/Visual-C3.","2024","2025-03-30 16:13:19","2025-03-30 16:13:19","2025-03-30 16:13:19","8656-8668","","","","","","Towards Real-World Writing Assistance","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/A3QGY98X/Li et al. - 2024 - Towards Real-World Writing Assistance A Chinese Character Checking Benchmark with Faked and Misspel.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"H33GIIXI","conferencePaper","2024","Deng, Shihan; Xu, Weikai; Sun, Hongda; Liu, Wei; Tan, Tao; Liujianfeng, Liujianfeng; Li, Ang; Luan, Jian; Wang, Bin; Yan, Rui; Shang, Shuo","Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.478","https://aclanthology.org/2024.acl-long.478","With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform are available at https://github.com/XiaoMi/MobileBench.","2024","2025-03-30 16:13:22","2025-03-30 16:13:23","2025-03-30 16:13:22","8813-8831","","","","","","Mobile-Bench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XGAHWXWX/Deng et al. - 2024 - Mobile-Bench An Evaluation Benchmark for LLM-based Mobile Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"KFWUTWLU","conferencePaper","2024","Thrush, Tristan; Moore, Jared; Monares, Miguel; Potts, Christopher; Kiela, Douwe","I am a Strange Dataset: Metalinguistic Tests for Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.482","https://aclanthology.org/2024.acl-long.482","Statements involving metalinguistic selfreference (“This paper has six sections.”) are prevalent in many domains. Can current large language models (LLMs) handle such language? In this paper, we present “I am a Strange Dataset”, a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like “The penultimate word in this sentence is” (where a correct continuation is “is”). In verification, models judge the truth of statements like “The penultimate word in this sentence is sentence.” (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89–93% range. The dataset and evaluation toolkit are available at https://github.com/ TristanThrush/i-am-a-strange-dataset.","2024","2025-03-30 16:13:26","2025-03-30 16:13:26","2025-03-30 16:13:26","8888-8907","","","","","","I am a Strange Dataset","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7YDTTCC9/Thrush et al. - 2024 - I am a Strange Dataset Metalinguistic Tests for Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"ZWZAV2AL","conferencePaper","2024","Chen, Zhe; Liu, Heyang; Yu, Wenyi; Sun, Guangzhi; Liu, Hongcheng; Wu, Ji; Zhang, Chao; Wang, Yu; Wang, Yanfeng","M3AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.489","https://aclanthology.org/2024.acl-long.489","Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M3AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the slide text and spoken words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual speech recognition, speech synthesis, and slide and script generation tasks demonstrate that the diversity of M3AV makes it a challenging dataset1.","2024","2025-03-30 16:13:29","2025-03-30 16:13:30","2025-03-30 16:13:29","9041-9060","","","","","","M3AV","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IC4B8RYR/Chen et al. - 2024 - M3AV A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"PC3B3R2Y","conferencePaper","2024","Wang, Yuhao; Liao, Yusheng; Liu, Heyang; Liu, Hongcheng; Wang, Yanfeng; Wang, Yu","MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.498","https://aclanthology.org/2024.acl-long.498","Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models’ struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the selfawareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights. The experiment results reveal that current MLLMs possess limited selfawareness capabilities, pointing to a crucial area for future advancement in the development of trustworthy MLLMs. Code and data are available at https://github.com/YHWmz/ MM-SAP.","2024","2025-03-30 16:13:34","2025-03-30 16:13:34","2025-03-30 16:13:34","9192-9205","","","","","","MM-SAP","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CV64JZD5/Wang et al. - 2024 - MM-SAP A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models i.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"SK2JLA2P","conferencePaper","2024","Yehuda, Yakir; Malkiel, Itzik; Barkan, Oren; Weill, Jonathan; Ronen, Royi; Koenigstein, Noam","InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.506","https://aclanthology.org/2024.acl-long.506","Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 87% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy of 81%, all without relying on external knowledge 1.","2024","2025-03-30 16:13:37","2025-03-30 16:13:37","2025-03-30 16:13:37","9333-9347","","","","","","InterrogateLLM","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z95R59LL/Yehuda et al. - 2024 - InterrogateLLM Zero-Resource Hallucination Detection in LLM-Generated Answers.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"MXBISDFI","conferencePaper","2024","Mahari, Robert; Stammbach, Dominik; Ash, Elliott; Pentland, Alex","LePaRD: A Large-Scale Dataset of Judicial Citations to Precedent","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.532","https://aclanthology.org/2024.acl-long.532","We present the Legal Passage Retrieval Dataset, LePaRD. LePaRD contains millions of examples of U.S. federal judges citing precedent in context. The dataset aims to facilitate work on legal passage retrieval, a challenging practice-oriented legal retrieval and reasoning task. Legal passage retrieval seeks to predict relevant passages from precedential court decisions given the context of a legal argument. We extensively evaluate various approaches on LePaRD, and find that classification-based retrieval appears to work best. Our best models only achieve a recall of 59% when trained on data corresponding to the 10,000 most-cited passages, underscoring the difficulty of legal passage retrieval. By publishing LePaRD, we provide a large-scale and high quality resource to foster further research on legal passage retrieval. We hope that research on this practiceoriented NLP task will help expand access to justice by reducing the burden associated with legal research via computational assistance. Warning: Extracts from judicial opinions may contain offensive language.","2024","2025-03-30 16:13:39","2025-03-30 16:13:39","2025-03-30 16:13:39","9863-9877","","","","","","LePaRD","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VLSK2GRM/Mahari et al. - 2024 - LePaRD A Large-Scale Dataset of Judicial Citations to Precedent.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"LCJF9XZT","conferencePaper","2024","Pan, Shilong; Tian, Zhiliang; Ding, Liang; Zheng, Haoqi; Huang, Zhen; Wen, Zhihua; Li, Dongsheng","POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.537","https://aclanthology.org/2024.acl-long.537","Low-resource languages (LRLs) face challenges in supervised neural machine translation (NMT) due to limited parallel data, prompting research in unsupervised NMT (UNMT). UNMT, without requiring ground truth, provides solutions for LRL translations using synthetic pseudo-parallel data and parallel data from auxiliary language pairs. However, they usually encounter translation errors, including errors from synthetic data and from auxiliary language pairs with linguistic biases. We argue that large language models (LLMs) mitigate UNMT’s translation errors by dynamically organizing auxiliary languages in prompts to improve LRL translations. In this paper, we propose PrObability-driven Meta-graph Prompter (POMP), an approach employing a dynamic graph to organize multiple auxiliary languages, to prompt LLMs in LRL translations. POMP proposes a language-specific meta-graph that dynamically samples multiple translation paths to organize auxiliary languages in constructing prompts. Following the path, POMP prompts LLMs to translate with a mixture of auxiliary languages. We achieve the meta-graph’s evolution by back-propagating evaluation scores to update probabilities on the graph. Our experimental improvements show POMP’s effectiveness on LRLs’ translation.","2024","2025-03-30 16:13:43","2025-03-30 16:13:43","2025-03-30 16:13:43","9976-9992","","","","","","POMP","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RVXGBXVX/Pan et al. - 2024 - POMP Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Tr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"PPEVJU8S","conferencePaper","2024","Fernandez, Nigel; Scarlatos, Alexander; Lan, Andrew","SyllabusQA: A Course Logistics Question Answering Dataset","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.557","https://aclanthology.org/2024.acl-long.557","Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SYLLABUSQA 1, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5, 078 open-ended course logisticsrelated question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.","2024","2025-03-30 16:13:46","2025-03-30 16:13:46","2025-03-30 16:13:46","10344-10369","","","","","","SyllabusQA","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/F8MWGHE7/Fernandez et al. - 2024 - SyllabusQA A Course Logistics Question Answering Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"WWJ9IXRI","conferencePaper","2024","Braun, Daniel; Matthes, Florian","AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.559","https://aclanthology.org/2024.acl-long.559","Legal tasks and datasets are often used as benchmarks for the capabilities of language models. However, openly available annotated datasets are rare. In this paper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, we present a first baseline for the task of detecting potentially void clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. Our results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. While the fine-tuned models often performed better with regard to precision, GPT-3.5 outperformed the other approaches with regard to recall. An analysis of the errors indicates that one of the main challenges could be the correct interpretation of complex clauses, rather than the decision boundaries of what is permissible and what is not.","2024","2025-03-30 16:13:48","2025-03-30 16:13:48","2025-03-30 16:13:48","10389-10405","","","","","","AGB-DE","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EBTQNI6V/Braun and Matthes - 2024 - AGB-DE A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"LL9ZGLJY","conferencePaper","2024","Siska, Charlotte; Marazopoulou, Katerina; Ailem, Melissa; Bono, James","Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.560","https://aclanthology.org/2024.acl-long.560","Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model’s average performance across the test prompts of a benchmark to evaluate the model’s performance. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from a real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. We find that (1) the correlation in model performance across test prompts is non-random, (2) accounting for correlations across test prompts can change model rankings on major benchmarks, (3) explanatory factors for these correlations include semantic similarity and common LLM failure points.","2024","2025-03-30 16:13:51","2025-03-30 16:13:51","2025-03-30 16:13:51","10406-10421","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YQB9MG6T/Siska et al. - 2024 - Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"K7HFR2QS","conferencePaper","2024","Luo, Fuwen; Chen, Chi; Wan, Zihao; Kang, Zhaolu; Yan, Qidong; Li, Yingjie; Wang, Xiaolong; Wang, Siyu; Wang, Ziyue; Mi, Xiaoyue; Li, Peng; Ma, Ning; Sun, Maosong; Liu, Yang","CODIS: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.573","https://aclanthology.org/2024.acl-long.573","Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a contextdependent manner. View our project website at https://thunlp-mt.github.io/CODIS.","2024","2025-03-30 16:13:54","2025-03-30 16:13:54","2025-03-30 16:13:54","10639-10659","","","","","","CODIS","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5MGU64EQ/Luo et al. - 2024 - CODIS Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"H2HT2XMT","conferencePaper","2024","Zhang, Tong; Qin, Peixin; Deng, Yang; Huang, Chen; Lei, Wenqiang; Liu, Junhong; Jin, Dingnan; Liang, Hongru; Chua, Tat-Seng","CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.578","https://aclanthology.org/2024.acl-long.578","Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ∼ 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-theshelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at https://github.com/SCUNLP/CLAMBER.","2024","2025-03-30 16:13:57","2025-03-30 16:13:57","2025-03-30 16:13:57","10746-10766","","","","","","CLAMBER","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TPDYTHCG/Zhang et al. - 2024 - CLAMBER A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Mod.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"ZJBNUK7H","conferencePaper","2024","Du, Weihong; Liao, Wenrui; Liang, Hongru; Lei, Wenqiang","PAGED: A Benchmark for Procedural Graphs Extraction from Documents","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.583","https://aclanthology.org/2024.acl-long.583","Automatic extraction of procedural graphs from documents creates a low-cost way for users to easily understand a complex procedure by skimming visual graphs. Despite the progress in recent studies, it remains unanswered: whether the existing studies have well solved this task (Q1) and whether the emerging large language models (LLMs) can bring new opportunities to this task (Q2). To this end, we propose a new benchmark PAGED, equipped with a large high-quality dataset and standard evaluations. It investigates five state-of-the-art baselines, revealing that they fail to extract optimal procedural graphs well because of their heavy reliance on hand-written rules and limited available data. We further involve three advanced LLMs in PAGED and enhance them with a novel self-refine strategy. The results point out the advantages of LLMs in identifying textual elements and their gaps in building logical structures. We hope PAGED can serve as a major landmark for automatic procedural graph extraction and the investigations in PAGED can provide valuable insights into the research on logical reasoning among non-sequential elements. The code and dataset are available in https://github.com/SCUNLP/PAGED.","2024","2025-03-30 16:14:00","2025-03-30 16:14:00","2025-03-30 16:14:00","10829-10846","","","","","","PAGED","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XNG6VR3G/Du et al. - 2024 - PAGED A Benchmark for Procedural Graphs Extraction from Documents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"PKLTEDMU","conferencePaper","2024","Niu, Cheng; Wu, Yuanhao; Zhu, Juno; Xu, Siliang; Shum, KaShun; Zhong, Randy; Song, Juntong; Zhang, Tong","RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.585","https://aclanthology.org/2024.acl-long.585","","2024","2025-03-30 16:14:03","2025-03-30 16:14:03","2025-03-30 16:14:03","10862-10878","","","","","","RAGTruth","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/V7FV4H5K/Niu et al. - 2024 - RAGTruth A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"8NLRAUT2","conferencePaper","2024","Singh, Harman; Gupta, Nitish; Bharadwaj, Shikhar; Tewari, Dinesh; Talukdar, Partha","IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.595","https://aclanthology.org/2024.acl-long.595","","2024","2025-03-30 16:14:05","2025-03-30 16:14:05","2025-03-30 16:14:05","11047-11073","","","","","","IndicGenBench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7S36JLSH/Singh et al. - 2024 - IndicGenBench A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languag.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"MVWCGPMK","conferencePaper","2024","Zhang, Yuanchi; Wang, Yile; Liu, Zijun; Wang, Shuo; Wang, Xiaolong; Li, Peng; Sun, Maosong; Liu, Yang","Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.603","https://aclanthology.org/2024.acl-long.603","","2024","2025-03-30 16:14:08","2025-03-30 16:14:08","2025-03-30 16:14:08","11189-11204","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/F9V77KII/Zhang et al. - 2024 - Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"VQNR4AGF","conferencePaper","2024","Sun, Jiaxing; Huang, Weiquan; Wu, Jiang; Gu, Chenya; Li, Wei; Zhang, Songyang; Yan, Hang; He, Conghui","Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.604","https://aclanthology.org/2024.acl-long.604","We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs’ reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM’s language orientation and the task’s domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs’ memorizationindependent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs’ strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM.","2024","2025-03-30 16:14:11","2025-03-30 16:14:11","2025-03-30 16:14:11","11205-11228","","","","","","Benchmarking Chinese Commonsense Reasoning of LLMs","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5HC6H9XS/Sun et al. - 2024 - Benchmarking Chinese Commonsense Reasoning of LLMs From Chinese-Specifics to Reasoning-Memorization.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"JGUTYWCC","conferencePaper","2024","Joshi, Abhinav; Paul, Shounak; Sharma, Akshat; Goyal, Pawan; Ghosh, Saptarshi; Modi, Ashutosh","IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.618","https://aclanthology.org/2024.acl-long.618","Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning. ILTUR contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domainspecific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/ IL-TUR/) where the research community can upload and compare legal text understanding systems.","2024","2025-03-30 16:14:13","2025-03-30 16:14:13","2025-03-30 16:14:13","11460-11499","","","","","","IL-TUR","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/W76TH2QH/Joshi et al. - 2024 - IL-TUR Benchmark for Indian Legal Text Understanding and Reasoning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"TZIIT9H6","conferencePaper","2024","Singh, Shivalika; Vargus, Freddie; D’souza, Daniel; Karlsson, Börje; Mahendiran, Abinaya; Ko, Wei-Yin; Shandilya, Herumb; Patel, Jay; Mataciunas, Deividas; O’Mahony, Laura; Zhang, Mike; Hettiarachchi, Ramith; Wilson, Joseph; Machado, Marina; Moura, Luisa; Krzemiński, Dominik; Fadaei, Hakimeh; Ergun, Irem; Okoh, Ifeoma; Alaagib, Aisha; Mudannayake, Oshan; Alyafeai, Zaid; Chien, Vu; Ruder, Sebastian; Guthikonda, Surya; Alghamdi, Emad; Gehrmann, Sebastian; Muennighoff, Niklas; Bartolo, Max; Kreutzer, Julia; Üstün, Ahmet; Fadaee, Marzieh; Hooker, Sara","Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.620","https://aclanthology.org/2024.acl-long.620","Datasets are foundational to many breakthroughs in modern artificial intelligence (AI). Many recent achievements in the space of natural language processing (NLP) can be attributed to the fine-tuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and augmenting existing datasets across 114 languages. In total, we contribute three key resources: we develop and opensource the Aya 1 Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as an important framework for future research collaborations that aim to bridge gaps in resources.","2024","2025-03-30 16:14:16","2025-03-30 16:14:16","2025-03-30 16:14:16","11521-11567","","","","","","Aya Dataset","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NXKQVWDX/Singh et al. - 2024 - Aya Dataset An Open-Access Collection for Multilingual Instruction Tuning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"IPWEI7IL","conferencePaper","2024","Liu, Xiao; Lei, Xuanyu; Wang, Shengyuan; Huang, Yue; Feng, Andrew; Wen, Bosi; Cheng, Jiale; Ke, Pei; Xu, Yifan; Tam, Weng Lam; Zhang, Xiaohan; Sun, Lichao; Gu, Xiaotao; Wang, Hongning; Zhang, Jing; Huang, Minlie; Dong, Yuxiao; Tang, Jie","AlignBench: Benchmarking Chinese Alignment of Large Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.624","https://aclanthology.org/2024.acl-long.624","Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, the effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce ALIGNBENCH, a comprehensive multidimensional benchmark for evaluating LLMs’ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references. To ensure the correctness of references, each knowledge-intensive query is accompanied with evidences collected from reliable web sources (including URLs and quotations) by our annotators. For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge (Zheng et al., 2023) approach with Chain-of-Thought to generate explanations and final ratings, ensuring high reliability and interpretability. All evaluation codes, data, and LLM generations are available at https://github.com/THUDM/ AlignBench.","2024","2025-03-30 16:14:19","2025-03-30 16:14:19","2025-03-30 16:14:19","11621-11640","","","","","","AlignBench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZYPH3Y87/Liu et al. - 2024 - AlignBench Benchmarking Chinese Alignment of Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"CSPK3HTE","conferencePaper","2024","Tu, Quan; Fan, Shilong; Tian, Zihang; Shen, Tianhao; Shang, Shuo; Gao, Xin; Yan, Rui","CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.638","https://aclanthology.org/2024.acl-long.638","Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in CharacterEval, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source, and reward model will be publicly accessible at https://github.com/ morecry/CharacterEval.","2024","2025-03-30 16:14:21","2025-03-30 16:14:21","2025-03-30 16:14:21","11836-11850","","","","","","CharacterEval","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9DRDJNRV/Tu et al. - 2024 - CharacterEval A Chinese Benchmark for Role-Playing Conversational Agent Evaluation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"23EPPBWC","conferencePaper","2024","Wang, Haoyu; Wang, Shuo; Yan, Yukun; Wang, Xujia; Yang, Zhiyu; Xu, Yuzhuang; Liu, Zhenghao; Yang, Liner; Ding, Ning; Han, Xu; Liu, Zhiyuan; Sun, Maosong","UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.644","https://aclanthology.org/2024.acl-long.644","","2024","2025-03-30 16:14:23","2025-03-30 16:14:23","2025-03-30 16:14:23","11929-11942","","","","","","UltraLink","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/K63WXAH7/Wang et al. - 2024 - UltraLink An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"7YY6GYUX","conferencePaper","2024","Kasner, Zdeněk; Dusek, Ondrej","Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.651","https://aclanthology.org/2024.acl-long.651","","2024","2025-03-30 16:14:26","2025-03-30 16:14:26","2025-03-30 16:14:26","12045-12072","","","","","","Beyond Traditional Benchmarks","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5LGGAZVN/Kasner and Dusek - 2024 - Beyond Traditional Benchmarks Analyzing Behaviors of Open LLMs on Data-to-Text Generation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"Q92UGFAK","conferencePaper","2024","Cao, Qingxing; Cheng, Junhao; Liang, Xiaodan; Lin, Liang","VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.658","https://aclanthology.org/2024.acl-long.658","","2024","2025-03-30 16:14:28","2025-03-30 16:14:28","2025-03-30 16:14:28","12161-12176","","","","","","VisDiaHalBench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GI7ZQ6C6/Cao et al. - 2024 - VisDiaHalBench A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Mo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"NCZIVY99","conferencePaper","2024","Dugan, Liam; Hwang, Alyssa; Trhlík, Filip; Zhu, Andrew; Ludan, Josh Magnus; Xu, Hainiu; Ippolito, Daphne; Callison-Burch, Chris","RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.674","https://aclanthology.org/2024.acl-long.674","Many commercial and open-source models claim to detect machine-generated text with extremely high accuracy (99% or more). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging—lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machinegenerated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-ofdomain and adversarial robustness of 8 openand 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our data1 along with a leaderboard2 to encourage future research.","2024","2025-03-30 16:14:31","2025-03-30 16:14:31","2025-03-30 16:14:31","12463-12492","","","","","","RAID","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4LC6G35D/Dugan et al. - 2024 - RAID A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"NGE2NEW2","conferencePaper","2024","Salaün, Olivier; Piedboeuf, Frédéric; Le Berre, Guillaume; Alfonso-Hermelo, David; Langlais, Philippe","EUROPA: A Legal Multilingual Keyphrase Generation Dataset","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.687","https://aclanthology.org/2024.acl-long.687","Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.","2024","2025-03-30 16:14:34","2025-03-30 16:14:34","2025-03-30 16:14:34","12718-12736","","","","","","EUROPA","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TUB4CYXU/Salaün et al. - 2024 - EUROPA A Legal Multilingual Keyphrase Generation Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"UBJN3DFU","conferencePaper","2024","Alwajih, Fakhraddin; Nagoudi, El Moatez Billah; Bhatia, Gagan; Mohamed, Abdelrahman; Abdul-Mageed, Muhammad","Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.689","https://aclanthology.org/2024.acl-long.689","Multimodal large language models (MLLMs) have proven effective in a wide range of tasks that require complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, even those with large speaker populations, such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed Peacock, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce Henna, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturallyaware Arabic MLLMs. The GitHub repository for the Peacock project is available at https: //github.com/UBC-NLP/peacock.","2024","2025-03-30 16:14:36","2025-03-30 16:14:37","2025-03-30 16:14:36","12753-12776","","","","","","Peacock","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/E2UFADA6/Alwajih et al. - 2024 - Peacock A Family of Arabic Multimodal Large Language Models and Benchmarks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"FKF6LDTX","journalArticle","","Zhao, Yilun; Liu, Hongjun; Long, Yitao; Zhang, Rui; Zhao, Chen; Cohan, Arman","KnowledgeFMATH: Knowledge-Intensive Math Reasoning in Finance Domains","","","","","","We introduce KnowledgeFMATH, a novel benchmark designed to evaluate LLMs’ capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, KnowledgeFMATH includes 1,259 problems with a hybrid of textual and tabular content. These problems require collegelevel knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a highquality benchmark for LLM assessment. We also construct a finance-domain knowledge bank and investigate various knowledge integration strategies. Finally, we evaluate a wide spectrum of 26 LLMs with different prompting strategies like Chain-of-Thought and Programof-Thought. Our experimental results reveal that the current best-performing system (i.e., GPT-4 with CoT prompting) achieves only 56.6% accuracy, leaving substantial room for improvement. Moreover, while augmenting LLMs with external knowledge can improve their performance (e.g., from 33.5% to 47.1% for GPT-3.5) , their accuracy remains significantly lower than the estimated human expert performance of 92%. We believe that KnowledgeFMATH can advance future research in the area of domain-specific knowledge retrieval and integration, particularly within the context of solving math reasoning problems.","","2025-03-30 16:14:39","2025-03-30 16:14:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SQAQLRB5/Zhao et al. - KnowledgeFMATH Knowledge-Intensive Math Reasoning in Finance Domains.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IFPRYTR4","conferencePaper","2024","Basu, Kinjal; Abdelaziz, Ibrahim; Chaudhury, Subhajit; Dan, Soham; Crouse, Maxwell; Munawar, Asim; Austel, Vernon; Kumaravel, Sadhana; Muthusamy, Vinod; Kapanipathi, Pavan; Lastras, Luis","API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.694","https://aclanthology.org/2024.acl-long.694","There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes1.","2024","2025-03-30 16:14:41","2025-03-30 16:14:41","2025-03-30 16:14:41","12859-12870","","","","","","API-BLEND","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VBGIQ7B8/Basu et al. - 2024 - API-BLEND A Comprehensive Corpora for Training and Benchmarking API LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"ARFHTF2F","conferencePaper","2024","Wang, Chenhao; Cao, Pengfei; Jin, Zhuoran; Chen, Yubo; Zeng, Daojian; Liu, Kang; Zhao, Jun","MULFE: A Multi-Level Benchmark for Free Text Model Editing","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.732","https://aclanthology.org/2024.acl-long.732","Adjusting the outdated behaviors of large langugae models (LLMs) after deployment remains a significant challenge. It motivates the model editing research, which is however mainly explored in a restricted task form with triplebased edit requests. Recent works have initiated a transition to a more practical and unified editing task that takes free-form text as edit requests. However, there are gaps in nuanced benchmark designs and re-evaluation of existing methods. To bridge the gaps, we introduce a multi-level benchmark for free text model editing (MULFE). The benchmark categorizes probe queries into three levels of generalization, ranging from basic literal memory to deeper understanding and reasoning. Based on the benchmark, we conduct extensive experiments across various base models, edit sizes, and editing methods, including adaptations of mainstream locate-and-edit and hypernetwork methods. The results highlight the inconsistent behaviors of edited models on different generalization levels. Higherlevel generalization remains a significant challenge. Based on the findings, we propose SIDE, a simple yet effective method based on in-context distillation to enhance the generalization performance. The benchmark dataset and evaluation scripts are publicly available at http://github.com/wchrepo/mulfe.","2024","2025-03-30 16:14:43","2025-03-30 16:14:43","2025-03-30 16:14:43","13570-13587","","","","","","MULFE","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3D9MGTGA/Wang et al. - 2024 - MULFE A Multi-Level Benchmark for Free Text Model Editing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"MRNJXSEG","conferencePaper","2024","Patil, Vaidehi; Ribeiro, Leonardo; Liu, Mengwen; Bansal, Mohit; Dreyer, Markus","REFINESUMM: Self-Refining MLLM for Generating a Multimodal Summarization Dataset","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.743","https://aclanthology.org/2024.acl-long.743","","2024","2025-03-30 16:14:47","2025-03-30 16:14:47","2025-03-30 16:14:47","13773-13786","","","","","","REFINESUMM","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QKNFUTLQ/Patil et al. - 2024 - REFINESUMM Self-Refining MLLM for Generating a Multimodal Summarization Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"SY4GNXL9","conferencePaper","2024","Alzahrani, Norah; Alyahya, Hisham; Alnumay, Yazeed; AlRashed, Sultan; Alsubaie, Shaykhah; Almushayqih, Yousef; Mirza, Faisal; Alotaibi, Nouf; Al-Twairesh, Nora; Alowisheq, Areeb; Bari, M Saiful; Khan, Haidar","When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.744","https://aclanthology.org/2024.acl-long.744","Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value — we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at https://github.com/National-Centerfor-AI-Saudi-Arabia/lm-evaluation-harness.","2024","2025-03-30 16:14:50","2025-03-30 16:14:50","2025-03-30 16:14:50","13787-13805","","","","","","When Benchmarks are Targets","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GXXJ8BFQ/Alzahrani et al. - 2024 - When Benchmarks are Targets Revealing the Sensitivity of Large Language Model Leaderboards.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"PEWSYZXF","conferencePaper","2024","Zhang, Enshi; Trujillo, Rafael; Poellabauer, Christian","The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.752","https://aclanthology.org/2024.acl-long.752","Research in the field of speech emotion recognition (SER) relies on the availability of comprehensive datasets to make it possible to design accurate emotion detection models. This study introduces the Multimodal Emotion Recognition and Sentiment Analysis (MERSA) dataset, which includes both natural and scripted speech recordings, transcribed text, physiological data, and self-reported emotional surveys from 150 participants collected over a two-week period. This work also presents a novel emotion recognition approach that uses a transformer-based model, integrating pre-trained wav2vec 2.0 and BERT for feature extractions and additional LSTM layers to learn hidden representations from fused representations from speech and text. Our model predicts emotions on dimensions of arousal, valence, and dominance. We trained and evaluated the model on the MSP-PODCAST dataset and achieved competitive results from the best-performing model regarding the concordance correlation coefficient (CCC). Further, this paper demonstrates the effectiveness of this model through crossdomain evaluations on both IEMOCAP and MERSA datasets.","2024","2025-03-30 16:14:52","2025-03-30 16:14:52","2025-03-30 16:14:52","13960-13970","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XRTQAHUB/Zhang et al. - 2024 - The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"9ZZUWKK2","conferencePaper","2024","Schroeder, Hope; Roy, Deb; Kabbara, Jad","Fora: A corpus and framework for the study of facilitated dialogue","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.754","https://aclanthology.org/2024.acl-long.754","Facilitated dialogue is increasingly popular as a method of civic engagement and as a method for gathering social insight, but resources for its study are scant. We present Fora, a unique collection of annotated facilitated dialogues. We compile 262 facilitated conversations that were hosted with partner organizations seeking to engage their members and surface insights regarding issues like education, elections, and public health, primarily through the sharing of personal experience. Alongside this corpus of 39,911 speaker turns, we present a framework for the analysis of facilitated dialogue. We taxonomize key personal sharing behaviors and facilitation strategies in the corpus, annotate a 25% sample (10,000+ speaker turns) of the data accordingly, and evaluate and establish baselines on a number of tasks essential to the identification of these phenomena in dialogue. We describe the data, and relate facilitator behavior to turn-taking and participant sharing. We outline how this research can inform future work in understanding and improving facilitated dialogue, parsing spoken conversation, and improving the behavior of dialogue agents.","2024","2025-03-30 16:15:00","2025-03-30 16:15:00","2025-03-30 16:15:00","13985-14001","","","","","","Fora","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CHGLXR25/Schroeder et al. - 2024 - Fora A corpus and framework for the study of facilitated dialogue.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"YG2BM66A","journalArticle","","Felkner, Virginia K; Thompson, Jennifer A","GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction","","","","","","Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.","","2025-03-30 16:15:02","2025-03-30 16:15:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/X23NJ3BZ/Felkner and Thompson - GPT is Not an Annotator The Necessity of Human Annotation in Fairness Benchmark Construction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZ4CN3E4","conferencePaper","2024","Li, Lei; Wang, Yuqi; Xu, Runxin; Wang, Peiyi; Feng, Xiachong; Kong, Lingpeng; Liu, Qi","Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.775","https://aclanthology.org/2024.acl-long.775","","2024","2025-03-30 16:15:11","2025-03-30 16:15:11","2025-03-30 16:15:11","14369-14387","","","","","","Multimodal ArXiv","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZQ86N3HQ/Li et al. - 2024 - Multimodal ArXiv A Dataset for Improving Scientific Comprehension of Large Vision-Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"QQBQMSR5","journalArticle","","Faisal, Fahim; Ahia, Orevaoghene; Srivastava, Aarohi; Ahuja, Kabir; Chiang, David; Tsvetkov, Yulia; Anastasopoulos, Antonios","A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages","","","","","","","","2025-03-30 16:15:14","2025-03-30 16:15:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GS3DZZ54/Faisal et al. - A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTP4GF7B","conferencePaper","2024","Arora, Aryaman; Jurafsky, Dan; Potts, Christopher","CausalGym: Benchmarking causal interpretability methods on linguistic tasks","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.785","https://aclanthology.org/2024.acl-long.785","Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M–6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler–gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.","2024","2025-03-30 16:15:17","2025-03-30 16:15:17","2025-03-30 16:15:17","14638-14663","","","","","","CausalGym","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/T6D5C9KE/Arora et al. - 2024 - CausalGym Benchmarking causal interpretability methods on linguistic tasks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"EUC9H37X","conferencePaper","2024","Niklaus, Joel; Matoshi, Veton; Stürmer, Matthias; Chalkidis, Ilias; Ho, Daniel","MultiLegalPile: A 689GB Multilingual Legal Corpus","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.805","https://aclanthology.org/2024.acl-long.805","Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, few datasets are available for specialized critical domains such as law and the available ones are often small and only in English. To fill this gap, we curate and release MULTILEGALPILE, a 689GB corpus in 24 languages from 17 jurisdictions. MULTILEGALPILE includes diverse legal data sources and allows for pretraining NLP models under fair use, with most of the dataset licensed very permissively. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the languagespecific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, trained models, and all code under the most open licenses possible.","2024","2025-03-30 16:15:20","2025-03-30 16:15:20","2025-03-30 16:15:20","15077-15094","","","","","","MultiLegalPile","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6Z6WPRLH/Niklaus et al. - 2024 - MultiLegalPile A 689GB Multilingual Legal Corpus.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"ZUMXINXM","conferencePaper","2024","Kwan, Wai-Chung; Zeng, Xingshan; Wang, Yufei; Sun, Yusen; Li, Liangyou; Jiang, Yuxin; Shang, Lifeng; Liu, Qun; Wong, Kam-Fai","M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.832","https://aclanthology.org/2024.acl-long.832","Managing long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMs’ long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length.1 Our evaluation of 11 prominent LLMs reveals that 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area 2.","2024","2025-03-30 16:15:23","2025-03-30 16:15:23","2025-03-30 16:15:23","15568-15592","","","","","","M4LE","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RX6AM58R/Kwan et al. - 2024 - M4LE A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Larg.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"UAJEMS94","conferencePaper","2024","Zhao, Chenye; Caragea, Cornelia","EZ-STANCE: A Large Dataset for English Zero-Shot Stance Detection","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.838","https://aclanthology.org/2024.acl-long.838","Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in favor, against, or neutral toward a target that is unseen during training. In this paper, we present EZ-STANCE, a large English ZSSD dataset with 47,316 annotated text-target pairs. In contrast to VAST (Allaway and McKeown, 2020), which is the only other large existing ZSSD dataset for English, EZ-STANCE is 2.5 times larger, includes both noun-phrase targets and claim targets that cover a wide range of domains, provides two challenging subtasks for ZSSD: target-based ZSSD and domain-based ZSSD, and contains much harder examples for the neutral class. We evaluate EZ-STANCE using state-of-the-art deep learning models. Furthermore, we propose to transform ZSSD into the NLI task by applying simple yet effective prompts to noun-phrase targets. Our experimental results show that EZ-STANCE is a challenging new benchmark, which provides significant research opportunities on English ZSSD. We publicly release our dataset and code at https://github.com/chenyez/EZ-STANCE.","2024","2025-03-30 16:15:27","2025-03-30 16:15:27","2025-03-30 16:15:27","15697-15714","","","","","","EZ-STANCE","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FUAXE7TT/Zhao and Caragea - 2024 - EZ-STANCE A Large Dataset for English Zero-Shot Stance Detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"53XJVXPY","conferencePaper","2024","Soldaini, Luca; Kinney, Rodney; Bhagia, Akshita; Schwenk, Dustin; Atkinson, David; Authur, Russell; Bogin, Ben; Chandu, Khyathi; Dumas, Jennifer; Elazar, Yanai; Hofmann, Valentin; Jha, Ananya; Kumar, Sachin; Lucy, Li; Lyu, Xinxi; Lambert, Nathan; Magnusson, Ian; Morrison, Jacob; Muennighoff, Niklas; Naik, Aakanksha; Nam, Crystal; Peters, Matthew; Ravichander, Abhilasha; Richardson, Kyle; Shen, Zejiang; Strubell, Emma; Subramani, Nishant; Tafjord, Oyvind; Walsh, Evan; Zettlemoyer, Luke; Smith, Noah; Hajishirzi, Hannaneh; Beltagy, Iz; Groeneveld, Dirk; Dodge, Jesse; Lo, Kyle","Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.840","https://aclanthology.org/2024.acl-long.840","","2024","2025-03-30 16:15:30","2025-03-30 16:15:30","2025-03-30 16:15:30","15725-15788","","","","","","Dolma","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2XVYIFD2/Soldaini et al. - 2024 - Dolma an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"PNS6Z3RN","conferencePaper","2024","Khan, Mohammed; Mehta, Priyam; Sankar, Ananth; Kumaravelan, Umashankar; Doddapaneni, Sumanth; B, Suriyaprasaad; G, Varun; Jain, Sparsh; Kunchukuttan, Anoop; Kumar, Pratyush; Dabre, Raj; Khapra, Mitesh","IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.843","https://aclanthology.org/2024.acl-long.843","","2024","2025-03-30 16:15:33","2025-03-30 16:15:33","2025-03-30 16:15:33","15831-15879","","","","","","IndicLLMSuite","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6MMXLXYC/Khan et al. - 2024 - IndicLLMSuite A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"DIBX7JFY","journalArticle","","Chen, Zhuang; Wu, Jincenzi; Zhou, Jinfeng; Wen, Bosi; Bi, Guanqun; Jiang, Gongyao; Cao, Yaru; Hu, Mengting; Lai, Yunghwei; Xiong, Zexuan; Huang, Minlie","T MBENCH: Benchmarking Theory of Mind in Large Language Models","","","","","","Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce T MBENCH with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on T MBENCH, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with T MBENCH is to enable an efficient and effective evaluation of LLMs’ ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.","","2025-03-30 16:15:35","2025-03-30 16:15:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Q5WIE65N/Chen et al. - T MBENCH Benchmarking Theory of Mind in Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJ4XBTPH","conferencePaper","2024","Casola, Silvia; Frenda, Simona; Lo, Soda Marem; Sezerer, Erhan; Uva, Antonio; Basile, Valerio; Bosco, Cristina; Pedrani, Alessandro; Rubagotti, Chiara; Patti, Viviana; Bernardi, Davide","MultiPICo: Multilingual Perspectivist Irony Corpus","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.849","https://aclanthology.org/2024.acl-long.849","Recently, several scholars have contributed to the growth of a new theoretical framework in NLP called perspectivism. This approach aims to leverage data annotated by different individuals to model diverse perspectives that affect their opinions on subjective phenomena such as irony. In this context, we propose MultiPICo, a multilingual perspectivist corpus of ironic short conversations in different languages and linguistic varieties extracted from Twitter and Reddit. The corpus includes sociodemographic information about its annotators. Our analysis of the annotated corpus shows how different demographic cohorts may significantly disagree on their annotation of irony and how certain cultural factors influence the perception of the phenomenon and the agreement on the annotation. Moreover, we show how disaggregated annotations and rich annotator metadata can be exploited to benchmark the ability of large language models to recognize irony, their positionality with respect to sociodemographic groups, and the efficacy of perspective-taking prompting for irony detection in multiple languages.","2024","2025-03-30 16:15:37","2025-03-30 16:15:37","2025-03-30 16:15:37","16008-16021","","","","","","MultiPICo","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YGBFF3JU/Casola et al. - 2024 - MultiPICo Multilingual Perspectivist Irony Corpus.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"QL3TNCYI","conferencePaper","2024","Trivedi, Harsh; Khot, Tushar; Hartmann, Mareike; Manku, Ruskin; Dong, Vinty; Li, Edward; Gupta, Shashank; Sabharwal, Ashish; Balasubramanian, Niranjan","AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.850","https://aclanthology.org/2024.acl-long.850","Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine,1 a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our ‘normal’ tasks and ~30% of ‘challenge’ tasks, while other models solve at least 16% fewer. This highlights the benchmark’s difficulty and AppWorld’s potential to push the frontiers of interactive coding agents.","2024","2025-03-30 16:15:40","2025-03-30 16:15:40","2025-03-30 16:15:40","16022-16076","","","","","","AppWorld","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SSSFLNK4/Trivedi et al. - 2024 - AppWorld A Controllable World of Apps and People for Benchmarking Interactive Coding Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"BGEJAH79","conferencePaper","2024","Liu, Yu Lu; Blodgett, Su Lin; Cheung, Jackie; Liao, Q. Vera; Olteanu, Alexandra; Xiao, Ziang","ECBD: Evidence-Centered Benchmark Design for NLP","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2024.acl-long.861","https://aclanthology.org/2024.acl-long.861","Benchmarking is seen as critical to assessing progress in NLP. However, creating a benchmark involves many design decisions (e.g., which datasets to include, which metrics to use) that often rely on tacit, untested assumptions about what the benchmark is intended to measure or is actually measuring. There is currently no principled way of analyzing these decisions and how they impact the validity of the benchmark’s measurements. To address this gap, we draw on evidence-centered design in educational assessments and propose EvidenceCentered Benchmark Design (ECBD), a framework which formalizes the benchmark design process into five modules. ECBD specifies the role each module plays in helping practitioners collect evidence about capabilities of interest. Specifically, each module requires benchmark designers to describe, justify, and support benchmark design choices—e.g., clearly specifying the capabilities the benchmark aims to measure or how evidence about those capabilities is collected from model responses. To demonstrate the use of ECBD, we conduct case studies with three benchmarks: BoolQ, SuperGLUE, and HELM. Our analysis reveals common trends in benchmark design and documentation that could threaten the validity of benchmarks’ measurements.","2024","2025-03-30 16:15:43","2025-03-30 16:15:43","2025-03-30 16:15:43","16349-16365","","","","","","ECBD","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EG6UFYE6/Liu et al. - 2024 - ECBD Evidence-Centered Benchmark Design for NLP.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","","","","","","","","","","","","",""
"GS6L386S","conferencePaper","2024","Zhu, Andrew; Hwang, Alyssa; Dugan, Liam; Callison-Burch, Chris","FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.2","https://aclanthology.org/2024.acl-short.2","","2024","2025-03-30 16:15:47","2025-03-30 16:15:47","2025-03-30 16:15:47","18-37","","","","","","FanOutQA","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SJLXHXVD/Zhu et al. - 2024 - FanOutQA A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"JH9W3ANF","conferencePaper","2024","Liang, Zhenwen; Guo, Kehan; Liu, Gang; Guo, Taicheng; Zhou, Yujun; Yang, Tianyu; Jiao, Jiajun; Pi, Renjie; Zhang, Jipeng; Zhang, Xiangliang","SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.11","https://aclanthology.org/2024.acl-short.11","The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and freeresponse formats, ensuring a comprehensive evaluation of AI models’ abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both opensource and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models.","2024","2025-03-30 16:15:49","2025-03-30 16:15:49","2025-03-30 16:15:49","109-119","","","","","","SceMQA","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UIII2D9M/Liang et al. - 2024 - SceMQA A Scientific College Entrance Level Multimodal Question Answering Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"Y65WKZ3G","journalArticle","","Gui, Honghao; Yuan, Lin; Ye, Hongbin; Zhang, Ningyu; Sun, Mengshu; Liang, Lei; Chen, Huajun","IEPILE: Unearthing Large-Scale Schema-Based Information Extraction Corpus","","","","","","Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPILE, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPILE by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimentally, IEPILE enhance the performance of LLMs for IE, with notable improvements in zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.","","2025-03-30 16:15:50","2025-03-30 16:15:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GR4P2MRN/Gui et al. - IEPILE Unearthing Large-Scale Schema-Based Information Extraction Corpus.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IFJYYRGM","conferencePaper","2024","Ahmed, Shafiuddin Rehan; Wang, Zhiyong; Baker, George; Stowe, Kevin; Martin, James","Generating Harder Cross-document Event Coreference Resolution Datasets using Metaphoric Paraphrasing","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.27","https://aclanthology.org/2024.acl-short.27","","2024","2025-03-30 16:15:52","2025-03-30 16:15:52","2025-03-30 16:15:52","276-286","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GB2RXAHL/Ahmed et al. - 2024 - Generating Harder Cross-document Event Coreference Resolution Datasets using Metaphoric Paraphrasing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"GDNCBGF5","conferencePaper","2024","Ho, Gia-Bao; Tan, Chang; Darban, Zahra; Salehi, Mahsa; Haf, Reza; Buntine, Wray","MTP: A Dataset for Multi-Modal Turning Points in Casual Conversations","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.30","https://aclanthology.org/2024.acl-short.30","Detecting critical moments, such as emotional outbursts or changes in decisions during conversations, is crucial for understanding shifts in human behavior and their consequences. Our work introduces a novel problem setting focusing on these moments as turning points (TPs), accompanied by a meticulously curated, high-consensus, human-annotated multi-modal dataset. We provide precise timestamps, descriptions, and visual-textual evidence highlighting changes in emotions, behaviors, perspectives, and decisions at these turning points. We also propose a framework, TPMaven, utilizing state-of-the-art vision-language models to construct a narrative from the videos and large language models to classify and detect turning points in our multi-modal dataset. Evaluation results show that TPMaven achieves an F1-score of 0.88 in classification and 0.61 in detection, with additional explanations aligning with human expectations.","2024","2025-03-30 16:15:54","2025-03-30 16:15:54","2025-03-30 16:15:54","314-326","","","","","","MTP","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QCV74A3K/Ho et al. - 2024 - MTP A Dataset for Multi-Modal Turning Points in Casual Conversations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"DAJBZTRX","conferencePaper","2024","Du, Mengfei; Wu, Binhao; Li, Zejun; Huang, Xuanjing; Wei, Zhongyu","EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.33","https://aclanthology.org/2024.acl-short.33","The recent rapid development of Large VisionLanguage Models (LVLMs) has indicated their potential for embodied tasks. However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs. The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective. Experiments expose the insufficient capacity of current LVLMs (even GPT4V). We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs’ embodied spatial understanding.","2024","2025-03-30 16:15:56","2025-03-30 16:15:56","2025-03-30 16:15:56","346-355","","","","","","EmbSpatial-Bench","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4DV7YSBJ/Du et al. - 2024 - EmbSpatial-Bench Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language M.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"7FJ6QRW2","conferencePaper","2024","Wretblad, Niklas; Riseby, Fredrik; Biswas, Rahul; Ahmadi, Amin; Holmström, Oskar","Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.34","https://aclanthology.org/2024.acl-short.34","Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of ‘noise,’ such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRDBench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold SQL queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the benchmark’s reliability. Surprisingly, when evaluating models on corrected SQL queries, zero-shot baselines surpassed the performance of state-of-the-art prompting methods. We conclude that informative noise labels and reliable benchmarks are crucial to developing new Textto-SQL methods that can handle varying types of noise. All datasets, annotations, and code are available at this URL.","2024","2025-03-30 16:15:58","2025-03-30 16:15:58","2025-03-30 16:15:58","356-369","","","","","","Understanding the Effects of Noise in Text-to-SQL","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IGQMKXDX/Wretblad et al. - 2024 - Understanding the Effects of Noise in Text-to-SQL An Examination of the BIRD-Bench Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"DFJKIR3Z","conferencePaper","2024","Reddy, Varshini; Koncel-Kedziorski, Rik; Lai, Viet; Krumdick, Michael; Lovering, Charles; Tanner, Chris","DocFinQA: A Long-Context Financial Reasoning Dataset","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.42","https://aclanthology.org/2024.acl-short.42","For large language models (LLMs) to be effective in the financial domain – where each decision can have a significant impact – it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents spanning hundreds of pages, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. Based on our experiments, DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case study on a subset of the longest documents in DocFinQA and find that models particularly struggle with these documents. Addressing these challenges may have a wide-reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis. DocFinQA dataset is publicly accessible1.","2024","2025-03-30 16:16:01","2025-03-30 16:16:01","2025-03-30 16:16:01","445-458","","","","","","DocFinQA","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DAVHNM6J/Reddy et al. - 2024 - DocFinQA A Long-Context Financial Reasoning Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"QIDKHMT9","conferencePaper","2024","Haq, Saiful; Sharma, Ashutosh; Khattab, Omar; Chhaya, Niyati; Bhattacharyya, Pushpak","IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian Languages","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.46","https://aclanthology.org/2024.acl-short.46","In this paper, we introduce Neural Information Retrieval resources for 11 widely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian language families (Indo-Aryan and Dravidian). These resources include (a) INDICMARCO, a multilingual version of the MS MARCO dataset in 11 Indian Languages created using Machine Translation, and (b) IndicColBERT, a collection of 11 distinct Monolingual Neural Information Retrieval models, each trained on one of the 11 languages in the INDIC-MARCO dataset. To the best of our knowledge, IndicIRSuite is the first attempt at building large-scale Neural Information Retrieval resources for a large number of Indian languages, and we hope that it will help accelerate research in Neural IR for Indian Languages. Experiments demonstrate that Indic-ColBERT achieves 47.47% improvement in the MRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian languages except Oriya, 12.26% improvement in the NDCG@10 score averaged over the MIRACL Bengali and Hindi Language baselines, and 20% improvement in the MRR@100 Score over the Mr. Tydi Bengali Language baseline. IndicIRSuite is available at github.com/saifulhaq95/IndicIRSuite.","2024","2025-03-30 16:16:04","2025-03-30 16:16:04","2025-03-30 16:16:04","501-509","","","","","","IndicIRSuite","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KP4B8GQL/Haq et al. - 2024 - IndicIRSuite Multilingual Dataset and Neural Information Models for Indian Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"DHX4SDUG","conferencePaper","2024","Sun, Bin; Li, Jianfeng; Zhou, Hao; Meng, Fandong; Li, Kan; Zhou, Jie","Exploring Conditional Variational Mechanism to Pinyin Input Method for Addressing One-to-Many Mappings in Low-Resource Scenarios","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.56","https://aclanthology.org/2024.acl-short.56","Pinyin input method engine (IME) refers to the transformation tool from pinyin sequence to Chinese characters, which is widely used on mobile phone applications. Due to the homophones, Pinyin IME suffers from the oneto-many mapping problem in the process of pinyin sequences to Chinese characters. To solve the above issue, this paper makes the first exploration to leverage an effective conditional variational mechanism (CVM) for pinyin IME. However, to ensure the stable and smooth operation of Pinyin IME under low-resource conditions (e.g., on offline mobile devices), we should balance diversity, accuracy, and efficiency with CVM, which is still challenging. To this end, we employ a novel strategy that simplifies the complexity of semantic encoding by facilitating the interaction between pinyin and the Chinese character information during the construction of continuous latent variables. Concurrently, the accuracy of the outcomes is enhanced by capitalizing on the discrete latent variables. Experimental results demonstrate the superior performance of our method.","2024","2025-03-30 16:16:06","2025-03-30 16:16:06","2025-03-30 16:16:06","616-629","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZX5S5AVE/Sun et al. - 2024 - Exploring Conditional Variational Mechanism to Pinyin Input Method for Addressing One-to-Many Mappin.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"8RDCA3QU","conferencePaper","2024","Singh, Anushka; Sai, Ananya; Dabre, Raj; Puduppully, Ratish; Kunchukuttan, Anoop; Khapra, Mitesh","How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.58","https://aclanthology.org/2024.acl-short.58","","2024","2025-03-30 16:16:08","2025-03-30 16:16:08","2025-03-30 16:16:08","640-649","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PTG7JHFY/Singh et al. - 2024 - How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"G5EPV3AY","conferencePaper","2024","Adeyemi, Mofetoluwa; Oladipo, Akintunde; Pradeep, Ronak; Lin, Jimmy","Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.59","https://aclanthology.org/2024.acl-short.59","Large language models (LLMs) as listwise rerankers have shown impressive zero-shot capabilities in various passage ranking tasks. Despite their success, there is still a gap in existing literature on their effectiveness in reranking low-resource languages. To address this, we investigate how LLMs function as listwise rerankers in cross-lingual information retrieval (CLIR) systems with queries in English and passages in four African languages: Hausa, Somali, Swahili, and Yoruba. We analyze and compare the effectiveness of monolingual reranking using either query or document translations. We also evaluate the effectiveness of LLMs when leveraging their own generated translations. To grasp the general picture, we examine the effectiveness of multiple LLMs—the proprietary models RankGPT4 and RankGPT3.5, along with the open-source model RankZephyr. While the document translation setting, i.e., both queries and documents are in English, leads to the best reranking effectiveness, our results indicate that for specific LLMs, reranking in the African language setting achieves competitive effectiveness with the cross-lingual setting, and even performs better when using the LLM’s own translations.","2024","2025-03-30 16:16:11","2025-03-30 16:16:11","2025-03-30 16:16:11","650-656","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/L4KI7UR8/Adeyemi et al. - 2024 - Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"G3VKVXVZ","journalArticle","","Keleg, Amr; Magdy, Walid; Goldwater, Sharon","Estimating the Level of Dialectness Predicts Interannotator Agreement in Multi-dialect Arabic Datasets","","","","","","On annotating multi-dialect Arabic datasets, it is common to randomly assign the samples across a pool of native Arabic speakers. Recent analyses recommended routing dialectal samples to native speakers of their respective dialects to build higher-quality datasets. However, automatically identifying the dialect of samples is hard. Moreover, the pool of annotators who are native speakers of specific Arabic dialects might be scarce. Arabic Level of Dialectness (ALDi) was recently introduced as a quantitative variable that measures how sentences diverge from Standard Arabic. On randomly assigning samples to annotators, we hypothesize that samples of higher ALDi scores are harder to label especially if they are written in dialects that the annotators do not speak. We test this by analyzing the relation between ALDi scores and the annotators’ agreement, on 15 public datasets having raw individual sample annotations for various sentence-classification tasks. We find strong evidence supporting our hypothesis for 11 of them. Consequently, we recommend prioritizing routing samples of high ALDi scores to native speakers of each sample’s dialect, for which the dialect could be automatically identified at higher accuracies.","","2025-03-30 16:16:12","2025-03-30 16:16:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YSJJ69BU/Keleg et al. - Estimating the Level of Dialectness Predicts Interannotator Agreement in Multi-dialect Arabic Datase.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NEYH88TM","conferencePaper","2024","Xu, Zhipeng; Liu, Zhenghao; Yan, Yukun; Liu, Zhiyuan; Yu, Ge; Xiong, Chenyan","Cleaner Pretraining Corpus Curation with Neural Web Scraping","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.72","https://aclanthology.org/2024.acl-short.72","The web contains large-scale, diverse, and abundant information to satisfy the informationseeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https: //github.com/OpenMatch/NeuScraper.","2024","2025-03-30 16:16:17","2025-03-30 16:16:17","2025-03-30 16:16:17","802-812","","","","","","","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PH5UCNDW/Xu et al. - 2024 - Cleaner Pretraining Corpus Curation with Neural Web Scraping.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"KAFRL7N7","conferencePaper","2024","Bhutani, Mukul; Robinson, Kevin; Prabhakaran, Vinodkumar; Dave, Shachi; Dev, Sunipa","SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.75","https://aclanthology.org/2024.acl-short.75","While generative multilingual models are rapidly being deployed, their safety and fairness evaluations are largely limited to resources collected in English. This is especially problematic for evaluations targeting inherently socio-cultural phenomena such as stereotyping, where it is important to build multilingual resources that reflect the stereotypes prevalent in respective language communities. However, gathering these resources, at scale, in varied languages and regions pose a significant challenge as it requires broad socio-cultural knowledge and can also be prohibitively expensive. To overcome this critical gap, we employ a recently introduced approach that couples LLM generations for scale with culturally situated validations for reliability, and build SeeGULL Multilingual, a global-scale multilingual dataset of social stereotypes, containing over 25K stereotypes, spanning 23 pairs of languages and regions they are common in,1 with human annotations, and demonstrate its utility in identifying gaps in model evaluations. Content warning: Stereotypes shared in this paper can be offensive.","2024","2025-03-30 16:16:20","2025-03-30 16:16:20","2025-03-30 16:16:20","842-854","","","","","","SeeGULL Multilingual","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/78RL4B7Z/Bhutani et al. - 2024 - SeeGULL Multilingual a Dataset of Geo-Culturally Situated Stereotypes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""
"ABRVLGIL","conferencePaper","2024","Horvitz, Zachary; Chen, Jingru; Aditya, Rahul; Srivastava, Harshvardhan; West, Robert; Yu, Zhou; McKeown, Kathleen","Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2024.acl-short.76","https://aclanthology.org/2024.acl-short.76","Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. We investigate whether large language models (LLMs) can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to “unfun” jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset where we find that GPT-4’s synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.","2024","2025-03-30 16:16:23","2025-03-30 16:16:23","2025-03-30 16:16:23","855-869","","","","","","Getting Serious about Humor","","","","","Association for Computational Linguistics","Bangkok, Thailand","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AP6K4YZM/Horvitz et al. - 2024 - Getting Serious about Humor Crafting Humor Datasets with Unfunny Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","","","","","","","","","","","","",""