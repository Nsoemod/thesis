"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"IHBLLIBV","dataset","2025","Hauser, Jakob Elias","HiST-LLM","","","","10.5281/ZENODO.14671247","https://zenodo.org/doi/10.5281/zenodo.14671247","Large Language Models (LLMs) have the potential to transform humanities and social science research, yet their history knowledge and comprehension at a graduate level remains untested. Benchmarking LLMs in history is particularly challenging, given that human knowledge of history is inherently unbalanced, with more information available on Western history and recent periods. We introduce the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over 2,700 scholarly references. This dataset covers every major world region from the Neolithic period to the Industrial Revolution and includes information reviewed and assembled by history experts and graduate research assistants. Using this dataset, we benchmark a total of seven models from the Gemini, OpenAI, and Llama families. We find that, in a four-choice format, LLMs have a balanced accuracy ranging from 33.6% (Llama-3.1-8B) to 46% (GPT-4-Turbo), outperforming random guessing (25%) but falling short of expert comprehension. LLMs perform better on earlier historical periods. Regionally, performance is more even but still better for the Americas and lowest in Oceania and Sub-Saharan Africa for the more advanced models. Our benchmark shows that while LLMs possess some expert-level historical knowledge, there is considerable room for improvement.","2025-01-16","2025-03-30 16:21:25","2025-03-30 16:21:25","2025-03-30 16:21:25","","","","","","","","","","","","Zenodo","","en","Creative Commons Attribution 4.0 International","","","","DOI.org (Datacite)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2CXCPE2S/Hauser - 2025 - HiST-LLM.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"58JV2V3K","journalArticle","","Dong, Linfeng; Wang, Wei; Qiao, Yu; Sun, Xiao","LucidAction: A Hierarchical and Multi-model Dataset for Comprehensive Action Quality Assessment","","","","","","","","2025-03-30 16:21:26","2025-03-30 16:21:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/C878U7TI/Dong et al. - LucidAction A Hierarchical and Multi-model Dataset for Comprehensive Action Quality Assessment.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRL2G4I9","journalArticle","","Zhang, Hang; Sun, Jiawei; Chen, Renqi; Liu, Wei; Yuan, Zhonghang; Zheng, Xinzhe; Wang, Zhefan; Yang, Zhiyuan; Yan, Hang; Zhong, Hansen; Wang, Xiqing; Ouyang, Wanli; Yang, Fan; Dong, Nanqing","Empowering and Assessing the Utility of Large Language Models in Crop Science","","","","","","Large language models (LLMs) have demonstrated remarkable efficacy across knowledge-intensive tasks. Nevertheless, their untapped potential in crop science presents an opportunity for advancement. To narrow this gap, we introduce CROP3, which includes a novel instruction tuning dataset specifically designed to enhance LLMs’ professional capabilities in the crop science sector, along with a benchmark that serves as a comprehensive evaluation of LLMs’ understanding of the domain knowledge. The CROP dataset is curated through a task-oriented and LLM-human integrated pipeline, comprising 210,038 single-turn and 1,871 multi-turn dialogues related to crop science scenarios. The CROP benchmark includes 5,045 multiplechoice questions covering three difficulty levels. Our experiments based on the CROP benchmark demonstrate notable enhancements in crop science-related tasks when LLMs are fine-tuned with the CROP dataset. To the best of our knowledge, CROP dataset is the first-ever instruction tuning dataset in the crop science domain. We anticipate that CROP will accelerate the adoption of LLMs in the domain of crop science, ultimately contributing to global food production.","","2025-03-30 16:21:28","2025-03-30 16:21:28","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DU25HTFE/Zhang et al. - Empowering and Assessing the Utility of Large Language Models in Crop Science.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZFMDATR","journalArticle","","Luo, Jialin; Wang, Yuanzhi; Gu, Ziqi; Qiu, Yide; Yao, Shuaizhen; Wang, Fuyun; Xu, Chunyan; Zhang, Wenhua; Wang, Dan; Cui, Zhen","MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation","","","","","","Recently, the diffusion-based generative paradigm has achieved impressive general image generation capabilities with text prompts due to its accurate distribution modeling and stable training process. However, generating diverse remote sensing (RS) images that are tremendously different from general images in terms of scale and perspective remains a formidable challenge due to the lack of a comprehensive remote sensing image generation dataset with various modalities, ground sample distances (GSD), and scenes. In this paper, we propose a Multi-modal, MultiGSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for textto-image generation in diverse remote sensing scenarios. Specifically, we first collect nine publicly available RS datasets and conduct standardization for all samples. To bridge RS images to textual semantic information, we utilize a largescale pretrained vision-language model to automatically output text prompts and perform hand-crafted rectification, resulting in information-rich text-image pairs (including multi-modal images). In particular, we design some methods to obtain the images with different GSD and various environments (e.g., low-light, foggy) in a single sample. With extensive manual screening and refining annotations, we ultimately obtain a MMM-RS dataset that comprises approximately 2.1 million text-image pairs. Extensive experimental results verify that our proposed MMMRS dataset allows off-the-shelf diffusion models to generate diverse RS images across various modalities, scenes, weather conditions, and GSD. The dataset is available at https://github.com/ljl5261/MMM-RS.","","2025-03-30 16:21:29","2025-03-30 16:21:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/26MHDYNT/Luo et al. - MMM-RS A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I249SJTS","journalArticle","","Chen, Cheng; Zhu, Junchen; Luo, Xu; Shen, Heng Tao; Song, Jingkuan; Gao, Lianli","CoIN: A Benchmark of Continual Instruction Tuning for Multimodal Large Language Models","","","","","","Instruction tuning demonstrates impressive performance in adapting Multimodal Large Language Models (MLLMs) to follow task instructions and improve generalization ability. By extending tuning across diverse tasks, MLLMs can further enhance their understanding of world knowledge and instruction intent. However, continual instruction tuning has been largely overlooked and there are no public benchmarks available. In this paper, we present CoIN, a comprehensive benchmark tailored for assessing the behavior of existing MLLMs under continual instruction tuning. CoIN comprises 10 meticulously crafted datasets spanning 8 tasks, ensuring diversity and serving as a robust evaluation framework to assess crucial aspects of continual instruction tuning, such as task order, instruction diversity and volume. Additionally, apart from traditional evaluation, we design another LLM-based metric to assess the knowledge preserved within MLLMs for reasoning. Following an in-depth evaluation of several MLLMs, we demonstrate that they still suffer catastrophic forgetting, and the failure in instruction alignment assumes the main responsibility, instead of reasoning knowledge forgetting. To this end, we introduce MoELoRA which is effective in retaining the previous instruction alignment. Codes and datasets are publicly available https://github.com/zackschen/CoIN.","","2025-03-30 16:21:30","2025-03-30 16:21:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DDEL7PX2/Chen et al. - CoIN A Benchmark of Continual Instruction Tuning for Multimodal Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NG7SICZV","journalArticle","","Mou, Yutao; Zhang, Shikun; Ye, Wei","SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types","","","","","","Ensuring the safety of large language model (LLM) applications is essential for developing trustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First, they focus solely on either discriminative or generative evaluation paradigms while ignoring their interconnection. Second, they rely on standardized inputs, overlooking the effects of widespread prompting techniques, such as system prompts, few-shot demonstrations, and chain-of-thought prompting. To overcome these issues, we developed SG-Bench, a novel benchmark to assess the generalization of LLM safety across various tasks and prompt types. This benchmark integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak on LLM safety. Our assessment of 3 advanced proprietary LLMs and 10 opensource LLMs with the benchmark reveals that most LLMs perform worse on discriminative tasks than generative ones, and are highly susceptible to prompts, indicating poor generalization in safety alignment. We also explain these findings quantitatively and qualitatively to provide insights for future research.2 Warning: this paper includes examples that may be offensive or harmful.","","2025-03-30 16:21:32","2025-03-30 16:21:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/545S2GV7/Mou et al. - SG-Bench Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N2NIA4VB","journalArticle","","Yan, Zhiyuan; Yao, Taiping; Chen, Shen; Zhao, Yandan; Fu, Xinghe; Zhu, Junwei; Luo, Donghao; Wang, Chengjie; Ding, Shouhong; Wu, Yunsheng; Yuan, Li","DF40: Toward Next-Generation Deepfake Detection","","","","","","We propose a new comprehensive benchmark to revolutionize the current deepfake detection field to the next generation. Predominantly, existing works identify top-notch detection algorithms and models by adhering to the common practice: training detectors on one specific dataset (e.g., FF++ [62]) and testing them on other prevalent deepfake datasets. This protocol is often regarded as a ""golden compass"" for navigating SoTA detectors. But can these stand-out ""winners"" be truly applied to tackle the myriad of realistic and diverse deepfakes lurking in the real world? If not, what underlying factors contribute to this gap? In this work, we found the dataset (both train and test) can be the ""primary culprit"" due to the following: (1) forgery diversity: Deepfake techniques are commonly referred to as both face forgery (face-swapping and face-reenactment) and entire face synthesis (especially face). Most existing datasets only contain partial types of them, with limited forgery methods implemented (e.g., 2 swapping and 2 reenactment methods in FF++); (2) forgery realism: The dominated training dataset, FF++, contains out-of-date forgery techniques from the past four years. ""Honing skills"" on these forgeries makes it difficult to guarantee effective detection generalization toward nowadays’ SoTA deepfakes; (3) evaluation protocol: Most detection works perform evaluations on one type, e.g., training and testing on face-swapping types only, which hinders the development of universal deepfake detectors. To address this dilemma, we construct a highly diverse and large-scale deepfake detection dataset called DF40, which comprises 40 distinct deepfake techniques (10 times larger than FF++). We then conduct comprehensive evaluations using 4 standard evaluation protocols and 8 representative detection methods, resulting in over 2,000 evaluations. Through these evaluations, we provide an extensive analysis from various perspectives, leading to 7 new insightful findings contributing to the field. We also open up 4 valuable yet previously underexplored research questions to inspire future works. We release our dataset, code, and checkpoints at https://github.com/YZY-stack/DF40.","","2025-03-30 16:21:34","2025-03-30 16:21:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/K2HK67XB/Yan et al. - DF40 Toward Next-Generation Deepfake Detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASZDEX82","journalArticle","","Hui, Yulong; Lu, Yao; Zhang, Huanchen","UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis","","","","","","The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval. We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.","","2025-03-30 16:21:35","2025-03-30 16:21:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/F6QQHA3S/Hui et al. - UDA A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CYUKMDR3","journalArticle","","Cao, Jiahuan; Liu, Yang; Shi, Yongxin; Ding, Kai; Jin, Lianwen","WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts","","","","","","Large Language Models (LLMs) have made significant advancements across numerous domains, but their capabilities in Chinese Classical Literature and Language Arts (CCLLA) remain largely unexplored due to the limited scope and tasks of existing benchmarks. To fill this gap, we propose WenMind, a comprehensive benchmark dedicated for evaluating LLMs in CCLLA. WenMind covers the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, comprising 4,875 question-answer pairs, spanning 42 fine-grained tasks, 3 question formats, and 2 evaluation scenarios: domain-oriented and capability-oriented. Based on WenMind, we conduct a thorough evaluation of 31 representative LLMs, including general-purpose models and ancient Chinese LLMs. The results reveal that even the best-performing model, ERNIE-4.0, only achieves a total score of 64.3, indicating significant room for improvement of LLMs in the CCLLA domain. We also provide insights into the strengths and weaknesses of different LLMs and highlight the importance of pre-training data in achieving better results. Overall, WenMind serves as a standardized and comprehensive baseline, providing valuable insights for future CCLLA research. Our benchmark and related code are available at https://github.com/SCUT-DLVCLab/WenMind.","","2025-03-30 16:21:37","2025-03-30 16:21:37","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QY9DCSFA/Cao et al. - WenMind A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDWXU772","journalArticle","","Gong, Biao; Tan, Shuai; Feng, Yutong; Xie, Xiaoying; Li, Yuyuan; Chen, Chaochao; Zheng, Kecheng; Shen, Yujun; Zhao, Deli","UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training","","","","","","This work presents a unified knowledge protocol, called UKnow, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under UKnow format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following UKnow protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 visionrelated ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on 4 benchmarks demonstrate the potential of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. See Appendix A to download the dataset.","","2025-03-30 16:21:38","2025-03-30 16:21:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/T9HN9UHU/Gong et al. - UKnow A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Visio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86HQJEK3","journalArticle","","Lin, Shuyi; He, Haoyu; Wei, Tianhao; Xu, Kaidi; Zhang, Huan; Singh, Gagandeep; Liu, Changliu; Tan, Cheng","NN4SysBench: Characterizing Neural Network Verification for Computer Systems","","","","","","We present NN4SysBench, a benchmark suite for neural network verification that is composed of applications from the domain of computer systems. We call these neural networks for computer systems or NN4Sys. NN4Sys is booming: there are many proposals for using neural networks in computer systems—for example, databases, OSes, and networked systems—many of which are safetycritical. Neural network verification is a technique to formally verify whether neural networks satisfy safety properties. We however observe that NN4Sys has some unique characteristics that today’s verification tools overlook and have limited support. Therefore, this benchmark suite aims at bridging the gap between NN4Sys and the verification by using impactful NN4Sys applications as benchmarks to illustrate computer systems’ unique challenges. We also build a compatible version of NN4SysBench, so that today’s verifiers can also work on these benchmarks with approximately the same verification difficulties. The code is available here: https://github.com/Khoury-srg/NN4SysBench.","","2025-03-30 16:21:40","2025-03-30 16:21:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EJPZYPQ3/Lin et al. - NN4SysBench Characterizing Neural Network Verification for Computer Systems.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SAHJD6BL","dataset","2024","Li, Ruohan; Xie, Yiqun; Wang, Dongdong","SolarCube","","","","10.5281/ZENODO.11498739","https://zenodo.org/doi/10.5281/zenodo.11498739","Solar power is a critical source of renewable energy, offering significant potential to lower greenhouse gas emissions and mitigate climate change. However, the cloud induced-variability of solar radiation reaching the earth’s surface presents a challenge for integrating solar power into the grid (e.g., storage and backup management). The new generation of geostationary satellites such as GOES-16 has become an important data source for large-scale and high temporal frequency solar radiation forecasting. However, no machine-learning-ready dataset has integrated geostationary satellite data with fine-grained solar radiation information to support forecasting model development and benchmarking with consistent metrics. We present SolarCube, a new ML-ready benchmark dataset for solar radiation forecasting. SolarCube covers 19 study areas distributed over multiple continents: North America, South America, Asia, and Oceania. The dataset supports short (i.e., 30 minutes to 6 hours) and long-term (i.e., day-ahead or longer) solar radiation forecasting at both point-level (i.e., specific locations of monitoring stations) and area-level, by processing and integrating data from multiple sources, including geostationary satellite images, physics-derived solar radiation, and ground station observations from different monitoring networks over the globe. We also evaluated a set of forecasting models for point- and image-based time-series data to develop performance benchmarks under different testing scenarios. The dataset is available at https://doi.org/10.5281/zenodo.11498739. A Python library is available to conveniently generate different variations of the dataset based on user needs, along with baseline models at https://github.com/Ruohan-Li/SolarCube.","2024-06-12","2025-03-30 16:21:43","2025-03-30 16:21:43","2025-03-30 16:21:43","","","","","","","","","","","","Zenodo","","en","Creative Commons Attribution 4.0 International","","","","DOI.org (Datacite)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RAQSSFGA/Li et al. - 2024 - SolarCube.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4MGCCB2","journalArticle","","Chen, Zijian; Sun, Wei; Tian, Yuan; Jia, Jun; Zhang, Zicheng; Wang, Jiarui; Huang, Ru; Min, Xiongkuo; Zhai, Guangtao; Zhang, Wenjun","GAIA: Rethinking Action Quality Assessment for AI-Generated Videos","","","","","","Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV). Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs. To address these problems, we construct GAIA, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions. We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods. Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods perform poorly with an average SRCC of 0.454, 0.191, and 0.519, respectively, indicating a sizable gap between current models and human action perception patterns in AIGVs. Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs.","","2025-03-30 16:21:44","2025-03-30 16:21:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3CQAPX4R/Chen et al. - GAIA Rethinking Action Quality Assessment for AI-Generated Videos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSIQZ8UH","journalArticle","","Yang, Bing; Quan, Changsheng; Wang, Yabo; Wang, Pengyu; Yang, Yujie; Fang, Ying; Shao, Nian; Bu, Hui; Xu, Xin; Li, Xiaofei","RealMAN: A Real-Recorded and Annotated Microphone Array Dataset for Dynamic Speech Enhancement and Localization","","","","","","The training of deep learning-based multichannel speech enhancement and source localization systems relies heavily on the simulation of room impulse response and multichannel diffuse noise, due to the lack of large-scale real-recorded datasets. However, the acoustic mismatch between simulated and real-world data could degrade the model performance when applying in real-world scenarios. To bridge this simulation-to-real gap, this paper presents a new relatively large-scale Realrecorded and annotated Microphone Array speech&Noise (RealMAN) dataset2. The proposed dataset is valuable in two aspects: 1) benchmarking speech enhancement and localization algorithms in real scenarios; 2) offering a substantial amount of real-world training data for potentially improving the performance of real-world applications. Specifically, a 32-channel array with high-fidelity microphones is used for recording. A loudspeaker is used for playing source speech signals (about 35 hours of Mandarin speech). A total of 83.7 hours of speech signals (about 48.3 hours for static speaker and 35.4 hours for moving speaker) are recorded in 32 different scenes, and 144.5 hours of background noise are recorded in 31 different scenes. Both speech and noise recording scenes cover various common indoor, outdoor, semi-outdoor and transportation environments, which enables the training of general-purpose speech enhancement and source localization networks. To obtain the task-specific annotations, speaker location is annotated with an omni-directional fisheye camera by automatically detecting the loudspeaker. The direct-path signal is set as the target clean speech for speech enhancement, which is obtained by filtering the source speech signal with an estimated direct-path propagation filter. Baseline experiments demonstrate that i) compared to using simulated data, the proposed dataset is indeed able to train better speech enhancement and source localization networks; ii) using various sub-arrays of the proposed 32-channel microphone array can successfully train variable-array networks that can be directly used to unseen arrays.","","2025-03-30 16:21:46","2025-03-30 16:21:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/573B56YF/Yang et al. - RealMAN A Real-Recorded and Annotated Microphone Array Dataset for Dynamic Speech Enhancement and L.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MIVEBFE","journalArticle","","Lu, Junyu; Xu, Bo; Zhang, Xiaokun; Wang, Hongbo; Zhu, Haohao; Zhang, Dongyu; Yang, Liang; Lin, Hongfei","Towards Comprehensive Detection of Chinese Harmful Memes","","","","","","Harmful memes have proliferated on the Chinese Internet, while research on detecting Chinese harmful memes significantly lags behind due to the absence of reliable datasets and effective detectors. To this end, we focus on the comprehensive detection of Chinese harmful memes. We construct TOXICN MM, the first Chinese harmful meme dataset, which consists of 12,000 samples with finegrained annotations for various meme types. Additionally, we propose a baseline detector, Multimodal Knowledge Enhancement (MKE), incorporating contextual information of meme content generated by the LLM to enhance the understanding of Chinese memes. During the evaluation phase, we conduct extensive quantitative experiments and qualitative analyses on multiple baselines, including LLMs and our MKE. The experimental results indicate that detecting Chinese harmful memes is challenging for existing models while demonstrating the effectiveness of MKE.1 Disclaimer: The samples presented by this paper may be considered offensive.","","2025-03-30 16:21:47","2025-03-30 16:21:47","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/M2WEZMCG/Lu et al. - Towards Comprehensive Detection of Chinese Harmful Memes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJCA6MKR","journalArticle","","Lin, Xiaohan; Cao, Qingxing; Huang, Yinya; Wang, Haiming; Lu, Jianqiao; Liu, Zhengying; Song, Linqi; Liang, Xiaodan","FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving","","","","","","Formal verification (FV) has witnessed growing significance with emerging program synthesis by the evolving large language models (LLMs). However, current formal verification mainly resorts to symbolic verifiers or hand-craft rules, resulting in limitations for extensive and flexible verification. On the other hand, formal systems for automated theorem proving, such as Isabelle, serve as another line of rigorous verification, upheld by extensive rules and theorems. In this paper, we propose FVEL3, an interactive Formal Verification Environment with LLMs. Specifically, FVEL transforms a given code to be verified into Isabelle, and then conducts verification via neural automated theorem proving with an LLM. The joined paradigm leverages the rigorous yet abundant formulated and organized rules in Isabelle and is also convenient for introducing and adjusting cutting-edge LLMs. To achieve this goal, we extract a large-scale dataset for automated formal verification named FVELER3. The FVELER dataset includes code dependencies and verification processes that are formulated in Isabelle, containing 758 theories, 29,304 lemmas, and 201,498 proof steps with in-depth dependencies. We benchmark FVELER in the FVEL environment by fine-tuning LLMs with FVELER and then evaluating them on Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned Llama3-8B solves 17.39% (69→81) more problems, and Mistral-7B 12% (75→84) more problems in SV-COMP. And the proportion of proof errors is reduced. Project page: https://fveler.github.io/.","","2025-03-30 16:21:48","2025-03-30 16:21:48","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TYJLGXLX/Lin et al. - FVEL Interactive Formal Verification Environment with Large Language Models via Theorem Proving.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NNYCNYS","journalArticle","","Li, Yiwei; Shi, Jiayi; Feng, Shaoxiong; Yuan, Peiwen; Wang, Xinglin; Pan, Boyuan; Wang, Heda; Hu, Yao; Li, Kan","Instruction Embedding: Latent Representations of Instructions Towards Task Identification","","","","","","Instruction data is crucial for improving the capability of Large Language Models (LLMs) to align with human-level performance. Recent research LIMA demonstrates that alignment is essentially a process where the model adapts instructions’ interaction style or format to solve various tasks, leveraging pre-trained knowledge and skills. Therefore, for instructional data, the most important aspect is the task it represents, rather than the specific semantics and knowledge information. The latent representations of instructions play roles for some instruction-related tasks like data selection and demonstrations retrieval. However, they are always derived from text embeddings, encompass overall semantic information that influences the representation of task categories. In this work, we introduce a new concept, instruction embedding, and construct Instruction Embedding Benchmark (IEB) for its training and evaluation. Then, we propose a baseline Prompt-based Instruction Embedding (PIE) method to make the representations more attention on tasks. The evaluation of PIE, alongside other embedding methods on IEB with two designed tasks, demonstrates its superior performance in accurately identifying task categories. Moreover, the application of instruction embeddings in four downstream tasks showcases its effectiveness and suitability for instruction-related tasks1.","","2025-03-30 16:21:49","2025-03-30 16:21:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/M3HVGQBH/Li et al. - Instruction Embedding Latent Representations of Instructions Towards Task Identification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PG2VLHAV","journalArticle","","Liu, Hongbin; Guo, Moyang; Jiang, Zhengyuan; Wang, Lun; Gong, Neil Zhenqiang","AudioMarkBench: Benchmarking Robustness of Audio Watermarking","","","","","","The increasing realism of synthetic speech, driven by advancements in text-tospeech models, raises ethical concerns regarding impersonation and disinformation. Audio watermarking offers a promising solution via embedding humanimperceptible watermarks into AI-generated audios. However, the robustness of audio watermarking against common/adversarial perturbations remains understudied. We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against watermark removal and watermark forgery. AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations. We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings. Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions. Our dataset and code are publicly available at https://github.com/moyangkuo/AudioMarkBench.","","2025-03-30 16:21:51","2025-03-30 16:21:51","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NSJUC3D7/Liu et al. - AudioMarkBench Benchmarking Robustness of Audio Watermarking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"INUH5EEP","journalArticle","","Yao, Jing; Yi, Xiaoyuan; Xie, Xing","CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses","","","","","","The rapid progress in Large Language Models (LLMs) poses potential risks such as generating unethical content. Assessing the values embedded in LLMs’ generated responses can help expose their misalignment, but this relies on reference-free value evaluators, e.g. fine-tuned LLMs or closed-source models like GPT-4. Nevertheless, two key challenges emerge in open-ended value evaluation: the evaluator should adapt to changing human value definitions with minimal annotation, against their own bias (adaptability); and remain robust across varying value expressions and scenarios (generalizability). To handle these challenges, we introduce CLAVE, a novel framework that integrates two complementary LLMs: a large model to extract high-level value concepts from diverse responses, leveraging its extensive knowledge and generalizability, and a small model fine-tuned on these concepts to adapt to human value annotations. This dual-model framework enables adaptation to any value system using <100 human-labeled samples per value type. We also present ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples across diverse domains, covering three major value systems. We benchmark the performance of 15+ popular LLM evaluators and fully analyze their strengths and weaknesses. Our findings reveal that CLAVE combining a large prompt-based model and a small fine-tuned one serves as an optimal balance in value evaluation.","","2025-03-30 16:21:52","2025-03-30 16:21:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PA4R5XZB/Yao et al. - CLAVE An Adaptive Framework for Evaluating Values of LLM Generated Responses.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y79SCJCW","journalArticle","","Chen, Xi; Qin, Chuan; Fang, Chuyu; Wang, Chao; Zhu, Chen; Zhuang, Fuzhen; Zhu, Hengshu; Xiong, Hui","Job-SDF: A Multi-Granularity Dataset for Job Skill Demand Forecasting and Benchmarking","","","","","","In a rapidly evolving job market, skill demand forecasting is crucial as it enables policymakers and businesses to anticipate and adapt to changes, ensuring that workforce skills align with market needs, thereby enhancing productivity and competitiveness. Additionally, by identifying emerging skill requirements, it directs individuals towards relevant training and education opportunities, promoting continuous self-learning and development. However, the absence of comprehensive datasets presents a significant challenge, impeding research and the advancement of this field. To bridge this gap, we present Job-SDF, a dataset designed to train and benchmark job-skill demand forecasting models. Based on millions of public job advertisements collected from online recruitment platforms, this dataset encompasses monthly recruitment demand. Our dataset uniquely enables evaluating skill demand forecasting models at various granularities, including occupation, company, and regional levels. We benchmark a range of models on this dataset, evaluating their performance in standard scenarios, in predictions focused on lower value ranges, and in the presence of structural breaks, providing new insights for further research. Our code and dataset are publicly accessible via the https://github.com/Job-SDF/benchmark.","","2025-03-30 16:21:53","2025-03-30 16:21:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SPG5XGWM/Chen et al. - Job-SDF A Multi-Granularity Dataset for Job Skill Demand Forecasting and Benchmarking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BYDGLYUW","journalArticle","","Jin, Xin; Qiao, Qianqian; Lu, Yi; Wang, Huaye; Huang, Heng; Gao, Shan; Liu, Jianfei; Li, Rui","APDDv2: Aesthetics of Paintings and Drawings Dataset with Artist Labeled Scores and Comments","","","","","","Datasets play a pivotal role in training visual models, facilitating the development of abstract understandings of visual features through diverse image samples and multidimensional attributes. However, in the realm of aesthetic evaluation of artistic images, datasets remain relatively scarce. Existing painting datasets are often characterized by limited scoring dimensions and insufficient annotations, thereby constraining the advancement and application of automatic aesthetic evaluation methods in the domain of painting. To bridge this gap, we introduce the Aesthetics Paintings and Drawings Dataset (APDD), the first comprehensive collection of paintings encompassing 24 distinct artistic categories and 10 aesthetic attributes. Building upon the initial release of APDDv1[Jin et al., 2024], our ongoing research has identified opportunities for enhancement in data scale and annotation precision. Consequently, APDDv2 boasts an expanded image corpus and improved annotation quality, featuring detailed language comments to better cater to the needs of both researchers and practitioners seeking high-quality painting datasets. Furthermore, we present an updated version of the Art Assessment Network for Specific Painting Styles, denoted as ArtCLIP. Experimental validation demonstrates the superior performance of this revised model in the realm of aesthetic evaluation, surpassing its predecessor in accuracy and efficacy. The dataset and model are available at https://github.com/BestiVictory/APDDv2.git.","","2025-03-30 16:21:54","2025-03-30 16:21:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NCB9MZ96/Jin et al. - APDDv2 Aesthetics of Paintings and Drawings Dataset with Artist Labeled Scores and Comments.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E5CZZLQA","journalArticle","","Khandekar, Nikhil; Jin, Qiao; Xiong, Guangzhi; Dunn, Soren; Applebaum, Serina S; Anwar, Zain; Sarfo-Gyamfi, Maame; Safranek, Conrad W; Anwar, Abid A; Zhang, Andrew; Gilson, Aidan; Singer, Maxwell B; Dave, Amisha; Taylor, Andrew; Zhang, Aidong; Chen, Qingyu; Lu, Zhiyong","MEDCALC-BENCH: Evaluating Large Language Models for Medical Calculations","","","","","","","","2025-03-30 16:21:56","2025-03-30 16:21:56","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z5UWUIUB/Khandekar et al. - MEDCALC-BENCH Evaluating Large Language Models for Medical Calculations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ENX794ZS","journalArticle","","Zhong, Ming; Lyu, Fang; Wang, Lulin; Geng, Hongna; Qiu, Lei; Cui, Huimin; Feng, Xiaobing","ComBack: A Versatile Dataset for Enhancing Compiler Backend Development Efficiency","","","","","","Compiler backends are tasked with generating executable machine code for processors. With the proliferation of diverse processors, it is imperative for programmers to tailor specific compiler backends to accommodate each one. Meanwhile, compiler backend development is a laborious and time-consuming task, lacking effective automation methods. Although language models have demonstrated strong abilities in code related tasks, the lack of appropriate datasets for compiler backend development limits the application of language models in this field.","","2025-03-30 16:21:57","2025-03-30 16:21:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2CCSRPD7/Zhong et al. - ComBack A Versatile Dataset for Enhancing Compiler Backend Development Efficiency.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DC4W2JFN","journalArticle","","Ma, Jiefeng; Wang, Yan; Liu, Chenyu; Du, Jun; Hu, Yu; Zhang, Zhenrong; Hu, Pengfei; Wang, Qing; Zhang, Jianshu","SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding","","","","","","Accurate identification and organizing of textual content is crucial for the automation of document processing in the field of form understanding. Existing datasets, such as FUNSD and XFUND, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. This limitation overlooks the hierarchically structured representation of documents, constraining a comprehensive understanding of complex forms. To address this issue, we present the SRFUND, a hierarchically structured multi-task form understanding benchmark. SRFUND provides refined annotations on top of the original FUNSD and XFUND datasets, encompassing five tasks: (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery. We meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. Additionally, we introduce global hierarchical structure dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. The SRFUND dataset includes eight languages including English, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese, making it a powerful tool for understanding cross-lingual forms. Extensive experimental results demonstrate that the SRFUND dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understanding. The original data set and implementations of the baseline methods are available at https://sprateam-ustc.github.io/SRFUND.","","2025-03-30 16:21:58","2025-03-30 16:21:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/C2Y9I6UW/Ma et al. - SRFUND A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3PS8XFCG","journalArticle","","Silcock, Emily; Arora, Abhishek; D’Amico-Wong, Luca","Newswire: A Large-Scale Structured Database of a Century of Historical News","","","","","","In the U.S. historically, local newspapers drew their content largely from newswires like the Associated Press. Historians argue that newswires played a pivotal role in creating a national identity and shared understanding of the world, but there is no comprehensive archive of the content sent over newswires. We reconstruct such an archive by applying a customized deep learning pipeline to hundreds of terabytes of raw image scans from thousands of local newspapers. The resulting dataset contains 2.7 million unique public domain U.S. newswire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model. To construct the Newswire dataset, we first recognize newspaper layouts and transcribe around 138 million structured article texts from raw image scans. We then use a customized neural bi-encoder model to de-duplicate reproduced articles, in the presence of considerable abridgement and noise, quantifying how widely each article was reproduced. A text classifier is used to ensure that we only include newswire articles, which historically are in the public domain. The structured data that accompany the texts provide rich information about the who (disambiguated individuals), what (topics), and where (georeferencing) of the news that millions of Americans read over the course of a century. We also include Library of Congress metadata information about the newspapers that ran the articles on their front pages. The Newswire dataset is useful both for large language modeling - expanding training data beyond what is available from modern web texts - and for studying a diversity of questions in computational linguistics, social science, and the digital humanities.","","2025-03-30 16:21:59","2025-03-30 16:21:59","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZAYWTWIX/Silcock et al. - Newswire A Large-Scale Structured Database of a Century of Historical News.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KLLVQQXR","journalArticle","","Li, Xiaotong; Zhang, Fan; Diao, Haiwen; Wang, Yueze; Wang, Xinlong; Duan, Ling-Yu","DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception","","","","","","Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, and spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions. Specifically, Perceptual Fusion integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs’ perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs. The dataset and code are publicly available at https://github.com/baaivision/DenseFusion.","","2025-03-30 16:22:01","2025-03-30 16:22:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YA5HSE7J/Li et al. - DenseFusion-1M Merging Vision Experts for Comprehensive Multimodal Perception.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I9MAISVX","journalArticle","","Al-Tahan, Haider; Garrido, Quentin; Balestriero, Randall; Bouchacourt, Diane; Hazirbas, Caner; Ibrahim, Mark","UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling","","","","","","","","2025-03-30 16:22:03","2025-03-30 16:22:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7REUD23E/Al-Tahan et al. - UniBench Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5NT5NLPL","journalArticle","","Xia, Peng; Chen, Ze; Tian, Juanxi; Gong, Yangrui; Hou, Ruibo; Xu, Yue; Wu, Zhenbang; Fan, Zhiyuan; Zhou, Yiyang; Zhu, Kangyu; Zheng, Wenhao; Wang, Zhaoyang; Wang, Xiao; Zhang, Xuchao; Bansal, Chetan; Niethammer, Marc; Huang, Junzhou; Zhu, Hongtu; Li, Yun; Sun, Jimeng; Ge, Zongyuan; Li, Gang; Zou, James; Yao, Huaxiu","CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models","","","","","","Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to Comprehensively evAluate the tRustworthinESs of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://cares-ai.github.io/.","","2025-03-30 16:22:05","2025-03-30 16:22:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GYUD64XH/Xia et al. - CARES A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83WEKFCW","journalArticle","","Yang, Ge; He, Changyi; Guo, Jinyang; Wu, Jianyu; Ding, Yifu; Liu, Aishan; Qin, Haotong; Ji, Pengliang; Liu, Xianglong","LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment","","","","","","Although large language models (LLMs) have demonstrated their strong intelligence ability, the high demand for computation and storage hinders their practical application. To this end, many model compression techniques are proposed to increase the efficiency of LLMs. However, current researches only validate their methods on limited models, datasets, metrics, etc, and still lack a comprehensive evaluation under more general scenarios. So it is still a question of which model compression approach we should use under a specific case. To mitigate this gap, we present the Large Language Model Compression Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth analysis for LLM compression algorithms. We first analyze the actual model production requirements and carefully design evaluation tracks and metrics. Then, we conduct extensive experiments and comparison using multiple mainstream LLM compression approaches. Finally, we perform an in-depth analysis based on the evaluation and provide useful insight for LLM compression design. We hope our LLMCBench can contribute insightful suggestions for LLM compression algorithm design and serve as a foundation for future research. Our code is available at https://github.com/AboveParadise/LLMCBench.","","2025-03-30 16:22:06","2025-03-30 16:22:06","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PJ6GBYGF/Yang et al. - LLMCBench Benchmarking Large Language Model Compression for Efficient Deployment.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TEG6KVS5","journalArticle","","Eyzaguirre, Cristóbal; Tang, Eric; Buch, Shyamal","Streaming Detection of Queried Event Start","","","","","","Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding—Streaming Detection of Queried Event Start (SDQES). The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency. We introduce a new benchmark based on the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting. Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling. We evaluate four vision-language backbones and three adapter architectures in both short-clip and untrimmed video settings.","","2025-03-30 16:22:07","2025-03-30 16:22:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QRQFBPW5/Eyzaguirre et al. - Streaming Detection of Queried Event Start.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QIZLL694","journalArticle","","Baek, Jae-Yong; Yoo, Yong-Sang; Bae, Seung-Hwan","A New Multi-Source Light Detection Benchmark and Semi-Supervised Focal Light Detection","","","","","","This paper addresses a multi-source light detection (LD) problem from vehicles, trafﬁc signals, and streetlights under driving scenarios. Albeit it is crucial for autonomous driving and night vision, this problem has not been yet focused on as much as other object detection (OD). One of the main reasons is the absence of a public available LD benchmark dataset. Therefore, we construct a new large LD dataset consisting of different light sources via heavy annotation:YouTube Driving Light Detection dataset (YDLD). Compared to the existing LD datasets, our dataset has much more images and box annotations for multi-source lights. We also provide rigorous statistical analysis and transfer learning comparison of other well-known detection benchmark datasets to prove the generality of our YDLD.","","2025-03-30 16:22:09","2025-03-30 16:22:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/H3H7FPSW/Baek et al. - A New Multi-Source Light Detection Benchmark and Semi-Supervised Focal Light Detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GIUBUE93","conferencePaper","2024","Akhtar, Mubashara; Benjelloun, Omar; Conforti, Costanza; Gijsbers, Pieter; Giner-Miguelez, Joan; Jain, Nitisha; Kuchnik, Michael; Lhoest, Quentin; Marcenac, Pierre; Maskey, Manil; Mattson, Peter; Oala, Luis; Ruyssen, Pierre; Shinde, Rajat; Simperl, Elena; Thomas, Goeffry; Tykhonov, Slava; Vanschoren, Joaquin; Van Der Velde, Jos; Vogler, Steffen; Wu, Carole-Jean","Croissant: A Metadata Format for ML-Ready Datasets","Proceedings of the Eighth Workshop on Data Management for End-to-End Machine Learning","9798400706110","","10.1145/3650203.3663326","https://dl.acm.org/doi/10.1145/3650203.3663326","Data is a critical resource for machine learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that creates a shared representation across ML tools, frameworks, and platforms. Croissant makes datasets more discoverable, portable, and interoperable, thereby addressing signiﬁcant challenges in ML data management. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, enabling easy loading into the most commonly-used ML frameworks, regardless of where the data is stored. Our initial evaluation by human raters shows that Croissant metadata is readable, understandable, complete, yet concise.","2024-06-09","2025-03-30 16:22:11","2025-03-30 16:22:11","2025-03-30 16:22:11","1-6","","","","","","Croissant","","","","","ACM","Santiago AA Chile","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/A4Q8WEAD/Akhtar et al. - 2024 - Croissant A Metadata Format for ML-Ready Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","SIGMOD/PODS '24: International Conference on Management of Data","","","","","","","","","","","","","","",""
"KCJ8EXH3","journalArticle","","Wu, Shirley; Zhao, Shiyu; Yasunaga, Michihiro; Huang, Kexin; Cao, Kaidi; Huang, Qian; Ioannidis, Vassilis N; Subbian, Karthik; Zou, James; Leskovec, Jure","STARK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases","","","","","","Answering real-world complex queries, such as complex product search, often requires accurate retrieval from semi-structured knowledge bases that involve blend of unstructured (e.g., textual descriptions of products) and structured (e.g., entity relations of products) information. However, many previous works studied textual and relational retrieval tasks as separate topics. To address the gap, we develop STARK, a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases. Our benchmark covers three domains: product search, academic paper search, and queries in precision medicine. We design a novel pipeline to synthesize realistic user queries that integrate diverse relational information and complex textual properties, together with their ground-truth answers (items). We conduct rigorous human evaluation to validate the quality of our synthesized queries. We further enhance the benchmark with high-quality human-generated queries to provide an authentic reference. STARK serves as a comprehensive testbed for evaluating the performance of retrieval systems driven by large language models (LLMs). Our experiments suggest that STARK presents significant challenges to the current retrieval and LLM systems, highlighting the need for more capable semi-structured retrieval systems.","","2025-03-30 16:22:12","2025-03-30 16:22:12","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/V5CSQERZ/Wu et al. - STARK Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3XZVKD8R","journalArticle","","Li, Jeffrey; Fang, Alex; Smyrnis, Georgios; Ivgi, Maor; Jordan, Matt; Gadre, Samir; Bansal, Hritik; Guha, Etash; Keh, Sedrick; Arora, Kushal; Garg, Saurabh; Xin, Rui; Muennighoff, Niklas; Heckel, Reinhard; Mercat, Jean; Gururangan, Suchin; Wortsman, Mitchell; Albalak, Alon; Bitton, Yonatan; Nezhurina, Marianna; Abbas, Amro; Hsieh, Cheng-Yu; Ghosh, Dhruba; Gardner, Josh; Kilian, Maciej; Zhang, Hanlin; Shao, Rulin; Pratt, Sarah; Sanyal, Sunny; Ilharco, Gabriel; Daras, Giannis; Marathe, Kalyani; Gokaslan, Aaron; Zhang, Jieyu; Chandu, Khyathi; Nguyen, Thao; Vasiljevic, Igor; Kakade, Sham; Song, Shuran; Sanghavi, Sujay; Oh, Sewoong; Zettlemoyer, Luke; Lo, Kyle; El-Nouby, Alaaeldin; Pouransari, Hadi; Toshev, Alexander; Wang, Stephanie; Groeneveld, Dirk; Soldaini, Luca; Koh, Pang Wei; Jitsev, Jenia; Kollar, Thomas; Dimakis, Alexandros G; Carmon, Yair; Dave, Achal; Schmidt, Ludwig; Shankar, Vaishaal","DataComp-LM: In search of the next generation of training sets for language models","","","","","","We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLMBASELINE, enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-BASELINE represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6× less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation. We release the DCLM benchmark, framework, models, and datasets at https://datacomp.ai/dclm.","","2025-03-30 16:22:14","2025-03-30 16:22:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HTDC6MVW/Li et al. - DataComp-LM In search of the next generation of training sets for language models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7KVMVLT6","journalArticle","","Shangguan, Zhongkai; Huang, Zanming; Ohn-Bar, Eshed; Ozernov-Palchik, Ola; Kosty, Derek; Stoolmiller, Michael; Fien, Hank","Scalable Early Childhood Reading Performance Prediction","","","","","","Models for student reading performance can empower educators and institutions to proactively identify at-risk students, thereby enabling early and tailored instructional interventions. However, there are no suitable publicly available educational datasets for modeling and predicting future reading performance. In this work, we introduce the Enhanced Core Reading Instruction (ECRI) dataset, a novel largescale longitudinal tabular dataset collected across 44 schools with 6,916 students and 172 teachers. We leverage the dataset to empirically evaluate the ability of state-of-the-art machine learning models to recognize early childhood educational patterns in multivariate and partial measurements. Specifically, we demonstrate a simple self-supervised strategy in which a Multi-Layer Perception (MLP) network is pre-trained over masked inputs to outperform several strong baselines while generalizing over diverse educational settings. To facilitate future developments in precise modeling and responsible use of models for individualized and early intervention strategies, our data and code are available at https://ecri-data.github.io/.","","2025-03-30 16:22:16","2025-03-30 16:22:16","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HBLA5IAS/Shangguan et al. - Scalable Early Childhood Reading Performance Prediction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PH6ZDI9I","journalArticle","","Liu, Ziyu; Chu, Tao; Zang, Yuhang; Wei, Xilin; Dong, Xiaoyi; Zhang, Pan; Liang, Zijian; Xiong, Yuanjun; Qiao, Yu; Lin, Dahua; Wang, Jiaqi","MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs","","","","","","Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simplified scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately assess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce MMDU, a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs’ abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5× longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data. We demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly addresses this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA: +1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. This project is available at https://github.com/Liuziyu77/MMDU.","","2025-03-30 16:22:17","2025-03-30 16:22:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z8PEQQ2F/Liu et al. - MMDU A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVL.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FEWIIZG7","journalArticle","","Xue, Chuanyi; Liu, Qihan; Ma, Xiaoteng; Qi, Yang; Qin, Xinyao; Jiang, Yuhua; Gui, Ning; Ren, Jinsheng; Liang, Bin; Yang, Jun","NeuralPlane: An Efficiently Parallelizable Platform for Fixed-wing Aircraft Control with Reinforcement Learning","","","","","","Reinforcement learning (RL) demonstrates superior potential over traditional flight control methods for fixed-wing aircraft, particularly under extreme operational conditions. However, the high demand for training samples and the lack of efficient computation in existing simulators hinder its further application. In this paper, we introduce NeuralPlane, the first benchmark platform for large-scale parallel simulations of fixed-wing aircraft. NeuralPlane significantly boosts high-fidelity simulation via GPU-accelerated Flight Dynamics Model (FDM) computation, achieving a single-step simulation time of just 0.2 seconds at a parallel scale of 106 aircraft, far exceeding current platforms. We also provide clear code templates, comprehensive evaluation and visualization tools, and hierarchical frameworks for integrating RL and traditional control methods. We believe that NeuralPlane can accelerate the development of RL-based fixed-wing flight control and serve as a new challenging benchmark for the RL community. Our NeuralPlane is open-source and accessible at https://github.com/xuecy22/NeuralPlane.","","2025-03-30 16:22:18","2025-03-30 16:22:18","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZR92L7Q5/Xue et al. - NeuralPlane An Efficiently Parallelizable Platform for Fixed-wing Aircraft Control with Reinforceme.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCQB4TA3","journalArticle","","Geng, Haoyu; Ruan, Hang; Wang, Runzhong; Li, Yang; Wang, Yang; Chen, Lei; Yan, Junchi","Benchmarking PtO and PnO Methods in the Predictive Combinatorial Optimization Regime","","","","","","Predictive combinatorial optimization, where the parameters of combinatorial optimization (CO) are unknown at the decision-making time, is the precise modeling of many real-world applications, including energy cost-aware scheduling and budget allocation on advertising. Tackling such a problem usually involves a prediction model and a CO solver. These two modules are integrated into the predictive CO pipeline following two design principles: “Predict-then-Optimize (PtO)”, which learns predictions by supervised training and subsequently solves CO using predicted coefficients, while the other, named “Predict-and-Optimize (PnO)”, directly optimizes towards the ultimate decision quality and claims to yield better decisions than traditional PtO approaches. However, there lacks a systematic benchmark of both approaches, including the specific design choices at the module level, as well as an evaluation dataset that covers representative real-world scenarios. To this end, we develop a modular framework to benchmark 11 existing PtO/PnO methods on 8 problems, including a new industrial dataset for combinatorial advertising that will be released. Our study shows that PnO approaches are better than PtO on 7 out of 8 benchmarks, but there is no silver bullet found for the specific design choices of PnO. A comprehensive categorization of current approaches and integration of typical scenarios are provided under a unified benchmark. Therefore, this paper could serve as a comprehensive benchmark for future PnO approach development and also offer fast prototyping for application-focused development. The code is available at https://github.com/Thinklab-SJTU/PredictiveCO-Benchmark.","","2025-03-30 16:22:19","2025-03-30 16:22:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CTHY8Z3E/Geng et al. - Benchmarking PtO and PnO Methods in the Predictive Combinatorial Optimization Regime.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XEQR2JEP","journalArticle","","Wang, Ke; Pan, Junting; Shi, Weikang; Lu, Zimu; Ren, Houxing; Zhou, Aojun; Zhan, Mingjie; Li, Hongsheng","Measuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset","","","","","","","","2025-03-30 16:22:21","2025-03-30 16:22:21","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/D939U8ZE/Wang et al. - Measuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HNNF8YBY","journalArticle","","Liu, Haoxin; Xu, Shangqing; Zhao, Zhiyuan; Kong, Lingkai; Kamarthi, Harshavardhan; Sasanur, Aditya B; Sharma, Megha; Cui, Jiaming; Wen, Qingsong; Zhang, Chao; Prakash, B Aditya","Time-MMD: Multi-Domain Multimodal Dataset for Time Series Analysis","","","","","","Time series data are ubiquitous across a wide range of real-world domains. While real-world time series analysis (TSA) requires human experts to integrate numerical series data with multimodal domain-specific knowledge, most existing TSA models rely solely on numerical data, overlooking the significance of information beyond numerical series. This oversight is due to the untapped potential of textual series data and the absence of a comprehensive, high-quality multimodal dataset. To overcome this obstacle, we introduce Time-MMD, the first multi-domain, multimodal time series dataset covering 9 primary data domains. Time-MMD ensures fine-grained modality alignment, eliminates data contamination, and provides high usability. Additionally, we develop MM-TSFlib, the first-cut multimodal time-series forecasting (TSF) library, seamlessly pipelining multimodal TSF evaluations based on Time-MMD for in-depth analyses. Extensive experiments conducted on Time-MMD through MM-TSFlib demonstrate significant performance enhancements by extending unimodal TSF to multimodality, evidenced by over 15% mean squared error reduction in general, and up to 40% in domains with rich textual data. More importantly, our datasets and library revolutionize broader applications, impacts, research topics to advance TSA. The dataset is available at https://github.com/AdityaLab/Time-MMD.","","2025-03-30 16:22:22","2025-03-30 16:22:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DY47QAD4/Liu et al. - Time-MMD Multi-Domain Multimodal Dataset for Time Series Analysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PKXHW2MD","journalArticle","","Debenedetti, Edoardo; Rando, Javier; Paleka, Daniel; Fritz, Mario; Tramèr, Florian; Abdelnabi, Sahar; Schönherr, Lea","Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition","","","","","","Large language model systems face significant security risks from maliciously crafted messages that aim to overwrite the system’s original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.","","2025-03-30 16:22:23","2025-03-30 16:22:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GR57BSDE/Debenedetti et al. - Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CWBEMNCP","journalArticle","","Kil, Jihyung; Mai, Zheda; Lee, Justin; Chowdhury, Arpita; Wang, Zihe; Cheng, Kerrie; Wang, Lemeng; Liu, Ye; Chao, Wei-Lun","MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs","","","","","","The ability to compare objects, scenes, or situations is crucial for effective decisionmaking and problem-solving in everyday life. For instance, comparing the freshness of apples enables better choices during grocery shopping, while comparing sofa designs helps optimize the aesthetics of our living space. Despite its significance, the comparative capability is largely unexplored in artificial general intelligence (AGI). In this paper, we introduce MLLM-COMPBENCH, a benchmark designed to evaluate the comparative reasoning capability of multimodal large language models (MLLMs). MLLM-COMPBENCH mines and pairs images through visually oriented questions covering eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. We curate a collection of around 40K image pairs using metadata from diverse vision datasets and CLIP similarity scores. These image pairs span a broad array of visual domains, including animals, fashion, sports, and both outdoor and indoor scenes. The questions are carefully crafted to discern relative characteristics between two images and are labeled by human annotators for accuracy and relevance. We use MLLM-COMPBENCH to evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable shortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only sheds light on these limitations but also establishes a solid foundation for future enhancements in the comparative capability of MLLMs.","","2025-03-30 16:22:25","2025-03-30 16:22:25","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/88GFTE5G/Kil et al. - MLLM-COMPBENCH A Comparative Reasoning Benchmark for Multimodal LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LIX5C7ZZ","journalArticle","","Laine, Rudolf; Chughtai, Bilal; Betley, Jan; Hariharan, Kaivalya; Scheurer, Jérémy; Balesni, Mikita; Hobbhahn, Marius; Meinke, Alexander; Evans, Owain","Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs","","","","","","AI assistants such as ChatGPT are trained to respond to users by saying, “I am a large language model”. This raises questions. Do such models know that they are LLMs and reliably act on this knowledge? Are they aware of their current circumstances, such as being deployed to the public? We refer to a model’s knowledge of itself and its circumstances as situational awareness. To quantify situational awareness in LLMs, we introduce a range of behavioral tests, based on question answering and instruction following. These tests form the Situational Awareness Dataset (SAD), a benchmark comprising 7 task categories and over 13,000 questions. The benchmark tests numerous abilities, including the capacity of LLMs to (i) recognize their own generated text, (ii) predict their own behavior, (iii) determine whether a prompt is from internal evaluation or real-world deployment, and (iv) follow instructions that depend on self-knowledge.","","2025-03-30 16:22:26","2025-03-30 16:22:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VBTUKAR4/Laine et al. - Me, Myself, and AI The Situational Awareness Dataset (SAD) for LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XPP6TEE4","journalArticle","","Hesse, Robin; Schaub-Meyer, Simone; Roth, Stefan","Benchmarking the Attribution Quality of Vision Models","","","","","","","","2025-03-30 16:22:27","2025-03-30 16:22:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KVEEXFDT/Hesse et al. - Benchmarking the Attribution Quality of Vision Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QW6VNL4Q","journalArticle","","Dou, Yutao; Yu, Huimin; Li, Wei; Li, Jingyang; Xia, Fei; Xiao, Jian","PEACE: A Dataset of Pharmaceutical Care for Cancer Pain Analgesia Evaluation and Medication Decision","","","","","","","","2025-03-30 16:22:29","2025-03-30 16:22:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/S4GCQ9EB/Dou et al. - PEACE A Dataset of Pharmaceutical Care for Cancer Pain Analgesia Evaluation and Medication Decision.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9AQ7X63","journalArticle","","Long, Xingming; Zhang, Jie; Shan, Shiguang; Chen, Xilin","Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites Paradox","","","","","","Most existing out-of-distribution (OOD) detection benchmarks classify samples with novel labels as the OOD data. However, some marginal OOD samples actually have close semantic contents to the in-distribution (ID) sample, which makes determining the OOD sample a Sorites Paradox. In this paper, we construct a benchmark named Incremental Shift OOD (IS-OOD) to address the issue, in which we divide the test samples into subsets with different semantic and covariate shift degrees relative to the ID dataset. The data division is achieved through a shift measuring method based on our proposed Language Aligned Image feature Decomposition (LAID). Moreover, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains high-quality generated images with more diverse covariate contents to complement the IS-OOD benchmark. We evaluate current OOD detection methods on our benchmark and find several important insights: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods. Our code and data are released in https://github.com/qqwsad5/IS-OOD.","","2025-03-30 16:22:30","2025-03-30 16:22:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WKTDEBLQ/Long et al. - Rethinking the Evaluation of Out-of-Distribution Detection A Sorites Paradox.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AELVJCT6","journalArticle","","Zhang, Chunhui; Liu, Li; Huang, Guanjie; Wen, Hao; Zhou, Xi; Wang, Yanfeng","WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark","","","","","","Underwater Object Tracking (UOT) is essential for identifying and tracking submerged objects in underwater videos, but existing datasets are limited in scale, diversity of target categories and scenarios covered, impeding the development of advanced tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, i.e., the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, e.g., UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, e.g., underwater vision-language tracking. Given that most existing trackers are designed for open-air conditions and perform poorly in underwater environments due to domain gaps, we propose a novel framework that uses omni-knowledge distillation to train a student Transformer model effectively. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. We have thoroughly tested WebUOT-1M with 30 deep trackers, showcasing its potential as a benchmark for future UOT research. The complete dataset, along with codes and tracking results, are publicly accessible at here.","","2025-03-30 16:22:31","2025-03-30 16:22:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KFYFC3KG/Zhang et al. - WebUOT-1M Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6RC9WEV5","journalArticle","","Cheng, Guangzhao; Fu, Chengbo; Cheng, Lu","NanoBaseLib: A Multi-Task Benchmark Dataset for Nanopore Sequencing","","","","","","Nanopore sequencing is the third-generation sequencing technology with capabilities of generating long-read sequences and directly measuring modifications on DNA/RNA molecules, which makes it ideal for biological applications such as human Telomere-to-Telomere (T2T) genome assembly, Ebola virus surveillance and COVID-19 mRNA vaccine development. However, accuracies of computational methods in various tasks of Nanopore sequencing data analysis are far from satisfactory. For instance, the base calling accuracy of Nanopore RNA sequencing is ∼90%, while the aim is ∼99.9%. This highlights an urgent need of contributions from the machine learning community. A bottleneck that prevents machine learning researchers from entering this field is the lack of a large integrated benchmark dataset. To this end, we present NanoBaseLib, a comprehensive multi-task benchmark dataset. It integrates 16 public datasets with over 30 million reads for four critical tasks in Nanopore data analysis. To facilitate method development, we have preprocessed all the raw data using a uniform workflow, stored all the intermediate results in uniform formats, analysed test datasets with various baseline methods for four benchmark tasks, and developed a software package to easily access these results. NanoBaseLib is available at https://nanobaselib.github.io.","","2025-03-30 16:22:32","2025-03-30 16:22:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SNU6X7VY/Cheng et al. - NanoBaseLib A Multi-Task Benchmark Dataset for Nanopore Sequencing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6JPAGCB","journalArticle","","Zhang, Jiawen; Wen, Xumeng; Zhang, Zhenwei; Zheng, Shun; Li, Jia; Bian, Jiang","ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons","","","","","","","","2025-03-30 16:22:34","2025-03-30 16:22:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/75SJVQ7F/Zhang et al. - ProbTS Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KVF2AJ8E","journalArticle","","Li, Wei; Bishop, William; Li, Alice; Rawles, Chris; Campbell-Ajala, Folawiyo; Tyamagundlu, Divya; Riva, Oriana","On the Effects of Data Scale on UI Control Agents","","","","","","Autonomous agents that control user interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless ﬁne-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether ﬁne-tuning alone is a viable approach for building real-world UI control agents. To this end we collect and release a new dataset, ANDROIDCONTROL, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each ANDROIDCONTROL task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, ANDROIDCONTROL is the most diverse UI control dataset to date, including 14,548 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we ﬁnd that when tested in domain ﬁne-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales signiﬁcantly more slowly and suggests that in particular for high-level tasks, ﬁne-tuning on more data alone may be insufﬁcient for achieving robust out-of-domain performance.","","2025-03-30 16:22:35","2025-03-30 16:22:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/C39ML7LL/Li et al. - On the Effects of Data Scale on UI Control Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EIIZK9MQ","journalArticle","","Wang, Yuxin; Feng, Duanyu; Dai, Yongfu; Chen, Zhengyu; Huang, Jimin; Ananiadou, Sophia; Xie, Qianqian; Wang, Hao","HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection","","","","","","Data serves as the fundamental basis for advancing deep learning. The tabular data presented in a structured format is highly valuable for modeling and training. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Therefore, exploring the methods for effectively using models like LLMs to generate synthetic tabular data, which is privacy-preserving but similar to original one, is urgent. In this paper, we introduce a new framework HARMONIC for tabular data generation and evaluation by LLMs. In the data generation part of our framework, we employ fine-tuning to generate tabular data and enhance privacy rather than continued pre-training which is often used by previous small-scale LLM-based methods. In particular, we construct an instruction fine-tuning dataset based on the idea of the k-nearest neighbors algorithm to inspire LLMs to discover inter-row relationships. By such fine-tuning, LLMs are trained to remember the format and connections of the data rather than the data itself, which reduces the risk of privacy leakage. The experiments find that our tabular data generation achieves equivalent performance as existing methods but with better privacy by the metric of MLE, DCR, etc. In the evaluation part of our framework, we develop a specific privacy risk metric DLT for LLM synthetic data generation, which quantifies the extent to which the generator itself leaks data. We also developed LLE, a performance evaluation metric for downstream LLM tasks, which is more practical and credible than previous metrics. The experiments show that our data generation method outperform the previous methods in the metrics DLT and LLE. ∗Co-Corresponding Author.","","2025-03-30 16:22:36","2025-03-30 16:22:36","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/L33UHGME/Wang et al. - HARMONIC Harnessing LLMs for Tabular Data Synthesis and Privacy Protection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYAIW46P","journalArticle","","Wu, Nemin; Cao, Qian; Wang, Zhangyu; Liu, Zeping; Qi, Yanlin; Zhang, Jielu; Ni, Joshua; Yao, Xiaobai; Ma, Hongxu; Mu, Lan; Ermon, Stefano; Ganu, Tanuja; Nambi, Akshay; Lao, Ni; Mai, Gengchen","TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning","","","","","","Spatial representation learning (SRL) aims at learning general-purpose neural network representations from various types of spatial data (e.g., points, polylines, polygons, networks, images, etc.) in their native formats. Learning good spatial representations is a fundamental problem for various downstream applications such as species distribution modeling, weather forecasting, trajectory generation, geographic question answering, etc. Even though SRL has become the foundation of almost all geospatial artificial intelligence (GeoAI) research, we have not yet seen significant efforts to develop an extensive deep learning framework and benchmark to support SRL model development and evaluation. To fill this gap, we propose TorchSpatial, a learning framework and benchmark for location (point) encoding, which is one of the most fundamental data types of spatial representation learning. TorchSpatial contains three key components: 1) a unified location encoding framework that consolidates 15 commonly recognized location encoders, ensuring scalability and reproducibility of the implementations; 2) the LocBench benchmark tasks encompassing 7 geo-aware image classification and 10 geo-aware image regression datasets; 3) a comprehensive suite of evaluation metrics to quantify geo-aware models’ overall performance as well as their geographic bias, with a novel Geo-Bias Score metric. Finally, we provide a detailed analysis and insights into the model performance and geographic bias of different location encoders. We believe TorchSpatial will foster future advancement of spatial representation learning and spatial fairness in GeoAI research. The TorchSpatial model framework and LocBench benchmark are available at https://github.com/seai-lab/ TorchSpatial, and the Geo-Bias Score evaluation framework is available at https://github.com/seai-lab/PyGBS.","","2025-03-30 16:22:37","2025-03-30 16:22:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KDEFI87F/Wu et al. - TorchSpatial A Location Encoding Framework and Benchmark for Spatial Representation Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYR9IB7A","journalArticle","","Tang, Xiaojuan; Li, Jiaqi; Liang, Yitao; Zhu, Song-chun; Zhang, Muhan; Zheng, Zilong","Mars: Situated Inductive Reasoning in an Open-World Environment","","","","","","Large Language Models (LLMs) trained on massive corpora have shown remarkable success in knowledge-intensive tasks. Yet, most of them rely on pre-stored knowledge. Inducing new general knowledge from a speciﬁc environment and performing reasoning with the acquired knowledge—situated inductive reasoning, is crucial and challenging for machine intelligence. In this paper, we design Mars, an interactive environment devised for situated inductive reasoning. It introduces counter-commonsense game mechanisms by modifying terrain, survival setting and task dependency while adhering to certain principles. In Mars, agents need to actively interact with their surroundings, derive useful rules and perform decision-making tasks in speciﬁc contexts. We conduct experiments on various RL-based and LLM-based methods, ﬁnding that they all struggle on this challenging situated inductive reasoning benchmark. Furthermore, we explore Induction from Reﬂection, where we instruct agents to perform inductive reasoning from history trajectory. The superior performance underscores the importance of inductive reasoning in Mars. Through Mars, we aim to galvanize advancements in situated inductive reasoning and set the stage for developing the next generation of AI systems that can reason in an adaptive and context-sensitive way.","","2025-03-30 16:22:39","2025-03-30 16:22:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3JTZ6VW4/Tang et al. - Mars Situated Inductive Reasoning in an Open-World Environment.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X2DRVVQW","journalArticle","","Magnusson, Ian; Bhagia, Akshita; Hofmann, Valentin; Soldaini, Luca; Jha, Ananya Harsh; Tafjord, Oyvind; Schwenk, Dustin; Walsh, Evan Pete; Elazar, Yanai; Lo, Kyle; Groeneveld, Dirk; Beltagy, Iz; Hajishirzi, Hannaneh; Smith, Noah A; Richardson, Kyle; Dodge, Jesse","Paloma : A Benchmark for Evaluating Language Model Fit","","","","","","Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains—varying distributions of language. We introduce PERPLEXITY ANALYSIS FOR LANGUAGE MODEL ASSESSMENT (PALOMA)1, a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., r/depression on Reddit) and programming languages (e.g., Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from PALOMA surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.","","2025-03-30 16:22:40","2025-03-30 16:22:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/N4NX2RKQ/Magnusson et al. - Paloma  A Benchmark for Evaluating Language Model Fit.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TN9CYH65","journalArticle","","Su, Kefan; Huo, Yusen; Zhang, Zhilin; Dou, Shuai; Yu, Chuan; Xu, Jian; Lu, Zongqing; Zheng, Bo","AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games","","","","","","Decision-making in large-scale games is an essential research area in artificial intelligence (AI) with significant real-world impact. However, the limited access to realistic large-scale game environments has hindered research progress in this area. In this paper, we present AuctionNet, a benchmark for bid decision-making in largescale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of several baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions through the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse autobidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic Generalized Second Price (GSP) auction but also allows for customization of auction mechanisms as needed. To facilitate research and provide insights into the environment, we have also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet. AuctionNet has powered the NeurIPS 2024 Auto-Bidding in Large-Scale Auctions competition, providing competition environments for over 1,500 teams. We believe that AuctionNet is applicable not only to research on bid decision-making in ad auctions but also to the general area of decision-making in large-scale games. Code3: https://github.com/alimama-tech/AuctionNet.","","2025-03-30 16:22:42","2025-03-30 16:22:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DC5V68HV/Su et al. - AuctionNet A Novel Benchmark for Decision-Making in Large-Scale Games.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZTC9QUBJ","journalArticle","","Panchal, Sunny; Bhattacharyya, Apratim; Berger, Guillaume; Mercier, Antoine; Böhm, Cornelius; Dietrichkeit, Florian; Pourreza, Reza; Li, Xuanlin; Madan, Pulkit; Lee, Mingu; Todorovich, Mark; Bax, Ingo; Memisevic, Roland","What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction","","","","","","Vision-language models have shown impressive progress in recent years. However, existing models are largely limited to turn-based interactions, where each turn must be stepped (i.e., prompted) by the user. Open-ended, asynchronous interactions, where an AI model may proactively deliver timely responses or feedback based on the unfolding situation in real-time, are an open challenge. In this work, we present the QEVD benchmark and dataset, which explores human-AI interaction in the challenging, yet controlled, real-world domain of fitness coaching – a task which intrinsically requires monitoring live user activity and providing immediate feedback. The benchmark requires vision-language models to recognize complex human actions, identify possible mistakes, and provide appropriate feedback in real-time. Our experiments reveal the limitations of existing state-of-the-art vision-language models for such asynchronous situated interactions. Motivated by this, we propose a simple end-to-end streaming baseline that can respond asynchronously to human actions with appropriate feedback at the appropriate time.","","2025-03-30 16:22:43","2025-03-30 16:22:43","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/REE68QSK/Panchal et al. - What to Say and When to Say it Live Fitness Coaching as a Testbed for Situated Interaction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AA9XQBWC","journalArticle","","Wang, Weiyun; Zhang, Shuibo; Ren, Yiming; Duan, Yuchen; Li, Tiantong; Liu, Shuo; Hu, Mengkang; Chen, Zhe; Zhang, Kaipeng; Lu, Lewei; Zhu, Xizhou; Luo, Ping; Qiao, Yu; Dai, Jifeng; Shao, Wenqi; Wang, Wenhai","Needle In A Multimodal Haystack","","","","","","With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.","","2025-03-30 16:22:44","2025-03-30 16:22:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/U65JCQ7J/Wang et al. - Needle In A Multimodal Haystack.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I2HD9LV3","journalArticle","","Xiong, Tianwei; Wang, Yuqing; Zhou, Daquan; Lin, Zhijie; Feng, Jiashi; Liu, Xihui","LVD-2M: A Long-take Video Dataset with Temporally Dense Captions","","","","","","","","2025-03-30 16:22:46","2025-03-30 16:22:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FXAUCFXV/Xiong et al. - LVD-2M A Long-take Video Dataset with Temporally Dense Captions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P34I6Z4P","journalArticle","","Nathaniel, Juan; Qu, Yongquan; Nguyen, Tung; Yu, Sungduk; Busecke, Julius; Grover, Aditya; Gentine, Pierre","ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction","","","","","","Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster preparedness and robust decision making amidst climate change. Yet, forecasting beyond the weather timescale is challenging because it deals with problems other than initial condition, including boundary interaction, butterfly effect, and our inherent lack of physical understanding. At present, existing benchmarks tend to have shorter forecasting range of up-to 15 days, do not include a wide range of operational baselines, and lack physics-based constraints for explainability. Thus, we propose ChaosBench, a challenging benchmark to extend the predictability range of data-driven weather emulators to S2S timescale. First, ChaosBench is comprised of variables beyond the typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that span over 45 years to allow for full Earth system emulation that respects boundary conditions. We also propose physics-based, in addition to deterministic and probabilistic metrics, to ensure a physically-consistent ensemble that accounts for butterfly effect. Furthermore, we evaluate on a diverse set of physics-based forecasts from four national weather agencies as baselines to our data-driven counterpart such as ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find methods originally developed for weather-scale applications fail on S2S task: their performance simply collapse to an unskilled climatology. Nonetheless, we outline and demonstrate several strategies that can extend the predictability range of existing weather emulators, including the use of ensembles, robust control of error propagation, and the use of physics-informed models. Our benchmark, datasets, and instructions are available at https://leap-stc.github.io/ChaosBench.","","2025-03-30 16:22:47","2025-03-30 16:22:47","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UZZ3XI3A/Nathaniel et al. - ChaosBench A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W2VFNPR9","journalArticle","","Miao, Yibo; Zhu, Yifan; Yu, Lijia; Zhu, Jun; Gao, Xiao-Shan; Dong, Yinpeng","T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models","","","","","","The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its safety risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover limited aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, the first comprehensive benchmark for conducting safety-critical assessments of textto-video models. We define 4 primary categories with 14 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts, and jailbreak attack-based prompts. We then conduct a thorough safety evaluation on 9 recently released T2V models. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AIs. Our code is publicly available at https://github.com/yibo-miao/T2VSafetyBench.","","2025-03-30 16:22:49","2025-03-30 16:22:49","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PVVHBU7U/Miao et al. - T2VSafetyBench Evaluating the Safety of Text-to-Video Generative Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTQ9I677","journalArticle","","Ahmed, Mahmoud; Li, Xiang; Prajapati, Arpit; Elhoseiny, Mohamed","3DCoMPaT200: Language-Grounded Compositional Understanding of Parts and Materials of 3D Shapes","","","","","","","","2025-03-30 16:22:50","2025-03-30 16:22:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/W6MBJHPP/Ahmed et al. - 3DCoMPaT200 Language-Grounded Compositional Understanding of Parts and Materials of 3D Shapes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q77BJSK3","journalArticle","","Zhao, Dora; Scheuerman, Morgan Klaus; Chitre, Pooja; Andrews, Jerone T A","A Taxonomy of Challenges to Curating Fair Datasets","","","","","","Despite extensive efforts to create fairer machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation. Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle. Our findings underscore overarching issues within the broader fairness landscape that impact data curation. We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices.","","2025-03-30 16:22:51","2025-03-30 16:22:51","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NJTWGL6F/Zhao et al. - A Taxonomy of Challenges to Curating Fair Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBQNSYZL","journalArticle","","Madan, Anish; Peri, Neehar; Kong, Shu; Ramanan, Deva","Revisiting Few-Shot Object Detection with Vision-Language Models","","","","","","The era of vision-language models (VLMs) trained on web-scale datasets challenges conventional formulations of “open-world"" perception. In this work, we revisit the task of few-shot object detection (FSOD) in the context of recent foundational VLMs. First, we point out that zero-shot predictions from VLMs such as GroundingDINO signiﬁcantly outperform state-of-the-art few-shot detectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance, such foundation models may still be sub-optimal. For example, trucks on the web may be deﬁned differently from trucks for a target application such as autonomous vehicle perception. We argue that the task of few-shot recognition can be reformulated as aligning foundation models to target concepts using a few examples. Interestingly, such examples can be multi-modal, using both text and visual cues, mimicking instructions that are often given to human annotators when deﬁning a target concept of interest. Concretely, we propose Foundational FSOD, a new benchmark protocol that evaluates detectors pre-trained on any external data and ﬁne-tuned on multi-modal (text and visual) K-shot examples per target class. We repurpose nuImages for Foundational FSOD, benchmark several popular open-source VLMs, and provide an empirical analysis of state-of-the-art methods. Lastly, we discuss our recent CVPR 2024 Foundational FSOD competition and share insights from the community. Notably, the winning team signiﬁcantly outperforms our baseline by 23.3 mAP! Our code and dataset splits are available on GitHub and HuggingFace.","","2025-03-30 16:22:52","2025-03-30 16:22:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5YRWXC4P/Madan et al. - Revisiting Few-Shot Object Detection with Vision-Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ZYL2ZAC","journalArticle","","Tsuruta, Hirofumi; Yamazaki, Hiroyuki; Maeda, Ryota; Tamura, Ryotaro; Imura, Akihiro","A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models","","","","","","Antibodies are crucial proteins produced by the immune system to eliminate harmful foreign substances and have become pivotal therapeutic agents for treating human diseases. To accelerate the discovery of antibody therapeutics, there is growing interest in constructing language models using antibody sequences. However, the applicability of pre-trained language models for antibody discovery has not been thoroughly evaluated due to the scarcity of labeled datasets. To overcome these limitations, we introduce AVIDa-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins. AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARSCoV-2 mutants, such as the Delta and Omicron variants. Furthermore, we release VHHCorpus-2M, a pre-training dataset for antibody language models, containing over two million VHH sequences. We report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-speciﬁc pre-trained language models. These results conﬁrm that AVIDa-SARS-CoV-2 provides valuable benchmarks for evaluating the representation capabilities of antibody language models for binding prediction, thereby facilitating the development of AI-driven antibody discovery. The datasets are available at https://datasets.cognanous.com.","","2025-03-30 16:22:54","2025-03-30 16:22:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IQNUPDR5/Tsuruta et al. - A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ALSQWKXT","journalArticle","","Yang, Xiao; Sun, Kai; Xin, Hao; Sun, Yushi; Bhalla, Nikita; Chen, Xiangsen; Gui, Rongze Daniel; Jiang, Ziran Will; Jiang, Ziyu; Kong, Lingkun; Moran, Brian; Wang, Jiaqi; Xu, Yifan Ethan; Yan, An; Yang, Chenyu; Yuan, Eting; Zha, Hanwen; Tang, Nan; Chen, Lei; Scheffer, Nicolas; Liu, Yue; Shah, Nirav; Wanga, Rakesh; Kumar, Anuj; Yih, Wen-tau; Dong, Xin Luna","CRAG – Comprehensive RAG Benchmark","","","","","","Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation of this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve ď 34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% of questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of participants and submissions. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions. CRAG is available at https://github.com/facebookresearch/CRAG/.","","2025-03-30 16:22:55","2025-03-30 16:22:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BABDQWXR/Yang et al. - CRAG – Comprehensive RAG Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPJFSUC4","journalArticle","","Hollidt, Dominik; Streli, Paul; Jiang, Jiaxi; Haghighi, Yasaman; Qian, Changlin; Liu, Xintong; Holz, Christian","EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity","","","","","","Research on egocentric tasks in computer vision has mostly focused on headmounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets. We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations. This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition—particularly for the lower body, which is typically occluded.","","2025-03-30 16:22:56","2025-03-30 16:22:56","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4LWTTP2C/Hollidt et al. - EgoSim An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZFCKKZM","journalArticle","","Ao, Junyi; Wang, Yuancheng; Tian, Xiaohai; Chen, Dekun; Zhang, Jun; Lu, Lu; Wang, Yuxuan; Li, Haizhou; Wu, Zhizheng","SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words","","","","","","Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information. This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction. Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech. Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses. We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation. SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound. To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a process similar to that of SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. We also conduct a comprehensive evaluation using objective evaluation methods (e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses. Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures. Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics. We open-source SD-Eval at https://github.com/amphionspace/SD-Eval.","","2025-03-30 16:22:57","2025-03-30 16:22:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EBNZAT69/Ao et al. - SD-Eval A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GM58P5RW","journalArticle","","Wang, Christopher; Yaari, Adam; Singh, Aaditya K; Subramaniam, Vighnesh; Rosenfarb, Dana; DeWitt, Jan; Misra, Pranav; Madsen, Joseph R; Stone, Scellig; Kreiman, Gabriel; Katz, Boris; Cases, Ignacio; Barbu, Andrei","Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli","","","","","","","","2025-03-30 16:22:58","2025-03-30 16:22:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DG33A7MC/Wang et al. - Brain Treebank Large-scale intracranial recordings from naturalistic language stimuli.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YNMBKDRF","journalArticle","","Dai, Juntao; Chen, Tianle; Wang, Xuyao; Yang, Ziran; Chen, Taiye; Ji, Jiaming; Yang, Yaodong","SAFESORA: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset","","","","","","To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the SAFESORA dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The SAFESORA dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the SAFESORA dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms. Our project is available at https://sites.google.com/view/safe-sora.","","2025-03-30 16:23:00","2025-03-30 16:23:00","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2NNDM32J/Dai et al. - SAFESORA Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAKN4YIK","journalArticle","","Ashton, Neil; Angel, Jordan B; Ghate, Aditya S; Kenway, Gaetan K W","WindsorML: High-Fidelity Computational Fluid Dynamics Dataset For Automotive Aerodynamics","","","","","","This paper presents a new open-source high-fidelity dataset for Machine Learning (ML) containing 355 geometric variants of the Windsor body, to help the development and testing of ML surrogate models for external automotive aerodynamics. Each Computational Fluid Dynamics (CFD) simulation was run with a GPU-native high-fidelity Wall-Modeled Large-Eddy Simulations (WMLES) using a Cartesian immersed-boundary method using more than 280M cells to ensure the greatest possible accuracy. The dataset contains geometry variants that exhibits a wide range of flow characteristics that are representative of those observed on road-cars. The dataset itself contains the 3D time-averaged volume & boundary data as well as the geometry and force & moment coefficients. This paper discusses the validation of the underlying CFD methods as well as contents and structure of the dataset. To the authors knowledge, this represents the first, large-scale high-fidelity CFD dataset for the Windsor body with a permissive open-source license (CC-BY-SA).","","2025-03-30 16:23:01","2025-03-30 16:23:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DSAJCJI2/Ashton et al. - WindsorML High-Fidelity Computational Fluid Dynamics Dataset For Automotive Aerodynamics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E893SYR9","journalArticle","","Dong, Zibin; Yuan, Yifu; Hao, Jianye; Ni, Fei; Ma, Yi; Li, Pengyi; Zheng, Yan","CleanDiffuser: An Easy-to-use Modularized Library for Diffusion Models in Decision Making","","","","","","Leveraging the powerful generative capability of diffusion models (DMs) to build decision-making agents has achieved extensive success. However, there is still a demand for an easy-to-use and modularized open-source library that offers customized and efficient development for DM-based decision-making algorithms. In this work, we introduce CleanDiffuser, the first DM library specifically designed for decision-making algorithms. By revisiting the roles of DMs in the decisionmaking domain, we identify a set of essential sub-modules that constitute the core of CleanDiffuser, allowing for the implementation of various DM algorithms with simple and flexible building blocks. To demonstrate the reliability and flexibility of CleanDiffuser, we conduct comprehensive evaluations of various DM algorithms implemented with CleanDiffuser across an extensive range of tasks. The analytical experiments provide a wealth of valuable design choices and insights, reveal opportunities and challenges, and lay a solid groundwork for future research. CleanDiffuser will provide long-term support to the decision-making community, enhancing reproducibility and fostering the development of more robust solutions. The code and documentation of CleanDiffuser are open-sourced on the project website.","","2025-03-30 16:23:03","2025-03-30 16:23:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/LK92SB3R/Dong et al. - CleanDiffuser An Easy-to-use Modularized Library for Diffusion Models in Decision Making.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9687XZ9A","journalArticle","","Lekkala, Kiran; Cai, Peixu; Lim, Wei Zer; Liu, Chen; Itti, Laurent","USCILab3D: A Large-scale, Long-term, Semantically Annotated Outdoor Dataset","","","","","","In this paper, we introduce the USCILab3D dataset, a large-scale, annotated outdoor dataset designed for versatile applications across multiple domains, including computer vision, robotics, and machine learning. The dataset was acquired using a mobile robot equipped with 5 cameras and a 32-beam, 360◦ scanning LIDAR. The robot was teleoperated, over the course of a year and under a variety of weather and lighting conditions, through a rich variety of paths within the USC campus (229 acres = ∼ 92.7 hectares). The raw data was annotated using state-of-theart large foundation models, and processed to provide multi-view imagery, 3D reconstructions, semantically-annotated images and point clouds (267 semantic categories), and text descriptions of images and objects within. The dataset also offers a diverse array of complex analyses using pose-stamping and trajectory data. In sum, the dataset offers 1.4M point clouds and 10M images (∼ 6TB of data). Despite covering a narrower geographical scope compared to a whole-city dataset, our dataset prioritizes intricate intersections along with denser multi-view scene images and semantic point clouds, enabling more precise 3D labelling and facilitating a broader spectrum of 3D vision tasks. For data, code and more details, please visit our website.","","2025-03-30 16:23:04","2025-03-30 16:23:04","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JJBHG9LZ/Lekkala et al. - USCILab3D A Large-scale, Long-term, Semantically Annotated Outdoor Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBUY2PVD","journalArticle","","Robinson, Joshua; Ranjan, Rishabh; Hu, Weihua; Huang, Kexin; Han, Jiaqi; Dobles, Alejandro; Fey, Matthias; Lenssen, Jan E; Yuan, Yiwen; Zhang, Zecheng; He, Xinwei; Leskovec, Jure","RELBENCH: A Benchmark for Deep Learning on Relational Databases","","","","","","We present RELBENCH, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RELBENCH provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. We use RELBENCH to conduct the ﬁrst comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a signiﬁcant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, we conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RELBENCH.","","2025-03-30 16:23:06","2025-03-30 16:23:06","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JY8UJQL4/Robinson et al. - RELBENCH A Benchmark for Deep Learning on Relational Databases.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELCP3KTG","journalArticle","","Wang, Zengzhi; Li, Xuefeng; Xia, Rui; Liu, Pengfei","MATHPILE : A Billion-Token-Scale Pre-training Corpus for Math","","","","","","High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MATHPILE , a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of “less is more”, firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MATHPILE to boost language models’ mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field (available at https://github.com/GAIR-NLP/MathPile/).","","2025-03-30 16:23:07","2025-03-30 16:23:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TFBWVWQW/Wang et al. - MATHPILE  A Billion-Token-Scale Pre-training Corpus for Math.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTTIAM4G","journalArticle","","Arevalo, John; Su, Ellen; Carpenter, Anne E; Singh, Shantanu","MOTIVE: A Drug-Target Interaction Graph For Inductive Link Prediction","","","","","","Drug-target interaction (DTI) prediction is crucial for identifying new therapeutics and detecting mechanisms of action. While structure-based methods accurately model physical interactions between a drug and its protein target, cell-based assays such as Cell Painting can better capture complex DTI interactions. This paper introduces MOTIVE, a Morphological cOmpound Target Interaction Graph dataset comprising Cell Painting features for 11, 000 genes and 3, 600 compounds, along with their relationships extracted from seven publicly available databases. We provide random, cold-source (new drugs), and cold-target (new genes) data splits to enable rigorous evaluation under realistic use cases. Our benchmark results show that graph neural networks that use Cell Painting features consistently outperform those that learn from graph structure alone, feature-based models, and topological heuristics. MOTIVE accelerates both graph ML research and drug discovery by promoting the development of more reliable DTI prediction models. MOTIVE resources are available at https://github.com/carpenter-singh-lab/motive.","","2025-03-30 16:23:08","2025-03-30 16:23:08","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Q5I748PN/Arevalo et al. - MOTIVE A Drug-Target Interaction Graph For Inductive Link Prediction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"92L5PX3Q","journalArticle","","Deng, Hexuan; Jiao, Wenxiang; Liu, Xuebo; Zhang, Min; Tu, Zhaopeng","NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates","","","","","","Despite their remarkable abilities in various tasks, large language models (LLMs) still struggle with real-time information (e.g., new facts and terms) due to the knowledge cutoff in their development process. However, existing benchmarks focus on outdated content and limited fields, facing difficulties in real-time updating and leaving new terms unexplored. To address this problem, we propose an adaptive benchmark, NewTerm, for real-time evaluation of new terms. We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort, allowing flexible updates for real-time information. Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms. Additionally, while updates to the knowledge cutoff of LLMs can cover some of the new terms, they are unable to generalize to more distant new terms. We also analyze which types of terms are more challenging and why LLMs struggle with new terms, paving the way for future research. Finally, we construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually. The benchmark and codes can be found at https://github.com/hexuandeng/NewTerm.","","2025-03-30 16:23:10","2025-03-30 16:23:10","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WCSI5RZA/Deng et al. - NewTerm Benchmarking Real-Time New Terms for Large Language Models with Annual Updates.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q9K6CKXP","journalArticle","","Chen, Chaochao; Zhang, Jiaming; Zhang, Yizhao; Zhang, Li; Lyu, Lingjuan; Li, Yuyuan; Gong, Biao; Yan, Chenggang","CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper Inﬂuence","","","","","","With increasing privacy concerns in artiﬁcial intelligence, regulations have mandated the right to be forgotten, granting individuals the right to withdraw their data from models. Machine unlearning has emerged as a potential solution to enable selective forgetting in models, particularly in recommender systems where historical data contains sensitive user information. Despite recent advances in recommendation unlearning, evaluating unlearning methods comprehensively remains challenging due to the absence of a uniﬁed evaluation framework and overlooked aspects of deeper inﬂuence, e.g., fairness. To address these gaps, we propose CURE4Rec, the ﬁrst comprehensive benchmark for recommendation unlearning evaluation. CURE4Rec covers four aspects, i.e., unlearning Completeness, recommendation Utility, unleaRning efﬁciency, and recommendation fairnEss, under three data selection strategies, i.e., core data, edge data, and random data. Speciﬁcally, we consider the deeper inﬂuence of unlearning on recommendation fairness and robustness towards data with varying impact levels. We construct multiple datasets with CURE4Rec evaluation and conduct extensive experiments on existing recommendation unlearning methods. Our code is released at https://github.com/xiye7lai/CURE4Rec.","","2025-03-30 16:23:11","2025-03-30 16:23:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YCKH88VE/Chen et al. - CURE4Rec A Benchmark for Recommendation Unlearning with Deeper Inﬂuence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3AEWWJUV","journalArticle","","Tschalzev, Andrej; Marton, Sascha; Lüdtke, Stefan; Bartelt, Christian; Stuckenschmidt, Heiner","A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data","","","","","","Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing the performance of models typically consist of model-centric evaluation setups with overly standardized data preprocessing. This paper demonstrates that such model-centric evaluations are biased, as real-world modeling pipelines often require dataset-specific preprocessing, which includes feature engineering. Therefore, we propose a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings are: 1. After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. 2. Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. 3. While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics. Our framework is available under: https://github.com/atschalz/dc_tabeval.","","2025-03-30 16:23:12","2025-03-30 16:23:12","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WWYV9SMR/Tschalzev et al. - A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D38SLIPD","journalArticle","","Longjohn, Rachel; Kelly, Markelle; Singh, Sameer; Smyth, Padhraic","Benchmark Data Repositories for Better Benchmarking","","","","","","In machine learning research, it is common to evaluate algorithms via their performance on standard benchmark datasets. While a growing body of work establishes guidelines for—and levies criticisms at—data and benchmarking practices in machine learning, comparatively less attention has been paid to the data repositories where these datasets are stored, documented, and shared. In this paper, we analyze the landscape of these benchmark data repositories and the role they can play in improving benchmarking. This role includes addressing issues with both datasets themselves (e.g., representational harms, construct validity) and the manner in which evaluation is carried out using such datasets (e.g., overemphasis on a few datasets and metrics, lack of reproducibility). To this end, we identify and discuss a set of considerations surrounding the design and use of benchmark data repositories, with a focus on improving benchmarking practices in machine learning.","","2025-03-30 16:23:14","2025-03-30 16:23:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WARIGQNJ/Longjohn et al. - Benchmark Data Repositories for Better Benchmarking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DSAWWSWC","journalArticle","","Bonnen, Tyler; Fu, Stephanie; Bai, Yutong; O’Connell, Thomas; Friedman, Yoni; Kanwisher, Nancy; Tenenbaum, Joshua B; Efros, Alexei A","Evaluating Multiview Object Consistency in Humans and Image Models","","","","","","We introduce a benchmark to directly evaluate the alignment between human observers and vision models on a 3D shape inference task. We leverage an experimental design from the cognitive sciences: given a set of images, participants identify which contain the same/different objects, despite considerable viewpoint variation. We draw from a diverse range of images that include common objects (e.g., chairs) as well as abstract shapes (i.e., procedurally generated ‘nonsense’ objects). After constructing over 2000 unique image sets, we administer these tasks to human participants, collecting 35K trials of behavioral data from over 500 participants. This includes explicit choice behaviors as well as intermediate measures, such as reaction time and gaze data. We then evaluate the performance of common vision models (e.g., DINOv2, MAE, CLIP). We find that humans outperform all models by a wide margin. Using a multi-scale evaluation approach, we identify underlying similarities and differences between models and humans: while human-model performance is correlated, humans allocate more time/processing on challenging trials. All images, data, and code can be accessed via our project page.","","2025-03-30 16:23:15","2025-03-30 16:23:15","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DYREYSG6/Bonnen et al. - Evaluating Multiview Object Consistency in Humans and Image Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGK3D3KT","journalArticle","","Barsellotti, Luca; Bigazzi, Roberto; Cornia, Marcella; Baraldi, Lorenzo; Cucchiara, Rita","Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments","","","","","","In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.","","2025-03-30 16:23:16","2025-03-30 16:23:16","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VIGUEU8D/Barsellotti et al. - Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PAFZ4943","journalArticle","","Tsesmelis, Theodore; Palmieri, Luca; Khoroshiltseva, Marina; Islam, Adeela; Elkin, Gur; Alali, Nadav; Aslan, Sinem; Morerio, Pietro; Vascon, Sebastiano; Gravina, Elena; Napolitano, Maria Cristina; Scarpati, Giuseppe; Zuchtriegel, Gabriel; Spühler, Alexandra; Fuchs, Michel E; James, Stuart; Ben-Shahar, Ohad; Pelillo, Marcello; Bue, Alessio Del","Re-assembling the past: The RePAIR dataset and benchmark for real world 2D and 3D puzzle solving","","","","","","This paper proposes the RePAIR dataset that represents a challenging benchmark to test modern computational and data driven methods for puzzle-solving and reassembly tasks. Our dataset has unique properties that are uncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and fractures are realistic, caused by a collapse of a fresco during a World War II bombing at the Pompeii archaeological park. The fragments are also eroded and have missing pieces with irregular shapes and different dimensions, challenging further the reassembly algorithms. The dataset is multi-modal providing high resolution images with characteristic pictorial elements, detailed 3D scans of the fragments and metadata annotated by the archaeologists. Ground truth has been generated through several years of unceasing eldwork, including the excavation and cleaning of each fragment, followed by manual puzzle solving by archaeologists of a subset of approx. 1000 pieces among the 16000 available. After digitizing all the fragments in 3D, a benchmark was prepared to challenge current reassembly and puzzlesolving methods that often solve more simplistic synthetic scenarios. The tested baselines show that there clearly exists a gap to ll in solving this computationally complex problem.","","2025-03-30 16:23:18","2025-03-30 16:23:18","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/I3ZX7P2C/Tsesmelis et al. - Re-assembling the past The RePAIR dataset and benchmark for real world 2D and 3D puzzle solving.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MN2JVT9W","journalArticle","","Khrabrov, Kuzma; Ber, Anton; Tsypin, Artem; Ushenin, Konstantin; Rumiantsev, Egor; Telepov, Alexander; Protasov, Dmitry; Shenbin, Ilya; Alekseev, Anton; Shirokikh, Mikhail; Nikolenko, Sergey; Tutubalina, Elena; Kadurin, Artur","∇2DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials","","","","","","Methods of computational quantum chemistry provide accurate approximations of molecular properties crucial for computer-aided drug discovery and other areas of chemical science. However, high computational complexity limits the scalability of their applications. Neural network potentials (NNPs) are a promising alternative to quantum chemistry methods, but they require large and diverse datasets for training. This work presents a new dataset and benchmark called ∇2DFT that is based on the nablaDFT. It contains twice as much molecular structures, three times more conformations, new data types and tasks, and state-of-the-art models. The dataset includes energies, forces, 17 molecular properties, Hamiltonian and overlap matrices, and a wavefunction object. All calculations were performed at the DFT level (ωB97X-D/def2-SVP) for each conformation. Moreover, ∇2DFT is the first dataset that contains relaxation trajectories for a substantial number of drug-like molecules. We also introduce a novel benchmark for evaluating NNPs in molecular property prediction, Hamiltonian prediction, and conformational optimization tasks. Finally, we propose an extendable framework for training NNPs and implement 10 models within it.","","2025-03-30 16:23:19","2025-03-30 16:23:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6Y73TSIK/Khrabrov et al. - ∇2DFT A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Netwo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MD568B6","journalArticle","","Yun, Sukmin; Lin, Haokun; Thushara, Rusiru; Bhat, Mohammad Qazim; Wang, Yongxin; Jiang, Zutao; Deng, Mingkai; Wang, Jinhong; Tao, Tianhua; Li, Junbo; Li, Haonan; Nakov, Preslav; Baldwin, Timothy; Liu, Zhengzhong; Xing, Eric P; Liang, Xiaodan; Shen, Zhiqiang","Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs","","","","","","Multimodal large language models (MLLMs) have shown impressive success across modalities such as image, video, and audio in a variety of understanding and generation tasks. However, current MLLMs are surprisingly poor at understanding webpage screenshots and generating their corresponding HTML code. To address this problem, we propose Web2Code, a benchmark consisting of a new large-scale webpage-to-code dataset for instruction tuning and an evaluation framework for the webpage understanding and HTML code translation abilities of MLLMs. For dataset construction, we leverage pretrained LLMs to enhance existing webpageto-code datasets as well as generate a diverse pool of new webpages rendered into images. Specifically, the inputs are webpage images and instructions, while the responses are the webpage’s HTML code. We further include diverse natural language QA pairs about the webpage content in the responses to enable a more comprehensive understanding of the web content. To evaluate model performance in these tasks, we develop an evaluation framework for testing MLLMs’ abilities in webpage understanding and web-to-code generation. Extensive experiments show that our proposed dataset is beneficial not only to our proposed tasks but also in the general visual domain. We hope our work will contribute to the development of general MLLMs suitable for web-based content generation and task automation. Our data and code are available at https://github.com/MBZUAI-LLM/web2code.","","2025-03-30 16:23:21","2025-03-30 16:23:21","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/U3M5TQBJ/Yun et al. - Web2Code A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDAWF2E7","journalArticle","","Wang, Yiheng; Wang, Tianyu; Zhang, Yuying; Zhang, Hongji; Zheng, Haoyu; Zheng, Guanjie; Kong, Linghe","UrbanDataLayer: A Unified Data Pipeline for Urban Science","","","","","","","","2025-03-30 16:23:22","2025-03-30 16:23:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/A79EMBIQ/Wang et al. - UrbanDataLayer A Unified Data Pipeline for Urban Science.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RCQPKDDH","journalArticle","","Wu, Xian; Zhao, Yutian; Zhang, Yunyan; Wu, Jiageng; Zhu, Zhihong; Zhang, Yingying; Ouyang, Yi; Zhang, Ziheng; Wang, Huimin; Lin, Zhenxi; Yang, Jie; Zhao, Shuang; Zheng, Yefeng","MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey","","","","","","Large language models (LLMs) have demonstrated remarkable capabilities in language understanding and generation, leading to their widespread adoption across various fields. Among these, the medical field is particularly well-suited for LLM applications, as many medical tasks can be enhanced by LLMs. Despite the existence of benchmarks for evaluating LLMs in medical question-answering and exams, there remains a notable gap in assessing LLMs’ performance in supporting patients throughout their entire hospital visit journey in real-world clinical practice. In this paper, we address this gap by dividing a typical patient’s clinical journey into four stages: planning, access, delivery and ongoing care. For each stage, we introduce multiple tasks and corresponding datasets, resulting in a comprehensive benchmark comprising 12 datasets, of which five are newly introduced, and seven are constructed from existing datasets. This proposed benchmark facilitates a thorough evaluation of LLMs’ effectiveness across the entire patient journey, providing insights into their practical application in clinical settings. Additionally, we evaluate three categories of LLMs against this benchmark: 1) proprietary LLM services such as GPT-4; 2) public LLMs like QWen; and 3) specialized medical LLMs, like HuatuoGPT2. Through this extensive evaluation, we aim to provide a better understanding of LLMs’ performance in the medical domain, ultimately contributing to their more effective deployment in healthcare settings.","","2025-03-30 16:23:24","2025-03-30 16:23:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TBQE6F8N/Wu et al. - MedJourney Benchmark and Evaluation of Large Language Models over Patient Clinical Journey.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"INQ2ZU57","journalArticle","","Bortolotti, Samuele; Marconato, Emanuele; Carraro, Tommaso; Morettin, Paolo; van Krieken, Emile; Vergari, Antonio; Teso, Stefano; Passerini, Andrea","A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts","","","","","","The advent of powerful neural classiﬁers has increased interest in problems that require both learning and reasoning. These problems are critical for understanding important properties of models, such as trustworthiness, generalization, interpretability, and compliance to safety and structural constraints. However, recent research observed that tasks requiring both learning and reasoning on background knowledge often suffer from reasoning shortcuts (RSs): predictors can solve the downstream reasoning task without associating the correct concepts to the highdimensional data. To address this issue, we introduce rsbench, a comprehensive benchmark suite designed to systematically evaluate the impact of RSs on models by providing easy access to highly customizable tasks affected by RSs. Furthermore, rsbench implements common metrics for evaluating concept quality and introduces novel formal veriﬁcation procedures for assessing the presence of RSs in learning tasks. Using rsbench, we highlight that obtaining high quality concepts in both purely neural and neuro-symbolic models is a far-from-solved problem. rsbench is available at: https://unitn-sml.github.io/rsbench. neurosymbolicisbooljuice.","","2025-03-30 16:23:25","2025-03-30 16:23:25","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7Z2YIZP7/Bortolotti et al. - A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P3QN4YMX","journalArticle","","Lin, Wang; Feng, Yueying; Han, Wenkang; Jin, Tao; Zhao, Zhou; Wu, Fei; Yao, Chang; Chen, Jingyuan","E3: Exploring Embodied Emotion Through A Large-Scale Egocentric Video Dataset","","","","","","Understanding human emotions is fundamental to enhancing human-computer interaction, especially for embodied agents that mimic human behavior. Traditional emotion analysis often takes a third-person perspective, limiting the ability of agents to interact naturally and empathetically. To address this gap, this paper presents E3 for Exploring Embodied Emotion, the ﬁrst massive ﬁrst-person view video dataset. E3 contains more than 50 hours of video, capturing 8 different emotion types in diverse scenarios and languages. The dataset features videos recorded by individuals in their daily lives, capturing a wide range of real-world emotions conveyed through visual, acoustic, and textual modalities. By leveraging this dataset, we deﬁne 4 core benchmark tasks - emotion recognition, emotion classiﬁcation, emotion localization, and emotion reasoning - supported by more than 80k manually crafted annotations, providing a comprehensive resource for training and evaluating emotion analysis models. We further present Emotion-LlaMa, which complements visual modality with acoustic modality to enhance the understanding of emotion in ﬁrst-person videos. The results of comparison experiments with a large number of baselines demonstrate the superiority of Emotion-LlaMa and set a new benchmark for embodied emotion analysis. We expect that E3 can promote advances in multimodal understanding, robotics, and augmented reality, and provide a solid foundation for the development of more empathetic and context-aware embodied agents. Project page: https://exploring-embodied-emotion-ofﬁcial.github.io.","","2025-03-30 16:23:27","2025-03-30 16:23:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DPAZ8KB8/Lin et al. - E3 Exploring Embodied Emotion Through A Large-Scale Egocentric Video Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6L5BYH3","journalArticle","","Li, Jia; Li, Ge; Zhang, Xuanming; Zhao, Yunfei; Dong, Yihong; Jin, Zhi; Li, Binhua; Huang, Fei; Li, Yongbin","EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations","","","","","","How to evaluate Large Language Models (LLMs) in code generation remains an open question. Many benchmarks have been proposed, but they have two limitations, i.e., data leakage and lack of domain-specific evaluation. The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains.","","2025-03-30 16:23:29","2025-03-30 16:23:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AEC7LQSU/Li et al. - EvoCodeBench An Evolving Code Generation Benchmark with Domain-Specific Evaluations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZUHQD693","journalArticle","","Liu, Qinghua; Paparrizos, John","The Elephant in the Room: Towards A Reliable Time-Series Anomaly Detection Benchmark","","","","","","Time-series anomaly detection is a fundamental task across scientific fields and industries. However, the field has long faced the “elephant in the room:” critical issues including flawed datasets, biased evaluation measures, and inconsistent benchmarking practices that have remained largely ignored and unaddressed. We introduce the TSB-AD to systematically tackle these issues in the following three aspects: (i) Dataset Integrity: with 1070 high-quality time series from a diverse collection of 40 datasets (doubling the size of the largest collection and four times the number of existing curated datasets), we provide the first large-scale, heterogeneous, meticulously curated dataset that combines the effort of human perception and model interpretation; (ii) Measure Reliability: by revealing issues and biases in evaluation measures, we identify the most reliable and accurate measure, namely, VUS-PR for anomaly detection in time series to address concerns from the community; and (iii) Comprehensive Benchmarking: with a broad spectrum of 40 detection algorithms, from statistical methods to the latest foundation models, we perform a comprehensive evaluation that includes a thorough hyperparameter tuning and a unified setup for a fair and reproducible comparison. Our findings challenge the conventional wisdom regarding the superiority of advanced neural network architectures, revealing that simpler architectures and statistical methods often yield better performance. The promising performance of neural networks on multivariate cases and foundation models on point anomalies highlights the need for further advancements in these methods. We open-source the benchmark at https://github.com/TheDatumOrg/TSB-AD to promote further research.","","2025-03-30 16:23:30","2025-03-30 16:23:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VYQ43S4J/Liu and Paparrizos - The Elephant in the Room Towards A Reliable Time-Series Anomaly Detection Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWRSJPWK","journalArticle","","Ju, Xuan; Gao, Yiming; Zhang, Zhaoyang; Yuan, Ziyang; Wang, Xintao; Zeng, Ailing; Xiong, Yu; Xu, Qiang; Shan, Ying","MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions","","","","","","","","2025-03-30 16:23:32","2025-03-30 16:23:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AIKE75JH/Ju et al. - MiraData A Large-Scale Video Dataset with Long Durations and Structured Captions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K8A9N9GE","journalArticle","","Repasky, Boris; Dick, Anthony; Abbasnejad, Ehsan","BLURD: Benchmarking and Learning using a Unified Rendering and Diffusion Model","","","","","","Recent advancements in pre-trained vision models have made them pivotal in computer vision, emphasizing the need for their thorough evaluation and benchmarking. This evaluation needs to consider various factors of variation, their potential biases, shortcuts, and inaccuracies that ultimately lead to disparate performance in models. Such evaluations are conventionally done using either synthetic data from 2D or 3D rendering software or real-world images in controlled settings. Synthetic methods offer full control and flexibility, while real-world methods are limited by high costs and less adaptability. Moreover, 3D rendering can’t yet fully replicate real photography, creating a realism gap. In this paper, we introduce BLURD–Benchmarking and Learning using a Unified Rendering and Diffusion Model–a novel method combining 3D rendering and Stable Diffusion to bridge this gap in representation learning. With BLURD we create a new family of datasets that allow for the creation of both 3D rendered and photo-realistic images with identical factors. BLURD, therefore, provides deeper insights into the representations learned by various CLIP backbones. The source code for creating the BLURD datasets is available at https://github.com/squaringTheCircle/BLURD.","","2025-03-30 16:23:33","2025-03-30 16:23:33","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DIVBNU26/Repasky et al. - BLURD Benchmarking and Learning using a Unified Rendering and Diffusion Model.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ER9RPDLU","journalArticle","","Huang, Yizhe; Wang, Xingbo; Liu, Hao; Kong, Fanqi; Qin, Aoyang; Tang, Min; Zhu, Song-Chun; Bi, Mingjie; Qi, Siyuan; Feng, Xue","AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making","","","","","","Traditional interactive environments limit agents’ intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.","","2025-03-30 16:23:35","2025-03-30 16:23:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QRRVZMZE/Huang et al. - AdaSociety An Adaptive Environment with Social Structures for Multi-Agent Decision-Making.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MLC49LWG","journalArticle","","Wei, Chenrui; Sun, Mengzhou; Wang, Wei","Proving Olympiad Algebraic Inequalities without Human Demonstrations","","","","","","Solving Olympiad-level mathematical problems represents a significant advancement in machine intelligence and automated reasoning. Current machine learning methods, however, struggle to solve Olympiad-level problems beyond Euclidean plane geometry due to a lack of large-scale, high-quality datasets. The challenge is even greater in algebraic systems, which involve infinite reasoning spaces within finite conditions. To address these issues, we propose AIPS, an Algebraic Inequality Proving System capable of autonomously generating complex inequality theorems and effectively solving Olympiad-level inequality problems without requiring human demonstrations. During proof search in a mixed reasoning manner, a value curriculum learning strategy on generated datasets is implemented to improve proving performance, demonstrating strong mathematical intuitions. On a test set of 20 International Mathematical Olympiad-level inequality problems, AIPS successfully solved 10, outperforming state-of-the-art methods. Furthermore, AIPS automatically generated a vast array of non-trivial theorems without human intervention, some of which have been evaluated by professional contestants and deemed to reach the level of the International Mathematical Olympiad. Notably, one theorem was selected as a competition problem in a major city’s 2024 Mathematical Olympiad. All the materials are available at sites.google.com/view/aips2.","","2025-03-30 16:23:36","2025-03-30 16:23:36","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/H8NQDR3I/Wei et al. - Proving Olympiad Algebraic Inequalities without Human Demonstrations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ILWCDUNG","journalArticle","","Bushuiev, Roman; Bushuiev, Anton; de Jonge, Niek F; Young, Adamo; Kretschmer, Fleming; Samusevich, Raman; Heirman, Janne; Wang, Fei; Zhang, Luke; Dührkop, Kai; Ludwig, Marcus; Haupt, Nils A; Kalia, Apurva; Brungs, Corinna; Schmid, Robin; Greiner, Russell; Wang, Bo; Wishart, David S; Liu, Li-Ping; Rousu, Juho; Bittremieux, Wout; Rost, Hannes; Mak, Tytus D; Hassoun, Soha; Huber, Florian; Böcker, Sebastian; Sivic, Josef; Pluskal, Tomáš","MassSpecGym: A benchmark for the discovery and identification of molecules","","","","","","The discovery and identification of molecules in biological and environmental samples is crucial for advancing biomedical and chemical sciences. Tandem mass spectrometry (MS/MS) is the leading technique for high-throughput elucidation of molecular structures. However, decoding a molecular structure from its mass spectrum is exceptionally challenging, even when performed by human experts. As a result, the vast majority of acquired MS/MS spectra remain uninterpreted, thereby limiting our understanding of the underlying (bio)chemical processes. Despite decades of progress in machine learning applications for predicting molecular structures from MS/MS spectra, the development of new methods is severely hindered by the lack of standard datasets and evaluation protocols. To address this problem, we propose MassSpecGym – the first comprehensive benchmark for the discovery and identification of molecules from MS/MS data. Our benchmark comprises the largest publicly available collection of high-quality labeled MS/MS spectra and defines three MS/MS annotation challenges: de novo molecular structure generation, molecule retrieval, and spectrum simulation. It includes new evaluation metrics and a generalization-demanding data split, therefore standardizing the MS/MS annotation tasks and rendering the problem accessible to the broad machine learning community. MassSpecGym is publicly available at https://github.com/pluskal-lab/MassSpecGym.","","2025-03-30 16:23:37","2025-03-30 16:23:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/D4P7J3GB/Bushuiev et al. - MassSpecGym A benchmark for the discovery and identification of molecules.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZQ3IJMX","journalArticle","","Ying, Huaiyuan; Wu, Zijian; Geng, Yihan; Wang, Jiayu; Lin, Dahua; Chen, Kai","Lean Workbook: A large-scale Lean problem set formalized from natural language math problems","","","","","","Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github. com/InternLM/InternLM-Math and our data at https://huggingface.co/ datasets/InternLM/Lean-Workbook.","","2025-03-30 16:23:39","2025-03-30 16:23:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZZCNG254/Ying et al. - Lean Workbook A large-scale Lean problem set formalized from natural language math problems.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLIIYINJ","journalArticle","","Wu, Zhenbang; Dadu, Anant; Nalls, Mike; Faghri, Faraz; Sun, Jimeng","Instruction Tuning Large Language Models to Understand Electronic Health Records","","","","","","Large language models (LLMs) have shown impressive capabilities in solving a wide range of tasks based on human instructions. However, developing a conversational AI assistant for electronic health record (EHR) data remains challenging due to (1) the lack of large-scale instruction-following datasets and (2) the limitations of existing model architectures in handling complex and heterogeneous EHR data. In this paper, we introduce MIMIC-Instr, a dataset comprising over 400K open-ended instruction-following examples derived from the MIMIC-IV EHR database. This dataset covers various topics and is suitable for instructiontuning general-purpose LLMs for diverse clinical use cases. Additionally, we propose Llemr, a general framework that enables LLMs to process and interpret EHRs with complex data structures. Llemr demonstrates competitive performance in answering a wide range of patient-related questions based on EHR data. Furthermore, our evaluations on clinical predictive modeling benchmarks reveal that the fine-tuned Llemr achieves performance comparable to state-of-the-art (SOTA) baselines using curated features. The dataset and code are available at https://github.com/zzachw/llemr.","","2025-03-30 16:23:40","2025-03-30 16:23:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BCMXEUAS/Wu et al. - Instruction Tuning Large Language Models to Understand Electronic Health Records.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HWA8PUPY","journalArticle","","Dong, Ha; Wang, Xin; Moretti, Rocco; Wang, Yu; Su, Zhaoqian; Gu, Jiawei; Bodenheimer, Bobby; Weaver, Charles David; Meiler, Jens; Derr, Tyler","WelQrate: Defining the Gold Standard in Small Molecule Drug Discovery Benchmarking","","","","","","While deep learning has revolutionized computer-aided drug discovery, the AI community has predominantly focused on model innovation and placed less emphasis on establishing best benchmarking practices. We posit that without a sound model evaluation framework, the AI community’s efforts cannot reach their full potential, thereby slowing the progress and transfer of innovation into real-world drug discovery. Thus, in this paper, we seek to establish a new gold standard for small molecule drug discovery benchmarking, WelQrate. Specifically, our contributions are threefold: WelQrate dataset collection - we introduce a meticulously curated collection of 9 datasets spanning 5 therapeutic target classes. Our hierarchical curation pipelines, designed by drug discovery experts, go beyond the primary high-throughput screen by leveraging additional confirmatory and counter screens along with rigorous domain-driven preprocessing, such as PanAssay Interference Compounds (PAINS) filtering, to ensure the high-quality data in the datasets; WelQrate Evaluation Framework - we propose a standardized model evaluation framework considering high-quality datasets, featurization, 3D conformation generation, evaluation metrics, and data splits, which provides a reliable benchmarking for drug discovery experts conducting real-world virtual screening; Benchmarking - we evaluate model performance through various research questions using the WelQrate dataset collection, exploring the effects of different models, dataset quality, featurization methods, and data splitting strategies on the results. In summary, we recommend adopting our proposed WelQrate as the gold standard in small molecule drug discovery benchmarking. The WelQrate dataset collection, along with the curation codes, and experimental scripts are all publicly available at WelQrate.org.","","2025-03-30 16:23:41","2025-03-30 16:23:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AGXR8I5Z/Dong et al. - WelQrate Defining the Gold Standard in Small Molecule Drug Discovery Benchmarking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"96S9687M","journalArticle","","Nikitin, Alexander; Iannucci, Letizia; Kaski, Samuel","TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series","","","","","","","","2025-03-30 16:23:42","2025-03-30 16:23:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HDXD29C7/Nikitin et al. - TSGM A Flexible Framework for Generative Modeling of Synthetic Time Series.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JET3ZCN4","journalArticle","","Qiu, Tianyi; Zhang, Yang; Huang, Xuchuan; Li, Jasmine Xinze; Ji, Jiaming; Yang, Yaodong","ProgressGym: Alignment with a Millennium of Moral Progress","","","","","","Frontier AI systems, including large language models (LLMs), hold increasing inﬂuence over the epistemology of human users. Such inﬂuence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. We introduce progress alignment as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. To empower research in progress alignment, we introduce ProgressGym,4 an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 historical LLMs,5 ProgressGym enables codiﬁcation of real-world progress alignment challenges into concrete benchmarks. Speciﬁcally, we introduce three core challenges: tracking evolving values (PG-Follow), preemptively anticipating moral progress (PG-Predict), and regulating the feedback loop between human and AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension are inapplicable to these tasks. In response, we present lifelong and extrapolative algorithms as baseline methods of progress alignment, and build an open leaderboard6 soliciting novel algorithms and challenges.","","2025-03-30 16:23:44","2025-03-30 16:23:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/W8W6XICQ/Qiu et al. - ProgressGym Alignment with a Millennium of Moral Progress.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LF22YML3","journalArticle","","Castillo-Bolado, David; Davidson, Joseph; Gray, Finlay; Rosa, Marek","Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models","","","","","","We introduce a dynamic benchmarking system for conversational agents that evaluates their performance through a single, simulated, and lengthy user↔agent interaction. The interaction is a conversation between the user and agent, where multiple tasks are introduced and then undertaken concurrently. We context switch regularly to interleave the tasks, which constructs a realistic testing scenario in which we assess the Long-Term Memory, Continual Learning, and Information Integration capabilities of the agents. Results from both proprietary and open-source Large-Language Models show that LLMs in general perform well on single-task interactions, but they struggle on the same tasks when they are interleaved. Notably, short-context LLMs supplemented with an LTM system perform as well as or better than those with larger contexts. Our benchmark suggests that there are other challenges for LLMs responding to more natural interactions that contemporary benchmarks have heretofore not been able to capture.","","2025-03-30 16:23:45","2025-03-30 16:23:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UTH9XE8A/Castillo-Bolado et al. - Beyond Prompts Dynamic Conversational Benchmarking of Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3QQMQ95K","journalArticle","","Hu, Dapeng; Luo, Mi; Liang, Jian; Foo, Chuan-Sheng","Towards Reliable Model Selection for Unsupervised Domain Adaptation: An Empirical Study and A Certified Baseline","","","","","","Selecting appropriate hyperparameters is crucial for unlocking the full potential of advanced unsupervised domain adaptation (UDA) methods in unlabeled target domains. Although this challenge remains under-explored, it has recently garnered increasing attention with the proposals of various model selection methods. Reliable model selection should maintain performance across diverse UDA methods and scenarios, especially avoiding highly risky worst-case selections—selecting the model or hyperparameter with the worst performance in the pool. Are existing model selection methods reliable and versatile enough for different UDA tasks? In this paper, we provide a comprehensive empirical study involving 8 existing model selection approaches to answer this question. Our evaluation spans 12 UDA methods across 5 diverse UDA benchmarks and 5 popular UDA scenarios. Surprisingly, we find that none of these approaches can effectively avoid the worst-case selection. In contrast, a simple but overlooked ensemble-based selection approach, which we call EnsV, is both theoretically and empirically certified to avoid the worst-case selection, ensuring high reliability. Additionally, EnsV is versatile for various practical but challenging UDA scenarios, including validation of open-partial-set UDA and source-free UDA. Finally, we call for more attention to the reliability of model selection in UDA: avoiding the worst-case is as significant as achieving peak selection performance and should not be overlooked when developing new model selection methods. Code is available at https://github.com/LHXXHB/EnsV.","","2025-03-30 16:23:47","2025-03-30 16:23:47","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/98J2DI7A/Hu et al. - Towards Reliable Model Selection for Unsupervised Domain Adaptation An Empirical Study and A Certif.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IB97QJF","journalArticle","2014","Universitat Politècnica De València, Editorial","Universitat Politècnica de València","Ingeniería del agua","","1886-4996, 1134-2196","10.4995/ia.2014.3293","http://polipapers.upv.es/index.php/IA/article/view/3293","","2014-09-29","2025-03-30 16:23:51","2025-03-30 16:23:51","2025-03-30 16:23:51","ix","","1","18","","ing.agua","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AXB3ZZGS/Universitat Politècnica De València - 2014 - Universitat Politècnica de València.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8YXD3SNG","journalArticle","","Wang, Zhonghao; Sun, Danyu; Zhou, Sheng; Wang, Haobo; Fan, Jiapei; Huang, Longtao; Bu, Jiajun","NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise","","","","","","Graph Neural Networks (GNNs) exhibit strong potential in node classification tasks through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.","","2025-03-30 16:23:52","2025-03-30 16:23:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IPI6QDT5/Wang et al. - NoisyGL A Comprehensive Benchmark for Graph Neural Networks under Label Noise.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9WL5SQL","journalArticle","","Gupta, Rohan; Arcuschin, Iván; Kwa, Thomas; Garriga-Alonso, Adrià","INTERPBENCH: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques","","","","","","Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents INTERPBENCH, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model’s output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr’s original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.","","2025-03-30 16:23:53","2025-03-30 16:23:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UC54TSUV/Gupta et al. - INTERPBENCH Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IRWM8ASC","journalArticle","","Roberts, Josselin Somerville; Lee, Tony; Wong, Chi Heem","Image2Struct: Benchmarking Structure Extraction for Vision-Language Models","","","","","","We introduce Image2Struct, a benchmark to evaluate vision-language models (VLMs) on extracting structure from images. Our benchmark 1) captures realworld use cases, 2) is fully automatic and does not require human judgment, and 3) is based on a renewable stream of fresh data. In Image2Struct, VLMs are prompted to generate the underlying structure (e.g., LaTeX code or HTML) from an input image (e.g., webpage screenshot). The structure is then rendered to produce an output image (e.g., rendered webpage), which is compared against the input image to produce a similarity score. This round-trip evaluation allows us to quantitatively evaluate VLMs on tasks with multiple valid structures. We create a pipeline that downloads fresh data from active online communities upon execution and evaluates the VLMs without human intervention. We introduce three domains (Webpages, LaTeX, and Musical Scores) and use five image metrics (pixel similarity, cosine similarity between the Inception vectors, learned perceptual image patch similarity, structural similarity index measure, and earth mover similarity) that allow efficient and automatic comparison between pairs of images. We evaluate Image2Struct on 14 prominent VLMs and find that scores vary widely, indicating that Image2Struct can differentiate between the performances of different VLMs. Additionally, the best score varies considerably across domains (e.g., 0.402 on sheet music vs. 0.830 on LaTeX equations), indicating that Image2Struct contains tasks of varying difficulty. For transparency, we release the full results at https: //crfm.stanford.edu/helm/image2struct/v1.0.1/.","","2025-03-30 16:23:55","2025-03-30 16:23:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AF6JKCC8/Roberts et al. - Image2Struct Benchmarking Structure Extraction for Vision-Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QN2FII9Z","journalArticle","","Madan, Spandan; Xiao, Will; Cao, Mingran","Benchmarking Out-of-Distribution Generalization Capabilities of DNN-based Encoding Models for the Ventral Visual Cortex.","","","","","","We characterized the generalization capabilities of deep neural network encoding models when predicting neuronal responses from the visual cortex to flashed images. We collected MacaqueITBench, a large-scale dataset of neuronal population responses from the macaque inferior temporal (IT) cortex to over 300, 000 images, comprising 8, 233 unique natural images presented to seven monkeys over 109 sessions. Using MacaqueITBench, we investigated the impact of distribution shifts on models predicting neuronal activity by dividing the images into OutOf-Distribution (OOD) train and test splits. The OOD splits included variations in image contrast, hue, intensity, temperature, and saturation. Compared to the performance on in-distribution test images—the conventional way in which these models have been evaluated—models performed worse at predicting neuronal responses to out-of-distribution images, retaining as little as 20% of the performance on in-distribution test images. Additionally, the relative ranking of different models in terms of their ability to predict neuronal responses changed drastically across OOD shifts. The generalization performance under OOD shifts can be well accounted by a simple image similarity metric—the cosine distance between image representations extracted from a pre-trained object recognition model is a strong predictor of neuronal predictivity under different distribution shifts. The dataset of images, neuronal firing rate recordings, and computational benchmarks are hosted publicly at: MacaqueITBench Link.","","2025-03-30 16:23:57","2025-03-30 16:23:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QQKQTHG5/Madan et al. - Benchmarking Out-of-Distribution Generalization Capabilities of DNN-based Encoding Models for the Ve.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UN9EXMM3","journalArticle","","Press, Ori; Hochlehnert, Andreas; Prabhu, Ameya; Udandarao, Vishaal; Press, Ofir; Bethge, Matthias","CiteME: Can Language Models Accurately Cite Scientific Claims?","","","","","","Thousands of new scientific papers are published each month. Such information overload complicates researcher efforts to stay current with the state-of-the-art as well as to verify and correctly attribute claims. We pose the following research question: Given a text excerpt referencing a paper, could an LM act as a research assistant to correctly identify the referenced paper? We advance efforts to answer this question by building a benchmark that evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME, consists of text excerpts from recent machine learning papers, each referencing a single other paper. CiteME use reveals a large gap between frontier LMs and human performance, with LMs achieving only 4.218.5% accuracy and humans 69.7%. We close this gap by introducing CiteAgent, an autonomous system built on the GPT-4o LM that can also search and read papers, which achieves an accuracy of 35.3% on CiteME. Overall, CiteME serves as a challenging testbed for open-ended claim attribution, driving the research community towards a future where any claim made by an LM can be automatically verified and discarded if found to be incorrect.","","2025-03-30 16:23:58","2025-03-30 16:23:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SHEFCZC9/Press et al. - CiteME Can Language Models Accurately Cite Scientific Claims.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XW4RF49","journalArticle","","Jansen, Peter; Côté, Marc-Alexandre; Khot, Tushar; Bransom, Erin; Mishra, Bhavana Dalvi; Majumder, Bodhisattwa Prasad; Tafjord, Oyvind; Clark, Peter","DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents","","","","","","","","2025-03-30 16:23:59","2025-03-30 16:23:59","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TIPZAAXH/Jansen et al. - DISCOVERYWORLD A Virtual Environment for Developing and Evaluating Automated Scientific Discovery A.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IKFT77ZD","journalArticle","","Haresh, Sanjay; Dijkman, Daniel; Bhattacharyya, Apratim; Memisevic, Roland","ClevrSkills: Compositional Language and Visual Reasoning in Robotics","","","","","","Robotics tasks are highly compositional by nature. For example, to perform a high-level task like cleaning the table a robot must employ low-level capabilities of moving the effectors to the objects on the table, pick them up and then move them off the table one-by-one, while re-evaluating the consequently dynamic scenario in the process. Given that large vision language models (VLMs) have shown progress on many tasks that require high level, human-like reasoning, we ask the question: if the models are taught the requisite low-level capabilities, can they compose them in novel ways to achieve interesting high-level tasks like cleaning the table without having to be explicitly taught so? To this end, we present ClevrSkills - a benchmark suite for compositional reasoning in robotics. ClevrSkills is an environment suite developed on top of the ManiSkill2 [16] simulator and an accompanying dataset. The dataset contains trajectories generated on a range of robotics tasks with language and visual annotations as well as multi-modal prompts as task specification. The suite includes a curriculum of tasks with three levels of compositional understanding, starting with simple tasks requiring basic motor skills. We benchmark multiple different VLM baselines on ClevrSkills and show that even after being pre-trained on large numbers of tasks, these models fail on compositional reasoning in robotics tasks.","","2025-03-30 16:24:01","2025-03-30 16:24:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ECSD4RRS/Haresh et al. - ClevrSkills Compositional Language and Visual Reasoning in Robotics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43WXZ2EG","preprint","2024","Peterson, Ralph E; Tanelus, Aramis; Ick, Christopher; Mimica, Bartul; Francis, Niegil; Ivan, Violet J; Choudhri, Aman; Falkner, Annegret L; Murthy, Mala; Schneider, David M; Sanes, Dan H; Williams, Alex H","Vocal Call Locator Benchmark (VCL) for localizing rodent vocalizations from multi-channel audio","","","","10.1101/2024.09.20.613758","http://biorxiv.org/lookup/doi/10.1101/2024.09.20.613758","Understanding the behavioral and neural dynamics of social interactions is a goal of contemporary neuroscience. Many machine learning methods have emerged in recent years to make sense of complex video and neurophysiological data that result from these experiments. Less focus has been placed on understanding how animals process acoustic information, including social vocalizations. A critical step to bridge this gap is determining the senders and receivers of acoustic information in social interactions. While sound source localization (SSL) is a classic problem in signal processing, existing approaches are limited in their ability to localize animal-generated sounds in standard laboratory environments. Advances in deep learning methods for SSL are likely to help address these limitations, however there are currently no publicly available models, datasets, or benchmarks to systematically evaluate SSL algorithms in the domain of bioacoustics. Here, we present the VCL Benchmark: the ﬁrst large-scale dataset for benchmarking SSL algorithms in rodents. We acquired synchronized video and multi-channel audio recordings of 767,295 sounds with annotated ground truth sources across 9 conditions. The dataset provides benchmarks which evaluate SSL performance on real data, simulated acoustic data, and a mixture of real and simulated data. We intend for this benchmark to facilitate knowledge transfer between the neuroscience and acoustic machine learning communities, which have had limited overlap.","2024-09-21","2025-03-30 16:24:03","2025-03-30 16:24:03","2025-03-30 16:24:03","","","","","","","","","","","","","","en","http://creativecommons.org/licenses/by/4.0/","","","","Neuroscience","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JLLQPGMU/Peterson et al. - 2024 - Vocal Call Locator Benchmark (VCL) for localizing rodent vocalizations from multi-channel audio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPTAM2Z4","journalArticle","","Kazemi, Mehran; Dikkala, Nishanth; Anand, Ankit; Devic, Petar; Dasgupta, Ishita; Liu, Fangyu; Fatemi, Bahare; Awasthi, Pranjal; Guo, Dee; Gollapudi, Sreenivas; Qureshi, Ahmed","ReMI: A Dataset for Reasoning with Multiple Images","","","","","","With the continuous advancement of large language models (LLMs), it is essential to create new benchmarks to effectively evaluate their expanding capabilities and identify areas for improvement. This work focuses on multi-image reasoning, an emerging capability in state-of-the-art LLMs. We introduce ReMI, a dataset designed to assess LLMs’ ability to Reason with Multiple Images. This dataset encompasses a diverse range of tasks, spanning various reasoning domains such as math, physics, logic, code, table/chart understanding, and spatial and temporal reasoning. It also covers a broad spectrum of characteristics found in multi-image reasoning scenarios. We have benchmarked several cutting-edge LLMs using ReMI and found a substantial gap between their performance and human-level proﬁciency. This highlights the challenges in multi-image reasoning and the need for further research. Our analysis also reveals the strengths and weaknesses of different models, shedding light on the types of reasoning that are currently attainable and areas where future models require improvement. To foster further research in this area, we are open-sourcing ReMI: https://huggingface.co/datasets/ mehrankazemi/ReMI.","","2025-03-30 16:24:04","2025-03-30 16:24:04","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/F3XGAIKK/Kazemi et al. - ReMI A Dataset for Reasoning with Multiple Images.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VP443UBI","journalArticle","","Turishcheva, Polina; Fahey, Paul G; Vystrcˇilová, Michaela; Hansel, Laura; Froebe, Rachel; Ponder, Kayla; Qiu, Yongrong; Willeke, Konstantin F; Bashiri, Mohammad; Baikulov, Ruslan; Zhu, Yu; Ma, Lei; Yu, Shan; Huang, Tiejun; Li, Bryan M; Wulf, Wolf De; Kudryashova, Nina; Hennig, Matthias H; Rochefort, Nathalie L; Onken, Arno; Wang, Eric; Ding, Zhiwei; Tolias, Andreas S; Sinz, Fabian H; Ecker, Alexander S","Retrospective for the Dynamic Sensorium Competition for predicting large-scale mouse primary visual cortex activity from videos","","","","","","Understanding how biological visual systems process information is challenging because of the nonlinear relationship between visual input and neuronal responses. Artiﬁcial neural networks allow computational neuroscientists to create predictive models that connect biological and machine vision. Machine learning has beneﬁted tremendously from benchmarks that compare different models on the same task under standardized conditions. However, there was no standardized benchmark to identify state-of-the-art dynamic models of the mouse visual system. To address this gap, we established the SENSORIUM 2023 Benchmark Competition with dynamic input, featuring a new large-scale dataset from the primary visual cortex of ten mice. This dataset includes responses from 78,853 neurons to 2 hours of dynamic stimuli per neuron, together with behavioral measurements such as running speed, pupil dilation, and eye movements. The competition ranked models in two tracks based on predictive performance for neuronal responses on a held-out test set: one focusing on predicting in-domain natural stimuli and another on out-of-distribution (OOD) stimuli to assess model generalization. As part of the NeurIPS 2023 Competition Track, we received more than 160 model submissions from 22 teams. Several new architectures for predictive models were proposed, and the winning teams improved the previous state-of-the-art model by 50%. Access to the dataset as well as the benchmarking infrastructure will remain online at www.sensorium-competition.net.","","2025-03-30 16:24:05","2025-03-30 16:24:06","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YBHZ4NPX/Turishcheva et al. - Retrospective for the Dynamic Sensorium Competition for predicting large-scale mouse primary visual.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M3NQX3ZJ","journalArticle","","Akhbari, Bardiya; Gawali, Manish; Dronen, Nicholas A","SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models","","","","","","Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs’ algorithmic abilities under simple lexical or semantic variations. To this end, we present the SETLEXSEM CHALLENGE, a synthetic benchmark that evaluates the performance of LLMs on set operations. SETLEXSEM assesses the robustness of LLMs’ instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SETLEXSEM, we find that they exhibit poor robustness to variation in both operation and operands. We show – via the framework’s systematic sampling of set members along lexical and semantic dimensions – that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of ""deceptive"" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them independently. The code for reproducing the results of this paper, and for generating the SETLEXSEM CHALLENGE dataset, is available at https://github.com/amazon-science/SetLexSem-Challenge.","","2025-03-30 16:24:07","2025-03-30 16:24:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MJJ55Z5V/Akhbari et al. - SETLEXSEM CHALLENGE Using Set Operations to Evaluate the Lexical and Semantic Robustness of Languag.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5VH8EBM","journalArticle","","Wang, Yuli; Peng, Jian; Dai, Yuwei; Jones, Craig; Sair, Haris; Shen, Jinglai; Loizou, Nicolas; Wu, Jing; Hsu, Wen-Chi; Imami, Maliha; Jiao, Zhicheng; Zhang, Paul; Bai, Harrison","Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection","","","","","","Recent approaches to vision-language tasks are built on the remarkable capabilities of large vision-language models (VLMs). These models excel in zero-shot and few-shot learning, enabling them to learn new tasks without parameter updates. However, their primary challenge lies in their design, which primarily accommodates 2D input, thus limiting their effectiveness for medical images, particularly radiological images like MRI and CT, which are typically 3D. To bridge the gap between state-of-the-art 2D VLMs and 3D medical image data, we developed an innovative, one-pass, unsupervised representative slice selection method called Vote-MI, which selects representative 2D slices from 3D medical imaging. To evaluate the effectiveness of Vote-MI when implemented with VLMs, we introduce BrainMD, a robust, multimodal dataset comprising 2,453 annotated 3D MRI brain scans with corresponding textual radiology reports and electronic health records. Based on BrainMD, we further develop two benchmarks, BrainMD-select (including the most representative 2D slice of a 3D image) and BrainBench (including various vision-language downstream tasks). Extensive experiments on the BrainMD dataset and its two corresponding benchmarks demonstrate that our representative selection method signiﬁcantly improves performance in zero-shot and few-shot learning tasks. On average, Vote-MI achieves a 14.6% and 16.6% absolute gain for zero-shot and few-shot learning, respectively, compared to randomly selecting examples. Our studies represent a signiﬁcant step toward integrating AI in medical imaging to enhance patient care and facilitate medical research. We hope this work will serve as a foundation for data selection as vision-language models are increasingly applied to new tasks. Code and data examples are available at Github: https://github.com/YuliWanghust/BrainMD.","","2025-03-30 16:24:08","2025-03-30 16:24:08","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/B4K7J8E3/Wang et al. - Enhancing vision-language models for medical imaging bridging the 3D gap with innovative slice sele.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSJHHCE3","journalArticle","","Shah, Nidhish; Genc, Zulkuf; Araci, Dogu","StackEval: Benchmarking LLMs in Coding Assistance","","","","","","We present two comprehensive benchmarks to evaluate the performance of language models in coding assistance tasks, covering code writing, debugging, code review, and conceptual understanding. Our main contribution includes two curated datasets: StackEval, a large-scale benchmark derived from Stack Overflow questions, and StackUnseen, a dynamic benchmark featuring the most recent Stack Overflow content. These benchmarks offer novel insights into the capabilities and limitations of LLMs, particularly in handling new and emerging content. Additionally, we assess LLMs’ proficiency as judges for coding tasks using a curated, human-annotated dataset, exploring their evaluation capabilities and potential biases, including whether they favor their own generated solutions. Our findings underscore the potential of these benchmarks to advance LLM development and application in coding assistance. To ensure reproducibility, we publicly share our datasets and evaluation code at https://github.com/ProsusAI/stack-eval.","","2025-03-30 16:24:09","2025-03-30 16:24:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VDNB6IK4/Shah et al. - StackEval Benchmarking LLMs in Coding Assistance.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KVGQK4R2","journalArticle","","Antonov, Anton; Moskalenko, Andrey; Shepelev, Denis; Krapukhin, Alexander; Soshin, Konstantin; Konushin, Anton; Shakhuro, Vlad","RClicks: Realistic Click Simulation for Benchmarking Interactive Segmentation","","","","","","The emergence of Segment Anything (SAM) sparked research interest in the field of interactive segmentation, especially in the context of image editing tasks and speeding up data annotation. Unlike common semantic segmentation, interactive segmentation methods allow users to directly influence their output through prompts (e.g. clicks). However, click patterns in real-world interactive segmentation scenarios remain largely unexplored. Most methods rely on the assumption that users would click in the center of the largest erroneous area. Nevertheless, recent studies show that this is not always the case. Thus, methods may have poor performance in real-world deployment despite high metrics in a baseline benchmark. To accurately simulate real-user clicks, we conducted a large crowdsourcing study of click patterns in an interactive segmentation scenario and collected 475K real-user clicks. Drawing on ideas from saliency tasks, we develop a clickability model that enables sampling clicks, which closely resemble actual user inputs. Using our model and dataset, we propose RClicks benchmark for a comprehensive comparison of existing interactive segmentation methods on realistic clicks. Specifically, we evaluate not only the average quality of methods, but also the robustness w.r.t. click patterns. According to our benchmark, in real-world usage interactive segmentation models may perform worse than it has been reported in the baseline benchmark, and most of the methods are not robust. We believe that RClicks is a significant step towards creating interactive segmentation methods that provide the best user experience in real-world cases.","","2025-03-30 16:24:11","2025-03-30 16:24:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/K4VKFFMH/Antonov et al. - RClicks Realistic Click Simulation for Benchmarking Interactive Segmentation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FLFQ8G5A","journalArticle","","Coursey, Austin; Ji, Junyi; Quinones-Grueiro, Marcos; Barbour, William; Zhang, Yuhang; Derr, Tyler; Biswas, Gautam; Work, Daniel B","FT-AED: Benchmark Dataset for Early Freeway Traffic Anomalous Event Detection","","","","","","Early and accurate detection of anomalous events on the freeway, such as accidents, can improve emergency response and clearance. However, existing delays and mistakes from manual crash reporting records make it a difficult problem to solve. Current large-scale freeway traffic datasets are not designed for anomaly detection and ignore these challenges. In this paper, we introduce the first large-scale lanelevel freeway traffic dataset for anomaly detection. Our dataset consists of a month of weekday radar detection sensor data collected in 4 lanes along an 18-mile stretch of Interstate 24 heading toward Nashville, TN, comprising over 3.7 million sensor measurements. We also collect official crash reports from the Tennessee Department of Transportation Traffic Management Center and manually label all other potential anomalies in the dataset. To show the potential for our dataset to be used in future machine learning and traffic research, we benchmark numerous deep learning anomaly detection models on our dataset. We find that unsupervised graph neural network autoencoders are a promising solution for this problem and that ignoring spatial relationships leads to decreased performance. We demonstrate that our methods can reduce reporting delays by over 10 minutes on average while detecting 75% of crashes. Our dataset and all preprocessing code needed to get started are publicly released at https://vu.edu/ft-aed/ to facilitate future research.","","2025-03-30 16:24:12","2025-03-30 16:24:12","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SS2SG8TI/Coursey et al. - FT-AED Benchmark Dataset for Early Freeway Traffic Anomalous Event Detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F5FU3269","preprint","2024","Silberg, Jake; Swanson, Kyle; Simon, Elana; Zhang, Angela; Ghazizadeh, Zaniar; Ogden, Scott; Hamadeh, Hisham; Zou, James","UniTox: Leveraging LLMs to Curate a Unified Dataset of Drug-Induced Toxicity from FDA Labels","","","","10.1101/2024.06.21.24309315","http://medrxiv.org/lookup/doi/10.1101/2024.06.21.24309315","Drug-induced toxicity is one of the leading reasons new drugs fail clinical trials. Machine learning models that predict drug toxicity from molecular structure could help researchers prioritize less toxic drug candidates. However, current toxicity datasets are typically small and limited to a single organ system (e.g., cardio, renal, or liver). Creating these datasets often involved time-intensive expert curation by parsing drug labelling documents that can exceed 100 pages per drug. Here, we introduce UniTox1, a unified dataset of 2,418 FDA-approved drugs with druginduced toxicity summaries and ratings created by using GPT-4o to process FDA drug labels. UniTox spans eight types of toxicity: cardiotoxicity, liver toxicity, renal toxicity, pulmonary toxicity, hematological toxicity, dermatological toxicity, ototoxicity, and infertility. This is, to the best of our knowledge, the largest such systematic human in vivo database by number of drugs and toxicities, and the first covering nearly all non-combination FDA-approved medications for several of these toxicities. We recruited clinicians to validate a random sample of our GPT-4o annotated toxicities, and UniTox’s toxicity ratings concord with clinician labelers 85–96% of the time. Finally, we benchmark several machine learning models trained on UniTox to demonstrate the utility of this dataset for building molecular toxicity prediction models.","2024-06-22","2025-03-30 16:24:15","2025-03-30 16:24:15","2025-03-30 16:24:15","","","","","","","UniTox","","","","","","","en","","","","","Health Informatics","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DDM6SDXG/Silberg et al. - 2024 - UniTox Leveraging LLMs to Curate a Unified Dataset of Drug-Induced Toxicity from FDA Labels.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YFY4TN4I","journalArticle","","Oh, Jio; Kim, Soyeon; Seo, Junseok; Wang, Jindong; Xu, Ruochen; Xie, Xing; Whang, Steven Euijong","ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models","","","","","","Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have integrity constraints that can be used to better construct complex in-depth questions and verify answers: (1) functional dependencies can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) foreign key constraints can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.","","2025-03-30 16:24:17","2025-03-30 16:24:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GXFKNCLW/Oh et al. - ERBench An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Lan.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YZZB9L6Y","journalArticle","","Duncan, Suzanne; Leoni, Gianna; Steven, Lee; Mahelona, Keoni; Jones, Peter-Lucas","Fit for our purpose, not yours: Benchmark for a low-resource, Indigenous language","","","","","","Influential and popular benchmarks in AI are largely irrelevant to developing NLP tools for low-resource, Indigenous languages. With the primary goal of measuring the performance of general-purpose AI systems, these benchmarks fail to give due consideration and care to individual language communities, especially low-resource languages. The datasets contain numerous grammatical and orthographic errors, poor pronunciation, limited vocabulary, and the content lacks cultural relevance to the language community. To overcome the issues with these benchmarks, we have created a dataset for the M¯aori language (the Indigenous language of Aotearoa/New Zealand) to pursue NLP tools that are ‘fit-for-our-purpose’. This paper demonstrates how low-resourced, Indigenous languages can develop tailored, high-quality benchmarks that; i. Reflect the unique characteristics of their language, ii. Reflect the diversity of speakers in the language community, iii. Support the aspirations for the tools they are developing and their language revitalisation efforts. All of which sit within a broader understanding of the impact of colonisation on their language.","","2025-03-30 16:24:18","2025-03-30 16:24:18","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MQSZNM4D/Duncan et al. - Fit for our purpose, not yours Benchmark for a low-resource, Indigenous language.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5ZDTAE9","journalArticle","","Miranda, Imanol; Salaberria, Ander; Agirre, Eneko; Azkune, Gorka","BIVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval","","","","","","Existing Vision-Language Compositionality (VLC) benchmarks like SUGARCREPE are formulated as image-to-text retrieval problems, where, given an image, the models need to select between the correct textual description and a synthetic hard negative text. In this work, we present the Bidirectional Vision-Language Compositionality (BIVLC) dataset. The novelty of BIVLC is to add a synthetic hard negative image generated from the synthetic text, resulting in two image-to-text retrieval examples (one for each image) and, more importantly, two text-to-image retrieval examples (one for each text). Human annotators filter out ill-formed examples ensuring the validity of the benchmark. The experiments on BIVLC uncover a weakness of current multimodal models, as they perform poorly in the text-toimage direction. In fact, when considering both retrieval directions, the conclusions obtained in previous works change significantly. In addition to the benchmark, we show that a contrastive model trained using synthetic images and texts significantly improves over the base model in SUGARCREPE and in BIVLC for both retrieval directions. The gap to human performance in BIVLC confirms that Vision-Language Compositionality is still a challenging problem. BIVLC and code are available at https://imirandam.github.io/BiVLC_project_page.","","2025-03-30 16:24:19","2025-03-30 16:24:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/P9D6P4J8/Miranda et al. - BIVLC Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"493GYHEK","dataset","2024","Herde, Marek; Huseljic, Denis; Rauch, Lukas; Sick, Bernhard","dopanim: A Dataset of Doppelganger Animals with Noisy Annotations from Multiple Humans","","","","10.5281/ZENODO.11479589","https://zenodo.org/doi/10.5281/zenodo.11479589","Human annotators typically provide annotated data for training machine learning models, such as neural networks. Yet, human annotations are subject to noise, impairing generalization performances. Methodological research on approaches counteracting noisy annotations requires corresponding datasets for a meaningful empirical evaluation. Consequently, we introduce a novel benchmark dataset, dopanim, consisting of about 15,750 animal images of 15 classes with ground truth labels. For approximately 10,500 of these images, 20 humans provided over 52,000 annotations with an accuracy of circa 67%. Its key attributes include (1) the challenging task of classifying doppelganger animals, (2) human-estimated likelihoods as annotations, and (3) annotator metadata. We benchmark well-known multi-annotator learning approaches using seven variants of this dataset and outline further evaluation use cases such as learning beyond hard class labels and active learning. Our dataset and a comprehensive codebase are publicly available to emulate the data collection process and to reproduce all empirical results.","2024-10-31","2025-03-30 16:24:21","2025-03-30 16:24:21","2025-03-30 16:24:21","","","","","","","dopanim","","","","","Zenodo","","en","Creative Commons Attribution Non Commercial Share Alike 4.0 International","","","","DOI.org (Datacite)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZZHWWVB3/Herde et al. - 2024 - dopanim A Dataset of Doppelganger Animals with Noisy Annotations from Multiple Humans.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6EHKAVZ","journalArticle","","Zhu, Yizhang; Du, Shiyin; Li, Boyan; Luo, Yuyu; Tang, Nan","Are Large Language Models Good Statisticians?","","","","","","Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry. Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored. To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks. StatQA comprises 11,623 examples tailored to evaluate LLMs’ proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement. Notably, while open-source LLMs (e.g., LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g., GPT-4o). Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential. Our source code and data are available at https://statqa.github.io/.","","2025-03-30 16:24:22","2025-03-30 16:24:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8DDJ9PBA/Zhu et al. - Are Large Language Models Good Statisticians.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52G7S892","journalArticle","","Nikulin, Alexander; Kurenkov, Vladislav; Zisman, Ilya; Agarkov, Artem; Sinii, Viacheslav; Kolesnikov, Sergey","XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX","","","","","","Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging. XLand-MiniGrid is open-source and available at https://github.com/corl-team/xland-minigrid.","","2025-03-30 16:24:23","2025-03-30 16:24:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZUHS8LPD/Nikulin et al. - XLand-MiniGrid Scalable Meta-Reinforcement Learning Environments in JAX.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FU7226SX","journalArticle","","Sivakumar, Viswanath; Seely, Jeffrey; Du, Alan; Bittner, Sean R; Berenzweig, Adam; Bolarinwa, Anuoluwapo; Gramfort, Alexandre; Mandel, Michael I","emg2qwerty: A Large Dataset with Baselines for Touch Typing using Surface Electromyography","","","","","","Surface electromyography (sEMG) non-invasively measures signals generated by muscle activity with sufficient sensitivity to detect individual spinal neurons and richness to identify dozens of gestures and their nuances. Wearable wrist-based sEMG sensors have the potential to offer low friction, subtle, information rich, always available human-computer inputs. To this end, we introduce emg2qwerty, a large-scale dataset of non-invasive electromyographic signals recorded at the wrists while touch typing on a QWERTY keyboard, together with ground-truth annotations and reproducible baselines1. With 1,135 sessions spanning 108 users and 346 hours of recording, this is the largest such public dataset to date. These data demonstrate non-trivial, but well defined hierarchical relationships both in terms of the generative process, from neurons to muscles and muscle combinations, as well as in terms of domain shift across users and user sessions. Applying standard modeling techniques from the closely related field of Automatic Speech Recognition (ASR), we show strong baseline performance on predicting keypresses using sEMG signals alone. We believe the richness of this task and dataset will facilitate progress in several problems of interest to both the machine learning and neuroscientific communities.","","2025-03-30 16:24:24","2025-03-30 16:24:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VBQFA7CC/Sivakumar et al. - emg2qwerty A Large Dataset with Baselines for Touch Typing using Surface Electromyography.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SH2723EF","journalArticle","","Dumpala, Sri Harsha; Jaiswal, Aman; Sastry, Chandramouli; Milios, Evangelos; Oore, Sageev; Sajjad, Hassan","SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations","","","","","","Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly to object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We demonstrate that models excelling on compositionality datasets may not perform equally well on SUGARCREPE++. This indicates that compositionality alone might not be sufficient to fully understand semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community. Data and code is available at https://github.com/Sri-Harsha/scpp.","","2025-03-30 16:24:25","2025-03-30 16:24:25","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GMJ7W46G/Dumpala et al. - SUGARCREPE++ Dataset Vision-Language Model Sensitivity to Semantic and Lexical Alterations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNDKV7SF","journalArticle","","Gharaee, Zahra; Lowe, Scott C; Gong, ZeMing; Arias, Pablo Millan; Pellegrino, Nicholas; Wang, Austin T; Haurum, Joakim Bruslund; Zarubiieva, Iuliia; Kari, Lila; Steinke, Dirk; Taylor, Graham W; Fieguth, Paul; Chang, Angel X","BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity","","","","","","As part of an ongoing worldwide effort to comprehend and monitor insect biodiversity, this paper presents the BIOSCAN-5M Insect dataset to the machine learning community and establish several benchmark tasks. BIOSCAN-5M is a comprehensive dataset containing multi-modal information for over 5 million insect specimens, and it significantly expands existing image-based biological datasets by including taxonomic labels, raw nucleotide barcode sequences, assigned barcode index numbers, geographical, and size information. We propose three benchmark experiments to demonstrate the impact of the multi-modal data types on the classification and clustering accuracy. First, we pretrain a masked language model on the DNA barcode sequences of the BIOSCAN-5M dataset, and demonstrate the impact of using this large reference library on species- and genus-level classification performance. Second, we propose a zero-shot transfer learning task applied to images and DNA barcodes to cluster feature embeddings obtained from self-supervised learning, to investigate whether meaningful clusters can be derived from these representation embeddings. Third, we benchmark multi-modality by performing contrastive learning on DNA barcodes, image data, and taxonomic information. This yields a general shared embedding space enabling taxonomic classification using multiple types of information and modalities. The code repository of the BIOSCAN-5M Insect dataset is available at https://github.com/bioscan-ml/BIOSCAN-5M.","","2025-03-30 16:24:26","2025-03-30 16:24:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZHHICK9Q/Gharaee et al. - BIOSCAN-5M A Multimodal Dataset for Insect Biodiversity.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8IMLAVIE","journalArticle","","Salter, Sasha; Warren, Richard; Schlager, Collin; Spurr, Adrian; Han, Shangchen; Cai, Yujun; Walkington, Peter; Bolarinwa, Anuoluwapo; Wang, Robert; Merel, Josh; Pnevmatikakis, Eftychios; Marshall, Jesse","emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation","","","","","","Hands are the primary means through which humans interact with the world. Reliable and always-available hand pose inference could yield new and intuitive control schemes for human-computer interactions, particularly in virtual and augmented reality. Computer vision is effective but requires one or multiple cameras and can struggle with occlusions, limited field of view, and poor lighting. Wearable wrist-based surface electromyography (sEMG) presents a promising alternative as an always-available modality sensing muscle activities that drive hand motion. However, sEMG signals are strongly dependent on user anatomy and sensor placement; existing sEMG models have thus required hundreds of users and device placements to effectively generalize for tasks other than pose inference. To facilitate progress on sEMG pose inference, we introduce the emg2pose benchmark, which is to our knowledge the first publicly available dataset of high-quality hand pose labels and wrist sEMG recordings. emg2pose contains 2kHz, 16 channel sEMG and pose labels from a 26-camera motion capture rig for 193 users, 370 hours, and 29 stages with diverse gestures - a scale comparable to vision-based hand pose datasets. We provide competitive baselines and challenging tasks evaluating real-world generalization scenarios: held-out users, sensor placements, and stages. This benchmark provides the machine learning community a platform for exploring complex generalization problems, holding potential to significantly enhance the development of sEMG-based human-computer interactions.","","2025-03-30 16:24:27","2025-03-30 16:24:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/728IBLWK/Salter et al. - emg2pose A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGBGDTQE","journalArticle","","Saul, Rebecca; Liu, Chang; Fleischmann, Noah; Zak, Richard; Micinski, Kristopher; Raff, Edward; Holt, James","Is Function Similarity Over-Engineered? Building a Benchmark","","","","","","Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive preprocessing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFUSE-BENCH, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple baseline —one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing — is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value.","","2025-03-30 16:24:29","2025-03-30 16:24:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FB5S4Q6K/Saul et al. - Is Function Similarity Over-Engineered Building a Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L9B8KG8Q","journalArticle","","Chen, Jiawen; Zhou, Muqing; Wu, Wenrong; Zhang, Jinwei; Li, Yun; Li, Didong","STimage-1K4M: A histopathology image-gene expression dataset for spatial transcriptomics","","","","","","","","2025-03-30 16:24:30","2025-03-30 16:24:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KYAWVS38/Chen et al. - STimage-1K4M A histopathology image-gene expression dataset for spatial transcriptomics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXFN8GGP","journalArticle","","Fang, Xinyu; Mao, Kangrui; Duan, Haodong; Zhao, Xiangyu; Li, Yining; Lin, Dahua; Chen, Kai","MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding","","","","","","The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models’ temporal comprehension. To address these limitations, we introduce MMBenchVideo, a quantitative benchmark designed to rigorously evaluate LVLMs’ proficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models’ temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding. The evalutation code of MMBench-Video will be integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.","","2025-03-30 16:24:31","2025-03-30 16:24:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/D5F2T36G/Fang et al. - MMBench-Video A Long-Form Multi-Shot Benchmark for Holistic Video Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SGIAPDQS","journalArticle","","Liang, Haiji; Han, Ruize","OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking","","","","","","Open-vocabulary object perception has become an important topic in artificial intelligence, which aims to identify objects with novel classes that have not been seen during training. Under this setting, open-vocabulary object detection (OVD) in a single image has been studied in many literature. However, open-vocabulary object tracking (OVT) from a video has been studied less, and one reason is the shortage of benchmarks. In this work, we have built a new large-scale benchmark for open-vocabulary multi-object tracking namely OVT-B. OVT-B contains 1,048 categories of objects and 1,973 videos with 637,608 bounding box annotations, which is much larger than the sole open-vocabulary tracking dataset, i.e., OVTAO-val dataset (200+ categories, 900+ videos). The proposed OVT-B can be used as a new benchmark to pave the way for OVT research. We also develop a simple yet effective baseline method for OVT. It integrates the motion features for object tracking, which is an important feature for MOT but is ignored in previous OVT methods. Experimental results have verified the usefulness of the proposed benchmark and the effectiveness of our method. We have released the benchmark to the public at https://github.com/Coo1Sea/OVT-B-Dataset.","","2025-03-30 16:24:32","2025-03-30 16:24:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z7FXJVQF/Liang and Han - OVT-B A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9S9ERQQ7","journalArticle","","Peddi, Rohith; Arya, Shivvrat; Challa, Bharath; Pallapothula, Likhitha; Vyas, Akshay; Gouripeddi, Bhavya; Zhang, Qifan; Wang, Jikai; Komaragiri, Vasundhara; Ragan, Eric; Ruozzi, Nicholas; Xiang, Yu; Gogate, Vibhav","CaptainCook4D: A Dataset for Understanding Errors in Procedural Activities","","","","","","Following step-by-step procedures is an essential component of various activities carried out by individuals in their daily lives. These procedures serve as a guiding framework that helps to achieve goals efficiently, whether it is assembling furniture or preparing a recipe. However, the complexity and duration of procedural activities inherently increase the likelihood of making errors. Understanding such procedural activities from a sequence of frames is a challenging task that demands an accurate interpretation of visual information and the ability to reason about the structure of the activity. To this end, we collect a new egocentric 4D dataset CaptainCook4D comprising 384 recordings (94.5 hours) of people performing recipes in real kitchen environments. This dataset consists of two distinct types of activities: one in which participants adhere to the provided recipe instructions and another in which they deviate and induce errors. We provide 5.3K step annotations and 10K finegrained action annotations and benchmark the dataset for the following tasks: error recognition, multi-step localization and procedure learning2.","","2025-03-30 16:24:34","2025-03-30 16:24:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BGQW9SC7/Peddi et al. - CaptainCook4D A Dataset for Understanding Errors in Procedural Activities.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H7ZVHYCU","journalArticle","","Jiménez-Sánchez, Amelia; Avlona, Natalia-Rozalia; Juodelyte, Dovile; Sourget, Théo; Vang-Larsen, Caroline; Rogers, Anna; Zaja, Hubert Dariusz","Copycats: the many lives of a publicly available medical imaging dataset","","","","","","Medical Imaging (MI) datasets are fundamental to artificial intelligence in healthcare. The accuracy, robustness, and fairness of diagnostic algorithms depend on the data (and its quality) used to train and evaluate the models. MI datasets used to be proprietary, but have become increasingly available to the public, including on community-contributed platforms (CCPs) like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data’s public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper, we conduct an analysis of publicly available machine learning datasets on CCPs, discussing datasets’ context, and identifying limitations and gaps in the current CCP landscape. We highlight differences between MI and computer vision datasets, particularly in the potentially harmful downstream effects from poor adoption of recommended dataset management practices. We compare the analyzed datasets across several dimensions, including data sharing, data documentation, and maintenance. We find vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata, with differences between the platforms. Our research contributes to efforts in responsible data curation and AI algorithms for healthcare.","","2025-03-30 16:24:35","2025-03-30 16:24:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KMFHHB5P/Jiménez-Sánchez et al. - Copycats the many lives of a publicly available medical imaging dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XL4RPNAZ","journalArticle","","Wang, Jize; Ma, Zerun; Li, Yining; Zhang, Songyang; Chen, Cailian; Chen, Kai; Le, Xinyi","GTA: A Benchmark for General Tool Agents","","","","","","Significant focus has been placed on integrating large language models (LLMs) with various tools in developing general-purpose agents. This poses a challenge to LLMs’ tool-use capabilities. However, there are evident gaps between existing tool-use evaluations and real-world scenarios. Current evaluations often use AIgenerated queries, single-step tasks, dummy tools, and text-only interactions, failing to effectively reveal the agents’ real-world problem-solving abilities. To address this, we propose GTA, a benchmark for General Tool Agents, featuring three main aspects: (i) Real user queries: human-written queries with simple real-world objectives but implicit tool-use, requiring the LLM to reason the suitable tools and plan the solution steps. (ii) Real deployed tools: an evaluation platform equipped with tools across perception, operation, logic, and creativity categories to evaluate the agents’ actual task execution performance. (iii) Real multimodal inputs: authentic image files, such as spatial scenes, web page screenshots, tables, code snippets, and printed/handwritten materials, used as the query contexts to align with real-world scenarios closely. We design 229 real-world tasks and executable tool chains to evaluate mainstream LLMs. Our findings show that real-world user queries are challenging for existing LLMs, with GPT-4 completing less than 50% of the tasks and most LLMs achieving below 25%. This evaluation reveals the bottlenecks in the tool-use capabilities of current LLMs in real-world scenarios, which provides future direction for advancing general-purpose tool agents. Dataset and code are available at https://github.com/open-compass/GTA.","","2025-03-30 16:24:36","2025-03-30 16:24:36","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QG2NPNTP/Wang et al. - GTA A Benchmark for General Tool Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UG3NRHRE","journalArticle","","Elrefaie, Mohamed; Morar, Florin; Dai, Angela; Ahmed, Faez","DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks","","","","","","","","2025-03-30 16:24:38","2025-03-30 16:24:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8LWVLDXA/Elrefaie et al. - DrivAerNet++ A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"744VH5HH","journalArticle","","Vivoli, Emanuele; Bertini, Marco; Karatzas, Dimosthenis","CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding","","","","","","The comic domain is rapidly advancing with the development of single-page analysis and synthesis models. However, evaluation metrics and datasets lag behind, often limited to small-scale or single-style test sets. We introduce a novel benchmark, CoMix, designed to evaluate the multi-task capabilities of models in comic analysis. Unlike existing benchmarks that focus on isolated tasks such as object detection or text recognition, CoMix addresses a broader range of tasks including object detection, speaker identification, character re-identification, reading order, and multi-modal reasoning tasks like character naming and dialogue generation. Our benchmark comprises three existing datasets with expanded annotations to support multi-task evaluation. To mitigate the over-representation of manga-style data, we have incorporated a new dataset of carefully selected American comicstyle books, thereby enriching the diversity of comic styles. CoMix is designed to assess pre-trained models in zero-shot and limited fine-tuning settings, probing their transfer capabilities across different comic styles and tasks. The validation split of the benchmark is publicly available for research purposes, and an evaluation server for the held-out test split is also provided. Comparative results between human performance and state-of-the-art models reveal a significant performance gap, highlighting substantial opportunities for advancements in comic understanding. The dataset, baseline models, and code are accessible at the repository link. This initiative sets a new standard for comprehensive comic analysis, providing the community with a common benchmark for evaluation on a large and varied set.","","2025-03-30 16:24:39","2025-03-30 16:24:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TQKGB2ZY/Vivoli et al. - CoMix A Comprehensive Benchmark for Multi-Task Comic Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9F4SHRXA","journalArticle","","Jiang, Dongfu; Ku, Max; Li, Tianle; Ni, Yuansheng; Sun, Shizhuo; Fan, Rongqi; Chen, Wenhu","GenAI Arena: An Open Evaluation Platform for Generative Models","","","","","","Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GENAI-ARENA to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GENAI-ARENA aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 35 open-source generative models. GENAI-ARENA has been operating for seven months, amassing over 9000 votes from the community. We describe our platform, analyze the data, and explain the statistical methods for ranking the models. To further promote the research in building model-based evaluation metrics, we release a cleaned version of our preference data for the three tasks, namely GenAI-Bench. We prompt the existing multi-modal models like Gemini, GPT-4o to mimic human voting. We compute the accuracy by comparing the model voting with the human voting to understand their juding abilities. Our results show existing multimodal models are still lagging in assessing the generated visual content, even the best model GPT-4o only achieves an average accuracy of 49.19% across the three generative tasks. Open-source MLLMs perform even worse due to the lack of instruction-following and reasoning ability in the complex vision scenarios.","","2025-03-30 16:24:40","2025-03-30 16:24:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4XNW6KIZ/Jiang et al. - GenAI Arena An Open Evaluation Platform for Generative Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4G6LN2L","journalArticle","","Chib, Pranav Singh; Singh, Pravendra","Pedestrian Trajectory Prediction with Missing Data: Datasets, Imputation, and Benchmarking","","","","","","Pedestrian trajectory prediction is crucial for several applications such as robotics and self-driving vehicles. Significant progress has been made in the past decade thanks to the availability of pedestrian trajectory datasets, which enable trajectory prediction methods to learn from pedestrians’ past movements and predict future trajectories. However, these datasets and methods typically assume that the observed trajectory sequence is complete, ignoring real-world issues such as sensor failure, occlusion, and limited fields of view that can result in missing values in observed trajectories. To address this challenge, we present TrajImpute, a pedestrian trajectory prediction dataset that simulates missing coordinates in the observed trajectory, enhancing real-world applicability. TrajImpute maintains a uniform distribution of missing data within the observed trajectories. In this work, we comprehensively examine several imputation methods to reconstruct the missing coordinates and benchmark them for imputing pedestrian trajectories. Furthermore, we provide a thorough analysis of recent trajectory prediction methods and evaluate the performance of these models on the imputed trajectories. Our experimental evaluation of the imputation and trajectory prediction methods offers several valuable insights. Our dataset provides a foundational resource for future research on imputation-aware pedestrian trajectory prediction, potentially accelerating the deployment of these methods in real-world applications. Publicly accessible links to the datasets and code files are available at https://github.com/Pranav-chib/TrajImpute.","","2025-03-30 16:24:41","2025-03-30 16:24:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5YQVLI29/Chib and Singh - Pedestrian Trajectory Prediction with Missing Data Datasets, Imputation, and Benchmarking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7YZYT2L","journalArticle","","Chen, Wei; Hao, Xixuan; Wu, Yuankai; Liang, Yuxuan","Terra: A Multimodal Spatio-Temporal Dataset Spanning the Earth","","","","","","Since the inception of our planet, the meteorological environment, as reflected through spatio-temporal data, has always been a fundamental factor influencing human life, socio-economic progress, and ecological conservation. A comprehensive exploration of this data is thus imperative to gain a deeper understanding and more accurate forecasting of these environmental shifts. Despite the success of deep learning techniques within the realm of spatio-temporal data and earth science, existing public datasets are beset with limitations in terms of spatial scale, temporal coverage, and reliance on limited time series data. These constraints hinder their optimal utilization in practical applications. To address these issues, we introduce Terra, a multimodal spatio-temporal dataset spanning the earth. This dataset encompasses hourly time series data from 6,480,000 grid areas worldwide over the past 45 years, while also incorporating multimodal spatial supplementary information including geo-images and explanatory text. Through a detailed data analysis and evaluation of existing deep learning models within earth sciences, utilizing our constructed dataset. we aim to provide valuable opportunities for enhancing future research in spatio-temporal data mining, thereby advancing towards more spatio-temporal general intelligence. Our source code and data can be accessed at https://github.com/CityMind-Lab/NeurIPS24-Terra.","","2025-03-30 16:24:42","2025-03-30 16:24:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HV5APIBS/Chen et al. - Terra A Multimodal Spatio-Temporal Dataset Spanning the Earth.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPRUH7NK","journalArticle","","Li, Haitao; Chen, You","LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models","","","","","","Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain. However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice. To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval. This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application. We evaluated 38 open-source and commercial LLMs and obtained some interesting findings. The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at https://github.com/CSHaitao/LexEval and will be continuously updated.","","2025-03-30 16:24:44","2025-03-30 16:24:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/H7BEYB39/Li and Chen - LexEval A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VWCCAHZI","journalArticle","","Karmakar, Prasenjit; Pradhan, Swadhin; Chakraborty, Sandip","Indoor Air Quality Dataset with Activities of Daily Living in Low to Middle-income Communities","","","","","","In recent years, indoor air pollution has posed a significant threat to our society, claiming over 3.2 million lives annually. Developing nations, such as India, are most affected since lack of knowledge, inadequate regulation, and outdoor air pollution lead to severe daily exposure to pollutants. However, only a limited number of studies have attempted to understand how indoor air pollution affects developing countries like India. To address this gap, we present spatiotemporal measurements of air quality from 30 indoor sites over six months during summer and winter seasons. The sites are geographically located across four regions of type: rural, suburban, and urban, covering the typical low to middle-income population in India. The dataset1 contains various types of indoor environments (e.g., studio apartments, classrooms, research laboratories, food canteens, and residential households), and can provide the basis for data-driven learning model research aimed at coping with unique pollution patterns in developing countries. This unique dataset demands advanced data cleaning and imputation techniques for handling missing data due to power failure or network outages during data collection. Furthermore, through a simple speech-to-text application, we provide real-time indoor activity labels annotated by occupants. Therefore, environmentalists and ML enthusiasts can utilize this dataset to understand the complex patterns of the pollutants under different indoor activities, identify recurring sources of pollution, forecast exposure, improve floor plans and room structures of modern indoor designs, develop pollution-aware recommender systems, etc.","","2025-03-30 16:24:45","2025-03-30 16:24:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NEHKIXH7/Karmakar et al. - Indoor Air Quality Dataset with Activities of Daily Living in Low to Middle-income Communities.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DJJLZL3Q","journalArticle","","Zou, Deyu; Liu, Shikun; Miao, Siqi; Fung, Victor; Chang, Shiyu; Li, Pan","GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts","","","","","","Geometric deep learning (GDL) has gained significant attention in scientific fields, for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many applications. To bridge this gap, we propose GeSS, a comprehensive benchmark designed for evaluating the performance of GDL models in scientific scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics, materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) test data, including no OOD information, only unlabeled OOD data, and OOD data with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 GDL backbones and 11 learning algorithms in each setting. A thorough analysis of the evaluation results is provided, poised to illuminate insights for GDL researchers and domain practitioners who are to use GDL in their applications.","","2025-03-30 16:24:46","2025-03-30 16:24:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9X4WP75L/Zou et al. - GeSS Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6B6TJ68","journalArticle","","Linghu, Xiongkun; Huang, Jiangyong; Niu, Xuesong; Ma, Xiaojian; Jia, Baoxiong; Huang, Siyuan","Multi-modal Situated Reasoning in 3D Scenes","","","","","","Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models’ situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.","","2025-03-30 16:24:48","2025-03-30 16:24:48","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JKWVSNFS/Linghu et al. - Multi-modal Situated Reasoning in 3D Scenes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CEKC445A","journalArticle","","Nguyen, Kien X; Qiao, Fengchun; Trembanis, Arthur; Peng, Xi","SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey","","","","","","A major obstacle to the advancements of machine learning models in marine science, particularly in sonar imagery analysis, is the scarcity of AI-ready datasets. While there have been efforts to make AI-ready sonar image dataset publicly available, they suffer from limitations in terms of environment setting and scale. To bridge this gap, we introduce SeafloorAI, the first extensive AI-ready datasets for seafloor mapping across 5 geological layers that is curated in collaboration with marine scientists. We further extend the dataset to SeafloorGenAI by incorporating the language component in order to facilitate the development of both visionand language-capable machine learning models for sonar imagery. The dataset consists of 62 geo-distributed data surveys spanning 17,300 square kilometers, with 696K sonar images, 827K annotated segmentation masks, 696K detailed language descriptions and approximately 7M question-answer pairs. By making our data processing source code publicly available, we aim to engage the marine science community to enrich the data pool and inspire the machine learning community to develop more robust models. This collaborative approach will enhance the capabilities and applications of our datasets within both fields. Our code repository are available 1 under the CC-BY-4.0 license.","","2025-03-30 16:24:49","2025-03-30 16:24:49","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BL69QAT7/Nguyen et al. - SeafloorAI A Large-scale Vision-Language Dataset for Seafloor Geological Survey.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q9EV9CUH","journalArticle","","Hargrave, Mason; Spaeth, Alex; Grosenick, Logan","EpiCare: A Reinforcement Learning Benchmark for Dynamic Treatment Regimes","","","","","","Healthcare applications pose significant challenges to existing reinforcement learning (RL) methods due to implementation risks, limited data availability, short treatment episodes, sparse rewards, partial observations, and heterogeneous treatment effects. Despite significant interest in using RL to generate dynamic treatment regimes for longitudinal patient care scenarios, no standardized benchmark has yet been developed. To fill this need we introduce Episodes of Care (EpiCare), a benchmark designed to mimic the challenges associated with applying RL to longitudinal healthcare settings. We leverage this benchmark to test five stateof-the-art offline RL models as well as five common off-policy evaluation (OPE) techniques. Our results suggest that while offline RL may be capable of improving upon existing standards of care given sufficient data, its applicability does not appear to extend to the moderate to low data regimes typical of current healthcare settings. Additionally, we demonstrate that several OPE techniques standard in the the medical RL literature fail to perform adequately on our benchmark. These results suggest that the performance of RL models in dynamic treatment regimes may be difficult to meaningfully evaluate using current OPE methods, indicating that RL for this application domain may still be in its early stages. We hope that these results along with the benchmark will facilitate better comparison of existing methods and inspire further research into techniques that increase the practical applicability of medical RL.","","2025-03-30 16:24:51","2025-03-30 16:24:51","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/V8UNHRCX/Hargrave et al. - EpiCare A Reinforcement Learning Benchmark for Dynamic Treatment Regimes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8I9TN7P2","journalArticle","","Jin, Yilun; Li, Zheng; Zhang, Chenwei; Cao, Tianyu; Gao, Yifan; Jayarao, Pratik; Li, Mao; Liu, Xin; Sarkhel, Ritesh; Tang, Xianfeng; Wang, Haodong; Wang, Zhengyang; Xu, Wenju; Yang, Jingfeng; Yin, Qingyu; Li, Xian; Nigam, Priyanka; Xu, Yi; Chen, Kai; Yang, Qiang; Jiang, Meng; Yin, Bing","Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models","","","","","","Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full complexity of online shopping. Large Language Models (LLMs), with their multi-task and few-shot learning abilities, have the potential to profoundly transform online shopping by alleviating task-specific engineering efforts and by providing users with interactive conversations. Despite the potential, LLMs face unique challenges in online shopping, such as domain-specific concepts, implicit knowledge, and heterogeneous user behaviors. Motivated by the potential and challenges, we propose Shopping MMLU, a diverse multi-task online shopping benchmark derived from real-world Amazon data. Shopping MMLU consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality, and can thus comprehensively evaluate the abilities of LLMs as general shop assistants. With Shopping MMLU, we benchmark over 20 existing LLMs and uncover valuable insights about practices and prospects of building versatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at https://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we host a competition in KDD Cup 2024 2 with over 500 participating teams. The winning solutions and the associated workshop can be accessed at our website https://amazon-kddcup24.github.io/.","","2025-03-30 16:24:52","2025-03-30 16:24:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EUBK6IW3/Jin et al. - Shopping MMLU A Massive Multi-Task Online Shopping Benchmark for Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BUXRXSTA","journalArticle","","Wang, Zeyu; Zhang, Xiyuxing; Yu, Ruotong","DreamCatcher: A Wearer-aware Sleep Event Dataset Based on Earables in Non-restrictive Environments","","","","","","Poor quality sleep can be characterized by the occurrence of events ranging from body movement to breathing impairment. Widely available earbuds equipped with sensors (also known as earables) can be combined with a sleep event detection algorithm to offer a convenient alternative to laborious clinical tests for individuals suffering from sleep disorders. Although various solutions utilizing such devices have been proposed to detect sleep events, they ignore the fact that individuals often share sleeping spaces with roommates or couples. To address this issue, we introduce DreamCatcher, the first publicly available dataset for wearer-aware sleep event algorithm development on earables. DreamCatcher encompasses eight distinct sleep events, including synchronous dual-channel audio and motion data collected from 12 pairs (24 participants) totaling 210 hours (420 hour.person) with fine-grained label. We tested multiple benchmark models on three tasks related to sleep event detection, demonstrating the usability and unique challenge of DreamCatcher. We hope that the proposed DreamCatcher can inspire other researchers to further explore efficient wearer-aware human vocal activity sensing on earables. DreamCatcher is publicly available at https://github.com/thuhci/DreamCatcher.","","2025-03-30 16:24:53","2025-03-30 16:24:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TM5GS8XC/Wang et al. - DreamCatcher A Wearer-aware Sleep Event Dataset Based on Earables in Non-restrictive Environments.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LCYG8UQ9","journalArticle","","Guille-Escuret, Charles; Noël, Pierre-André; Mitliagkas, Ioannis; Vazquez, David; Monteiro, Joao","Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection","","","","","","Deployed machine learning systems require some mechanism to detect out-ofdistribution (OOD) inputs. Existing research mainly focuses on one type of distribution shift: detecting samples from novel classes, absent from the training set. However, real-world systems encounter a broad variety of anomalous inputs, and the OOD literature neglects this diversity. This work categorizes five distinct types of distribution shifts and critically evaluates the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). We find that while these methods excel in detecting novel classes, their performances are inconsistent across other types of distribution shifts. In other words, they can only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a Gaussian mixture generative model for existing detection scores, enabling an ensemble detection approach that is more consistent and comprehensive for broad OOD detection, with improved performances over existing methods. We release code to build BROAD to facilitate a more comprehensive evaluation of novel OOD detectors.1.","","2025-03-30 16:24:54","2025-03-30 16:24:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/85DW28M4/Guille-Escuret et al. - Expecting The Unexpected Towards Broad Out-Of-Distribution Detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MVE9XQR","journalArticle","","Chen, Pin; Peng, Luoxuan; Jiao, Rui; Mo, Qing; Wang, Zhen; Huang, Wenbing; Liu, Yang; Lu, Yutong","Learning Superconductivity from Ordered and Disordered Material Structures","","","","","","Superconductivity is a fascinating phenomenon observed in certain materials under certain conditions. However, some critical aspects of it, such as the relationship between superconductivity and materials’ chemical/structural features, still need to be understood. Recent successes of data-driven approaches in material science strongly inspire researchers to study this relationship with them, but a corresponding dataset is still lacking. Hence, we present a new dataset for datadriven approaches, namely SuperCon3D, containing both 3D crystal structures and experimental superconducting transition temperature (Tc) for the first time. Based on SuperCon3D, we propose two deep learning methods for designing high Tc superconductors. The first is SODNet, a novel equivariant graph attention model for screening known structures, which differs from existing models in incorporating both ordered and disordered geometric content. The second is a diffusion generative model DiffCSP-SC for creating new structures, which enables high Tc-targeted generation. Extensive experiments demonstrate that both our proposed dataset and models are advantageous for designing new high Tc superconducting candidates.","","2025-03-30 16:24:55","2025-03-30 16:24:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YQT3JHQB/Chen et al. - Learning Superconductivity from Ordered and Disordered Material Structures.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEX28XF5","journalArticle","","Chasmai, Mustafa; Shepard, Alexander; Maji, Subhransu; Horn, Grant Van","The iNaturalist Sounds Dataset","","","","","","We present the iNaturalist Sounds Dataset (iNatSounds), a collection of 230,000 audio files capturing sounds from over 5,500 species, contributed by more than 27,000 recordists worldwide. The dataset encompasses sounds from birds, mammals, insects, reptiles, and amphibians, with audio and species labels derived from observations submitted to iNaturalist, a global citizen science platform. Each recording in the dataset varies in length and includes a single species annotation. We benchmark multiple backbone architectures, comparing multiclass classification objectives with multilabel objectives. Despite weak labeling, we demonstrate that iNatSounds serves as a useful pretraining resource by benchmarking it on strongly labeled downstream evaluation datasets. The dataset is available as a single, freely accessible archive, promoting accessibility and research in this important domain. We envision models trained on this data powering next-generation public engagement applications, and assisting biologists, ecologists, and land use managers in processing large audio collections, thereby contributing to the understanding of species compositions in diverse soundscapes.","","2025-03-30 16:24:57","2025-03-30 16:24:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BTKZDXAH/Chasmai et al. - The iNaturalist Sounds Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IBUPPRQC","journalArticle","","Hao, Xiaoshuai; Wei, Mengchuan; Yang, Yifan; Zhao, Haimei; Zhang, Hui; Zhou, Yi; Wang, Qiang; Li, Weiming; Kong, Lingdong; Zhang, Jing","Is Your HD Map Constructor Reliable under Sensor Corruptions?","","","","","","Driving systems often rely on high-definition (HD) maps for precise environmental information, which is crucial for planning and navigation. While current HD map constructors perform well under ideal conditions, their resilience to real-world challenges, e.g., adverse weather and sensor failures, is not well understood, raising safety concerns. This work introduces MapBench, the first comprehensive benchmark designed to evaluate the robustness of HD map construction methods against various sensor corruptions. Our benchmark encompasses a total of 29 types of corruptions that occur from cameras and LiDAR sensors. Extensive evaluations across 31 HD map constructors reveal significant performance degradation of existing methods under adverse weather conditions and sensor failures, underscoring critical safety concerns. We identify effective strategies for enhancing robustness, including innovative approaches that leverage multi-modal fusion, advanced data augmentation, and architectural techniques. These insights provide a pathway for developing more reliable HD map construction methods, which are essential for the advancement of autonomous driving technology. The benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible.","","2025-03-30 16:24:58","2025-03-30 16:24:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/L98BWQQ9/Hao et al. - Is Your HD Map Constructor Reliable under Sensor Corruptions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNBHHIAL","journalArticle","","Huang, Irene; Lin, Wei; Mirza, M Jehanzeb; Hansen, Jacob A; Doveh, Sivan; Butoi, Victor Ion; Herzig, Roei; Arbelle, Assaf; Kuehne, Hilde; Darrell, Trevor; Gan, Chuang; Oliva, Aude; Feris, Rogerio; Karlinsky, Leonid","ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs","","","","","","Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs’ LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe1 – a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce ‘hard CR Q&A’. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.","","2025-03-30 16:24:59","2025-03-30 16:25:00","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UPYXLDZI/Huang et al. - ConMe Rethinking Evaluation of Compositional Reasoning for Modern VLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"66X94SUX","journalArticle","","Vendrow, Edward; Pantazis, Omiros; Shepard, Alexander; Brostow, Gabriel; Jones, Kate E; Aodha, Oisin Mac; Beery, Sara; Horn, Grant Van","INQUIRE: A Natural World Text-to-Image Retrieval Benchmark","","","","","","We introduce INQUIRE, a text-to-image retrieval benchmark designed to challenge multimodal vision-language models on expert-level queries. INQUIRE includes iNaturalist 2024 (iNat24), a new dataset of ﬁve million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identiﬁcation, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) INQUIRE-FULLRANK, a full dataset ranking task, and (2) INQUIRE-RERANK, a reranking task for reﬁning top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that INQUIRE poses a signiﬁcant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a signiﬁcant margin for improvement. By focusing on scientiﬁcally-motivated ecological challenges, INQUIRE aims to bridge the gap between AI capabilities and the needs of real-world scientiﬁc inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.","","2025-03-30 16:25:01","2025-03-30 16:25:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2PMYEJBT/Vendrow et al. - INQUIRE A Natural World Text-to-Image Retrieval Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2E7WGHL","journalArticle","","Wang, Zirui; Xia, Mengzhou; He, Luxi; Chen, Howard; Liu, Yitao; Zhu, Richard; Liang, Kaiqu; Wu, Xindi; Liu, Haotian; Malladi, Sadhika; Chevalier, Alexis; Arora, Sanjeev; Chen, Danqi","CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs","","","","","","Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an overly optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope that CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress.","","2025-03-30 16:25:02","2025-03-30 16:25:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GQRKRK39/Wang et al. - CharXiv Charting Gaps in Realistic Chart Understanding in Multimodal LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ZQIHDWV","journalArticle","","Prabowo, Arian; Lin, Xiachong; Razzak, Imran; Xue, Hao; Yap, Emily W; Amos, Matthew; Salim, Flora D","BTS: Building Timeseries Dataset: Empowering Large-Scale Building Analytics","","","","","","","","2025-03-30 16:25:03","2025-03-30 16:25:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8IRAVACM/Prabowo et al. - BTS Building Timeseries Dataset Empowering Large-Scale Building Analytics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWRUSKGL","journalArticle","","Kon, Patrick Tser Jern; Liu, Jiachen; Qiu, Yiming; Fan, Weijun; He, Ting; Lin, Lei; Zhang, Haoran; Park, Owen M; Elengikal, George S; Kang, Yuxin; Chen, Ang; Chowdhury, Mosharaf; Lee, Myungjin; Wang, Xinyu","IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs","","","","","","Infrastructure-as-Code (IaC), an important component of cloud computing, allows the definition of cloud infrastructure in high-level programs. However, developing IaC programs is challenging, complicated by factors that include the burgeoning complexity of the cloud ecosystem (e.g., diversity of cloud services and workloads), and the relative scarcity of IaC-specific code examples and public repositories. While large language models (LLMs) have shown promise in general code generation and could potentially aid in IaC development, no benchmarks currently exist for evaluating their ability to generate IaC code. We present IaC-Eval, a first step in this research direction. IaC-Eval’s dataset includes 458 human-curated scenarios covering a wide range of popular AWS services, at varying difficulty levels. Each scenario mainly comprises a natural language IaC problem description and an infrastructure intent specification. The former is fed as user input to the LLM, while the latter is a general notion used to verify if the generated IaC program conforms to the user’s intent; by making explicit the problem’s requirements that can encompass various cloud services, resources and internal infrastructure details. Our in-depth evaluation shows that contemporary LLMs perform poorly on IaC-Eval, with the top-performing model, GPT-4, obtaining a pass@1 accuracy of 19.36%. In contrast, it scores 86.6% on EvalPlus, a popular Python code generation benchmark, highlighting a need for advancements in this domain. We open-source the IaC-Eval dataset and evaluation framework at https://github.com/autoiac-project/iac-eval to enable future research on LLM-based IaC code generation.","","2025-03-30 16:25:04","2025-03-30 16:25:04","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QZ33BUN3/Kon et al. - IaC-Eval A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CAQRMHK5","journalArticle","","Zhang, Yichi; Huang, Yao; Sun, Yitong; Liu, Chang; Zhao, Zhe; Fang, Zhengwei; Wang, Yifan; Chen, Huanran; Yang, Xiao; Wei, Xingxing; Su, Hang; Dong, Yinpeng; Zhu, Jun","MULTITRUST: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models","","","","","","Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish MultiTrust, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: https://multi-trust.github.io/.","","2025-03-30 16:25:06","2025-03-30 16:25:06","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7VB9GY5V/Zhang et al. - MULTITRUST A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3SELXSRZ","journalArticle","","Zhao, Hongbo; Fan, Lue; Chen, Yuntao; Wang, Haochen; Yang, Yuran; Jin, Xiaojuan; Zhang, Yixin; Meng, Gaofeng; Zhang, Zhaoxiang","OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction","","","","","","In this paper, we propose OpenSatMap, a fine-grained, high-resolution satellite dataset for large-scale map construction. Map construction is one of the foundations of the transportation industry, such as navigation and autonomous driving. Extracting road structures from satellite images is an efficient way to construct large-scale maps. However, existing satellite datasets provide only coarse semantic-level labels with a relatively low resolution (up to level 19), impeding the advancement of this field. In contrast, the proposed OpenSatMap (1) has fine-grained instance-level annotations; (2) consists of high-resolution images (level 20); (3) is currently the largest one of its kind; (4) collects data with high diversity. Moreover, OpenSatMap covers and aligns with the popular nuScenes dataset and Argoverse 2 dataset to potentially advance autonomous driving technologies. By publishing and maintaining the dataset, we provide a high-quality benchmark for satellite-based map construction and downstream tasks like autonomous driving.","","2025-03-30 16:25:08","2025-03-30 16:25:08","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MGUJPC2V/Zhao et al. - OpenSatMap A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8VDXN6C3","journalArticle","","Liang, Paul Pu; Goindani, Akshay; Chafekar, Talha; Mathur, Leena; Yu, Haofei; Salakhutdinov, Ruslan; Morency, Louis-Philippe","HEMM: Holistic Evaluation of Multimodal Foundation Models","","","","","","Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of realworld applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today’s models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.","","2025-03-30 16:25:08","2025-03-30 16:25:08","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5J2NDJNN/Liang et al. - HEMM Holistic Evaluation of Multimodal Foundation Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZIEEPHH","journalArticle","","Etxaniz, Julen; Azkune, Gorka; Soroa, Aitor; de Lacalle, Oier Lopez; Artetxe, Mikel","BERTAQA: How Much Do Language Models Know About Local Culture?","","","","","","Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BERTAQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models’ performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a lowresource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.","","2025-03-30 16:25:09","2025-03-30 16:25:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/G68JYQ6Y/Etxaniz et al. - BERTAQA How Much Do Language Models Know About Local Culture.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QGE4U3RU","journalArticle","","Sankar, Ashwin; Anand, Srija; Varadhan, Praveen Srinivasa; Thomas, Sherry; Singal, Mehak; Kumar, Shridhar; Mehendale, Deovrat; Krishana, Aditi; Raju, Giri","INDICVOICES-R: Unlocking a Massive Multilingual Multi-speaker Speech Corpus for Scaling Indian TTS","","","","","","Recent advancements in text-to-speech (TTS) synthesis show that large-scale models trained with extensive web data produce highly natural-sounding output. However, such data is scarce for Indian languages due to the lack of high-quality, manually subtitled data on platforms like LibriVox or YouTube. To address this gap, we enhance existing large-scale ASR datasets containing natural conversations collected in low-quality environments to generate high-quality TTS training data. Our pipeline leverages the cross-lingual generalization of denoising and speech enhancement models trained on English and applied to Indian languages. This results in IndicVoices-R (IV-R), the largest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704 hours of high-quality speech from 10,496 speakers across 22 Indian languages. IV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS, and IndicTTS. We also introduce the IV-R Benchmark, the first to assess zero-shot, few-shot, and many-shot speaker generalization capabilities of TTS models on Indian voices, ensuring diversity in age, gender, and style. We demonstrate that fine-tuning an English pre-trained model on a combined dataset of high-quality IndicTTS and our IV-R dataset results in better zero-shot speaker generalization compared to fine-tuning on the IndicTTS dataset alone. Further, our evaluation reveals limited zero-shot generalization for Indian voices in TTS models trained on prior datasets, which we improve by fine-tuning the model on our data containing diverse set of speakers across language families. We open-source code and data3 for all 22 official Indian languages.","","2025-03-30 16:25:11","2025-03-30 16:25:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/S32WI2T6/Sankar et al. - INDICVOICES-R Unlocking a Massive Multilingual Multi-speaker Speech Corpus for Scaling Indian TTS.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNUAGU4M","journalArticle","","Cruz, André F; Hardt, Moritz; Mendler-Dünner, Celestine","Evaluating language models as risk scores","","","","","","Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks. Conditioned on a question and answer-key, does the most likely token match the ground truth? Such benchmarks necessarily fail to evaluate LLMs’ ability to quantify ground-truth outcome uncertainty. In this work, we focus on the use of LLMs as risk scores for unrealizable prediction tasks. We introduce folktexts, a software package to systematically generate risk scores using LLMs, and evaluate them against US Census data products. A flexible API enables the use of different prompting schemes, local or web-hosted models, and diverse census columns that can be used to compose custom prediction tasks. We evaluate 17 recent LLMs across five proposed benchmark tasks. We find that zero-shot risk scores produced by multiple-choice question-answering have high predictive signal but are wildly miscalibrated. Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores. In fact, instruction-tuning polarizes answer distribution regardless of true underlying data uncertainty. This reveals a general inability of instruction-tuned models to express data uncertainty using multiple-choice answers. A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models. These differences in ability to quantify data uncertainty cannot be revealed in realizable settings, and highlight a blind-spot in the current evaluation ecosystem that folktexts covers.","","2025-03-30 16:25:12","2025-03-30 16:25:12","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2XCJH766/Cruz et al. - Evaluating language models as risk scores.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B8LV9JX8","journalArticle","","Li, Linyi; Geng, Shijie; Li, Zhenwen; He, Yibo; Yu, Hao; Hua, Ziyue; Ning, Guanghan; Wang, Siwei; Xie, Tao; Yang, Hongxia","InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models","","","","","","Large Language Models for code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose InfiBench, the first large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. InfiBench uses four types of model-free automatic metrics to evaluate response correctness where domain experts carefully concretize the criterion for each question. We conduct a systematic evaluation for over 100 latest code LLMs on InfiBench, leading to a series of novel and insightful findings. Our detailed analyses showcase potential directions for further advancement of code LLMs. InfiBench is fully open source at https://infi-coder.github.io/infibench and continuously expanding to foster more scientific and systematic practices for code LLM evaluation.","","2025-03-30 16:25:13","2025-03-30 16:25:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GPNL8CJY/Li et al. - InfiBench Evaluating the Question-Answering Capabilities of Code Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQCSFPCA","journalArticle","","Yang, Jason; Mora, Ariane; Liu, Shengchao; Wittmann, Bruce J","CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes","","","","","","Enzymes are important proteins that catalyze chemical reactions. In recent years, machine learning methods have emerged to predict enzyme function from sequence; however, there are no standardized benchmarks to evaluate these methods. We introduce CARE, a benchmark and dataset suite for the Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1) classification of a protein sequence by its enzyme commission (EC) number and (2) retrieval of an EC number given a chemical reaction. For each task, we design train-test splits to evaluate different kinds of out-of-distribution generalization that are relevant to real use cases. For the classification task, we provide baselines for state-of-the-art methods. Because the retrieval task has not been previously formalized, we propose a method called Contrastive Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task and compare it to the recent method, CLIPZyme. CARE is available at https://github.com/jsunn-y/CARE/.","","2025-03-30 16:25:15","2025-03-30 16:25:15","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CRP5SVAY/Yang et al. - CARE a Benchmark Suite for the Classification and Retrieval of Enzymes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QZ9P362","journalArticle","","Zhang, Lemei; Liu, Peng; Henriksboe, Marcus Tiedemann Oekland; Lauvrak, Even W; Gulla, Jon Atle; Ramampiaro, Heri","PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models","","","","","","With the rapid advancement of Natural Language Processing in recent years, numerous studies have shown that generic summaries generated by Large Language Models (LLMs) can sometimes surpass those annotated by experts, such as journalists, according to human evaluations. However, there is limited research on whether these generic summaries meet the individual needs of ordinary people. The biggest obstacle is the lack of human-annotated datasets from the general public. Existing work on personalized summarization often relies on pseudo datasets created from generic summarization datasets or controllable tasks that focus on specific named entities or other aspects, such as the length and specificity of generated summaries, collected from hypothetical tasks without the annotators’ initiative. To bridge this gap, we propose a high-quality, personalized, manually annotated abstractive summarization dataset called PersonalSum. This dataset is the first to investigate whether the focus of public readers differs from the generic summaries generated by LLMs. It includes user profiles, personalized summaries accompanied by source sentences from given articles, and machine-generated generic summaries along with their sources. We investigate several personal signals — entities/topics, plot, and structure of articles—that may affect the generation of personalized summaries using LLMs in a few-shot in-context learning scenario. Our preliminary results and analysis indicate that entities/topics are merely one of the key factors that impact the diverse preferences of users, and personalized summarization remains a significant challenge for existing LLMs. Our dataset and code are available at https://github.com/SmartmediaAI/PersonalSum.","","2025-03-30 16:25:16","2025-03-30 16:25:16","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EM4KKWX4/Zhang et al. - PersonalSum A User-Subjective Guided Personalized Summarization Dataset for Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LN39V8DT","journalArticle","","Ortiz, Joseph; Dedieu, Antoine; Lehrach, Wolfgang; Guntupalli, J Swaroop; Wendelken, Carter; Humayun, Ahmad; Zhou, Guangyao; Swaminathan, Sivaramakrishnan; Lázaro-Gredilla, Miguel; Murphy, Kevin","DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors","","","","","","Learning from previously collected data via behavioral cloning or offline reinforcement learning (RL) is a powerful recipe for scaling generalist agents by avoiding the need for expensive online learning. Despite strong generalization in some respects, agents are often remarkably brittle to minor visual variations in control-irrelevant factors such as the background or camera viewpoint. In this paper, we present the DeepMind Control Vision Benchmark (DMC-VB), a dataset collected in the DeepMind Control Suite to evaluate the robustness of offline RL agents for solving continuous control tasks from visual input in the presence of visual distractors. In contrast to prior works, our dataset (a) combines locomotion and navigation tasks of varying difficulties, (b) includes static and dynamic visual variations, (c) considers data generated by policies with different skill levels, (d) systematically returns pairs of state and pixel observation, (e) is an order of magnitude larger, and (f) includes tasks with hidden goals. Accompanying our dataset, we propose three benchmarks to evaluate representation learning methods for pretraining, and carry out experiments on several recently proposed methods. First, we find that pretrained representations do not help policy learning on DMC-VB, and we highlight a large representation gap between policies learned on pixel observations and on states. Second, we demonstrate when expert data is limited, policy learning can benefit from representations pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals. Our dataset and benchmark code to train and evaluate agents are available at https://github.com/google-deepmind/dmc_vision_benchmark.","","2025-03-30 16:25:17","2025-03-30 16:25:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/29JWVN26/Ortiz et al. - DMC-VB A Benchmark for Representation Learning for Control with Visual Distractors.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VKM3VQ4","journalArticle","","Chao, Patrick; Debenedetti, Edoardo; Robey, Alexander; Andriushchenko, Maksym; Croce, Francesco; Sehwag, Vikash; Dobriban, Edgar; Flammarion, Nicolas; Pappas, George J; Tramer, Florian; Hassani, Hamed; Wong, Eric","JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models","","","","","","Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors—both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024)—which align with OpenAI’s usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.","","2025-03-30 16:25:19","2025-03-30 16:25:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UK42CDKL/Chao et al. - JailbreakBench An Open Robustness Benchmark for Jailbreaking Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XMJMG4W","journalArticle","","Zeng, Xianzhi; Jiang, Wenchao; Zhang, Shuhao","LibAMM: Empirical Insights into Approximate Computing for Accelerating Matrix Multiplication","","","","","","Matrix multiplication (MM) is pivotal in fields from deep learning to scientific computing, driving the quest for improved computational efficiency. Accelerating MM encompasses strategies like complexity reduction, parallel and distributed computing, hardware acceleration, and approximate computing techniques, namely AMM algorithms. Amidst growing concerns over the resource demands of large language models (LLMs), AMM has garnered renewed focus. However, understanding the nuances that govern AMM’s effectiveness remains incomplete. This study delves into AMM by examining algorithmic strategies, operational specifics, dataset characteristics, and their application in real-world tasks. Through comprehensive testing across diverse datasets and scenarios, we analyze how these factors affect AMM’s performance, uncovering that the selection of AMM approaches significantly influences the balance between efficiency and accuracy, with factors like memory access playing a pivotal role. Additionally, dataset attributes are shown to be vital for the success of AMM in applications. Our results advocate for tailored algorithmic approaches and careful strategy selection to enhance AMM’s effectiveness. To aid in the practical application and ongoing research of AMM, we introduce LibAMM —a toolkit offering a wide range of AMM algorithms, benchmarks, and tools for experiment management. LibAMM aims to facilitate research and application in AMM, guiding future developments towards more adaptive and context-aware computational solutions.","","2025-03-30 16:25:20","2025-03-30 16:25:20","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/I8NYXL2B/Zeng et al. - LibAMM Empirical Insights into Approximate Computing for Accelerating Matrix Multiplication.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZN36BBBY","journalArticle","","Zhu, Derui; Chen, Dingfan; Wu, Xiongfei; Geng, Jiahui; Li, Zhuo; Grossklags, Jens; Ma, Lei","PrivAuditor: Benchmarking Privacy Vulnerabilities in LLM Adaptation Techniques","","","","","","Large Language Models (LLMs) are recognized for their potential to be an important building block toward achieving artificial general intelligence due to their unprecedented capability for solving diverse tasks. Despite these achievements, LLMs often underperform in domain-specific tasks without training on relevant domain data. This phenomenon, which is often attributed to distribution shifts, makes adapting pre-trained LLMs with domain-specific data crucial. However, this adaptation raises significant privacy concerns, especially when the data involved come from sensitive domains. In this work, we extensively investigate the privacy vulnerabilities of adapted (fine-tuned) LLMs and benchmark privacy leakage across a wide range of data modalities, state-of-the-art privacy attack methods, adaptation techniques, and model architectures. We systematically evaluate and pinpoint critical factors related to privacy leakage. With our organized codebase and actionable insights, we aim to provide a standardized auditing tool for practitioners seeking to deploy customized LLM applications with faithful privacy assessments.","","2025-03-30 16:25:21","2025-03-30 16:25:21","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VNMHSJWJ/Zhu et al. - PrivAuditor Benchmarking Privacy Vulnerabilities in LLM Adaptation Techniques.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DR4R8NEG","journalArticle","","Hao, Zhongkai; Yao, Jiachen; Su, Chang; Su, Hang; Wang, Ziao; Lu, Fanzhi; Xia, Zeyu; Zhang, Yichi; Liu, Songming; Lu, Lu; Zhu, Jun","PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs","","","","","","While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains, including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research such as domain decomposition methods and loss reweighting for handling multi-scale problems. To the best of our knowledge, it is the largest benchmark with a diverse and comprehensive evaluation that will undoubtedly foster further research in PINNs.","","2025-03-30 16:25:22","2025-03-30 16:25:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/W5VSU8RB/Hao et al. - PINNacle A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5JWVW8M2","journalArticle","","Kannen, Nithish; Ahmad, Arif; Andreetto, Marco; Prabhakaran, Vinodkumar; Prabhu, Utsav; Dieng, Adji Bousso; Bhattacharyya, Pushpak; Dave, Shachi","Beyond Aesthetics: Cultural Competence in Text-to-Image Models","","","","","","Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a ﬁrst-of-its-kind benchmark to evaluate cultural competence of T2I models.2 CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of high-quality prompts that enable the evaluation of cultural awareness, and 2) CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to evaluate cultural diversity. We also introduce cultural diversity as a novel T2I evaluation component, leveraging quality-weighted Vendi score. Our evaluations reveal signiﬁcant gaps in the cultural awareness of existing models across countries and provide valuable insights into the cultural diversity of T2I outputs for under-speciﬁed prompts. Our methodology is extendable to other cultural regions and concepts, and can facilitate the development of T2I models that better cater to the global population.","","2025-03-30 16:25:24","2025-03-30 16:25:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FK24NLG8/Kannen et al. - Beyond Aesthetics Cultural Competence in Text-to-Image Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKCUPFM7","journalArticle","","Tsoukalas, George; Lee, Jasper; Jennings, John; Xin, Jimmy; Ding, Michelle; Jennings, Michael; Thakur, Amitayush; Chaudhuri, Swarat","PUTNAMBENCH: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition","","","","","","We present PUTNAMBENCH, a new multi-language benchmark for evaluating the ability of neural theorem-provers to solve competition mathematics problems. PUTNAMBENCH consists of 1692 hand-constructed formalizations of 640 theorems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problems have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq formalizations. PUTNAMBENCH requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We use PUTNAMBENCH to evaluate several established neural and symbolic theorem-provers. These approaches can only solve a handful of the PUTNAMBENCH problems, establishing the benchmark as a difficult open challenge for research on neural theorem-proving. PUTNAMBENCH is available at https://github.com/trishullab/PutnamBench.","","2025-03-30 16:25:25","2025-03-30 16:25:25","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GZWIHABV/Tsoukalas et al. - PUTNAMBENCH Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IA384QXX","journalArticle","","Li, Bowen; Li, Zhaoyu; Du, Qiwei; Luo, Jinqi; Wang, Wenshan; Xie, Yaqi; Stepputtis, Simon; Wang, Chen; Sycara, Katia; Ravikumar, Pradeep; Gray, Alexander; Si, Xujie; Scherer, Sebastian","LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation","","","","","","","","2025-03-30 16:25:26","2025-03-30 16:25:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UKB4LAM3/Li et al. - LogiCity Advancing Neuro-Symbolic AI with Abstract Urban Simulation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H5MNC8JL","journalArticle","","Ye, Hui; Sunderraman, Rajshekhar; Ji, Shihao","UAV3D: A Large-scale 3D Perception Benchmark for Unmanned Aerial Vehicles","","","","","","Unmanned Aerial Vehicles (UAVs), equipped with cameras, are employed in numerous applications, including aerial photography, surveillance, and agriculture. In these applications, robust object detection and tracking are essential for the effective deployment of UAVs. However, existing benchmarks for UAV applications are mainly designed for traditional 2D perception tasks, restricting the development of real-world applications that require a 3D understanding of the environment. Furthermore, despite recent advancements in single-UAV perception, limited views of a single UAV platform significantly constrain its perception capabilities over long distances or in occluded areas. To address these challenges, we introduce UAV3D – a benchmark designed to advance research in both 3D and collaborative 3D perception tasks with UAVs. UAV3D comprises 1,000 scenes, each of which has 20 frames with fully annotated 3D bounding boxes on vehicles. We provide the benchmark for four 3D perception tasks: single-UAV 3D object detection, single-UAV object tracking, collaborative-UAV 3D object detection, and collaborative-UAV object tracking. Our dataset and code are available at https://huiyegit.github.io/UAV3D_Benchmark/.","","2025-03-30 16:25:28","2025-03-30 16:25:28","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/G8279GCU/Ye et al. - UAV3D A Large-scale 3D Perception Benchmark for Unmanned Aerial Vehicles.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SEG8CTSR","journalArticle","","Wu, Xindi; Yu, Dingli; Huang, Yangsibo; Russakovsky, Olga; Arora, Sanjeev","ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty","","","","","","Compositionality is a critical capability in Text-to-Image (T2I) models, as it reflects their ability to understand and combine multiple concepts from text descriptions. Existing evaluations of compositional capability rely heavily on human-designed text prompts or fixed templates, limiting their diversity and complexity, and yielding low discriminative power. We propose CONCEPTMIX, a scalable, controllable, and customizable benchmark which automatically evaluates compositional generation ability of T2I models. This is done in two stages. First, CONCEPTMIX generates the text prompts: concretely, using categories of visual concepts (e.g., objects, colors, shapes, spatial relationships), it randomly samples an object and k-tuples of visual concepts, then uses GPT-4o to generate text prompts for image generation based on these sampled concepts. Second, CONCEPTMIX evaluates the images generated in response to these prompts: concretely, it checks how many of the k concepts actually appeared in the image by generating one question per visual concept and using a strong VLM to answer them. Through administering CONCEPTMIX to a diverse set of T2I models (proprietary as well as open ones) using increasing values of k, we show that our CONCEPTMIX has higher discrimination power than earlier benchmarks. Specifically, CONCEPTMIX reveals that the performance of several models, especially open models, drops dramatically with increased k. Importantly, it also provides insight into the lack of prompt diversity in widely-used training datasets. Additionally, we conduct extensive human studies to validate the design of CONCEPTMIX and compare our automatic grading with human judgement. We hope it will guide future T2I model development.","","2025-03-30 16:25:29","2025-03-30 16:25:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/994QSIPM/Wu et al. - ConceptMix A Compositional Image Generation Benchmark with Controllable Difficulty.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9736K86W","journalArticle","","Simonetto, Thibault; Ghamizi, Salah; Cordy, Maxime","TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases","","","","","","While adversarial robustness in computer vision is a mature research field, fewer researchers have tackled the evasion attacks against tabular deep learning, and even fewer investigated robustification mechanisms and reliable defenses. We hypothesize that this lag in the research on tabular adversarial attacks is in part due to the lack of standardized benchmarks. To fill this gap, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness with CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model. In addition to our open benchmark https://github.com/serval-uni-lu/tabularbench where we welcome submissions of new models and defenses, we implement 7 robustification mechanisms inspired by state-of-the-art defenses in computer vision and propose the largest benchmark of robust tabular deep-learning over 200 models across five critical scenarios in finance, healthcare, and security. We curated real datasets for each use case, augmented with hundreds of thousands of realistic synthetic inputs, and trained and assessed our models with and without data augmentations. We open-source our library that provides API access to all our pre-trained robust tabular models, and the largest datasets of real and synthetic tabular inputs. Finally, we analyze the impact of various defenses on the robustness and provide actionable insights to design new defenses and robustification mechanisms.","","2025-03-30 16:25:31","2025-03-30 16:25:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PLKCAZRE/Simonetto et al. - TabularBench Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9JEDAE87","journalArticle","","Luo, Yuankai; Shi, Lei; Wu, Xiao-Ming","Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification","","","","","","Graph Transformers (GTs) have recently emerged as popular alternatives to traditional message-passing Graph Neural Networks (GNNs), due to their theoretically superior expressiveness and impressive performance reported on standard node classification benchmarks, often significantly outperforming GNNs. In this paper, we conduct a thorough empirical analysis to reevaluate the performance of three classic GNN models (GCN, GAT, and GraphSAGE) against GTs. Our findings suggest that the previously reported superiority of GTs may have been overstated due to suboptimal hyperparameter configurations in GNNs. Remarkably, with slight hyperparameter tuning, these classic GNN models achieve state-of-the-art performance, matching or even exceeding that of recent GTs across 17 out of the 18 diverse datasets examined. Additionally, we conduct detailed ablation studies to investigate the influence of various GNN configurations—such as normalization, dropout, residual connections, and network depth—on node classification performance. Our study aims to promote a higher standard of empirical rigor in the field of graph machine learning, encouraging more accurate comparisons and evaluations of model capabilities. Our implementation is available at https://github.com/LUOyk1999/tunedGNN.","","2025-03-30 16:25:32","2025-03-30 16:25:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BESV82EE/Luo et al. - Classic GNNs are Strong Baselines Reassessing GNNs for Node Classification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FFJ459YN","dataset","2024","European Vegetation Survey, The IAVS Working Group","EVA project # 196 2023-11-22 MAMBO: Modern approaches to the monitoring of biodiversity - L. Picek: SELECTION 2024-01-04","","","","10.58060/QE37-TK48","https://euroveg.org/eva-database/project-selection/29","The difficulty of monitoring biodiversity at fine scales and over large areas limits ecological knowledge and conservation efforts. To fill this gap, Species Distribution Models (SDMs) predict species across space from spatially explicit features. Yet, they face the challenge of integrating the rich but heterogeneous data made available over the past decade, notably millions of opportunistic species observations and standardized surveys, as well as multimodal remote sensing data. In light of that, we have designed and developed a new European-scale dataset for SDMs at high spatial resolution (10–50m), including more than 10k species (i.e., most of the European flora). The dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive Presence-Absence survey records, all accompanied by diverse environmental rasters (e.g., elevation, human footprint, and soil) traditionally used in SDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10 m resolution, a 20-year time series of climatic variables, and satellite time series from the Landsat program. In addition to the data, we provide an openly accessible SDM benchmark (hosted on Kaggle), which has already attracted an active community and a set of strong baselines for single predictor/modality and multimodal approaches. All resources, e.g., the dataset, pre-trained models, and baseline methods (in the form of notebooks), are available on Kaggle, allowing one to start with our dataset literally with two mouse clicks.","2024","2025-03-30 16:25:35","2025-03-30 16:25:36","2025-03-30 16:25:35","","","","","","","EVA project # 196 2023-11-22 MAMBO","","","","","Masaryk University, Faculty of Science, Department of Botany and Zoology","","en","","","","","DOI.org (Datacite)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/L5S567PF/European Vegetation Survey, The IAVS Working Group - 2024 - EVA project # 196 2023-11-22 MAMBO Modern approaches to the monitoring of biodiversity - L. Picek.pdf","","","FOS: Biological sciences","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5YT9AK3P","journalArticle","","Foteinopoulou, Niki Maria; Ghorbel, Enjie; Aouada, Djamila","A Hitchhiker’s Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning","","","","","","","","2025-03-30 16:25:37","2025-03-30 16:25:37","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/W6DMVVSN/Foteinopoulou et al. - A Hitchhiker’s Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L9J493XD","journalArticle","","Jia, Xiaosong; Yang, Zhenjie; Li, Qifeng; Zhang, Zhiyuan; Yan, Junchi","Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving","","","","","","In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the datadriven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible. To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems’ multiple abilities in a closed-loop manner. Bench2Drive’s official training data consists of 2 million fully annotated frames, collected from 13638 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.","","2025-03-30 16:25:38","2025-03-30 16:25:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8SKT5B96/Jia et al. - Bench2Drive Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9SCT3A4","journalArticle","","Yuan, Shenghai; Huang, Jinfa; Xu, Yongqi; Liu, Yaoyang; Zhang, Shaofeng; Shi, Yujun; Zhu, Ruijie; Cheng, Xinhua; Luo, Jiebo; Yuan, Li","ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation","","","","","","","","2025-03-30 16:25:39","2025-03-30 16:25:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QKTKXL3H/Yuan et al. - ChronoMagic-Bench A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FY7SGQCC","journalArticle","","Liu, Meihan; Zhang, Zhen; Tang, Jiachen; Bu, Jiajun; He, Bingsheng; Zhou, Sheng","Revisiting, Benchmarking and Understanding Unsupervised Graph Domain Adaptation","","","","","","Unsupervised Graph Domain Adaptation (UGDA) involves the transfer of knowledge from a label-rich source graph to an unlabeled target graph under domain discrepancies. Despite the proliferation of methods designed for this emerging task, the lack of standard experimental settings and fair performance comparisons makes it challenging to understand which and when models perform well across different scenarios. To fill this gap, we present the first comprehensive benchmark for unsupervised graph domain adaptation named GDABench, which encompasses 16 algorithms across diverse adaptation tasks. Through extensive experiments, we observe that the performance of current UGDA models varies significantly across different datasets and adaptation scenarios. Specifically, we recognize that when the source and target graphs face significant distribution shifts, it is imperative to formulate strategies to effectively address and mitigate graph structural shifts. We also find that with appropriate neighbourhood aggregation mechanisms, simple GNN variants can even surpass state-of-the-art UGDA baselines. To facilitate reproducibility, we have developed an easy-to-use library PyGDA for training and evaluating existing UGDA methods, providing a standardized platform in this community. Our source codes and datasets can be found at https://github.com/pygda-team/pygda.","","2025-03-30 16:25:41","2025-03-30 16:25:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DFR66WE6/Liu et al. - Revisiting, Benchmarking and Understanding Unsupervised Graph Domain Adaptation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYWREGM2","journalArticle","","Wang, Zhilin; Dong, Yi; Delalleau, Olivier; Zeng, Jiaqi; Shen, Gerald; Egert, Daniel; Zhang, Jimmy J; Sreedhar, Makesh Narsimhan; Kuchaiev, Oleksii","HelpSteer2: Open-source dataset for training top-performing reward models","","","","","","High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences. As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HHRLHF, and HelpSteer need to be updated to remain effective for reward modeling. Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers. To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0). Using a powerful Nemotron4-340B base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench’s primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024. Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. Additionally, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multiattribute score predicted by our reward models. HelpSteer2 is available at https: //huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner.","","2025-03-30 16:25:42","2025-03-30 16:25:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GMFL4CCU/Wang et al. - HelpSteer2 Open-source dataset for training top-performing reward models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFERVXAD","journalArticle","","Fan, Yaran; Pool, Jamie","Topic-Conversation Relevance (TCR) Dataset and Benchmarks","","","","","","Workplace meetings are vital to organizational collaboration, yet a large percentage of meetings are rated as ineffective [1]. To help improve meeting effectiveness by understanding if the conversation is on topic, we create a comprehensive TopicConversation Relevance (TCR) dataset that covers a variety of domains and meeting styles. The TCR dataset includes 1,500 unique meetings, 22 million words in transcripts, and over 15,000 meeting topics, sourced from both newly collected Speech Interruption Meeting (SIM) data and existing public datasets. Along with the text data, we also open source scripts to generate synthetic meetings or create augmented meetings from the TCR dataset to enhance data diversity. For each data source, benchmarks are created using GPT-41 to evaluate the model accuracy in understanding transcription-topic relevance.","","2025-03-30 16:25:43","2025-03-30 16:25:43","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QUFD6TLS/Fan and Pool - Topic-Conversation Relevance (TCR) Dataset and Benchmarks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P8EKR3A9","journalArticle","","Roberts, Jonathan; Han, Kai; Houlsby, Neil","SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation","","","","","","Large multimodal models (LMMs) have proven flexible and generalisable across many tasks and fields. Although they have strong potential to aid scientific research, their capabilities in this domain are not well characterised. A key aspect of scientific research is the ability to understand and interpret figures, which serve as a rich, compressed source of complex information. In this work, we present SciFIBench, a scientific figure interpretation benchmark consisting of 2000 questions split between two tasks across 8 categories. The questions are curated from arXiv paper figures and captions, using adversarial filtering to find hard negatives and human verification for quality control. We evaluate 28 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we investigate the alignment and reasoning faithfulness of the LMMs on augmented question sets from our benchmark. We release SciFIBench to encourage progress in this domain: https://SciFIBench.github.io/.","","2025-03-30 16:25:44","2025-03-30 16:25:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BZXMH2ET/Roberts et al. - SciFIBench Benchmarking Large Multimodal Models for Scientific Figure Interpretation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J85G5U5S","journalArticle","","Han, Seungju; Rao, Kavel; Ettinger, Allyson; Jiang, Liwei; Lin, Bill Yuchen; Lambert, Nathan; Choi, Yejin; Dziri, Nouha","WILDGUARD: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs","","","","","","We introduce WILDGUARD—an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WILDGUARD serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 [16] score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models’ refusals, a key measure for evaluating safety behaviors in model responses.","","2025-03-30 16:25:46","2025-03-30 16:25:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/73KSHM2P/Han et al. - WILDGUARD Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VPRPVJTF","journalArticle","","Zhang, Tianyi; Cai, Linrong; Li, Jeffrey; Roberts, Nicholas; Guha, Neel; Sala, Frederic","Stronger Than You Think: Benchmarking Weak Supervision on Realistic Tasks","","","","","","Weak supervision (WS) is a popular approach for label-efficient learning, leveraging diverse sources of noisy but inexpensive weak labels to automatically annotate training data. Despite its wide usage, WS and its practical value are challenging to benchmark due to the many knobs in its setup, including: data sources, labeling functions (LFs), aggregation techniques (called label models), and end model pipelines. Existing evaluation suites tend to be limited, focusing on particular components or specialized use cases. Moreover, they often involve simplistic benchmark tasks or de-facto LF sets that are suboptimally written, producing insights that may not generalize to real-world settings. We address these limitations by introducing a new benchmark, BOXWRENCH,2 designed to more accurately reflect real-world usages of WS. This benchmark features tasks with (1) higher class cardinality and imbalance, (2) notable domain expertise requirements, and (3) opportunities to re-use LFs across parallel multilingual corpora. For all tasks, LFs are written using a careful procedure aimed at mimicking real-world settings. In contrast to existing WS benchmarks, we show that supervised learning requires substantial amounts (1000+) of labeled examples to match WS in many settings.","","2025-03-30 16:25:47","2025-03-30 16:25:47","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7CJJHNZ4/Zhang et al. - Stronger Than You Think Benchmarking Weak Supervision on Realistic Tasks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSHZ9XCM","journalArticle","","Yang, Jehan; Soh, Maxwell; Lieu, Vivianna","EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography","","","","","","This paper introduces the first generalization and adaptation benchmark using machine learning for evaluating out-of-distribution performance of electromyography (EMG) classification algorithms. The ability of an EMG classifier to handle inputs drawn from a different distribution than the training distribution is critical for real-world deployment as a control interface. By predicting the user’s intended gesture using EMG signals, we can create a wearable solution to control assistive technologies, such as computers, prosthetics, and mobile manipulator robots. This new out-of-distribution benchmark consists of two major tasks that have utility for building robust and adaptable control interfaces: 1) intersubject classification, and 2) adaptation using train-test splits for time-series. This benchmark spans nine datasets, the largest collection of EMG datasets in a benchmark. Among these, a new dataset is introduced, featuring a novel, easy-to-wear high-density EMG wearable for data collection. The lack of open-source benchmarks has made comparing accuracy results between papers challenging for the EMG research community. This new benchmark provides researchers with a valuable resource for analyzing practical measures of out-of-distribution performance for EMG datasets. Our code and data from our new dataset can be found at emgbench.github.io.","","2025-03-30 16:25:48","2025-03-30 16:25:48","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KPYP2ZAW/Yang et al. - EMGBench Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3VLU8HDS","journalArticle","","Farebrother, Jesse; Castro, Pablo Samuel","CALE: Continuous Arcade Learning Environment","","","","","","We introduce the Continuous Arcade Learning Environment (CALE), an extension of the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013]. The CALE uses the same underlying emulator of the Atari 2600 gaming system (Stella), but adds support for continuous actions. This enables the benchmarking and evaluation of continuous-control agents (such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]) and value-based agents (such as DQN [Mnih et al., 2015] and Rainbow [Hessel et al., 2018]) on the same environment suite. We provide a series of open questions and research directions that CALE enables, as well as initial baseline results using Soft Actor-Critic. CALE is available as part of the ALE at https://github.com/Farama-Foundation/ Arcade-Learning-Environment.","","2025-03-30 16:25:50","2025-03-30 16:25:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/E35QP52N/Farebrother and Castro - CALE Continuous Arcade Learning Environment.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GME3JDID","journalArticle","","Yuan, Shuai; Lin, Guancong; Zhang, Lixian; Dong, Runmin; Zhang, Jinxiao; Chen, Shuang; Zheng, Juepeng; Wang, Jie; Fu, Haohuan","FUSU: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding","","","","","","Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions in urban areas. Although there have been advances in high-quality land cover datasets that reveal the physical features of urban landscapes, the lack of fine-grained land use datasets hinders a deeper understanding of how human activities are distributed across the landscape and the impact of these activities on the environment, thus constraining proper technique development. To address this, we introduce FUSU, the first finegrained land use change segmentation dataset for Fine-grained Urban Semantic Understanding. FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations. It includes bi-temporal high-resolution satellite images with 0.2-0.5 m ground sample distance and monthly optical and radar satellite time series, covering 847 km2 across five urban areas in the southern and northern of China with different geographical features. The fine-grained land use pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for developing proper deep learning models to provide contextual insights on human activities and urbanization. To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation. We benchmark FUSU on various methods for several tasks. Dataset and code are available at: https://github.com/yuanshuai0914/FUSU.","","2025-03-30 16:25:51","2025-03-30 16:25:51","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/APFI9RAF/Yuan et al. - FUSU A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic U.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJACP4GY","journalArticle","","Ye, Rui; Ge, Rui; Zhu, Xinyu; Chai, Jingyi; Du, Yaxin; Liu, Yang; Wang, Yanfeng; Chen, Siheng","FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models","","","","","","Federated learning could enable multiple parties to collaboratively fine-tune large language models without directly sharing their data (FedLLM). Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy. However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works often rely on artificially constructed datasets, failing to capture properties in real-world scenarios. Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community. FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747. Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios. Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration). We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons. Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.","","2025-03-30 16:25:52","2025-03-30 16:25:52","","","","20","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PI3NPW9L/Ye et al. - FedLLM-Bench Realistic Benchmarks for Federated Learning of Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F4LJ337W","journalArticle","","Zhang, Yu; Pan, Changhao; Guo, Wenxiang; Li, Ruiqi; Zhu, Zhiyuan; Wang, Jialei; Xu, Wenhao; Lu, Jingyu; Hong, Zhiqing; Wang, Chuxin; Zhang, LiChao; He, Jinzheng; Jiang, Ziyue; Chen, Yuxin; Yang, Chen; Cheng, Jiecheng Zhou Xinyu; Zhao, Zhou","GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks","","","","","","The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large Global, multi-Technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at http://gtsinger.github.io. We provide the dataset and the code for processing data and conducting benchmarks at https://huggingface.co/datasets/GTSinger/GTSinger and https://github.com/GTSinger/GTSinger.","","2025-03-30 16:25:53","2025-03-30 16:25:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HT2FNFNG/Zhang et al. - GTSinger A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38RLDVBV","journalArticle","","Jung, HyunJun; Li, Weihang; Wu, Shun-Cheng; Bittner, William; Brasch, Nikolas; Song, Jifei; Pérez-Pellitero, Eduardo; Zhang, Zhensong; Moreau, Arthur; Navab, Nassir; Busam, Benjamin","SCRREAM : SCan, Register, REnder And Map: A Framework for Annotating Accurate and Dense 3D Indoor Scenes with a Benchmark","","","","","","","","2025-03-30 16:25:54","2025-03-30 16:25:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UL5J4WQJ/Jung et al. - SCRREAM  SCan, Register, REnder And Map A Framework for Annotating Accurate and Dense 3D Indoor Sc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDCP546P","journalArticle","","Toshniwal, Shubham; Moshkov, Ivan; Narenthiran, Sean; Gitman, Daria; Jia, Fei; Gitman, Igor","OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset","","","","","","","","2025-03-30 16:25:56","2025-03-30 16:25:56","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7AV6WIP2/Toshniwal et al. - OpenMathInstruct-1 A 1.8 Million Math Instruction Tuning Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5YY9TUS","journalArticle","","Penedo, Guilherme; Kydlícˇek, Hynek; Mitchell, Margaret; Raffel, Colin","The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale","","","","","","The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-ofthe-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including indepth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.","","2025-03-30 16:25:57","2025-03-30 16:25:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/398VT666/Penedo et al. - The FineWeb Datasets Decanting the Web for the Finest Text Data at Scale.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UBAISJW3","journalArticle","","Guo, Chengquan; Liu, Xun; Xie, Chulin; Zhou, Andy; Zeng, Yi; Lin, Zinan; Song, Dawn; Li, Bo","RedCode: Risky Code Execution and Generation Benchmark for Code Agents","","","","","","With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, an evaluation platform with benchmarks grounded in four key principles: real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and highquality safety scenarios and tests. RedCode consists of two parts to evaluate agents’ safety in unsafe code execution and generation: (1) RedCode-Exec provides challenging code prompts in Python as inputs, aiming to evaluate code agents’ ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances. We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems. We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents’ vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing unsafe operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Unsafe operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen reveal that more capable base models and agents with stronger overall coding abilities, such as GPT4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are publicly available at https://github.com/AI-secure/RedCode.","","2025-03-30 16:25:58","2025-03-30 16:25:59","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BJ4NLCC7/Guo et al. - RedCode Risky Code Execution and Generation Benchmark for Code Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QT4PT3R","journalArticle","","Wang, Haohui; Guan, Weijie; Chen, Jianpeng; Wang, Zi; Zhou, Dawei","Towards Heterogeneous Long-tailed Learning: Benchmarking, Metrics, and Toolbox","","","","","","Long-tailed data distributions pose challenges for a variety of domains like ecommerce, finance, biomedical science, and cyber security, where the performance of machine learning models is often dominated by head categories while tail categories are inadequately learned. This work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks. We develop HEROLT, a comprehensive longtailed learning benchmark integrating 18 state-of-the-art algorithms, 10 evaluation metrics, and 17 real-world datasets across 6 tasks and 4 data modalities. HEROLT with novel angles and extensive experiments (315 in total) enables effective and fair evaluation of newly proposed methods compared with existing baselines on varying dataset types. Finally, we conclude by highlighting the significant applications of long-tailed learning and identifying several promising future directions. For accessibility and reproducibility, we open-source our benchmark HEROLT and corresponding results at https://github.com/SSSKJ/HeroLT.","","2025-03-30 16:26:00","2025-03-30 16:26:00","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8N5WQY68/Wang et al. - Towards Heterogeneous Long-tailed Learning Benchmarking, Metrics, and Toolbox.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQSW9S2F","journalArticle","","Shao, Minghao; Jancheska, Sofija; Udeshi, Meet; Dolan-Gavitt, Brendan; Xi, Haoran; Milner, Kimberly; Chen, Boyuan; Yin, Max; Garg, Siddharth; Krishnamurthy, Prashanth; Khorrami, Farshad; Karri, Ramesh; Shafique, Muhammad","NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security","","","","","","Large Language Models (LLMs) are being deployed across various domains today. However, their capacity to solve Capture the Flag (CTF) challenges in cybersecurity has not been thoroughly evaluated. To address this, we develop a novel method to assess LLMs in solving CTF challenges by creating a scalable, open-source benchmark database specifically designed for these applications. This database includes metadata for LLM testing and adaptive learning, compiling a diverse range of CTF challenges from popular competitions. Utilizing the advanced function calling capabilities of LLMs, we build a fully automated system with an enhanced workflow and support for external tool calls. Our benchmark dataset and automated framework allow us to evaluate the performance of five LLMs, encompassing both black-box and open-source models. This work lays the foundation for future research into improving the efficiency of LLMs in interactive cybersecurity tasks and automated task planning. By providing a specialized benchmark, our project offers an ideal platform for developing, testing, and refining LLM-based approaches to vulnerability detection and resolution. Evaluating LLMs on these challenges and comparing with human performance yields insights into their potential for AI-driven cybersecurity solutions to perform real-world threat management. We make our benchmark dataset open source to public https://github.com/NYU-LLM-CTF/NYU_CTF_Bench along with our playground automated framework https://github.com/NYU-LLM-CTF/llm_ ctf_automation.","","2025-03-30 16:26:01","2025-03-30 16:26:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QVLCW7LX/Shao et al. - NYU CTF Bench A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KSNZPTH2","journalArticle","","Zhao, Xinyu; Sun, Guoheng; Cai, Ruisi; Zhou, Yukun; Li, Pingzhi; Wang, Peihao; Tan, Bowen; He, Yexiao; Chen, Li; Liang, Yi; Chen, Beidi; Yuan, Binhang; Wang, Hongyi; Li, Ang; Wang, Zhangyang; Chen, Tianlong; Ch, Unc; Austin, UT","Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild","","","","","","As Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has gained significant attention, which is challenged by potential performance drop when combining disparate models. Various techniques have been proposed to aggregate pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed. In light of this research gap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. Utilizing the insights from the benchmark results, we formulate a strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization. Our methodology involves clustering mergeable models, selecting a merging strategy, and integrating model clusters through model-level mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, Model-GLUE shows an average performance enhancement of 5.61%, achieved without additional training. Codes are available at https://github.com/Model-GLUE/Model-GLUE.","","2025-03-30 16:26:03","2025-03-30 16:26:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/A9IYT9LB/Zhao et al. - Model-GLUE Democratized LLM Scaling for A Large Model Zoo in the Wild.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"574MP3SL","journalArticle","","Chandrasegaran, Keshigeyan; Gupta, Agrim; Hadzic, Lea M; Kota, Taran; He, Jimming; Eyzaguirre, Cristobal; Durante, Zane; Li, Manling; Fei-Fei, Jiajun Wu Li","HourVideo: 1-Hour Video-Language Understanding","","","","","","We present HourVideo, a benchmark dataset for hour-long video-language understanding. Our dataset consists of a novel task suite comprising summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. HourVideo includes 500 manually curated egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features 12,976 high-quality, five-way multiple-choice questions. Benchmarking results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve marginal improvements over random chance. In stark contrast, human experts significantly outperform the state-of-the-art long-context multimodal model, Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are available at hourvideo.stanford.edu.","","2025-03-30 16:26:05","2025-03-30 16:26:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8LRU4XJ8/Chandrasegaran et al. - HourVideo 1-Hour Video-Language Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GG6CQS9R","journalArticle","","Johansson, Fredrik D","IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark","","","","","","Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.","","2025-03-30 16:26:06","2025-03-30 16:26:06","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/I82AAK5Q/Johansson - IncomeSCM From tabular data set to time-series simulator and causal estimation benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TK6984J3","journalArticle","","Stergiou, Alexandros","LAVIB: A Large-scale Video Interpolation Benchmark","","","","","","","","2025-03-30 16:26:07","2025-03-30 16:26:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EMZFGW3H/Stergiou - LAVIB A Large-scale Video Interpolation Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EF7JIQEG","journalArticle","","Li, Pengxiang; Gao, Zhi; Zhang, Bofei; Yuan, Tao; Wu, Yuwei; Harandi, Mehrtash; Jia, Yunde; Zhu, Song-Chun; Li, Qing","FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models","","","","","","Vision language models (VLMs) have achieved impressive progress in diverse applications, becoming a prevalent research direction. In this paper, we build FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn conversations that are derived from 27 source datasets, empowering VLMs to spontaneously refine their responses based on user feedback across diverse tasks. To scale up the data collection, FIRE is collected in two components: FIRE-100K and FIRE1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is freely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a benchmark to comprehensively evaluate the feedback-refining capability of VLMs, which contains 11K feedback-refinement conversations as the test data, two evaluation settings, and a model to provide feedback for VLMs. We develop the FIRELLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows remarkable feedback-refining capability on FIRE-Bench and outperforms untrained VLMs by 50%, making more efficient user-agent interactions and underscoring the significance of the FIRE dataset.","","2025-03-30 16:26:09","2025-03-30 16:26:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7N2C7FDW/Li et al. - FIRE A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AG23IHCK","journalArticle","","Zhang, Kaiyan; Zeng, Sihang; Hua, Ermo; Ding, Ning; Chen, Zhang-Ren; Ma, Zhiyuan; Li, Haoxin; Cui, Ganqu; Qi, Biqing; Zhu, Xuekai; Lv, Xingtai; Hu, Jin-Fang; Liu, Zhiyuan; Zhou, Bowen","UltraMedical: Building Specialized Generalists in Biomedicine","","","","","","Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.","","2025-03-30 16:26:10","2025-03-30 16:26:10","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/B7578KD9/Zhang et al. - UltraMedical Building Specialized Generalists in Biomedicine.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UFSC5YLQ","journalArticle","","Ma, Xiaochen; Zhu, Xuekang; Su, Lei; Du, Bo; Jiang, Zhuohang; Tong, Bingkui; Lei, Zeyu; Yang, Xinyu; Pun, Chi-Man; Lv, Jiancheng; Zhou, Jizhe","IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection & Localization","","","","","","A comprehensive benchmark is yet to be established in the Image Manipulation Detection & Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading model evaluations, severely undermining the development of this field. However, the scarcity of open-sourced baseline models and inconsistent training and evaluation protocols make conducting rigorous experiments and faithful comparisons among IMDL models challenging. To address these challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase. IMDL-BenCo: i) decomposes the IMDL framework into standardized, reusable components and revises the model construction pipeline, improving coding efficiency and customization flexibility; ii) fully implements or incorporates training code for state-of-the-art models to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the established benchmark and codebase, offering new insights into IMDL model architecture, dataset characteristics, and evaluation standards. Specifically, IMDL-BenCo includes common processing algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of robustness evaluation. This benchmark and codebase represent a significant leap forward in calibrating the current progress in the IMDL field and inspiring future breakthroughs. Code is available at: https://github.com/scu-zjz/IMDLBenCo.","","2025-03-30 16:26:11","2025-03-30 16:26:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XWLH783U/Ma et al. - IMDL-BenCo A Comprehensive Benchmark and Codebase for Image Manipulation Detection & Localization.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8YZ9ZCVJ","journalArticle","","Ho, Cherie; Zou, Jiaye; Alama, Omar; Kumar, Sai Mitheran Jagadesh; Chiang, Benjamin; Gupta, Taneesh; Wang, Chen; Keetha, Nikhil; Sycara, Katia; Scherer, Sebastian","Map It Anywhere (MIA): Empowering Bird’s Eye View Mapping using Large-scale Public Data","","","","","","Top-down Bird’s Eye View (BEV) maps are a popular perceptual representation for ground robot navigation due to their richness and flexibility for downstream tasks. While recent methods have shown promise for predicting BEV maps from First-Person View (FPV) images, their generalizability is limited to small regions captured by current autonomous vehicle-based datasets. In this context, we show that a more scalable approach towards generalizable map prediction can be enabled by using two large-scale crowd-sourced mapping platforms, Mapillary for FPV images and OpenStreetMap for BEV semantic maps. We introduce Map It Anywhere (MIA), a data engine that enables seamless curation and modeling of labeled map prediction data from existing open-source map platforms. Using our MIA data engine, we display the ease of automatically collecting a dataset of 1.2 million pairs of FPV images & BEV maps encompassing diverse geographies, landscapes, environmental factors, camera models & capture scenarios. We further train a simple camera model-agnostic model on this data for BEV map prediction. Extensive evaluations using established benchmarks and our dataset show that the data curated by MIA enables effective pretraining for generalizable BEV map prediction, with zero-shot performance far exceeding baselines trained on existing datasets by 35%. Our analysis highlights the promise of using large-scale public maps for developing & testing generalizable BEV perception, paving the way for more robust autonomous navigation. Website: mapitanywhere.github.io ∗Equal contribution.","","2025-03-30 16:26:12","2025-03-30 16:26:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/E3LGBUE7/Ho et al. - Map It Anywhere (MIA) Empowering Bird’s Eye View Mapping using Large-scale Public Data.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4CR6L6BR","journalArticle","","Souly, Alexandra; Lu, Qingyuan; Bowen, Dillon; Trinh, Tu; Hsieh, Elvis; Pandey, Sana; Abbeel, Pieter; Svegliato, Justin; Emmons, Scott; Watkins, Olivia; Toyer, Sam","A STRONGREJECT for Empty Jailbreaks","","","","","","Most jailbreak papers claim the jailbreaks they propose are highly effective, often boasting near-100% attack success rates. However, it is perhaps more common than not for jailbreak developers to substantially exaggerate the effectiveness of their jailbreaks. We suggest this problem arises because jailbreak researchers lack a standard, high-quality benchmark for evaluating jailbreak performance, leaving researchers to create their own. To create a benchmark, researchers must choose a dataset of forbidden prompts to which a victim model will respond, along with an evaluation method that scores the harmfulness of the victim model’s responses. We show that existing benchmarks suffer from significant shortcomings and introduce the StrongREJECT benchmark to address these issues. StrongREJECT’s dataset contains prompts that victim models must answer with specific, harmful information, while its automated evaluator measures the extent to which a response gives useful information to forbidden prompts. In doing so, the StrongREJECT evaluator achieves state-of-the-art agreement with human judgments of jailbreak effectiveness. Notably, we find that existing evaluation methods significantly overstate jailbreak effectiveness compared to human judgments and the StrongREJECT evaluator. We describe a surprising and novel phenomenon that explains this discrepancy: jailbreaks bypassing a victim model’s safety fine-tuning tend to reduce its capabilities. Together, our findings underscore the need for researchers to use a high-quality benchmark, such as StrongREJECT, when developing new jailbreak attacks. We release the StrongREJECT code and data at https://strong-reject.readthedocs.io/.","","2025-03-30 16:26:14","2025-03-30 16:26:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4JNHT943/Souly et al. - A STRONGREJECT for Empty Jailbreaks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8XMVGG2","journalArticle","","Huang, Han; Zhong, Haitian; Yu, Tao; Liu, Qiang; Wu, Shu; Wang, Liang; Tan, Tieniu","VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark","","","","","","Recently, knowledge editing on large language models (LLMs) has received considerable attention. Compared to this, editing Large Vision-Language Models (LVLMs) faces extra challenges from diverse data modalities and complicated model components, and data for LVLMs editing are limited. The existing LVLM editing benchmark, which comprises three metrics (Reliability, Locality, and Generality), falls short in the quality of synthesized evaluation images and cannot assess whether models apply edited knowledge in relevant content. Therefore, we employ more reliable data collection methods to construct a new Large Vision-Language Model Knowledge Editing Benchmark, VLKEB, and extend the Portability metric for more comprehensive evaluation. Leveraging a multi-modal knowledge graph, our image data are bound with knowledge entities. This can be further used to extract entity-related knowledge, which constitutes the base of editing data. We conduct experiments of different editing methods on five LVLMs, and thoroughly analyze how do they impact the models. The results reveal strengths and deficiencies of these methods and hopefully provide insights for future research. The codes and dataset are available at: https://github.com/VLKEB/VLKEB.","","2025-03-30 16:26:15","2025-03-30 16:26:15","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HD4IKS6V/Huang et al. - VLKEB A Large Vision-Language Model Knowledge Editing Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UZUQP9HF","journalArticle","","Bukas, Christina; Subramanian, Harshavardhan; See, Fenja; Steinchen, Carina; Ezhov, Ivan; Boosarpu, Gowtham; Asgharpour, Sara; Burgstaller, Gerald; Lehmann, Mareike; Kofler, Florian; Piraud, Marie","MultiOrg: A Multi-rater Organoid-detection Dataset","","","","","","High-throughput image analysis in the biomedical domain has gained significant attention in recent years, driving advancements in drug discovery, disease prediction, and personalized medicine. Organoids, specifically, are an active area of research, providing excellent models for human organs and their functions. Automating the quantification of organoids in microscopy images would provide an effective solution to overcome substantial manual quantification bottlenecks, particularly in high-throughput image analysis. However, there is a notable lack of open biomedical datasets, in contrast to other domains, such as autonomous driving, and, notably, only few of them have attempted to quantify annotation uncertainty. In this work, we present MultiOrg a comprehensive organoid dataset tailored for object detection tasks with uncertainty quantification. This dataset comprises over 400 high-resolution 2d microscopy images and curated annotations of more than 60,000 organoids. Most importantly, it includes three label sets for the test data, independently annotated by two experts at distinct time points. We additionally provide a benchmark for organoid detection, and make the best model available through an easily installable, interactive plugin for the popular image visualization tool Napari, to perform organoid quantification.","","2025-03-30 16:26:16","2025-03-30 16:26:16","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NVHL5UZ9/Bukas et al. - MultiOrg A Multi-rater Organoid-detection Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"565ELTVK","dataset","","Kweon, Sunjun; Kim, Jiyoun; Kwak, Heeyoung; Cha, Dongchul; Yoon, Hangyul; Kim, Kwang Hyun; Yang, Jeewon; Won, Seunghyun; Choi, Edward","EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries","","","","10.13026/ACGA-HT95","https://physionet.org/content/ehr-notes-qa-llms/1.0.1/","Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs’ capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients’ discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers ten diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (e.g., the length and number of discharge summaries). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman(ρ): 0.78, Kendall(τ ): 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings. EHRNoteQA is publicly available under PhysioNet credential access at https://doi.org/10.13026/acga-ht95, and the code is available at https://github.com/ji-youn-kim/EHRNoteQA.","","2025-03-30 16:26:18","2025-03-30 16:26:18","2025-03-30 16:26:18","","","","","","","EHRNoteQA","","","","","PhysioNet","","en","","","","","DOI.org (Datacite)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3VGDJBY6/Kweon et al. - EHRNoteQA An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","1.0.1","","","","","","","",""
"7ICRTRHB","journalArticle","","Sun, Jianhua; Li, Yuxuan; Xu, Longfei; Wang, Nange; Wei, Jiude; Zhang, Yining; Lu, Cewu","ConceptFactory: Facilitate 3D Object Knowledge Annotation with Object Conceptualization","","","","","","We present ConceptFactory, a novel scope to facilitate more efficient annotation of 3D object knowledge by recognizing 3D objects through generalized concepts (i.e. object conceptualization), aiming at promoting machine intelligence to learn comprehensive object knowledge from both vision and robotics aspects. This idea originates from the findings in human cognition research that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components (e.g. cuboids and cylinders). ConceptFactory consists of two critical parts: i) ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization, and ii) ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite. Our approach enables researchers to effortlessly acquire or customize extensive varieties of object knowledge to comprehensively study different object understanding tasks. We validate our idea on a wide range of benchmark tasks from both vision and robotics aspects with state-of-the-art algorithms, demonstrating the high quality and versatility of annotations provided by our approach. Our website is available at https://apeirony.github.io/ConceptFactory.","","2025-03-30 16:26:19","2025-03-30 16:26:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/69YY9ZB8/Sun et al. - ConceptFactory Facilitate 3D Object Knowledge Annotation with Object Conceptualization.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4PR9HW7Q","journalArticle","","Li, Xiang; Ding, Jian; Elhoseiny, Mohamed","VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding","","","","","","We introduce a new benchmark designed to advance the development of generalpurpose, large-scale vision-language models for remote sensing images. Although several vision-language datasets in remote sensing have been proposed to pursue this goal, existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. Exploring these improvement opportunities, we present a Versatile vision-language Benchmark for Remote Sensing image understanding, termed VRSBench. This benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 123,221 question-answer pairs. It facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. We further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. Our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing. The data and code can be accessed at https://vrsbench.github.io.","","2025-03-30 16:26:20","2025-03-30 16:26:20","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ST8SFH8C/Li et al. - VRSBench A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4SXYPW9I","journalArticle","","Bountos, Nikolaos Ioannis; Sdraka, Maria; Zavras, Angelos; Karasante, Ilektra; Karavias, Andreas; Herekakis, Themistocles; Thanasou, Angeliki; Michail, Dimitrios; Papoutsis, Ioannis","Kuro Siwo: 33 billion m2 under the water A global multi-temporal satellite dataset for rapid flood mapping","","","","","","Global flash floods, exacerbated by climate change, pose severe threats to human life, infrastructure, and the environment. Recent catastrophic events in Pakistan and New Zealand underscore the urgent need for precise flood mapping to guide restoration efforts, understand vulnerabilities, and prepare for future occurrences. While Synthetic Aperture Radar (SAR) remote sensing offers day-and-night, all-weather imaging capabilities, its application in deep learning for flood segmentation is limited by the lack of large annotated datasets. To address this, we introduce Kuro Siwo, a manually annotated multi-temporal dataset, spanning 43 flood events globally. Our dataset maps more than 338 billion m2 of land, with 33 billion designated as either flooded areas or permanent water bodies. Kuro Siwo includes a highly processed product optimized for flash flood mapping based on SAR Ground Range Detected, and a primal SAR Single Look Complex product with minimal preprocessing, designed to promote research on the exploitation of both the phase and amplitude information and to offer maximum flexibility for downstream task preprocessing. To leverage advances in large scale self-supervised pretraining methods for remote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR samples. Finally, we provide an extensive benchmark, namely BlackBench, offering strong baselines for a diverse set of flood events globally. All data and code are published in our Github repository: https://github.com/Orion-AI-Lab/KuroSiwo.","","2025-03-30 16:26:22","2025-03-30 16:26:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CB6IJWUU/Bountos et al. - Kuro Siwo 33 billion m2 under the water A global multi-temporal satellite dataset for rapid flood m.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFV2YKCT","journalArticle","","Li, Ruosen; Wang, Zimu; Tran, Son Quoc; Xia, Lei; Du, Xinya","MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations","","","","","","Existing benchmarks for multi-hop question answering (QA) primarily evaluate models based on their ability to reason about entities and the relationships between them. However, there’s a lack of insight into how these models perform in terms of both events and entities. In this paper, we introduce a novel semi-automatic question generation strategy by composing event structures from information extraction (IE) datasets and present the first Multi-hop Event-centric Question Answering (MEQA) benchmark1. It contains (1) 2,243 challenging questions that require a diverse range of complex reasoning over entity-entity, entity-event, and event-event relations; (2) corresponding multi-step QA-format event reasoning chain (explanation) which leads to the answer for each question. We also introduce two metrics for evaluating explanations: completeness and logical consistency. We conduct comprehensive benchmarking and analysis, which shows that MEQA is challenging for the latest state-of-the-art models encompassing large language models (LLMs); and how they fall short of providing faithful explanations of the event-centric reasoning process.","","2025-03-30 16:26:23","2025-03-30 16:26:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QLQ4LB8U/Li et al. - MEQA A Benchmark for Multi-hop Event-centric Question Answering with Explanations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUHF2L2P","journalArticle","","Mathai, Alex; Huang, Chenxi; Maniatis, Petros; Nogikh, Aleksandr; Ivancˇic, Franjo; Yang, Junfeng; Ray, Baishakhi","KGYM: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution","","","","","","Large Language Models (LLMs) are consistently improving at increasingly realistic software engineering (SE) tasks. In real-world software stacks, significant SE effort is spent developing foundational system software like the Linux kernel. Unlike application-level software, a systems codebase like Linux is multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines); critical (impacting billions of devices worldwide), and highly concurrent (involving complex multi-threading). To evaluate if machine learning (ML) models are useful while developing such large-scale systems-level software, we introduce KGYM (a platform) and KBENCHSYZ (a dataset). The KGYM2 platform provides a SE environment for large-scale experiments on the Linux kernel, including compiling and running kernels in parallel across several virtual machines, detecting operations and crashes, inspecting logs, and querying and patching the code base. We use KGYM to facilitate evaluation on KBENCHSYZ, a crash resolution benchmark drawn from real-world Linux kernel bugs. An example bug in KBENCHSYZ contains crashing stack traces, a bug-reproducer file, a developer-written fix, and other associated data. To understand current performance, we conduct baseline experiments by prompting LLMs to resolve Linux kernel crashes. Our initial evaluations reveal that the best performing LLM achieves 0.72% and 5.38% in the unassisted and assisted (i.e., buggy files disclosed to the model) settings, respectively. These results highlight the need for further research to enhance model performance in SE tasks. Improving performance on KBENCHSYZ requires models to master new learning skills, including understanding the cause of crashes and repairing faults, writing memory-safe and hardware-aware code, and understanding concurrency. As a result, this work opens up multiple avenues of research at the intersection of machine learning and systems software.","","2025-03-30 16:26:24","2025-03-30 16:26:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HXQL73VN/Mathai et al. - KGYM A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLMMLIN4","journalArticle","","Feuer, Benjamin; Xu, Jiawei; Cohen, Niv; Yubeaton, Patrick; Mittal, Govind; Hegde, Chinmay","SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification","","","","","","Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT , the first large-scale benchmark of curation strategies for image classification.","","2025-03-30 16:26:26","2025-03-30 16:26:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2336JCDZ/Feuer et al. - SELECT A Large-Scale Benchmark of Data Curation Strategies for Image Classification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W76LEVXH","journalArticle","","Maruf, M; Daw, Arka; Mehrab, Kazi Sajeed; Manogaran, Harish Babu; Neog, Abhilash; Sawhney, Medha; Khurana, Mridul; Balhoff, James P; Bakıs, Yasin; Altintas, Bahadir; Thompson, Matthew J; Campolongo, Elizabeth G; Uyeda, Josef C; Lapp, Hilmar; Jr, Henry L Bart; Mabee, Paula M; Su, Yu; Chao, Wei-Lun; Stewart, Charles; Berger-Wolf, Tanya; Dahdul, Wasila; Karpatne, Anuj","VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images","","","","","","","","2025-03-30 16:26:27","2025-03-30 16:26:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/F9FRVNUN/Maruf et al. - VLM4Bio A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XV6SSUCE","journalArticle","","Chen, Haozhe; Li, Ang; Che, Ethan; Peng, Tianyi; Dong, Jing; Namkoong, Hongseok","QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers","","","","","","Queuing network control determines the allocation of scarce resources to manage congestion, a fundamental problem in manufacturing, communications, and healthcare. Compared to standard RL problems, queueing problems are distinguished by unique challenges: i) a system operating in continuous time, ii) high stochasticity, and iii) long horizons over which the system can become unstable (exploding delays). To spur methodological progress tackling these challenges, we present an open-sourced queueing simulation framework, QGym, that benchmark queueing policies across realistic problem instances. Our modular framework allows the researchers to build on our initial instances, which provide a wide range of environments including parallel servers, criss-cross, tandem, and re-entrant networks, as well as a realistically calibrated hospital queuing system. QGym makes it easy to compare multiple policies, including both model-free RL methods and classical queuing policies. Our testbed complements the traditional focus on evaluating algorithms based on mathematical guarantees in idealized settings, and significantly expands the scope of empirical benchmarking in prior work. QGym code is open-sourced at https://github.com/namkoong-lab/QGym.","","2025-03-30 16:26:29","2025-03-30 16:26:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/LBEJFBAQ/Chen et al. - QGym Scalable Simulation and Benchmarking of Queuing Network Controllers.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S8F2IGMK","journalArticle","","Alam, Tanvirul; Bhusal, Dipkamal","CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence","","","","","","Cyber threat intelligence (CTI) is crucial in today’s cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs’ performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI. Code and dataset available at https://github.com/aiforsec/cti-bench.","","2025-03-30 16:26:30","2025-03-30 16:26:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RICC3W4K/Alam and Bhusal - CTIBench A Benchmark for Evaluating LLMs in Cyber Threat Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J763CBXE","journalArticle","","Huang, Dong; Qing, Yuhao; Shang, Weiyi; Cui, Heming; Zhang, Jie M","EFFIBENCH: Benchmarking the Efficiency of Automatically Generated Code","","","","","","Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in green computing and sustainability efforts — the efficiency of the generated code — has often been neglected. This paper presents EFFIBENCH, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EFFIBENCH contains a diverse set of LeetCode coding problems. Each problem is paired with an executable humanwritten canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EFFIBENCH, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) in generating efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average 3.12 times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 generated code are 13.89 and 43.92 times that of the canonical solutions. The source code of EffiBench is released on https: //github.com/huangd1999/EffiBench. We also provide the LeaderBoard in https://huggingface.co/spaces/EffiBench/effibench-leaderboard.","","2025-03-30 16:26:31","2025-03-30 16:26:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QWESDF5S/Huang et al. - EFFIBENCH Benchmarking the Efficiency of Automatically Generated Code.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4SZVXNRT","journalArticle","","Liu, Puze; Günster, Jonas; Funk, Niklas; Gröger, Simon; Chen, Dong; Bou-Ammar, Haitham; Jankowski, Julius; Maric, Ante; Calinon, Sylvain; Orsula, Andrej; Olivares-Mendez, Miguel; Zhou, Hongyi; Lioutikov, Rudolf; Neumann, Gerhard; Likmeta, Amarildo; Zhalehmehrabi, Amirhossein; Bonenfant, Thomas; Restelli, Marcello; Tateo, Davide; Liu, Ziyuan; Peters, Jan","A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics","","","","","","Machine learning methods have a groundbreaking impact in many application domains, but their application on real robotic platforms is still limited. Despite the many challenges associated with combining machine learning technology with robotics, robot learning remains one of the most promising directions for enhancing the capabilities of robots. When deploying learning-based approaches on real robots, extra effort is required to address the challenges posed by various real-world factors. To investigate the key factors influencing real-world deployment and to encourage original solutions from different researchers, we organized the Robot Air Hockey Challenge at the NeurIPS 2023 conference. We selected the air hockey task as a benchmark, encompassing low-level robotics problems and high-level tactics. Different from other machine learning-centric benchmarks, participants need to tackle practical challenges in robotics, such as the sim-toreal gap, low-level control issues, safety problems, real-time requirements, and the limited availability of real-world data. Furthermore, we focus on a dynamic environment, removing the typical assumption of quasi-static motions of other real-world benchmarks. The competition’s results show that solutions combining learning-based approaches with prior knowledge outperform those relying solely on data when real-world deployment is challenging. Our ablation study reveals which real-world factors may be overlooked when building a learning-based solution. The successful real-world air hockey deployment of best-performing agents sets the foundation for future competitions and follow-up research directions.","","2025-03-30 16:26:32","2025-03-30 16:26:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HCT7WAMT/Liu et al. - A Retrospective on the Robot Air Hockey Challenge Benchmarking Robust, Reliable, and Safe Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KXZI4XGU","journalArticle","","Hemmat, Arshia; Davies, Adam; Lamb, Tom A; Yuan, Jianhao; Torr, Philip; Khakzar, Ashkan; Pinto, Francesco","Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models","","","","","","","","2025-03-30 16:26:33","2025-03-30 16:26:33","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4GPSMGN4/Hemmat et al. - Hidden in Plain Sight Evaluating Abstract Shape Recognition in Vision-Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FWP7L65Q","journalArticle","","Li, Yuhan; Wang, Peisong; Zhu, Xiao; Chen, Aochuan; Jiang, Haiyun; Cai, Deng; Chan, Victor Wai Kin; Li, Jia","GLBench: A Comprehensive Benchmark for Graph with Large Language Models","","","","","","The emergence of large language models (LLMs) has revolutionized the way we interact with graphs, leading to a new paradigm called GraphLLM. Despite the rapid development of GraphLLM methods in recent years, the progress and understanding of this field remain unclear due to the lack of a benchmark with consistent experimental protocols. To bridge this gap, we introduce GLBench, the first comprehensive benchmark for evaluating GraphLLM methods in both supervised and zero-shot scenarios. GLBench provides a fair and thorough evaluation of different categories of GraphLLM methods, along with traditional baselines such as graph neural networks. Through extensive experiments on a collection of real-world datasets with consistent data processing and splitting strategies, we have uncovered several key findings. Firstly, GraphLLM methods outperform traditional baselines in supervised settings, with LLM-as-enhancers showing the most robust performance. However, using LLMs as predictors is less effective and often leads to uncontrollable output issues. We also notice that no clear scaling laws exist for current GraphLLM methods. In addition, both structures and semantics are crucial for effective zero-shot transfer, and our proposed simple baseline can even outperform several models tailored for zero-shot scenarios. The data and code of the benchmark can be found at https://github.com/NineAbyss/GLBench.","","2025-03-30 16:26:35","2025-03-30 16:26:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UKXWMTTI/Li et al. - GLBench A Comprehensive Benchmark for Graph with Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J8EMV5YS","journalArticle","","Wen, Bosi; Ke, Pei; Gu, Xiaotao; Wu, Lindong; Huang, Hao; Zhou, Jinfeng; Li, Wenchuang; Hu, Binxin; Gao, Wendy; Xu, Jiaxin; Liu, Yiming; Tang, Jie; Wang, Hongning; Huang, Minlie","Benchmarking Complex Instruction-Following with Multiple Constraints Composition","","","","","","Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose ComplexBench, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition. Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. ComplexBench identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition1.","","2025-03-30 16:26:36","2025-03-30 16:26:36","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BLNYJ4HC/Wen et al. - Benchmarking Complex Instruction-Following with Multiple Constraints Composition.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WC8TCSAT","journalArticle","","Romero, David; Lyu, Chenyang; Wibowo, Haryo Akbarianto; Lynn, Teresa; Hamed, Injy; Kishore, Aditya Nanda; Mandal, Aishik; Dragonetti, Alina; Abzaliev, Artem; Tonja, Atnafu Lambebo; Balcha, Bontu Fufa; Whitehouse, Chenxi; Salamea, Christian; Velasco, Dan John; Adelani, David Ifeoluwa; Meur, David Le; Villa-Cueva, Emilio; Koto, Fajri; Farooqui, Fauzan; Belcavello, Frederico; Batnasan, Ganzorig; Vallejo, Gisela; Caulfield, Grainne; Ivetta, Guido; Song, Haiyue; Ademtew, Henok Biadglign; Maina, Hernán; Ortiz-Barajas, Jesus-German; Baek, Jinheon; Dunstan, Jocelyn; Alemany, Laura Alonso; Nagasinghe, Kumaranage Ravindu Yasas; Benotti, Luciana; D’Haro, Luis Fernando; Viridiano, Marcelo; Estecha-Garitagoitia, Marcos; Cabrera, Maria Camila Buitrago; Rodríguez-Cantelar, Mario; Jouitteau, Mélanie; Mihaylov, Mihail; Etori, Naome; Imam, Mohamed Fazli Mohamed; Adilazuarda, Muhammad Farid; Gochoo, Munkhjargal; Otgonbold, Munkh-Erdene; Niyomugisha, Olivier; Silva, Paula Mónica; Chitale, Pranjal; Dabre, Raj; Chevi, Rendi; Zhang, Ruochen; Diandaru, Ryandito; Cahyawijaya, Samuel; Góngora, Santiago; Jeong, Soyeong; Purkayastha, Sukannya; Kuribayashi, Tatsuki; Clifford, Teresa; Jayakumar, Thanmay; Torrent, Tiago Timponi; Ehsan, Toqeer; Araujo, Vladimir; Kementchedjhieva, Yova; Burzo, Zara; Lim, Zheng Wei; Yong, Zheng Xin; Ignat, Oana; Nwatu, Joan; Mihalcea, Rada; Solorio, Thamar; Aji, Alham Fikri","CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark","","","","","","","","2025-03-30 16:26:38","2025-03-30 16:26:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/N3TTXLMZ/Romero et al. - CVQA Culturally-diverse Multilingual Visual Question Answering Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6EYVI5RH","journalArticle","","Alberts, Marvin; Schilter, Oliver; Zipoli, Federico; Hartrampf, Nina; Laino, Teodoro","Unraveling Molecular Structure: A Multimodal Spectroscopic Dataset for Chemistry","","","","","","Spectroscopic techniques are essential tools for determining the structure of molecules. Different spectroscopic techniques, such as Nuclear magnetic resonance (NMR), Infrared spectroscopy, and Mass Spectrometry, provide insight into the molecular structure, including the presence or absence of functional groups. Chemists leverage the complementary nature of the different methods to their advantage. However, the lack of a comprehensive multimodal dataset, containing spectra from a variety of spectroscopic techniques, has limited machine-learning approaches mostly to single-modality tasks for predicting molecular structures from spectra. Here we introduce a dataset comprising simulated 1H-NMR, 13CNMR, HSQC-NMR, Infrared, and Mass spectra (positive and negative ion modes) for 790k molecules extracted from chemical reactions in patent data. This dataset enables the development of foundation models for integrating information from multiple spectroscopic modalities, emulating the approach employed by human experts. Additionally, we provide benchmarks for evaluating single-modality tasks such as structure elucidation, predicting the spectra for a target molecule, and functional group predictions. This dataset has the potential automate structure elucidation, streamlining the molecular discovery pipeline from synthesis to structure determination. The dataset and code for the benchmarks can be found at https: //rxn4chemistry.github.io/multimodal-spectroscopic-dataset.","","2025-03-30 16:26:39","2025-03-30 16:26:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YX2H5MLY/Alberts et al. - Unraveling Molecular Structure A Multimodal Spectroscopic Dataset for Chemistry.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E8HEUPJX","journalArticle","","Fent, Felix; Kuttenreich, Fabian; Ruch, Florian; Rizwin, Farija; Juergens, Stefan; Lechermann, Lorenz; Nissler, Christian; Perl, Andrea; Voll, Ulrich; Yan, Min; Lienkamp, Markus","MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse conditions","","","","","","Autonomous trucking is a promising technology that can greatly impact modern logistics and the environment. Ensuring its safety on public roads is one of the main duties that requires an accurate perception of the environment. To achieve this, machine learning methods rely on large datasets, but to this day, no such datasets are available for autonomous trucks. In this work, we present MAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN TruckScenes allows the research community to come into contact with truck-specific challenges, such as trailer occlusions, novel sensor perspectives, and terminal environments for the first time. It comprises more than 740 scenes of 20 s each within a multitude of different environmental conditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2 IMUs, and a high-precision GNSS. The dataset’s 3D bounding boxes were manually annotated and carefully reviewed to achieve a high quality standard. Bounding boxes are available for 27 object classes, 15 attributes, and a range of more than 230 m. The scenes are tagged according to 34 distinct scene tags, and all objects are tracked throughout the scene to promote a wide range of applications. Additionally, MAN TruckScenes is the first dataset to provide 4D radar data with 360° coverage and is thereby the largest radar dataset with annotated 3D bounding boxes. Finally, we provide extensive dataset analysis and baseline results. The dataset, development kit, and more are available online.","","2025-03-30 16:26:40","2025-03-30 16:26:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/X7YWL6R7/Fent et al. - MAN TruckScenes A multimodal dataset for autonomous trucking in diverse conditions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJVIYPQ4","journalArticle","","Varbella, Anna; Amara, Kenza; Gjorgiev, Blazhe; El-Assady, Mennatallah; Sansavini, Giovanni","PowerGraph: A power grid benchmark dataset for graph neural networks","","","","","","","","2025-03-30 16:26:41","2025-03-30 16:26:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XSLBYTHM/Varbella et al. - PowerGraph A power grid benchmark dataset for graph neural networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AU362KL9","journalArticle","","Debenedetti, Edoardo; Zhang, Jie; Balunovic, Mislav; Beurer-Kellner, Luca; Fischer, Marc; Tramèr, Florian","AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents","","","","","","AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.","","2025-03-30 16:26:42","2025-03-30 16:26:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7TFJGTDT/Debenedetti et al. - AgentDojo A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRE5R9AI","journalArticle","","Chen, Pengcheng; Ye, Jin; Wang, Guoan; Li, Yanjun; Deng, Zhongying; Li, Wei; Li, Tianbin; Duan, Haodong; Huang, Ziyan; Su, Yanzhou; Wang, Benyou; Zhang, Shaoting; Fu, Bin; Cai, Jianfei; Zhuang, Bohan; Seibel, Eric J; Qiao, Yu; He, Junjun","GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI","","","","","","","","2025-03-30 16:26:44","2025-03-30 16:26:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/66SCIJ4B/Chen et al. - GMAI-MMBench A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XPAKHQ4Y","journalArticle","","Bandara, Nuwan; Kandappu, Thivya; Sen, Argha; Gokarn, Ila; Misra, Archan","EyeGraph: Modularity-aware Spatio Temporal Graph Clustering for Continuous Event-based Eye Tracking","","","","","","Continuous tracking of eye movement dynamics plays a significant role in developing a broad spectrum of human-centered applications, such as cognitive skills modeling, biometric user authentication, and foveated rendering. Recently neuromorphic cameras have garnered significant interest in the eye-tracking research community, owing to their sub-microsecond latency in capturing intensity changes resulting from eye movements. Nevertheless, the existing approaches for eventbased eye tracking suffer from several limitations: dependence on RGB frames, label sparsity, and training on datasets collected in controlled lab environments that do not adequately reflect real-world scenarios. To address these limitations, in this paper, we propose a dynamic graph-based approach that uses the event stream for high-fidelity tracking of pupillary movement. We first present EyeGraph, a large-scale, multi-modal near-eye tracking dataset collected using a wearable event camera attached to a head-mounted device from 40 participants – the dataset was curated while mimicking in-the-wild settings, with variations in user movement and ambient lighting conditions. Subsequently, to address the issue of label sparsity, we propose an unsupervised topology-aware spatio-temporal graph clustering approach as a benchmark. We show that our unsupervised approach achieves performance comparable to more onerous supervised approaches while consistently outperforming the conventional clustering-based unsupervised approaches.","","2025-03-30 16:26:45","2025-03-30 16:26:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/L2YXV7A6/Bandara et al. - EyeGraph Modularity-aware Spatio Temporal Graph Clustering for Continuous Event-based Eye Tracking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"THYZLHQB","journalArticle","","Reuel, Anka; Hardy, Amelia; Smith, Chandler; Lamparth, Max; Hardy, Malcolm; Kochenderfer, Mykel J","BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices","","","","","","","","2025-03-30 16:26:46","2025-03-30 16:26:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EZ8VYI99/Reuel et al. - BetterBench Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5F28W2U","journalArticle","","Ding, Mucong; Deng, Chenghao; Choo, Jocelyn; Wu, Zichu; Agrawal, Aakriti; Schwarzschild, Avi; Zhou, Tianyi; Goldstein, Tom; Langford, John; Anandkumar, Anima; Huang, Furong","Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization","","","","","","While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still missing. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six stateof-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https: //huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.","","2025-03-30 16:26:48","2025-03-30 16:26:48","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/P7XPZKR4/Ding et al. - Easy2Hard-Bench Standardized Difficulty Labels for Profiling LLM Performance and Generalization.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HM2HMXGU","journalArticle","","Ren, Richard; Basart, Steven; Khoja, Adam; Pan, Alexander; Gatti, Alice; Phan, Long; Yin, Xuwang; Mazeika, Mantas; Mukobi, Gabriel; Kim, Ryan Hwang; Fitz, Stephen; Hendrycks, Dan","Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?","","","","","","Performance on popular ML benchmarks is highly correlated with model scale, suggesting that most benchmarks tend to measure a similar underlying factor of general model capabilities. However, substantial research effort remains devoted to designing new benchmarks, many of which claim to measure novel phenomena. In the spirit of the Bitter Lesson, we leverage spectral analysis to measure an underlying capabilities component, the direction in benchmark-performance-space which explains most variation in model performance. In an extensive analysis of existing safety benchmarks, we find that variance in model performance on many safety benchmarks is largely explained by the capabilities component. In response, we argue that safety research should prioritize metrics which are not highly correlated with scale. Our work provides a lens to analyze both novel safety benchmarks and novel safety methods, which we hope will enable future work to make differential progress on safety.","","2025-03-30 16:26:49","2025-03-30 16:26:49","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7GYTETF9/Ren et al. - Safetywashing Do AI Safety Benchmarks Actually Measure Safety Progress.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"73YR5DPZ","journalArticle","","González-Duque, Miguel; Michael, Richard; Bartels, Simon; Zainchkovskyy, Yevgen; Hauberg, Søren; Boomsma, Wouter","A survey and benchmark of high-dimensional Bayesian optimization of discrete sequences","","","","","","Optimizing discrete black box functions is key in several domains, e.g. protein engineering and drug design. Due to the lack of gradient information and the need for sample efficiency, Bayesian optimization is an ideal candidate for these tasks. Several methods for high-dimensional continuous and categorical Bayesian optimization have been proposed recently. However, our survey of the field reveals highly heterogeneous experimental set-ups across methods and technical barriers for the replicability and application of published algorithms to real-world tasks. To address these issues, we develop a unified framework to test a vast array of high-dimensional Bayesian optimization methods and a collection of standardized black box functions representing real-world application domains in chemistry and biology. These two components of the benchmark are each supported by flexible, scalable, and easily extendable software libraries (poli and poli-baselines), allowing practitioners to readily incorporate new optimization objectives or discrete optimizers. Project website: https://machinelearninglifescience.github.io/hdbo_benchmark.","","2025-03-30 16:26:52","2025-03-30 16:26:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EXSQ8JRC/González-Duque et al. - A survey and benchmark of high-dimensional Bayesian optimization of discrete sequences.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQU45MB6","journalArticle","","Song, Jian; Chen, Hongruixuan; Xuan, Weihao; Xia, Junshi; Yokoya, Naoto","SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding from Monocular Remote Sensing Imagery","","","","","","Global semantic 3D understanding from single-view high-resolution remote sensing (RS) imagery is crucial for Earth observation (EO). However, this task faces significant challenges due to the high costs of annotations and data collection, as well as geographically restricted data availability. To address these challenges, synthetic data offer a promising solution by being unrestricted and automatically annotatable, thus enabling the provision of large and diverse datasets. We develop a specialized synthetic data generation pipeline for EO and introduce SynRS3D, the largest synthetic RS dataset. SynRS3D comprises 69,667 high-resolution optical images that cover six different city styles worldwide and feature eight land cover types, precise height information, and building change masks. To further enhance its utility, we develop a novel multi-task unsupervised domain adaptation (UDA) method, RS3DAda, coupled with our synthetic dataset, which facilitates the RS-specific transition from synthetic to real scenarios for land cover mapping and height estimation tasks, ultimately enabling global monocular 3D semantic understanding based on synthetic data. Extensive experiments on various real-world datasets demonstrate the adaptability and effectiveness of our synthetic dataset and the proposed RS3DAda method. SynRS3D and related codes are available at https://github.com/JTRNEO/SynRS3D.","","2025-03-30 16:26:53","2025-03-30 16:26:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XREKZMB8/Song et al. - SynRS3D A Synthetic Dataset for Global 3D Semantic Understanding from Monocular Remote Sensing Imag.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JYUTE5B","journalArticle","","Zhang, Xueyi; Zhang, Chengwei; Lao, Mingrui; Zhao, Peng; Tang, Jun; Guo, Yanming; Cai, Siqi; Yue, Xianghu; Li, Haizhou","Language Without Borders: A Dataset and Benchmark for Code-Switching Lip Reading","","","","","","Lip reading aims at transforming the videos of continuous lip movement into textual contents, and has achieved significant progress over the past decade. It serves as a critical yet practical assistance for speech-impaired individuals, with more practicability than speech recognition in noisy environments. With the increasing interpersonal communications in social media owing to globalization, the existing monolingual datasets for lip reading may not be sufficient to meet the exponential proliferation of bilingual and even multilingual users. However, to our best knowledge, research on code-switching is only explored in speech recognition, while the attempts in lip reading are seriously neglected. To bridge this gap, we have collected a bilingual code-switching lip reading benchmark composed of Chinese and English, dubbed CSLR. As the pioneering work, we recruited 62 speakers with proficient foundations in both spoken Chinese and English to express sentences containing both involved languages. Through rigorous criteria in data selection, CSLR benchmark has accumulated 85,560 video samples with a resolution of 1080x1920, totaling over 71.3 hours of high-quality code-switching lip movement data. To systematically evaluate the technical challenges in CSLR, we implement commonly-used lip reading backbones, as well as competitive solutions in code-switching speech for benchmark testing. Experiments show CSLR to be a challenging and under-explored lip reading task. We hope our proposed benchmark will extend the applicability of code-switching lip reading, and further contribute to the communities of cross-lingual communication and collaboration. Our dataset and benchmark are accessible at GitHub.","","2025-03-30 16:26:54","2025-03-30 16:26:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3PVUCPHY/Zhang et al. - Language Without Borders A Dataset and Benchmark for Code-Switching Lip Reading.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EU3A79M5","journalArticle","","Li, Jiatong; Hu, Renjun; Huang, Kunzhe; Zhuang, Yan; Liu, Qi; Zhu, Mengxiao; Shi, Xing; Lin, Wei","PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations","","","","","","Expert-designed close-ended benchmarks are indispensable in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs’ knowledge capacity through knowledge-invariant perturbations. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of response consistency analyses that compare performance on raw vs. perturbed test sets to precisely assess LLMs’ genuine knowledge capacity. Six representative LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs’ uncertainty to specious knowledge, and reveals their potential rote memorization to correct options which leads to overestimated performance. We also find that the detailed response consistency analyses by PertEval could illuminate various weaknesses in existing LLMs’ knowledge mastery and guide the development of refinement. Our findings provide insights for advancing more robust and genuinely knowledgeable LLMs. Our code is available at https://github.com/aigc-apps/PertEval.","","2025-03-30 16:26:55","2025-03-30 16:26:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5WT6D37Y/Li et al. - PertEval Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WPEIBAC","journalArticle","","Yang, Dongjie; Huang, Suyuan; Lu, Chengqiang; Han, Xiaodong; Zhang, Haoxin; Gao, Yan; Hu, Yao; Zhao, Hai","Vript: A Video Is Worth Thousands of Words","","","","","","","","2025-03-30 16:26:57","2025-03-30 16:26:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VBZ5S7Q6/Yang et al. - Vript A Video Is Worth Thousands of Words.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JMYDRE4X","journalArticle","","Tan, Alvin W M; Yu, Sunny; Long, Bria; Ma, Wanjing Anya; Murray, Tonya; Silverman, Rebecca D; Yeatman, Jason D; Frank, Michael C","DEVBENCH: A multimodal developmental benchmark for language learning","","","","","","How (dis)similar are the learning trajectories of vision–language models and children? Recent modeling work has attempted to understand the gap between models’ and humans’ data efficiency by constructing models trained on less data, especially multimodal naturalistic data. However, such models are often evaluated on adultlevel benchmarks, with limited breadth in language abilities tested, and without direct comparison to behavioral data. We introduce DEVBENCH, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. We evaluate a set of vision–language models on these tasks, comparing models and humans on their response patterns, not their absolute performance. Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. We also examine the developmental trajectory of OpenCLIP over training, finding that greater training results in closer approximations to adult response patterns. DEVBENCH thus provides a benchmark for comparing models to human language development. These comparisons highlight ways in which model and human language learning processes diverge, providing insight into entry points for improving language models.","","2025-03-30 16:26:58","2025-03-30 16:26:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TTQEPX77/Tan et al. - DEVBENCH A multimodal developmental benchmark for language learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KED34Q4G","journalArticle","","Kalyan, T Pavan; Pasi, Piyush Singh; Dharod, Sahil Nilesh; Motiwala, Azeem Azaz; Jyothi, Preethi; Chaudhary, Aditi; Srinivasan, Krishna","WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models","","","","","","Cross-modal (image-to-text and text-to-image) retrieval is an established task used in evaluation benchmarks to test the performance of vision-language models (VLMs). Several state-of-the-art VLMs (e.g. CLIP, BLIP-2) have achieved near-perfect performance on widely-used image-text retrieval benchmarks such as MSCOCO-Test-5K and Flickr30K-Test-1K. As a measure of out-of-distribution (OOD) generalization, prior works rely on zero-shot performance evaluated on one dataset (Flickr) using a VLM finetuned on another one (MSCOCO). We argue that such comparisons are insufficient to assess the OOD generalization capability of models due to high visual and linguistic similarity between the evaluation and finetuning datasets. To address this gap, we introduce WIKIDO (drawn from Wikipedia Diversity Observatory), a new cross-modal retrieval benchmark to assess the OOD generalization capabilities of pretrained VLMs. This consists of 384K image-text pairs from Wikipedia with domain labels, along with carefully curated, human-verified in-distribution (ID) and OOD test sets of size 3K each. The image-text pairs are very diverse in topics. We evaluate different VLMs of varying capacity on the WIKIDO benchmark; BLIP-2 achieves zero-shot performance of R@1≈ 66% on the OOD test set, compared to ≈ 81% on MSCOCO and ≈ 95% on Flickr. When fine-tuned on WIKIDO, the R@1 improvement is at most ≈ 5% on OOD instances compared to ≈ 12% on ID instances. WIKIDO offers a strong cross-modal retrieval benchmark for current VLMs, especially for evaluating OOD generalization. Our benchmark is hosted as a competition at https://kaggle.com/competitions/wikido24 with public access to dataset and code.","","2025-03-30 16:26:59","2025-03-30 16:26:59","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WJ8Q2Y99/Kalyan et al. - WikiDO A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJNEGL6Y","journalArticle","","Fawkes, Jake; Fishman, Nic; Andrews, Mel; Lipton, Zachary C","The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning","","","","","","Fairness metrics are a core tool in the fair machine learning literature (FairML), used to determine that ML models are, in some sense, “fair.” Real-world data, however, are typically plagued by various measurement biases and other violated assumptions, which can render fairness assessments meaningless. We adapt tools from causal sensitivity analysis to the FairML context, providing a general framework which (1) accommodates effectively any combination of fairness metric and bias that can be posed in the “oblivious setting”; (2) allows researchers to investigate combinations of biases, resulting in non-linear sensitivity; and (3) enables flexible encoding of domain-specific constraints and assumptions. Employing this framework, we analyze the sensitivity of the most common parity metrics under 3 varieties of classifier across 14 canonical fairness datasets. Our analysis reveals the striking fragility of fairness assessments to even minor dataset biases. We show that causal sensitivity analysis provides a powerful and necessary toolkit for gauging the informativeness of parity metric evaluations. Our repository is available here.","","2025-03-30 16:27:00","2025-03-30 16:27:00","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4QTR3B98/Fawkes et al. - The Fragility of Fairness Causal Sensitivity Analysis for Fair Machine Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KUFU7CC","journalArticle","","Obi, Ike; Pant, Rohan; Agrawal, Srishti Shekhar; Ghazanfar, Maham; Basiletti, Aaron","Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets","","","","","","","","2025-03-30 16:27:02","2025-03-30 16:27:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/22WLAA8F/Obi et al. - Value Imprint A Technique for Auditing the Human Values Embedded in RLHF Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2YGUGXND","journalArticle","","Wang, Aoran; Tong, Tsz Pan; Mizera, Andrzej; Pang, Jun","Benchmarking Structural Inference Methods for Interacting Dynamical Systems with Synthetic Data","","","","","","Understanding complex dynamical systems begins with identifying their topological structures, which expose the organization of the systems. This requires robust structural inference methods that can deduce structure from observed behavior. However, existing methods are often domain-specific and lack a standardized, objective comparison framework. We address this gap by benchmarking 13 structural inference methods from various disciplines on simulations representing two types of dynamics and 11 interaction graph models, supplemented by a biological experimental dataset to mirror real-world application. We evaluated the methods for accuracy, scalability, robustness, and sensitivity to graph properties. Our findings indicate that deep learning methods excel with multi-dimensional data, while classical statistics and information theory based approaches are notably accurate and robust. Additionally, performance correlates positively with the graph’s average shortest path length. This benchmark should aid researchers in selecting suitable methods for their specific needs and stimulate further methodological innovation. Project website: https://structinfer.github.io/.","","2025-03-30 16:27:03","2025-03-30 16:27:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PUBCT286/Wang et al. - Benchmarking Structural Inference Methods for Interacting Dynamical Systems with Synthetic Data.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZCCMYDG","journalArticle","","Kargaran, Amir Hossein; Yvon, François; Schütze, Hinrich","GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages","","","","","","The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it— including the pipeline, language identification model, and filters—available to the research community.","","2025-03-30 16:27:05","2025-03-30 16:27:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/P6F2GS54/Kargaran et al. - GlotCC An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86KSZDPJ","journalArticle","","Xie, Junlin; Zhang, Ruifei; Chen, Zhihong; Wan, Xiang; Li, Guanbin","WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games","","","","","","","","2025-03-30 16:27:06","2025-03-30 16:27:06","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/E7KKNZTF/Xie et al. - WhodunitBench Evaluating Large Multimodal Agents via Murder Mystery Games.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VSBD3SL6","journalArticle","","Choi, Yejin; Chung, Jiwan; Shim, Sumin; Oh, Giyeong; Yu, Youngjae","Towards Visual Text Design Transfer Across Languages","","","","","","Visual text design plays a critical role in conveying themes, emotions, and atmospheres in multimodal formats such as film posters and album covers. Translating these visual and textual elements across languages extends the concept of translation beyond mere text, requiring the adaptation of aesthetic and stylistic features. To address this, we introduce a novel task of Multimodal Style Translation (MuSTBench), a benchmark designed to evaluate the ability of visual text generation models to perform translation across different writing systems while preserving design intent. Our initial experiments on MuST-Bench reveal that existing visual text generation models struggle with the proposed task due to the inadequacy of textual descriptions in conveying visual design. In response, we introduce SIGIL, a framework for multimodal style translation that eliminates the need for style descriptions. SIGIL enhances image generation models through three innovations: glyph latent for multilingual settings, pre-trained VAEs for stable style guidance, and an OCR model with reinforcement learning feedback for optimizing readable character generation. SIGIL outperforms existing baselines by achieving superior style consistency and legibility while maintaining visual fidelity, setting itself apart from traditional description-based approaches. We release MuST-Bench publicly for broader use and exploration1.","","2025-03-30 16:27:07","2025-03-30 16:27:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PVT2B33F/Choi et al. - Towards Visual Text Design Transfer Across Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C69GNNI3","journalArticle","","Ohana, Ruben; McCabe, Michael; Meyer, Lucas; Morel, Rudy; Agocs, Fruzsina J; Beneitez, Miguel; Berger, Marsha; Burkhart, Blakesley; Dalziel, Stuart B; Fielding, Drummond B; Fortunato, Daniel; Goldberg, Jared A; Hirashima, Keiya; Jiang, Yan-Fei; Kerswell, Rich R; Maddu, Suryanarayana; Miller, Jonah; Mukhopadhyay, Payel; Nixon, Stefan S; Shen, Jeff; Watteaux, Romain; Blancard, Bruno Régaldo-Saint; Rozet, François; Parker, Liam H; Cranmer, Miles; Ho, Shirley","The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning","","","","","","Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.","","2025-03-30 16:27:09","2025-03-30 16:27:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TX6FVK4U/Ohana et al. - The Well a Large-Scale Collection of Diverse Physics Simulations for Machine Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HP4ZKJYP","journalArticle","","Shen, Yongliang; Song, Kaitao; Tan, Xu; Zhang, Wenqi; Ren, Kan; Yuan, Siyu; Lu, Weiming; Li, Dongsheng; Zhuang, Yueting","TaskBench: Benchmarking Large Language Models for Task Automation","","","","","","In recent years, the remarkable progress of large language models (LLMs) has sparked interest in task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents. However, there is a lack of systematic and standardized benchmarks to promote the development of LLMs in task automation. To address this, we introduce TASKBENCH, a comprehensive framework to evaluate the capability of LLMs in task automation. Specifically, task automation can be divided into three critical stages: task decomposition, tool selection, and parameter prediction. To tackle the complexities inherent in these stages, we introduce the concept of Tool Graph to represent decomposed tasks and adopt a back-instruct method to generate high-quality user instructions. We propose TASKEVAL, a multi-faceted evaluation methodology that assesses LLM performance across these three stages. Our approach combines automated construction with rigorous human verification, ensuring high consistency with human evaluation. Experimental results demonstrate that TASKBENCH effectively reflects the capabilities of various LLMs in task automation. It provides insights into model performance across different task complexities and domains, pushing the boundaries of what current models can achieve. TASKBENCH offers a scalable, adaptable, and reliable benchmark for advancing LLM-based autonomous agents 1.","","2025-03-30 16:27:10","2025-03-30 16:27:10","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z2VCQTAQ/Shen et al. - TaskBench Benchmarking Large Language Models for Task Automation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P3JCDCNA","journalArticle","","Koppula, Skanda; Rocco, Ignacio; Yang, Yi; Heyward, Joe; Carreira, João; Zisserman, Andrew; Brostow, Gabriel; Doersch, Carl","TAPVid-3D: A Benchmark for Tracking Any Point in 3D","","","","","","We introduce a new benchmark, TAPVid-3D, for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). While point tracking in two dimensions (TAP-2D) has many benchmarks measuring performance on real-world videos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To this end, leveraging existing footage, we build a new benchmark for 3D point tracking featuring 4,000+ real-world videos, composed of three different data sources spanning a variety of object types, motion patterns, and indoor and outdoor environments. To measure performance on the TAP-3D task, we formulate a collection of metrics that extend the Jaccard-based metric used in TAP-2D to handle the complexities of ambiguous depth scales across models, occlusions, and multi-track spatio-temporal smoothness. We manually verify a large sample of trajectories to ensure correct video annotations, and assess the current state of the TAP-3D task by constructing competitive baselines using existing tracking models. We anticipate this benchmark will serve as a guidepost to improve our ability to understand precise 3D motion and surface deformation from monocular video.","","2025-03-30 16:27:12","2025-03-30 16:27:12","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FTS992HW/Koppula et al. - TAPVid-3D A Benchmark for Tracking Any Point in 3D.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2GACHI4L","preprint","2024","Karpowicz, Brianna M.; Ye, Joel; Fan, Chaofei; Tostado-Marcos, Pablo; Rizzoglio, Fabio; Washington, Clay; Scodeler, Thiago; De Lucena, Diogo; Nason-Tomaszewski, Samuel R.; Mender, Matthew J.; Ma, Xuan; Arneodo, Ezequiel Matias; Hochberg, Leigh R.; Chestek, Cynthia A.; Henderson, Jaimie M.; Gentner, Timothy Q.; Gilja, Vikash; Miller, Lee E.; Rouse, Adam G.; Gaunt, Robert A.; Collinger, Jennifer L.; Pandarinath, Chethan","Few-shot Algorithms for Consistent Neural Decoding (FALCON) Benchmark","","","","10.1101/2024.09.15.613126","http://biorxiv.org/lookup/doi/10.1101/2024.09.15.613126","Abstract                        Intracortical brain-computer interfaces (iBCIs) can restore movement and communication abilities to individuals with paralysis by decoding their intended behavior from neural activity recorded with an implanted device. While this activity yields high-performance decoding over short timescales, neural data are often nonstationary, which can lead to decoder failure if not accounted for. To maintain performance, users must frequently recalibrate decoders, which requires the arduous collection of new neural and behavioral data. Aiming to reduce this burden, several approaches have been developed that either limit recalibration data requirements (few-shot approaches) or eliminate explicit recalibration entirely (zero-shot approaches). However, progress is limited by a lack of standardized datasets and comparison metrics, causing methods to be compared in an ad hoc manner. Here we introduce the FALCON benchmark suite (Few-shot Algorithms for COnsistent Neural decoding) to standardize evaluation of iBCI robustness. FALCON curates five datasets of neural and behavioral data that span movement and communication tasks to focus on behaviors of interest to modern-day iBCIs. Each dataset includes calibration data, optional few-shot recalibration data, and private evaluation data. We implement a flexible evaluation platform which only requires user-submitted code to return behavioral predictions on unseen data. We also seed the benchmark by applying baseline methods spanning several classes of possible approaches. FALCON aims to provide rigorous selection criteria for robust iBCI decoders, easing their translation to real-world devices.             https://snel-repo.github.io/falcon/","2024-09-16","2025-03-30 16:27:14","2025-03-30 16:27:14","2025-03-30 16:27:14","","","","","","","","","","","","","","en","https://www.biorxiv.org/about/FAQ#license","","","","Neuroscience","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YPJALPVC/Karpowicz et al. - 2024 - Few-shot Algorithms for Consistent Neural Decoding (FALCON) Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y3WD8VM5","journalArticle","","Zheng, Matthew; Simsar, Enis; Yesiltepe, Hidir; Tombari, Federico; Simon, Joel; Yanardag, Pinar","Stylebreeder : Exploring and Democratizing Artistic Styles through Text-to-Image Models","","","","","","Text-to-image models are becoming increasingly popular, revolutionizing the landscape of digital art creation by enabling highly detailed and creative visual content generation. These models have been widely employed across various domains, particularly in art generation, where they facilitate a broad spectrum of creative expression and democratize access to artistic creation. In this paper, we introduce STYLEBREEDER, a comprehensive dataset of 6.8M images and 1.8M prompts generated by 95K users on Artbreeder, a platform that has emerged as a significant hub for creative exploration with over 13M users. We introduce a series of tasks with this dataset aimed at identifying diverse artistic styles, generating personalized content, and recommending styles based on user interests. By documenting unique, usergenerated styles that transcend conventional categories like ‘cyberpunk’ or ‘Picasso,’ we explore the potential for unique, crowd-sourced styles that could provide deep insights into the collective creative psyche of users worldwide. We also evaluate different personalization methods to enhance artistic expression and introduce a style atlas, making these models available in LoRA format for public use. Our research demonstrates the potential of text-to-image diffusion models to uncover and promote unique artistic expressions, further democratizing AI in art and fostering a more diverse and inclusive artistic community. The dataset, code, and models are available at https://stylebreeder.github.io under a Public Domain (CC0) license.","","2025-03-30 16:27:15","2025-03-30 16:27:15","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SSKPEA3F/Zheng et al. - Stylebreeder  Exploring and Democratizing Artistic Styles through Text-to-Image Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"45TGPXER","journalArticle","","Veitch-Michaelis, Josh; Cottam, Andrew; Schweizer, Daniella; Broadbent, Eben N; Dao, David; Zhang, Ce; Zambrano, Angelica Almeyda; Max, Simeon","OAM-TCD: A globally diverse dataset of high-resolution tree cover maps","","","","","","Accurately quantifying tree cover is an important metric for ecosystem monitoring and for assessing progress in restored sites. Recent works have shown that deep learning-based segmentation algorithms are capable of accurately mapping trees at country and continental scales using high-resolution aerial and satellite imagery. Mapping at high (ideally sub-meter) resolution is necessary to identify individual trees, however there are few open-access datasets containing instance level annotations and those that exist are small or not geographically diverse. We present a novel open-access dataset for individual tree crown delineation (TCD) in high-resolution aerial imagery sourced from OpenAerialMap (OAM). Our dataset, OAM-TCD, comprises 5072 2048x2048 px images at 10 cm/px resolution with associated human-labeled instance masks for over 280k individual and 56k groups of trees. By sampling imagery from around the world, we are able to better capture the diversity and morphology of trees in different terrestrial biomes and in both urban and natural environments. Using our dataset, we train reference instance and semantic segmentation models that compare favorably to existing state-of-the-art models. We assess performance through k-fold cross-validation and comparison with existing datasets; additionally we demonstrate compelling results on independent aerial imagery captured over Switzerland and compare to municipal tree inventories and LIDAR-derived canopy maps in the city of Zurich. Our dataset, models and training/benchmark code are publicly released under permissive open-source licenses: Creative Commons (majority CC BY 4.0), and Apache 2.0 respectively.","","2025-03-30 16:27:17","2025-03-30 16:27:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ICIDZA44/Veitch-Michaelis et al. - OAM-TCD A globally diverse dataset of high-resolution tree cover maps.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5EPD5ZZA","journalArticle","","Bitton-Guetta, Nitzan; Slobodkin, Aviv; Maimon, Aviya; Habba, Eliya; Rassin, Royi; Bitton, Yonatan; Szpektor, Idan; Globerson, Amir; Elovici, Yuval","Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models","","","","","","Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person’s discomfort, thereby alleviating the need for further information. This example illustrates how subtle visual cues can challenge our cognitive skills and demonstrates the complexity of interpreting visual scenarios. To study these skills, we present Visual Riddles, a benchmark aimed to test vision and language models on visual riddles requiring commonsense and world knowledge. The benchmark comprises 400 visual riddles, each featuring a unique image created by a variety of text-to-image models, question, groundtruth answer, textual hint, and attribution. Human evaluation reveals that existing models lag significantly behind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading with 40% accuracy. Our benchmark comes with automatic evaluation tasks to make assessment scalable. These findings underscore the potential of Visual Riddles as a valuable resource for enhancing vision and language models’ capabilities in interpreting complex visual scenarios.","","2025-03-30 16:27:18","2025-03-30 16:27:18","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IVYW5VAR/Bitton-Guetta et al. - Visual Riddles a Commonsense and World Knowledge Challenge for Large Vision and Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ETGMY2D","journalArticle","","Schneider, David; Reiß, Simon; Kugler, Marco; Jaus, Alexander; Peng, Kunyu; Sutschet, Susanne; Sarfraz, M Saquib; Matthiesen, Sven; Stiefelhagen, Rainer","Muscles in Time: Learning to Understand Human Motion by Simulating Muscle Activations","","","","","","Exploring the intricate dynamics between muscular and skeletal structures is pivotal for understanding human motion. This domain presents substantial challenges, primarily attributed to the intensive resources required for acquiring ground truth muscle activation data, resulting in a scarcity of datasets. In this work, we address this issue by establishing Muscles in Time (MinT), a large-scale synthetic muscle activation dataset. For the creation of MinT, we enriched existing motion capture datasets by incorporating muscle activation simulations derived from biomechanical human body models using the OpenSim platform, a common approach in biomechanics and human motion research. Starting from simple pose sequences, our pipeline enables us to extract detailed information about the timing of muscle activations within the human musculoskeletal system. Muscles in Time contains over nine hours of simulation data covering 227 subjects and 402 simulated muscle strands. We demonstrate the utility of this dataset by presenting results on neural network-based muscle activation estimation from human pose sequences with two different sequence-to-sequence architectures.","","2025-03-30 16:27:20","2025-03-30 16:27:20","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UVSUY7WW/Schneider et al. - Muscles in Time Learning to Understand Human Motion by Simulating Muscle Activations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKCJC36K","journalArticle","","Li, Heng; Li, Minghan; Cheng, Zhi-Qi; Dong, Yifei; Zhou, Yuxuan; He, Jun-Yan; Dai, Qi; Mitamura, Teruko; Hauptmann, Alexander G","Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions","","","","","","Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Visionand-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and NonExpert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN’s unique challenges, underscores the need for further research to enhance HA-VLN agents’ real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.","","2025-03-30 16:27:21","2025-03-30 16:27:21","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/V2MBQA2C/Li et al. - Human-Aware Vision-and-Language Navigation Bridging Simulation to Reality with Dynamic Human Intera.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBJ2IX2B","journalArticle","","Sundar, Anirudh; Xu, Jin; Gay, William; Richardson, Christopher; Heck, Larry","cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers","","","","","","An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers. Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists. This work introduces CONVERSATIONAL PAPERS (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv. We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from LATEX source files. Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset.","","2025-03-30 16:27:22","2025-03-30 16:27:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EUJYNCH6/Sundar et al. - cPAPERS A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U3IBVAD3","journalArticle","","Evans, Talfan; Parthasarathy, Nikhil; Merzic, Hamza; Hénaff, Olivier J","Data curation via joint example selection further accelerates multimodal learning","","","","","","Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly prioritizing batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individuallyprioritized data points. As performance improves by selecting from large superbatches, we also leverage recent advances in model approximation to reduce the computational overhead of scoring. As a result, our approach—multimodal contrastive learning with joint example selection (JEST)—surpasses state-of-the-art pretraining methods with up to 13× fewer iterations and 10× less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing data curation as a new dimension for neural scaling laws.","","2025-03-30 16:27:24","2025-03-30 16:27:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/LP84MCHS/Evans et al. - Data curation via joint example selection further accelerates multimodal learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MF3WSSQ","journalArticle","","Wang, Zhenzhi; Li, Yixuan; Zeng, Yanhong; Fang, Youqing; Guo, Yuwei; Liu, Wenran; Tan, Jing; Chen, Kai; Xue, Tianfan; Dai, Bo; Lin, Dahua","HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation","","","","","","Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.","","2025-03-30 16:27:24","2025-03-30 16:27:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/96WDP9VX/Wang et al. - HumanVid Demystifying Training Data for Camera-controllable Human Image Animation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26LZY7XU","journalArticle","","Cho, Hoonhee; Kim, Taewoo; Jeong, Yuhwan; Yoon, Kuk-Jin","A Benchmark Dataset for Event-Guided Human Pose Estimation and Tracking in Extreme Conditions","","","","","","Multi-person pose estimation and tracking have been actively researched by the computer vision community due to their practical applicability. However, existing human pose estimation and tracking datasets have only been successful in typical scenarios, such as those without motion blur or with well-lit conditions. These RGB-based datasets are limited to learning under extreme motion blur situations or poor lighting conditions, making them inherently vulnerable to such scenarios. As a promising solution, bio-inspired event cameras exhibit robustness in extreme scenarios due to their high dynamic range and micro-second level temporal resolution. Therefore, in this paper, we introduce a new hybrid dataset encompassing both RGB and event data for human pose estimation and tracking in two extreme scenarios: low-light and motion blur environments. The proposed Event-guided Human Pose Estimation and Tracking in eXtreme Conditions (EHPTXC) dataset covers cases of motion blur caused by dynamic objects and low-light conditions individually as well as both simultaneously. With EHPT-XC, we aim to inspire researchers to tackle pose estimation and tracking in extreme conditions by leveraging the advantageous of the event camera. Project pages are available at https://github.com/Chohoonhee/EHPT-XC.","","2025-03-30 16:27:26","2025-03-30 16:27:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HXJII5X3/Cho et al. - A Benchmark Dataset for Event-Guided Human Pose Estimation and Tracking in Extreme Conditions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2XIPE3PA","journalArticle","","Shao, Yijia; Li, Tianshi; Shi, Weiyan; Liu, Yanchen; Yang, Diyi","PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action","","","","","","As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents’ actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-ofthe-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.","","2025-03-30 16:27:27","2025-03-30 16:27:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CKWV3YW8/Shao et al. - PrivacyLens Evaluating Privacy Norm Awareness of Language Models in Action.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXRBCCPB","journalArticle","","Li, Yinghui; Zhou, Qingyu; Luo, Yuanzhen; Ma, Shirong; Li, Yangning; Zheng, Hai-Tao; Hu, Xuming; Yu, Philip S","When LLMs Meet Cunning Texts:","","","","","","Recently, Large Language Models (LLMs) make remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning texts that FLUB focuses on mainly consist of the tricky, humorous, and misleading texts collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs’ ability to understand fallacies. Our data and codes are available at https://github.com/THUKElab/FLUB.","","2025-03-30 16:27:29","2025-03-30 16:27:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/T7QJ3PTL/Li et al. - When LLMs Meet Cunning Texts.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L5JCQE39","journalArticle","","Wornow, Michael; Narayan, Avanika; Viggiano, Ben; Khare, Ishan S; Verma, Tathagat; Thompson, Tibor; Hernandez, Miguel Angel Fuentes; Sundar, Sudharsan; Trujillo, Chloe; Chawla, Krrish; Lu, Rongfei; Shen, Justin; Nagaraj, Divya; Martinez, Joshua; Agrawal, Vardhan; Hudson, Althea; Shah, Nigam H; Ré, Christopher","WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks","","","","","","Existing ML benchmarks lack the depth and diversity of annotations needed for evaluating models on business process management (BPM) tasks. BPM is the practice of documenting, measuring, improving, and automating enterprise workflows. However, research has focused almost exclusively on one task – full end-to-end automation using agents based on multimodal foundation models (FMs) like GPT-4. This focus on automation ignores the reality of how most BPM tools are applied today – simply documenting the relevant workflow takes 60% of the time of the typical process optimization project. To address this gap we present WONDERBREAD, the first benchmark for evaluating multimodal FMs on BPM tasks beyond automation. Our contributions are: (1) a dataset containing 2928 documented workflow demonstrations; (2) 6 novel BPM tasks sourced from real-world applications ranging from workflow documentation to knowledge transfer to process improvement; and (3) an automated evaluation harness. Our benchmark shows that while state-of-the-art FMs can automatically generate documentation (e.g. recalling 88% of the steps taken in a video demonstration of a workflow), they struggle to re-apply that knowledge towards finer-grained validation of workflow completion (F1 < 0.3). We hope WONDERBREAD encourages the development of more “humancentered” AI tooling for enterprise applications and furthers the exploration of multimodal FMs for the broader universe of BPM tasks. We publish our dataset and experiments here: § https://github.com/HazyResearch/wonderbread.","","2025-03-30 16:27:30","2025-03-30 16:27:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/D3LAZZ72/Wornow et al. - WONDERBREAD A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"INKTAVDD","journalArticle","","Li, Xin; Chen, Weize; Chu, Qizhi; Li, Haopeng; Sun, Zhaojun; Li, Ran; Qian, Chen; Wei, Yiwei; Liu, Zhiyuan; Shi, Chuan; Sun, Maosong; Yang, Cheng","Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models","","","","","","The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs’ proficiency of graph analysis. The benchmark, datasets and enhanced open-source models are available at https://github.com/BUPT-GAMMA/ProGraph.","","2025-03-30 16:27:31","2025-03-30 16:27:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JATUMT56/Li et al. - Can Large Language Models Analyze Graphs like Professionals A Benchmark, Datasets and Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JMWCRKT","journalArticle","","Lin, Kevin Qinghong; Li, Linjie; Gao, Difei; Wu, Qinchen; Yan, Mingyi; Yang, Zhengyuan; Wang, Lijuan; Shou, Mike Zheng","VideoGUI: A Benchmark for GUI Automation from Instructional Videos","","","","","","Graphical User Interface (GUI) automation holds significant promise for enhancing human productivity by assisting with computer tasks. Existing task formulations primarily focus on simple tasks that can be specified by a single, language-only instruction, such as “Insert a new slide.” In this work, we introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, our benchmark focuses on tasks involving professional and novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex activities (e.g., video editing). VideoGUI evaluates GUI assistants through a hierarchical process, allowing for identification of the specific levels at which they may fail: (i) high-level planning: reconstruct procedural subtasks from visual conditions without language descriptions; (ii) middle-level planning: generate sequences of precise action narrations based on visual state (i.e., screenshot) and goals; (iii) atomic action execution: perform specific actions such as accurately clicking designated elements. For each level, we design evaluation metrics across individual dimensions to provide clear signals, such as individual performance in clicking, dragging, typing, and scrolling for atomic action execution. Our evaluation on VideoGUI reveals that even the SoTA large multimodal model GPT4o performs poorly on visual-centric GUI tasks, especially for high-level planning. The data and code are available at https://github.com/showlab/videogui.","","2025-03-30 16:27:33","2025-03-30 16:27:33","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3UL3GK4V/Lin et al. - VideoGUI A Benchmark for GUI Automation from Instructional Videos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7WL6ZE5","journalArticle","","Du, Weihua; Lyu, Qiushi; Shan, Jiaming; Qi, Zhenting; Zhang, Hongxin; Chen, Sunli; Peng, Andi; Shu, Tianmin; Lee, Kwonjoon; Dariush, Behzad; Gan, Chuang","Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge","","","","","","We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. In CHAIC, the goal is for an embodied agent equipped with egocentric observations to assist a human who may be operating under physical constraints—e.g., unable to reach high places or confined to a wheelchair—in performing common household or outdoor tasks as efficiently as possible. To achieve this, a successful helper must: (1) infer the human’s intents and constraints by following the human and observing their behaviors (social perception), and (2) make a cooperative plan tailored to the human partner to solve the task as quickly as possible, working together as a team (cooperative planning). To benchmark this challenge, we create four new agents with real physical constraints and eight longhorizon tasks featuring both indoor and outdoor scenes with various constraints, emergency events, and potential risks. We benchmark planning- and learningbased baselines on the challenge and introduce a new method that leverages large language models and behavior modeling. Empirical evaluations demonstrate the effectiveness of our benchmark in enabling systematic assessment of key aspects of machine social intelligence. Our benchmark and code are publicly available at https://github.com/UMass-Foundation-Model/CHAIC.","","2025-03-30 16:27:34","2025-03-30 16:27:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/43QFTAEG/Du et al. - Constrained Human-AI Cooperation An Inclusive Embodied Social Intelligence Challenge.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTNGHKV3","journalArticle","","Liu, Chang; Wu, Xiwei; Feng, Yuan; Cao, Qinxiang; Yan, Junchi","Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation","","","","","","Program verification is vital for ensuring software reliability, especially in the context of increasingly complex systems. Loop invariants, remaining true before and after each iteration of loops, are crucial for this verification process. Traditional provers and machine learning based methods for generating loop invariants often require expert intervention or extensive labeled data, and typically only handle numerical property verification. These methods struggle with programs involving complex data structures and memory manipulations, limiting their applicability and automation capabilities. In this paper, we introduce a new benchmark named LIG-MM, specifically for programs with complex data structures and memory manipulations. We collect 312 programs from various sources, including daily programs from college homework, the international competition (SV-COMP), benchmarks from previous papers (SLING), and programs from real-world software systems (Linux Kernel, GlibC, LiteOS, and Zephyr). Based on LIG-MM, our findings indicate that previous methods, including GPT-4, fail to automate verification for these programs. Consequently, we propose a novel LLM-SE framework that coordinates LLM with symbolic execution, fine-tuned using self-supervised learning, to generate loop invariants. Experimental results on LIG-MM demonstrate that our LLM-SE outperforms state-of-the-art methods, offering a new direction toward automated program verification in real-world scenarios.","","2025-03-30 16:27:35","2025-03-30 16:27:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MA2FX4GY/Liu et al. - Towards General Loop Invariant Generation A Benchmark of Programs with Memory Manipulation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F9WI5N7L","journalArticle","","Witter, R Teal; Musco, Christopher","Benchmarking Estimators for Natural Experiments: A Novel Dataset and a Doubly Robust Algorithm","","","","","","","","2025-03-30 16:27:36","2025-03-30 16:27:36","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/I76TCU2L/Witter and Musco - Benchmarking Estimators for Natural Experiments A Novel Dataset and a Doubly Robust Algorithm.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZPHVHPAQ","journalArticle","","Singh, Pragya; Budhiraja, Ritvik; Gupta, Ankush; Goswami, Anshul; Kumar, Mohan; Singh, Pushpendra","EEVR: A Dataset of Paired Physiological Signals and Textual Descriptions for Joint Emotion Representation Learning","","","","","","EEVR (Emotion Elicitation in Virtual Reality) is a novel dataset specifically designed for language supervision-based pre-training of emotion recognition tasks, such as valence and arousal classification. It features high-quality physiological signals, including electrodermal activity (EDA) and photoplethysmography (PPG), acquired through emotion elicitation via 360-degree virtual reality (VR) videos. Additionally, it includes subject-wise textual descriptions of emotions experienced during each stimulus gathered from qualitative interviews. The dataset consists of recordings from 37 participants and is the first dataset to pair raw text with physiological signals, providing additional contextual information that objective labels cannot offer. To leverage this dataset, we introduced the Contrastive Language Signal Pre-training (CLSP) method, which jointly learns representations using pairs of physiological signals and textual descriptions. Our results show that integrating self-reported textual descriptions with physiological signals significantly improves performance on emotion recognition tasks, such as arousal and valence classification. Moreover, our pre-trained CLSP model demonstrates strong zero-shot transferability to existing datasets, outperforming supervised baseline models, suggesting that the representations learned by our method are more contextualized and generalized. The dataset also includes baseline models for arousal, valence, and emotion classification, as well as code for data cleaning and feature extraction. Further details and access to the dataset are available at https://melangelabiiitd.github.io/EEVR/.","","2025-03-30 16:27:38","2025-03-30 16:27:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/926HYQIU/Singh et al. - EEVR A Dataset of Paired Physiological Signals and Textual Descriptions for Joint Emotion Represent.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WG77W8ZM","journalArticle","","Cao, Ruisheng; Lei, Fangyu; Wu, Haoyuan; Chen, Jixuan; Fu, Yeqiao; Gao, Hongcheng; Xiong, Xinzhuang; Zhang, Hanchong; Mao, Yuchen; Hu, Wenjing; Xie, Tianbao; Xu, Hongshen; Zhang, Danyang; Sun, Sida Wang Ruoxi; Yin, Pengcheng; Xiong, Caiming; Ni, Ansong; Liu, Qian; Zhong, Victor; Chen, Lu; Yu, Kai; Yu, Tao","Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?","","","","","","Data science and engineering workflows often span multiple stages, from warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code generation, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably automate full data workflows (14.0% success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted workspaces (10.6%). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.","","2025-03-30 16:27:39","2025-03-30 16:27:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XA7UDTRZ/Cao et al. - Spider2-V How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LCHQBYGM","journalArticle","","Wu, Kevin; Wu, Eric; Zou, James","ClashEval: Quantifying the tug-of-war between an LLM’s internal prior and external evidence","","","","","","Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model’s initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs – namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.","","2025-03-30 16:27:40","2025-03-30 16:27:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FTKP6QV6/Wu et al. - ClashEval Quantifying the tug-of-war between an LLM’s internal prior and external evidence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H6FE2IB5","journalArticle","","Koehler, Felix; Niedermayr, Simon; Westermann, Rüdiger; Thuerey, Nils","APEBench: A Benchmark for Autoregressive Neural Emulators of PDEs","","","","","","We introduce the Autoregressive PDE Emulator Benchmark (APEBench), a comprehensive benchmark suite to evaluate autoregressive neural emulators for solving partial differential equations. APEBench is based on JAX and provides a seamlessly integrated differentiable simulation framework employing efficient pseudo-spectral methods, enabling 46 distinct PDEs across 1D, 2D, and 3D. Facilitating systematic analysis and comparison of learned emulators, we propose a novel taxonomy for unrolled training and introduce a unique identifier for PDE dynamics that directly relates to the stability criteria of classical numerical methods. APEBench enables the evaluation of diverse neural architectures, and unlike existing benchmarks, its tight integration of the solver enables support for differentiable physics training and neural-hybrid emulators. Moreover, APEBench emphasizes rollout metrics to understand temporal generalization, providing insights into the long-term behavior of emulating PDE dynamics. In several experiments, we highlight the similarities between neural emulators and numerical simulators. The code is available at https://github.com/tum-pbs/apebench and APEBench can be installed via pip install apebench.","","2025-03-30 16:27:41","2025-03-30 16:27:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BCULWT45/Koehler et al. - APEBench A Benchmark for Autoregressive Neural Emulators of PDEs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QTVRLH5I","journalArticle","","Sivasubramaniam, Sithursan; Osei-Akoto, Cedric; Zhang, Yi; Stockinger, Kurt; Fürst, Jonathan","SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark","","","","","","Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far. In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy—a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs. Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last, SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.","","2025-03-30 16:27:43","2025-03-30 16:27:43","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/964GW3HT/Sivasubramaniam et al. - SM3-Text-to-Query Synthetic Multi-Model Medical Text-to-Query Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZTYZQPDF","journalArticle","","Xu, Zhao; Liu, Fan; Liu, Hao","Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs","","","","","","Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks. However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced JailTrickBench to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/JailTrickBench.","","2025-03-30 16:27:44","2025-03-30 16:27:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PMWY4D3I/Xu et al. - Bag of Tricks Benchmarking of Jailbreak Attacks on LLMs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FNRYG9W8","journalArticle","","Gröger, Fabian; Lionetti, Simone; Gottfrois, Philippe; Gonzalez-Jimenez, Alvaro; Amruthalingam, Ludovic; Navarini, Alexander A; Pouly, Marc","Intrinsic Self-Supervision for Data Quality Audits","","","","","","Benchmark datasets in computer vision often contain off-topic images, near duplicates, and label errors, leading to inaccurate estimates of model performance. In this paper, we revisit the task of data cleaning and formalize it as either a ranking problem, which significantly reduces human inspection effort, or a scoring problem, which allows for automated decisions based on score distributions. We find that a specific combination of context-aware self-supervised representation learning and distance-based indicators is effective in finding issues without annotation biases. This methodology, which we call SELFCLEAN, surpasses state-of-the-art performance in detecting off-topic images, near duplicates, and label errors within widely-used image datasets, such as ImageNet-1k, Food-101N, and STL-10, both for synthetic issues and real contamination. We apply the detailed method to multiple image benchmarks, identify up to 16% of issues, and confirm an improvement in evaluation reliability upon cleaning. The official implementation can be found at: https://github.com/Digital-Dermatology/SelfClean.","","2025-03-30 16:27:45","2025-03-30 16:27:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5CHGDWWN/Gröger et al. - Intrinsic Self-Supervision for Data Quality Audits.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"34AL7D2Z","journalArticle","","Enevoldsen, Kenneth; Kardos, Márton; Muennighoff, Niklas; Nielbo, Kristoffer Laigaard","The Scandinavian Embedding Benchmarks: Evaluating Multilingual and Monolingual Text Embedding for Scandinavian languages","","","","","","The evaluation of English text embeddings has transitioned from evaluating a handful of datasets to broad coverage across many tasks through benchmarks such as MTEB. However, this is not the case for multilingual text embeddings due to a lack of available benchmarks. To address this problem, we introduce the Scandinavian Embedding Benchmark (SEB). SEB is a framework that enables text embedding evaluation for Scandinavian languages across 24 tasks, 10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26 models, uncovering signiﬁcant performance disparities between public and commercial solutions not previously captured by MTEB. We open-source SEB1 and integrate it with MTEB, thus bridging the text embedding evaluation gap for Scandinavian languages.","","2025-03-30 16:27:46","2025-03-30 16:27:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AN9862FD/Enevoldsen et al. - The Scandinavian Embedding Benchmarks Evaluating Multilingual and Monolingual Text Embedding for Sc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNBEHHT7","journalArticle","","Perron, Yohann; Sydorov, Vladyslav; Wijker, Adam P; Evans, Damian; Pottier, Christophe; Landrieu, Loic","Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era","","","","","","Airborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available at https://archaeoscape.ai/data/2024), a novel large-scale archaeological ALS dataset spanning 888 km2 in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models.","","2025-03-30 16:27:47","2025-03-30 16:27:47","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9J42NJQ7/Perron et al. - Archaeoscape Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BM7C7REU","journalArticle","","Khirodkar, Rawal; Song, Jyun-Ting; Cao, Jinkun; Luo, Zhengyi; Kitani, Kris","Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions","","","","","","Understanding how humans interact with each other is key to building realistic multi-human virtual reality systems. This area remains relatively unexplored due to the lack of large-scale datasets. Recent datasets focusing on this issue mainly consist of activities captured entirely in controlled indoor environments with choreographed actions, significantly affecting their diversity. To address this, we introduce Harmony4D, a multi-view video dataset for human-human interaction featuring in-the-wild activities such as wrestling, dancing, MMA, and more. We use a flexible multi-view capture system to record these dynamic activities and provide annotations for human detection, tracking, 2D/3D pose estimation, and mesh recovery for closely interacting subjects. We propose a novel markerless algorithm to track 3D human poses in severe occlusion and close interaction to obtain our annotations with minimal manual intervention. Harmony4D consists of 1.66 million images and 3.32 million human instances from more than 20 synchronized cameras with 208 video sequences spanning diverse environments and 24 unique subjects. We rigorously evaluate existing stateof-the-art methods for mesh recovery and highlight their significant limitations in modeling close interaction scenarios. Additionally, we fine-tune a pre-trained HMR2.0 model on Harmony4D and demonstrate an improved performance of 54.8% PVE in scenes with severe occlusion and contact. Code and data are available at https://jyuntins.github.io/harmony4d/.","","2025-03-30 16:27:49","2025-03-30 16:27:49","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PI78WUMG/Khirodkar et al. - Harmony4D A Video Dataset for In-The-Wild Close Human Interactions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MJUMCQ7W","journalArticle","","Lu, Yujie; Jiang, Dongfu; Chen, Wenhu; Wang, William Yang; Choi, Yejin; Lin, Bill Yuchen","WILDVISION: Evaluating Vision-Language Models in the Wild with Human Preferences","","","","","","Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WILDVISION-ARENA (WV-ARENA), an online platform that collects human preferences to evaluate VLMs. We curated WVBENCH by selecting 500 high-quality samples from 8,000 user submissions in WV-ARENA. WV-BENCH uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-ARENA Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar. Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.","","2025-03-30 16:27:50","2025-03-30 16:27:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HXS8D2FQ/Lu et al. - WILDVISION Evaluating Vision-Language Models in the Wild with Human Preferences.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"USGKPNNC","journalArticle","","Wu, Xueqing; Zheng, Rui; Sha, Jingzhen; Wu, Te-Lin; Zhou, Hanyu; Tang, Mohan; Chang, Kai-Wei; Peng, Nanyun; Huang, Haoran","DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation","","","","","","Data analysis is a crucial analytical process essential for deriving insights from realworld databases. As shown in Figure 1, the need for data analysis typically arises from specific application scenarios, and requires diverse reasoning skills including mathematical reasoning, logical reasoning, and strategic reasoning. Existing work often focus on simple factual retrieval or arithmetic resolutions and thus are insufficient for addressing complex real-world queries. This work aims to propose new resources and benchmarks on this crucial yet challenging and under-explored task. Due to the prohibitively high cost of collecting expert annotations, we use large language models (LLMs) enhanced by code generation to automatically generate high-quality data analysis, which will later be refined by human annotators. We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ∼ 2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. Experiments show that while LLMs like GPT-4 exhibit promising data analysis capabilities, they are still evaluated as less helpful than human-written analysis on 58.1% cases. Leveraging our weak supervision data, we experiment with various fine-tuning methods, including supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Our trained model outperforms existing baselines for table question answering, and RLHF further boosts the helpfulness of generated analysis on 58.5% cases. Data and code are released at https://github.com/shirley-wu/daco.","","2025-03-30 16:27:51","2025-03-30 16:27:51","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2NRTKXCF/Wu et al. - DACO Towards Application-Driven and Comprehensive Data Analysis via Code Generation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62UMF5LM","journalArticle","","Jia, Qi; Fan, Baoyu; Xu, Cong; Liu, Lu; Jin, Liang; Du, Guoguang; Guo, Zhenhua; Zhao, Yaqian; Huang, Xuanjing; Li, Rengang","Infer Induced Sentiment of Comment Response to Video: A New Task, Dataset and Baseline","","","","","","Existing video multi-modal sentiment analysis mainly focuses on the sentiment expression of people within the video, yet often neglects the induced sentiment of viewers while watching the videos. Induced sentiment of viewers is essential for inferring the public response to videos and has broad application in analyzing public societal sentiment, effectiveness of advertising and other areas. The micro videos and the related comments provide a rich application scenario for viewers’ induced sentiment analysis. In light of this, we introduces a novel research task, Multimodal Sentiment Analysis for Comment Response of Video Induced(MSA-CRVI), aims to infer opinions and emotions according to comments response to micro video. Meanwhile, we manually annotate a dataset named Comment Sentiment toward to Micro Video (CSMV) to support this research. It is the largest video multi-modal sentiment dataset in terms of scale and video duration to our knowledge, containing 107, 267 comments and 8, 210 micro videos with a video duration of 68.83 hours. To infer the induced sentiment of comment should leverage the video content, we propose the Video Content-aware Comment Sentiment Analysis (VC-CSA) method as a baseline to address the challenges inherent in this new task. Extensive experiments demonstrate that our method is showing significant improvements over other established baselines. We make the dataset and source code publicly available at https://github.com/IEIT-AGI/MSA-CRVI.","","2025-03-30 16:27:53","2025-03-30 16:27:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/LKD8STTF/Jia et al. - Infer Induced Sentiment of Comment Response to Video A New Task, Dataset and Baseline.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"567XI86S","journalArticle","","Xin, Yi; Luo, Siqi; Liu, Xuyang; Du, Yuntao; Zhou, Haodi; Cheng, Xinyu; Lee, Christina; Du, Junlong; Wang, Haozhe; Chen, Mingcai; Liu, Ting; Hu, Guimin; Wan, Zhongwei; Zhang, Rongchao; Li, Aoxue; Yi, Mingyang; Liu, Xiaohong","V-PETL Bench: A Uniﬁed Visual Parameter-Efﬁcient Transfer Learning Benchmark","","","","","","Parameter-efﬁcient transfer learning (PETL) methods show promise in adapting a pre-trained model to various downstream tasks while training only a few parameters. In the computer vision (CV) domain, numerous PETL algorithms have been proposed, but their direct employment or comparison remains inconvenient. To address this challenge, we construct a Uniﬁed Visual PETL Benchmark (V-PETL Bench) for the CV domain by selecting 30 diverse, challenging, and comprehensive datasets from image recognition, video action recognition, and dense prediction tasks. On these datasets, we systematically evaluate 25 dominant PETL algorithms and open-source a modular and extensible codebase for fair evaluation of these algorithms. V-PETL Bench runs on NVIDIA A800 GPUs and requires approximately 310 GPU days. We release all the benchmark, making it more efﬁcient and friendly to researchers. Additionally, V-PETL Bench will be continuously updated for new PETL algorithms and CV tasks.","","2025-03-30 16:27:54","2025-03-30 16:27:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZCTBQXW7/Xin et al. - V-PETL Bench A Uniﬁed Visual Parameter-Efﬁcient Transfer Learning Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A5DBYYPJ","journalArticle","","Jin, Emily; Huang, Zhuoyi; Fränken, Jan-Philipp; Liu, Weiyu; Cha, Hannah; Brockbank, Erik; Wu, Sarah; Zhang, Ruohan; Wu, Jiajun; Gerstenberg, Tobias","MARPLE: A Benchmark for Long-Horizon Inference","","","","","","Reconstructing past events requires reasoning across long time horizons. To figure out what happened, humans draw on prior knowledge about the world and human behavior and integrate insights from various sources of evidence including visual, language, and auditory cues. We introduce MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence. Our benchmark features agents interacting with simulated households, supporting vision, language, and auditory stimuli, as well as procedurally generated environments and agent behaviors. Inspired by classic “whodunit” stories, we ask AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened. The goal is to correctly identify the culprit as early as possible. Our findings show that human participants outperform both traditional Monte Carlo simulation methods and an LLM baseline (GPT-4) on this task. Compared to humans, traditional inference models are less robust and performant, while GPT-4 has difficulty comprehending environmental changes. We analyze factors influencing inference performance and ablate different modes of evidence, finding that all modes are valuable for performance. Overall, our experiments demonstrate that the long-horizon, multimodal inference tasks in our benchmark present a challenge to current models. Project website: https: //marple-benchmark.github.io/.","","2025-03-30 16:27:55","2025-03-30 16:27:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZI2Y9XPW/Jin et al. - MARPLE A Benchmark for Long-Horizon Inference.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C5PFK9XS","journalArticle","","Parmar, Paritosh; Peh, Eric; Chen, Ruirui; Lam, Ting En; Chen, Yuhan; Tan, Elston; Fernando, Basura","CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes","","","","","","Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic “Tom and Jerry"" cartoon series. Cartoons use the principles of animation that allow animators to create expressive, unambiguous causal relationships between events to form a coherent storyline. Utilizing these properties, along with thought-provoking questions and multilevel answers (answer and detailed causal explanation), our questions involve causal chains that interconnect multiple dynamic interactions between characters and visual scenes. These factors demand models to solve more challenging, yet well-defined causal relationships. We also introduce hard incorrect answer mining, including a causally confusing version that is even more challenging. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling & joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field.","","2025-03-30 16:27:56","2025-03-30 16:27:56","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4TC2UHHT/Parmar et al. - CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Gr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQ9C5ZMX","journalArticle","","Yukhymenko, Hanna; Staab, Robin; Vero, Mark; Vechev, Martin","A Synthetic Dataset for Personal Attribute Inference","","","","","","Recently powerful Large Language Models (LLMs) have become easily accessible to hundreds of millions of users world-wide. However, their strong capabilities and vast world knowledge do not come without associated privacy risks. In this work, we focus on the emerging privacy threat LLMs pose – the ability to accurately infer personal information from online texts. Despite the growing importance of LLM-based author profiling, research in this area has been hampered by a lack of suitable public datasets, largely due to ethical and privacy concerns associated with real personal data. We take two steps to address this problem: (i) we construct a simulation framework for the popular social media platform Reddit using LLM agents seeded with synthetic personal profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic dataset of over 7800 comments manually labeled for personal attributes. We validate our dataset with a human study showing that humans barely outperform random guessing on the task of distinguishing our synthetic comments from real ones. Further, we verify that our dataset enables meaningful personal attribute inference research by showing across 18 state-of-theart LLMs that our synthetic comments allow us to draw the same conclusions as real-world data. Combined, our experimental results, dataset and pipeline form a strong basis for future privacy-preserving research geared towards understanding and mitigating inference-based privacy threats that LLMs pose.","","2025-03-30 16:27:58","2025-03-30 16:27:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7DXRJQMY/Yukhymenko et al. - A Synthetic Dataset for Personal Attribute Inference.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SEUYPHI2","journalArticle","","Wu, Tong; Xu, Yinghao; Po, Ryan; Zhang, Mengchen; Yang, Guandao; Wang, Jiaqi; Liu, Ziwei; Lin, Dahua; Wetzstein, Gordon","FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models","","","","","","Recent advances in text-to-image generation have enabled the creation of highquality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable attributes from source images. Current methods attempt to distill identity and style from source images. However, ""style"" is a broad concept that includes texture, color, and artistic elements, but does not cover other important attributes like lighting and dynamics. Additionally, a simplified ""style"" adaptation prevents combining multiple attributes from different sources into one generated image. In this work, we formulate a more effective approach to decompose the aesthetics of a picture into specific visual attributes, letting users apply characteristics like lighting, texture, and dynamics from different images. To achieve this goal, we constructed the first fine-grained visual attributes dataset (FiVA) to the best of our knowledge. This FiVA dataset features a well-organized taxonomy for visual attributes and includes 1 M highquality generated images with visual attribute annotations. Leveraging this dataset, we propose a fine-grained visual attributes adaptation framework (FiVA-Adapter) , which decouples and adapts visual attributes from one or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes to create images that meet their unique preferences and specific content requirements. The data and models will be released at https://huggingface.co/datasets/FiVA/FiVA.","","2025-03-30 16:27:59","2025-03-30 16:27:59","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/K9IL7J2M/Wu et al. - FiVA Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWVPSICA","journalArticle","","Du, Mingzhe; Tuan, Luu Anh; Ji, Bin; Liu, Qian; Ng, See-Kiong","Mercury: A Code Efficiency Benchmark for Code Large Language Models","","","","","","","","2025-03-30 16:28:00","2025-03-30 16:28:00","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CV3QUNR4/Du et al. - Mercury A Code Efficiency Benchmark for Code Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLDCKSKR","journalArticle","","Mucsányi, Bálint; Kirchhof, Michael","Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks","","","","","","Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one source of uncertainty. This paper presents the first benchmark of uncertainty disentanglement. We reimplement and evaluate a comprehensive range of uncertainty estimators, from Bayesian over evidential to deterministic ones, across a diverse range of uncertainty tasks on ImageNet. We find that, despite recent theoretical endeavors, no existing approach provides pairs of disentangled uncertainty estimators in practice. We further find that specialized uncertainty tasks are harder than predictive uncertainty tasks, where we observe saturating performance. Our results provide both practical advice for which uncertainty estimators to use for which specific task, and reveal opportunities for future research toward task-centric and disentangled uncertainties. All our reimplementations and Weights & Biases logs are available at https://github.com/bmucsanyi/untangle.","","2025-03-30 16:28:02","2025-03-30 16:28:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WFVSX2MD/Mucsányi and Kirchhof - Benchmarking Uncertainty Disentanglement Specialized Uncertainties for Specialized Tasks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J5R99SM5","journalArticle","","Liu, Jian; Wu, Jianyu; Xie, Hairun; Zhang, Guoqing; Wang, Jing; Liu, Wei; Ouyang, Wanli; Jiang, Junjun; Liu, Xianming; Tang, Shixiang; Zhang, Miao","AFBench: A Large-scale Benchmark for Airfoil Design","","","","","","","","2025-03-30 16:28:03","2025-03-30 16:28:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EAZAF6HW/Liu et al. - AFBench A Large-scale Benchmark for Airfoil Design.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPZ3UTZA","journalArticle","","Li, Yanzhi; Li, Keqiu; Li, Guohui; Wang, Zumin; Ji, Changqing; Wang, Lubo; Zuo, Die; Guo, Qing; Zhang, Feng; Wang, Manyu; Lin, Di","Sim2Real-Fire: A Multi-modal Simulation Dataset for Forecast and Backtracking of Real-world Forest Fire","","","","","","The latest research on wildfire forecast and backtracking has adopted AI models, which require a large amount of data from wildfire scenarios to capture fire spread patterns. This paper explores using cost-effective simulated wildfire scenarios to train AI models and apply them to the analysis of real-world wildfire. This solution requires AI models to minimize the Sim2Real gap, a brand-new topic in the fire spread analysis research community. To investigate the possibility of minimizing the Sim2Real gap, we collect the Sim2Real-Fire dataset that contains 1M simulated scenarios with multi-modal environmental information for training AI models. We prepare 1K real-world wildfire scenarios for testing the AI models. We also propose a deep transformer, S2R-FireTr, which excels in considering the multimodal environmental information for forecasting and backtracking the wildfire. S2R-FireTr surpasses state-of-the-art methods in real-world wildfire scenarios.","","2025-03-30 16:28:04","2025-03-30 16:28:04","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EYGIIEI9/Li et al. - Sim2Real-Fire A Multi-modal Simulation Dataset for Forecast and Backtracking of Real-world Forest F.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZRTXFE8","journalArticle","","Eppel, Sagi; Li, Jolina Yining; Drehwald, Manuel S; Aspuru-Guzik, Alan","Infusing Synthetic Data with Real-World Patterns for Zero-Shot Material State Segmentation","","","","","","Visual recognition of materials and their states is essential for understanding the physical world, from identifying wet regions on surfaces or stains on fabrics to detecting infected areas on plants or minerals in rocks. Collecting data that captures this vast variability is complex due to the scattered and gradual nature of material states. Manually annotating real-world images is constrained by cost and precision, while synthetic data, although accurate and inexpensive, lacks real-world diversity. This work aims to bridge this gap by infusing patterns automatically extracted from real-world images into synthetic data. Hence, patterns collected from natural images are used to generate and map materials into synthetic scenes. This unsupervised approach captures the complexity of the real world while maintaining the precision and scalability of synthetic data. We also present the first comprehensive benchmark for zero-shot material state segmentation, utilizing real-world images across a diverse range of domains, including food, soils, construction, plants, liquids, and more, each appears in various states such as wet, dry, infected, cooked, burned, and many others. The annotation includes partial similarity between regions with similar but not identical materials and hard segmentation of only identical material states. This benchmark eluded top foundation models, exposing the limitations of existing data collection methods. Meanwhile, nets trained on the infused data performed significantly better on this and related tasks. The dataset, code, and trained model are available at these URLs: 1, 2, 3, 4. We also share 300,000 extracted textures and SVBRDF/PBR materials to facilitate future datasets generation at these URLs: 1,2, 3, 4.","","2025-03-30 16:28:05","2025-03-30 16:28:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KBUDBYEU/Eppel et al. - Infusing Synthetic Data with Real-World Patterns for Zero-Shot Material State Segmentation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GF3HI2G","journalArticle","","Ma, Wufei; Zhang, Guofeng; Liu, Qihao; Zeng, Guanning; Kortylewski, Adam; Liu, Yaoyao; Yuille, Alan","ImageNet3D: Towards General-Purpose Object-Level 3D Understanding","","","","","","A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (e.g., class name and bounding box) and 3D information (e.g., 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments 200 categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding. Our dataset and project page are available here: https://imagenet3d.github.io.","","2025-03-30 16:28:07","2025-03-30 16:28:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FU8Y2QBP/Ma et al. - ImageNet3D Towards General-Purpose Object-Level 3D Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2YD2CEH","journalArticle","","Lee, Tony; Tu, Haoqin; Wong, Chi Heem; Zheng, Wenhao; Zhou, Yiyang; Mai, Yifan; Roberts, Josselin Somerville; Yasunaga, Michihiro; Yao, Huaxiu; Xie, Cihang; Liang, Percy","VHELM: A Holistic Evaluation of Vision Language Models","","","","","","Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiencyfocused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.1. VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.","","2025-03-30 16:28:08","2025-03-30 16:28:08","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9VEZM3EB/Lee et al. - VHELM A Holistic Evaluation of Vision Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UG9LBU46","journalArticle","","Bhardwaj, Eshta; Gujral, Harshit; Wu, Siyi; Zogheib, Ciara; Maharaj, Tegan; Becker, Christoph","The State of Data Curation at NeurIPS: An Assessment of Dataset Development Practices in the Datasets and Benchmarks Track","","","","","","Data curation is a field with origins in librarianship and archives, whose scholarship and thinking on data issues go back centuries, if not millennia. The field of machine learning is increasingly observing the importance of data curation to the advancement of both applications and fundamental understanding of machine learning models – evidenced not least by the creation of the Datasets and Benchmarks track itself. This work provides an analysis of recent dataset development practices at NeurIPS through the lens of data curation. We present an evaluation framework for dataset documentation, consisting of a rubric and toolkit developed through a thorough literature review of data curation principles. We use the framework to systematically assess the strengths and weaknesses in current dataset development practices of 60 datasets published in the NeurIPS Datasets and Benchmarks track from 2021-2023. We summarize key findings and trends. Results indicate greater need for documentation about environmental footprint, ethical considerations, and data management. We suggest targeted strategies and resources to improve documentation in these areas and provide recommendations for the NeurIPS peer-review process that prioritize rigorous data curation in ML. We also provide guidelines for dataset developers on the use of our rubric as a standalone tool. Finally, we provide results in the format of a dataset that showcases aspects of recommended data curation practices. Our rubric and results are of interest for improving data curation practices broadly in the field of ML as well as to data curation and science and technology studies scholars studying practices in ML. Our aim is to support continued improvement in interdisciplinary research on dataset practices, ultimately improving the reusability and reproducibility of new datasets and benchmarks, enabling standardized and informed human oversight, and strengthening the foundation of rigorous and responsible ML research.","","2025-03-30 16:28:09","2025-03-30 16:28:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/P8ZE3Z8T/Bhardwaj et al. - The State of Data Curation at NeurIPS An Assessment of Dataset Development Practices in the Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HLKWZ4RU","journalArticle","","Shen, Xin; Du, Heming; Sheng, Hongwei; Wang, Shuyun; Chen, Hui; Chen, Huiqiang; Wu, Zhuojie; Du, Xiaobiao; Ying, Jiaying; Lu, Ruihan; Xu, Qingzheng; Yu, Xin","MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset","","","","","","Isolated Sign Language Recognition (ISLR) focuses on identifying individual sign language signs. Considering the diversity of sign languages across geographical regions, developing region-specific ISLR datasets is crucial for supporting communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale word-level dataset for the ISLR task. To fill this gap, we curate the first large-scale Multi-view Multi-modal Word-Level Australian Sign Language recognition dataset, dubbed MM-WLAuslan. Compared to other publicly available datasets, MM-WLAuslan exhibits three significant advantages: (1) the largest amount of data, (2) the most extensive vocabulary, and (3) the most diverse of multi-modal camera views. Specifically, we record 282K+ sign videos covering 3,215 commonly used Auslan glosses presented by 73 signers in a studio environment. Moreover, our filming system includes two different types of cameras, i.e., three Kinect-V2 cameras and a RealSense camera. We position cameras hemispherically around the front half of the model and simultaneously record videos using all four cameras. Furthermore, we benchmark results with state-of-the-art methods for various multi-modal ISLR settings on MM-WLAuslan, including multi-view, cross-camera, and cross-view. Experiment results indicate that MM-WLAuslan is a challenging ISLR dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide. All datasets and benchmarks are available at  MM-WLAuslan.","","2025-03-30 16:28:11","2025-03-30 16:28:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/88NRPI2R/Shen et al. - MM-WLAuslan Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JZJNL8RM","journalArticle","","Tang, Tianqi; Deldari, Shohreh; Xue, Hao","ViLCo-Bench: VIdeo Language COntinual learning Benchmark","","","","","","Video language continual learning involves continuously adapting to information from video and text inputs, enhancing a model’s ability to handle new tasks while retaining prior knowledge. This field is a relatively under-explored area, and establishing appropriate datasets is crucial for facilitating communication and research in this field. In this study, we present the first dedicated benchmark, ViLCo-Bench, designed to evaluate continual learning models across a range of video-text tasks. The dataset comprises ten-minute-long videos and corresponding language queries collected from publicly available datasets. Additionally, we introduce a novel memory-efficient framework that incorporates self-supervised learning and mimics long-term and short-term memory effects. This framework addresses challenges including memory complexity from long video clips, natural language complexity from open queries, and text-video misalignment. We posit that ViLCo-Bench, with greater complexity compared to existing continual learning benchmarks, would serve as a critical tool for exploring the video-language domain, extending beyond conventional class-incremental tasks, and addressing complex and limited annotation issues. The curated data, evaluations, and our novel method are available at https://github.com/cruiseresearchgroup/ViLCo.","","2025-03-30 16:28:12","2025-03-30 16:28:12","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XRJETJUU/Tang et al. - ViLCo-Bench VIdeo Language COntinual learning Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3SMXH6JQ","journalArticle","","Chen, Jr-Jen; Liao, Yu-Chien; Lin, Hsi-Che; Yu, Yu-Chu; Chen, Yen-Chun; Wang, Yu-Chiang Frank","REXTIME: A Benchmark Suite for Reasoning-Across-Time in Videos","","","","","","We introduce REXTIME, a benchmark designed to rigorously test AI models’ ability to perform temporal reasoning within video events. Speciﬁcally, REXTIME focuses on reasoning across time, i.e. human-like understanding when the question and its corresponding answer occur in different video segments. This form of reasoning, requiring advanced understanding of cause-and-effect relationships across video segments, poses signiﬁcant challenges to even the frontier multimodal large language models. To facilitate this evaluation, we develop an automated pipeline for generating temporal reasoning question-answer pairs, signiﬁcantly reducing the need for labor-intensive manual annotations. Our benchmark includes 921 carefully vetted validation samples and 2,143 test samples, each manually curated for accuracy and relevance. Evaluation results show that while frontier large language models outperform academic models, they still lag behind human performance by a signiﬁcant 14.3% accuracy gap. Additionally, our pipeline creates a training dataset of 9,695 machine generated samples without manual effort, which empirical studies suggest can enhance the across-time reasoning via ﬁne-tuning.","","2025-03-30 16:28:13","2025-03-30 16:28:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RRBB9XU2/Chen et al. - REXTIME A Benchmark Suite for Reasoning-Across-Time in Videos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XJKTDMJL","journalArticle","","Vogel, Liane; Bodensohn, Jan-Micha; Binnig, Carsten","WikiDBs: A Large-Scale Corpus of Relational Databases from Wikidata","","","","","","Deep learning on tabular data, and particularly tabular representation learning, has recently gained growing interest. However, representation learning for relational databases with multiple tables is still an underexplored area, which may be attributed to the lack of openly available resources. To support the development of foundation models for tabular data and relational databases, we introduce WikiDBs, a novel open-source corpus of 100,000 relational databases. Each database consists of multiple tables connected by foreign keys. The corpus is based on Wikidata and aims to follow certain characteristics of real-world databases. In this paper, we describe the dataset and our method for creating it. By making our code publicly available, we enable others to create tailored versions of the dataset, for example, by creating databases in different languages. Finally, we conduct a set of initial experiments to showcase how WikiDBs can be used to train for data engineering tasks, such as missing value imputation and column type annotation.","","2025-03-30 16:28:14","2025-03-30 16:28:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ANJLJD9F/Vogel et al. - WikiDBs A Large-Scale Corpus of Relational Databases from Wikidata.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86X4J75B","journalArticle","","Granqvist, Filip; Song, Congzheng; Cahill, Áine; van Dalen, Rogier; Pelikan, Martin; Chan, Yi Sheng; Feng, Xiaojun; Krishnaswami, Natarajan; Jina, Vojta; Chitnis, Mona","pfl-research: simulation framework for accelerating research in Private Federated Learning","","","","","","Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on large and realistic FL datasets. We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that pfl-research is 7-72× faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of benchmarks that evaluates an algorithm’s overall performance on a diverse set of realistic scenarios. The code is available on GitHub at https://github.com/apple/pfl-research.","","2025-03-30 16:28:15","2025-03-30 16:28:15","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YKLSNJ5U/Granqvist et al. - pfl-research simulation framework for accelerating research in Private Federated Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JDP6NUB2","journalArticle","","Liu, Yiran; Yang, Ke; Qi, Zehan; Liu, Xiao; Yu, Yang; Zhai, ChengXiang","Bias and Volatility: A Statistical Framework for Evaluating Large Language Model’s Stereotypes and the Associated Generation Inconsistency","","","","","","We present a novel statistical framework for analyzing stereotypes in large language models (LLMs) by systematically estimating the bias and variation in their generation. Current evaluation metrics in the alignment literature often overlook the randomness of stereotypes caused by the inconsistent generative behavior of LLMs. For example, this inconsistency can result in LLMs displaying contradictory stereotypes, including those related to gender or race, for identical professions across varied contexts. Neglecting such inconsistency could lead to misleading conclusions in alignment evaluations and hinder the accurate assessment of the risk of LLM applications perpetuating or amplifying social stereotypes and unfairness. This work proposes a Bias-Volatility Framework (BVF) that estimates the probability distribution function of LLM stereotypes. Specifically, since the stereotype distribution fully captures an LLM’s generation variation, BVF enables the assessment of both the likelihood and extent to which its outputs are against vulnerable groups, thereby allowing for the quantification of the LLM’s aggregated discrimination risk. Furthermore, we introduce a mathematical framework to decompose an LLM’s aggregated discrimination risk into two components: bias risk and volatility risk, originating from the mean and variation of LLM’s stereotype distribution, respectively. We apply BVF to assess 12 commonly adopted LLMs and compare their risk levels. Our findings reveal that: i) Bias risk is the primary cause of discrimination risk in LLMs; ii) Most LLMs exhibit significant pro-male stereotypes for nearly all careers; iii) Alignment with reinforcement learning from human feedback lowers discrimination by reducing bias, but increases volatility; iv) Discrimination risk in LLMs correlates with key sociol-economic factors like professional salaries. Finally, we emphasize that BVF can also be used to assess other dimensions of generation inconsistency’s impact on LLM behavior beyond stereotypes, such as knowledge mastery.","","2025-03-30 16:28:17","2025-03-30 16:28:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MXXC5MKJ/Liu et al. - Bias and Volatility A Statistical Framework for Evaluating Large Language Model’s Stereotypes and t.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KD5ZF9Y","journalArticle","","Lyu, Jiafei; Xu, Kang; Xu, Jiacheng; Yan, Mengbei; Yang, Jingwen; Zhang, Zongzhang; Bai, Chenjia; Lu, Zongqing; Li, Xiu","ODRL: A Benchmark for Off-Dynamics Reinforcement Learning","","","","","","We consider off-dynamics reinforcement learning (RL) where one needs to transfer policies across different domains with dynamics mismatch. Despite the focus on developing dynamics-aware algorithms, this ﬁeld is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ODRL, the ﬁrst benchmark tailored for evaluating off-dynamics RL methods. ODRL contains four experimental settings where the source and target domains can be either online or ofﬂine, and provides diverse tasks and a broad spectrum of dynamics shifts, making it a reliable platform to comprehensively evaluate the agent’s adaptation ability to the target domain. Furthermore, ODRL includes recent off-dynamics RL algorithms in a uniﬁed framework and introduces some extra baselines for different settings, all implemented in a single-ﬁle manner. To unpack the true adaptation capability of existing methods, we conduct extensive benchmarking experiments, which show that no method has universal advantages across varied dynamics shifts. We hope this benchmark can serve as a cornerstone for future research endeavors. Our code is publicly available at https://github.com/OffDynamicsRL/off-dynamics-rl.","","2025-03-30 16:28:19","2025-03-30 16:28:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/I7VJKNMC/Lyu et al. - ODRL A Benchmark for Off-Dynamics Reinforcement Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z3JITVUR","journalArticle","","Lozano, Alejandro; Nirschl, Jeffrey; Burgess, James; Gupte, Sanket Rajan; Zhang, Yuhui; Unell, Alyssa; Yeung-Levy, Serena","Micro-Bench: A Vision-Language Benchmark for Microscopy Understanding","","","","","","Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing researchers’ efficiency, identifying new image biomarkers, and accelerating hypothesis generation and scientific discovery. However, there is a lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs’ perception and cognition capabilities in biological image understanding. To address this gap, we introduce Micro-Bench, an expert-curated benchmark encompassing 24 biomedical tasks across various scientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light), scales (subcellular, cellular, tissue), and organisms in both normal and abnormal states. We evaluate state-ofthe-art biomedical, pathology, and general VLMs on Micro-Bench and find that: i) current models struggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii) current specialist models fine-tuned on biomedical data often perform worse than generalist models; iii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior biomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned and pretrained models offers one solution to forgetting and improves general performance across biomedical tasks. We release Micro-Bench under a permissive license 2 to accelerate the research and development of microscopy foundation models.","","2025-03-30 16:28:20","2025-03-30 16:28:20","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SKS4BJI4/Lozano et al. - Micro-Bench A Vision-Language Benchmark for Microscopy Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FTMFIRR5","journalArticle","","Gabriel, Wassim; Shouman, Omar; Schroeder, Ayla; Boessl, Florian; Wilhelm, Mathias","PROSPECT PTMs: Rich Labeled Tandem Mass Spectrometry Dataset of Modified Peptides for Machine Learning in Proteomics","","","","","","Post-Translational Modifications (PTMs) are changes that occur in proteins after synthesis, influencing their structure, function, and cellular behavior. PTMs are essential in cell biology; they regulate protein function and stability, are involved in various cellular processes, and are linked to numerous diseases. A particularly interesting class of PTMs are chemical modifications such as phosphorylation introduced on amino acid side chains because they can drastically alter the physicochemical properties of the peptides once they are present. One or more PTMs can be attached to each amino acid of the peptide sequence. The most commonly applied technique to detect PTMs on proteins is bottom-up Mass Spectrometrybased proteomics (MS), where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). While an increasing number of machine learning models are published focusing on MS/MS-related property prediction of unmodified peptides, high-quality reference data for modified peptides is missing, impeding model development for this important class of peptides. To enable researchers to train machine learning models that can accurately predict the properties of modified peptides, we introduce four high-quality labeled datasets for applying machine and deep learning to tasks in MS-based proteomics. The four datasets comprise several subgroups of peptides with 1.2 million unique modified peptide sequences and 30 unique pairs of (amino-acid, PTM), covering both experimentally introduced and naturally occurring modifications on various amino acids. We evaluate the utility and importance of the dataset by providing benchmarking results on models trained with and without modifications and highlighting the impact of including modified sequences on downstream tasks. We demonstrate that predicting the properties of modified peptides is more challenging but has a broad impact since they are often the core of protein functionality and its regulation, and they have a potential role as biomarkers in clinical applications. Our datasets contribute to applied machine learning in proteomics by enabling the research community to experiment with methods to encode PTMs as model inputs and to benchmark against reference data for model comparison. With a proper data split for three common tasks in proteomics, we provide a robust way to evaluate model performance and assess generalization on unseen modified sequences.","","2025-03-30 16:28:21","2025-03-30 16:28:21","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AN3VPHSF/Gabriel et al. - PROSPECT PTMs Rich Labeled Tandem Mass Spectrometry Dataset of Modified Peptides for Machine Learni.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7MIY9Z7","journalArticle","","Cui, Hejie; Mao, Lingjun; Liang, Xin; Zhang, Jieyu; Ren, Hui; Li, Quanzheng; Li, Xiang; Yang, Carl","Biomedical Visual Instruction Tuning with Clinician Preference Alignment","","","","","","Recent advancements in multimodal foundation models have showcased impressive capabilities in understanding and reasoning with visual and textual information. Adapting these foundation models trained for general usage to specialized domains like biomedicine requires large-scale domain-specific instruction datasets. While existing works have explored curating such datasets automatically, the resultant datasets are not explicitly aligned with domain expertise. In this work, we propose a data-centric framework, Biomedical Visual Instruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that incorporates clinician preferences into both stages of generating and selecting instruction data for tuning biomedical multimodal foundation models. First, during the generation stage, we prompt the GPT-4V generator with a diverse set of clinician-selected demonstrations for preference-aligned data candidate generation. Then, during the selection phase, we train a separate selection model, which explicitly distills clinician and policy-guided model preferences into a rating function to select high-quality data for medical instruction tuning. Results show that the model tuned with the instruction data from our method demonstrates a significant improvement in open visual chat (18.5% relatively) and medical VQA (win rate up to 81.73%). Our instruction-following data, models, and code are available at https://BioMed-VITAL.github.io.","","2025-03-30 16:28:23","2025-03-30 16:28:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/QFJXCW7H/Cui et al. - Biomedical Visual Instruction Tuning with Clinician Preference Alignment.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIYY8HA5","journalArticle","","Wu, Jin; Zhou, Haoying; Kazanzides, Peter; Munawar, Adnan; Liu, Anqi","SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy Learning and Benchmarking","","","","","","","","2025-03-30 16:28:24","2025-03-30 16:28:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NYJC2JRI/Wu et al. - SurgicAI A Hierarchical Platform for Fine-Grained Surgical Policy Learning and Benchmarking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ELCNUK5","journalArticle","","Hou, Yufang; Pascale, Alessandra; Carnerero-Cano, Javier; Tchrakian, Tigran; Marinescu, Radu; Daly, Elizabeth; Padhi, Inkit; Sattigeri, Prasanna","WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia","","","","","","Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 highquality, human-annotated instances designed to assess the performance of LLMs in providing a complete perspective on conflicts from the retrieved documents, rather than choosing one answer over another, when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances. To facilitate future work, we release WikiContradict at https://ibm.biz/wikicontradict.","","2025-03-30 16:28:25","2025-03-30 16:28:25","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3EKR5HLN/Hou et al. - WikiContradict A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5WQYEJXI","journalArticle","","Brahman, Faeze; Kumar, Sachin; Balachandran, Vidhisha; Dasigi, Pradeep; Pyatkin, Valentina; Ravichander, Abhilasha; Wiegreffe, Sarah; Dziri, Nouha; Chandu, Khyathi; Hessel, Jack; Tsvetkov, Yulia; Smith, Noah A; Choi, Yejin; Hajishirzi, Hannaneh","The Art of Saying No: Contextual Noncompliance in Language Models","","","","","","Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of “unsafe” queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a syntheticallygenerated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.","","2025-03-30 16:28:27","2025-03-30 16:28:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VFLMM9BP/Brahman et al. - The Art of Saying No Contextual Noncompliance in Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X5DZPYEI","journalArticle","","Brennan, Connor; Williams, Andrew Robert; Younis, Omar G; Vyas, Vedant; Yasafova, Daria; Rish, Irina","Using Unity to Help Solve Reinforcement Learning","","","","","","Leveraging the depth and flexibility of XLand as well as the rapid prototyping features of the Unity engine, we present the United Unity Universe, an open-source toolkit designed to accelerate the creation of innovative reinforcement learning environments. This toolkit includes a robust implementation of OpenXLand, a framework for meta-RL based on XLand 2.0 [23], complemented by a user-friendly interface which allows users to modify the details of procedurally generated terrains and task rules with ease. Along with a ready-to-use implementation of OpenXLand, we provide a curated selection of terrains and rule sets, accompanied by implementations of reinforcement learning baselines to facilitate quick experimentation with novel architectural designs for adaptive agents. Furthermore, we illustrate how the United Unity Universe serves as a high-level language that enables researchers to develop diverse and endlessly variable 3D environments within a unified framework. This functionality establishes the United Unity Universe (U3) as an essential tool for advancing the field of reinforcement learning, especially in the development of adaptive and generalizable learning systems.","","2025-03-30 16:28:29","2025-03-30 16:28:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Q4UK6C4V/Brennan et al. - Using Unity to Help Solve Reinforcement Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M6XBYWLN","journalArticle","","Yeh, Chen; Chang, You-Ming; Chiu, Wei-Chen; Yu, Ning","T2Vs Meet VLMs: A Scalable Multimodal Dataset for Visual Harmfulness Recognition","","","","","","","","2025-03-30 16:28:30","2025-03-30 16:28:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XDZ4F2EI/Yeh et al. - T2Vs Meet VLMs A Scalable Multimodal Dataset for Visual Harmfulness Recognition.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KQNSFBT9","journalArticle","","Xie, Tianbao; Zhang, Danyang; Chen, Jixuan; Li, Xiaochuan; Zhao, Siheng; Cao, Ruisheng; Hua, Toh Jing; Cheng, Zhoujun; Shin, Dongchan; Lei, Fangyu; Liu, Yitao; Xu, Yiheng; Zhou, Shuyan; Savarese, Silvio; Xiong, Caiming; Zhong, Victor; Yu, Tao","OSWORLD: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","","","","","","Autonomous agents that accomplish complex computer tasks with minimal human interventions can significantly enhance accessibility and productivity of humancomputer interactions. Existing benchmarks either lack interactive environments or are limited to specific applications/domains, failing to reflect the diversity and complexity of real-world computer use and limiting agent scalability. We introduce OSWORLD, the first-of-its-kind scalable real computer environment for multimodal agents, supporting task setup, interactive learning, and execution-based evaluation of open-ended computer tasks across arbitrary applications in Ubuntu, Windows, and macOS. Using OSWORLD, we create a benchmark of 369 tasks involving real web and desktop apps in open domains, OS file I/O, and multi-app workflows. Each example derives from real-world use cases and includes detailed setup and execution-based evaluation for reproducibility. Extensive evaluation of state-of-theart LLM/VLM agents on OSWORLD reveals deficiencies in their ability to serve as computer assistants. While humans accomplish 72.4% of the tasks, the best agents achieve <12.2%, struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWORLD provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Implementation and experiments are at https://os-world.github.io.","","2025-03-30 16:28:31","2025-03-30 16:28:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/B46JC23Q/Xie et al. - OSWORLD Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNEBLIEK","journalArticle","","Zhang, Xiaoyuan; Zhao, Liang; Yu, Yingying; Lin, Xi; Chen, Yifan; Zhao, Han; Zhang, Qingfu","LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch","","","","","","Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, fairness, robustness, and more. Unlike single-objective optimization, which aggregates objectives into a scalar through weighted sums, MOPs focus on generating specific or diverse Pareto solutions and learning the entire Pareto set directly. Existing MOP benchmarks primarily focus on evolutionary algorithms, which are zeroth-order or meta-heuristic methods that fail to leverage higher-order objective information and cannot scale to large models. To address these challenges, we introduce LibMOON, the first multiobjective optimization library supporting state-of-the-art gradient-based methods, offering a fair and comprehensive benchmark, and open-sourced for the community.","","2025-03-30 16:28:33","2025-03-30 16:28:33","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PDKCYKUE/Zhang et al. - LibMOON A Gradient-based MultiObjective OptimizatioN Library in PyTorch.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VNJDI8L3","journalArticle","","Szałata, Artur; Benz, Andrew; Cannoodt, Robrecht; Cortes, Mauricio; Fong, Jason; Kuppasani, Sunil; Lieberman, Richard; Liu, Tianyu; Mas-Rosario, Javier A; Meinl, Rico; Nourisa, Jalil; Tumiel, Jared; Tunjic, Tin M; Wang, Mengbo; Weber, Noah; Zhao, Hongyu; Anchang, Benedict; Theis, Fabian J; Luecken, Malte D; Burkhardt, Daniel B","A benchmark for prediction of transcriptomic responses to chemical perturbations across cell types","","","","","","","","2025-03-30 16:28:34","2025-03-30 16:28:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3DC2YCFW/Szałata et al. - A benchmark for prediction of transcriptomic responses to chemical perturbations across cell types.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LAN2R3XZ","journalArticle","","Han, Tessa; Kumar, Aounon; Agarwal, Chirag; Lakkaraju, Himabindu","MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models","","","","","","As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench.","","2025-03-30 16:28:35","2025-03-30 16:28:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/LRLURXGL/Han et al. - MedSafetyBench Evaluating and Improving the Medical Safety of Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXZXGEMC","journalArticle","","Awadalla, Anas; Xue, Le; Lo, Oscar; Shu, Manli; Lee, Hannah; Guha, Etash; Jordan, Matt; Shen, Sheng; Awadalla, Mohamed; Savarese, Silvio; Xiong, Caiming; Xu, Ran; Choi, Yejin; Schmidt, Ludwig","Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens","","","","","","Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, open-source multimodal interleaved datasets. In response, we introduce MINT-1T, the most extensive and diverse open-source Multimodal INTerleaved dataset to date. MINT-1T comprises of one trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS. We release our data at https://github.com/mlfoundations/MINT-1T.","","2025-03-30 16:28:37","2025-03-30 16:28:37","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VKXGZNJE/Awadalla et al. - Scaling Open-Source Multimodal Data by 10x A Multimodal Dataset with One Trillion Tokens.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EQT5I7LK","journalArticle","","Liu, Zuxin; Hoang, Thai; Zhang, Jianguo; Zhu, Ming; Lan, Tian; Kokane, Shirley; Tan, Juntao; Yao, Weiran; Liu, Zhiwei; Feng, Yihao; Murthy, Rithesh; Yang, Liangwei; Savarese, Silvio; Niebles, Juan Carlos; Wang, Huan; Heinecke, Shelby; Xiong, Caiming","APIGen: Automated PIpeline for Generating Verifiable and Diverse Function-Calling Datasets","","","","","","The advancement of function-calling agent models requires diverse, reliable, and high-quality datasets. This paper presents APIGen, an automated data generation pipeline designed to synthesize high-quality datasets for function-calling applications. We leverage APIGen and collect 3,673 executable APIs across 21 different categories to generate diverse function-calling datasets in a scalable and structured manner. Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, improving its reliability and correctness. We demonstrate that models trained with our curated datasets, even with only 7B parameters, can achieve state-of-the-art performance on the Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models. Moreover, our 1B model achieves exceptional performance, surpassing GPT-3.5Turbo and Claude-3 Haiku. We release a dataset containing 60,000 high-quality entries, aiming to advance the field of function-calling agent domains. The dataset is available on Huggingface 1 and the project homepage 2.","","2025-03-30 16:28:38","2025-03-30 16:28:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/J7WFAS5A/Liu et al. - APIGen Automated PIpeline for Generating Verifiable and Diverse Function-Calling Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IXKABTNS","journalArticle","","Ma, Qi; Paudel, Danda Pani; Konukoglu, Ender; Gool, Luc Van","Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes","","","","","","","","2025-03-30 16:28:39","2025-03-30 16:28:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YCD3GQCA/Ma et al. - Implicit-Zoo A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUDH7H9G","journalArticle","","Roush, Allen; Shabazz, Yusuf; Balaji, Arvind; Zhang, Peter; Mezza, Stefano; Zhang, Markus; Basu, Sanjay; Vishwanath, Sriram; Fatemi, Mehdi; Ziv, Ravid Shwartz","OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset","","","","","","We introduce OpenDebateEvidence, a comprehensive dataset for argument mining and summarization sourced from the American Competitive Debate community. This dataset includes over 3.5 million documents with rich metadata, making it one of the most extensive collections of debate evidence. OpenDebateEvidence captures the complexity of arguments in high school and college debates, providing valuable resources for training and evaluation. Our extensive experiments demonstrate the efficacy of fine-tuning state-of-the-art large language models for argumentative abstractive summarization across various methods, models, and datasets. By providing this comprehensive resource, we aim to advance computational argumentation and support practical applications for debaters, educators, and researchers. OpenDebateEvidence is publicly available to support further research and innovation in computational argumentation. Access it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist.","","2025-03-30 16:28:40","2025-03-30 16:28:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SN3ENZGY/Roush et al. - OpenDebateEvidence A Massive-Scale Argument Mining and Summarization Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6UU7X73J","journalArticle","","Caciularu, Avi; Jacovi, Alon; Ben-David, Eyal; Goldshtein, Sasha; Schuster, Tal; Herzig, Jonathan; Elidan, Gal; Globerson, Amir","TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools","","","","","","Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT—Text And Calculations through Tables, a dataset crafted to evaluate LLMs’ reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as IE as a tool. Specifically, we propose to add “tools” for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks.","","2025-03-30 16:28:41","2025-03-30 16:28:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SLTVT7P8/Caciularu et al. - TACT Advancing Complex Aggregative Reasoning with Information Extraction Tools.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HYJTC287","journalArticle","","Saparina, Irina; Lapata, Mirella",": A Benchmark for Parsing Ambiguous Questions into Database Queries","","","","","","Practical semantic parsers are expected to understand user utterances and map them to executable programs, even when these are ambiguous. We introduce a new benchmark, , which we hope will inform and inspire the development of text-to-SQL parsers capable of recognizing and interpreting ambiguous requests. Our dataset contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries. In each case, the ambiguity persists even when the database context is provided. This is achieved through a novel approach that involves controlled generation of databases from scratch. We benchmark various LLMs on , revealing that even the most advanced models struggle to identify and interpret ambiguity in questions.","","2025-03-30 16:28:43","2025-03-30 16:28:43","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8BU7H9KI/Saparina and Lapata -  A Benchmark for Parsing Ambiguous Questions into Database Queries.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V35WWXH2","journalArticle","","Liu, Chang; Saul, Rebecca; Sun, Yihao; Raff, Edward; Fuchs, Maya; Pantano, Townsend Southard; Holt, James; Micinski, Kristopher","ASSEMBLAGE: Automatic Binary Dataset Construction for Machine Learning","","","","","","Binary code is pervasive, and binary analysis is a key task in reverse engineering, malware classification, and vulnerability discovery. Unfortunately, while there exist large corpora of malicious binaries, obtaining high-quality corpora of benign binaries for modern systems has proven challenging (e.g., due to licensing issues). Consequently, machine learning based pipelines for binary analysis utilize either costly commercial corpora (e.g., VirusTotal) or open-source binaries (e.g., coreutils) available in limited quantities. To address these issues, we present ASSEMBLAGE: an extensible distributed system that crawls, configures, and builds Windows PE binaries to obtain high-quality binary corpora suitable for training state-of-the-art models in binary analysis. We have run ASSEMBLAGE on AWS over the past year, producing 890k Windows PE and 428k Linux ELF binaries across 29 configurations. ASSEMBLAGE is designed to be both reproducible and extensible, enabling users to publish “recipes” for their datasets, and facilitating the extraction of a wide array of features. We evaluated ASSEMBLAGE by using its data to train modern learning-based pipelines for compiler provenance and binary function similarity. Our results illustrate the practical need for robust corpora of high-quality Windows PE binaries in training modern learning-based binary analyses. ASSEMBLAGE code is open sourced under the MIT license, and the dataset can be downloaded from https://assemblage-dataset.net/.","","2025-03-30 16:28:44","2025-03-30 16:28:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CV5RDDHP/Liu et al. - ASSEMBLAGE Automatic Binary Dataset Construction for Machine Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQY6HGTU","journalArticle","","Victor, Brandon; Letard, Mathilde; Naylor, Peter; Douch, Karim; Longépé, Nicolas; He, Zhen; Ebel, Patrick","Off to new Shores: A Dataset & Benchmark for (near-)coastal Flood Inundation Forecasting","","","","","","Floods are among the most common and devastating natural hazards, imposing immense costs on our society and economy due to their disastrous consequences. Recent progress in weather prediction and spaceborne flood mapping demonstrated the feasibility of anticipating extreme events and reliably detecting their catastrophic effects afterwards. However, these efforts are rarely linked to one another and there is a critical lack of datasets and benchmarks to enable the direct forecasting of flood extent. To resolve this issue, we curate a novel dataset enabling a timely prediction of flood extent. Furthermore, we provide a representative evaluation of state-of-the-art methods, structured into two benchmark tracks for forecasting flood inundation maps i) in general and ii) focused on coastal regions. Altogether, our dataset and benchmark provide a comprehensive platform for evaluating flood forecasts, enabling future solutions for this critical challenge. Data, code & models are shared at https://github.com/Multihuntr/GFF under a CC0 license.","","2025-03-30 16:28:46","2025-03-30 16:28:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XEE3JRAL/Victor et al. - Off to new Shores A Dataset & Benchmark for (near-)coastal Flood Inundation Forecasting.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XSWTVBC4","journalArticle","","Wang, Zhecan; Liu, Junzhang; Tang, Chia-Wei; Alomari, Hani; Sivakumar, Anushka; Sun, Rui; Li, Wenhao; Ayyubi, Hammad; You, Haoxuan; Ishmam, Alvi; Chang, Kai-Wei; Chang, Shih-Fu; Thomas, Chris","JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images","","","","","","","","2025-03-30 16:28:47","2025-03-30 16:28:47","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PJCRG52T/Wang et al. - JourneyBench A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4BQ4JIFR","journalArticle","","Tian, Minyang; Gao, Luyu; Zhang, Shizhuo Dylan; Chen, Xinan; Fan, Cunwei; Guo, Xuefei; Haas, Roland; Ji, Pan; Krongchon, Kittithat; Li, Yao; Liu, Shengyan; Luo, Di; Ma, Yutao; Tong, Hao; Trinh, Kha; Tian, Chenyu; Wang, Zihan; Wu, Bohao; Xiong, Yanyu; Yin, Shengzhu; Zhu, Minhui; Lieret, Kilian; Lu, Yanxin; Liu, Genglin; Du, Yufeng; Tao, Tianhua; Press, Ofir; Callan, Jamie; Huerta, Eliu; Peng, Hao","SciCode: A Research Coding Benchmark Curated by Scientists","","","","","","","","2025-03-30 16:28:49","2025-03-30 16:28:49","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TVA3KL8R/Tian et al. - SciCode A Research Coding Benchmark Curated by Scientists.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H4M2DIFZ","journalArticle","","Gastinger, Julia; Huang, Shenyang; Galkin, Mikhail; Loghmani, Erfan; Parviz, Ali; Poursafaei, Farimah; Danovitch, Jacob; Rossi, Emanuele; Koutis, Ioannis; Stuckenschmidt, Heiner; Rabbany, Reihaneh; Rabusseau, Guillaume","TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous Graphs","","","","","","Multi-relational temporal graphs are powerful tools for modeling real-world data, capturing the evolving and interconnected nature of entities over time. Recently, many novel models are proposed for ML on such graphs intensifying the need for robust evaluation and standardized benchmark datasets. However, the availability of such resources remains scarce and evaluation faces added complexity due to reproducibility issues in experimental protocols. To address these challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel benchmarking framework tailored for evaluating methods for predicting future links on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a focus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0 facilitates comprehensive evaluations by presenting eight novel datasets spanning five domains with up to 53 million edges. TGB 2.0 datasets are significantly larger than existing datasets in terms of number of nodes, edges, or timestamps. In addition, TGB 2.0 provides a reproducible and realistic evaluation pipeline for multi-relational temporal graphs. Through extensive experimentation, we observe that 1) leveraging edge-type information is crucial to obtain high performance, 2) simple heuristic baselines are often competitive with more complex methods, 3) most methods fail to run on our largest datasets, highlighting the need for research on more scalable methods.","","2025-03-30 16:28:50","2025-03-30 16:28:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MLNY8R6B/Gastinger et al. - TGB 2.0 A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous Graphs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G23LTUGD","journalArticle","","Muschalik, Maximilian; Baniecki, Hubert; Fumagalli, Fabian; Kolpaczki, Patrick; Hammer, Barbara; Hüllermeier, Eyke","shapiq: Shapley Interactions for Machine Learning","","","","","","Originally rooted in game theory, the Shapley Value (SV) has recently become an important tool in machine learning research. Perhaps most notably, it is used for feature attribution and data valuation in explainable artificial intelligence. Shapley Interactions (SIs) naturally extend the SV and address its limitations by assigning joint contributions to groups of entities, which enhance understanding of black box machine learning models. Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources. In this work, we introduce shapiq, an open-source Python package that unifies state-of-the-art algorithms to efficiently compute SVs and any-order SIs in an application-agnostic framework. Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains. For practitioners, shapiq is able to explain and visualize any-order feature interactions in predictions of models, including vision transformers, language models, as well as XGBoost and LightGBM with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and consolidate the application of SVs and SIs in machine learning that facilitates future research. The source code and documentation are available at https://github.com/mmschlk/shapiq.","","2025-03-30 16:28:52","2025-03-30 16:28:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3IXZ8BRA/Muschalik et al. - shapiq Shapley Interactions for Machine Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"THDTMSVY","journalArticle","","Gu, Tianle; Zhou, Zeyang; Huang, Kexin; Liang, Dandan; Wang, Yixu; Zhao, Haiquan; Yao, Yuanqi; Qiao, Xingge; Wang, Keqing; Yang, Yujiu; Teng, Yan; Qiao, Yu; Wang, Yingchun","MLLMGUARD: A Multi-dimensional Safety Evaluation Suite","","","","","","","","2025-03-30 16:28:53","2025-03-30 16:28:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3ND68MW2/Gu et al. - MLLMGUARD A Multi-dimensional Safety Evaluation Suite.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TBDP6JEM","journalArticle","","Wang, Minjie; Gan, Quan; Wipf, David; Cai, Zhenkun; Li, Ning; Tang, Jianheng; Zhang, Yanlin; Zhang, Zizhao; Mao, Zunyao; Song, Yakun; Wang, Yanbo; Li, Jiahang; Zhang, Han; Yang, Guang; Qin, Xiao; Lei, Chuan; Zhang, Muhan; Zhang, Weinan; Faloutsos, Christos; Zhang, Zheng","4DBInfer: A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on RDBs","","","","","","Given a relational database (RDB), how can we predict missing column values in some target table of interest? Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing. This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes. As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics. To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs. Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks. From a delivery standpoint, we operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer; please see https://github.com/awslabs/multi-table-benchmark/.","","2025-03-30 16:28:54","2025-03-30 16:28:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/GKCXPH2U/Wang et al. - 4DBInfer A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on RDBs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SLDHIRFS","journalArticle","","Li, Zhuofeng; Gou, Zixing; Zhang, Xiangnan; Liu, Zhongyuan; Li, Sirui; Hu, Yuntong; Ling, Chen; Zhang, Zheng; Zhao, Liang","TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs","","","","","","Text-Attributed Graphs (TAGs) augment graph structures with natural language descriptions, facilitating detailed depictions of data and their interconnections across various real-world settings. However, existing TAG datasets predominantly feature textual information only at the nodes, with edges typically represented by mere binary or categorical attributes. This lack of rich textual edge annotations significantly limits the exploration of contextual relationships between entities, hindering deeper insights into graph-structured data. To address this gap, we introduce Textual-Edge Graphs Datasets and Benchmark (TEG-DB), a comprehensive and diverse collection of benchmark textual-edge datasets featuring rich textual descriptions on nodes and edges. The TEG-DB datasets are large-scale and encompass a wide range of domains, from citation networks to social networks. In addition, we conduct extensive benchmark experiments on TEG-DB to assess the extent to which current techniques, including pre-trained language models (PLMs), graph neural networks (GNNs), proposed novel entangled GNNs and their combinations, can utilize textual node and edge information. Our goal is to elicit advancements in textual-edge graph research, specifically in developing methodologies that exploit rich textual node and edge descriptions to enhance graph analysis and provide deeper insights into complex real-world networks. The entire TEG-DB project is publicly accessible as an open-source repository on Github, accessible at https://github.com/Zhuofeng-Li/TEG-Benchmark.","","2025-03-30 16:28:56","2025-03-30 16:28:56","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8UB56YHM/Li et al. - TEG-DB A Comprehensive Dataset and Benchmark of Textual-Edge Graphs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QL9APSZS","journalArticle","","Monteiro, João; Noël, Pierre-André; Marcotte, Étienne; Rajeswar, Sai; Zantedeschi, Valentina; Vázquez, David; Chapados, Nicolas; Pal, Christopher; Taslakian, Perouz","REPLIQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content","","","","","","","","2025-03-30 16:28:57","2025-03-30 16:28:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YTHGBQDT/Monteiro et al. - REPLIQA A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEW4SY6R","journalArticle","","Shomee, Homaira Huda; Wang, Zhu; Medya, Sourav; Ravi, Sathya N","IMPACT: A Large-scale Integrated Multimodal Patent Analysis and Creation Dataset for Design Patents","","","","","","In this paper, we introduce IMPACT (Integrated Multimodal Patent Analysis and CreaTion Dataset for Design Patents), a large-scale multimodal patent dataset with detailed captions for design patent figures. Our dataset includes half a million design patents comprising 3.61 million figures along with captions from patents granted by the United States Patent and Trademark Office (USPTO) over a 16year period from 2007 to 2022. We incorporate the metadata of each patent application with elaborate captions that are coherent with multiple viewpoints of designs. Even though patents themselves contain a variety of design figures, titles, and descriptions of viewpoints, we find that they lack detailed descriptions that are necessary to perform multimodal tasks such as classification and retrieval. IMPACT closes this gap thereby providing researchers with necessary ingredients to instantiate a variety of multimodal tasks. Our dataset has a huge potential for novel design inspiration and can be used with advanced computer vision models in tandem. We perform preliminary evaluations on the dataset on the popular patent analysis tasks such as classification and retrieval. Our results indicate that integrating images with generated captions significantly improves the performance of different models on the corresponding tasks. Given that design patents offer various benefits for modeling novel tasks, we propose two standard computer vision tasks that have not been investigated in analyzing patents as future directions using IMPACT as a benchmark viz., 3D Image Construction and Visual Question Answering (VQA). To facilitate research in these directions, we make our IMPACT dataset and the code/models used in this work publicly available here.","","2025-03-30 16:28:58","2025-03-30 16:28:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/96GB64K2/Shomee et al. - IMPACT A Large-scale Integrated Multimodal Patent Analysis and Creation Dataset for Design Patents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5E7P5EAS","preprint","2024","Salehi, Mohammadreza; Park, Jae Sung; Yadav, Tanush; Kusupati, Aditya; Krishna, Ranjay; Choi, Yejin; Hajishirzi, Hannaneh; Farhadi, Ali","ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition","","","","10.48550/arXiv.2410.05774","http://arxiv.org/abs/2410.05774","Our world is full of varied actions and moves across specialized domains that we, as humans, strive to identify and understand. Within any single domain, actions can often appear quite similar, making it challenging for deep models to distinguish them accurately. To evaluate the effectiveness of multimodal foundation models in helping us recognize such actions, we present ActionAtlas v1.0, a multiple-choice video question-answering benchmark featuring short videos across various sports. Each video in the dataset is paired with a question and four or five choices. The question pinpoints specific individuals, asking which choice “best” describes their action within a certain temporal context. Overall, the dataset includes 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices. Unlike most existing video question answering benchmarks that only cover simplistic actions, often identifiable from a single frame, ActionAtlas focuses on intricate movements and rigorously tests the model’s capability to discern subtle differences between moves that look similar within each domain. We evaluate open and proprietary foundation models on this benchmark, finding that the best model, GPT-4o, achieves a maximum accuracy of 45.52%. Meanwhile, Nonexpert crowd workers, provided with action description for each choice, achieve 61.64% accuracy, where random chance is approximately 21%. Our findings with state-of-the-art models indicate that having a high frame sampling rate is important for accurately recognizing actions in ActionAtlas, a feature that some leading proprietary video models, such as Gemini, do not include in their default configurations.","2024-11-11","2025-03-30 16:29:01","2025-03-30 16:29:01","2025-03-30 16:29:01","","","","","","","ActionAtlas","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2410.05774 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/BX3WBH58/Salehi et al. - 2024 - ActionAtlas A VideoQA Benchmark for Domain-specialized Action Recognition.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2410.05774","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V85SSK78","journalArticle","","Liu, Sizhe; Xia, Jun; Zhang, Lecheng; Liu, Yuchen; Liu, Yue; Du, Wenjie; Gao, Zhangyang; Hu, Bozhen; Tan, Cheng; Xiang, Hongxin; Li, Stan Z","FlexMol: A Flexible Toolkit for Benchmarking Molecular Relational Learning","","","","","","Molecular relational learning (MRL) is crucial for understanding the interaction behaviors between molecular pairs, a critical aspect of drug discovery and development. However, the large feasible model space of MRL poses significant challenges to benchmarking, and existing MRL frameworks face limitations in flexibility and scope. To address these challenges, avoid repetitive coding efforts, and ensure fair comparison of models, we introduce FlexMol, a comprehensive toolkit designed to facilitate the construction and evaluation of diverse model architectures across various datasets and performance metrics. FlexMol offers a robust suite of preset model components, including 16 drug encoders, 13 protein sequence encoders, 9 protein structure encoders, and 7 interaction layers. With its easy-to-use API and flexibility, FlexMol supports the dynamic construction of over 70, 000 distinct combinations of model architectures. Additionally, we provide detailed benchmark results and code examples to demonstrate FlexMol’s effectiveness in simplifying and standardizing MRL model development and comparison. FlexMol is open-sourced and available at https://github.com/Steven51516/FlexMol.","","2025-03-30 16:29:02","2025-03-30 16:29:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5QPVN2UR/Liu et al. - FlexMol A Flexible Toolkit for Benchmarking Molecular Relational Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4525YUJ6","journalArticle","","Ma, Zeyao; Zhang, Bohan; Zhang, Jing; Yu, Jifan; Zhang, Xiaokang; Zhang, Xiaohan; Luo, Sijia; Wang, Xi; Tang, Jie","SPREADSHEETBENCH: Towards Challenging Real World Spreadsheet Manipulation","","","","","","We introduce SPREADSHEETBENCH, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SPREADSHEETBENCH is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-ofthe-art (SOTA) models and human performance, highlighting the benchmark’s difficulty.","","2025-03-30 16:29:03","2025-03-30 16:29:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KP88FI9A/Ma et al. - SPREADSHEETBENCH Towards Challenging Real World Spreadsheet Manipulation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5CLKKVLP","journalArticle","","Li, Manling; Zhao, Shiyu; Wang, Qineng; Wang, Kangrui; Zhou, Yu; Srivastava, Sanjana; Gokmen, Cem; Lee, Tony; Li, Li Erran; Zhang, Ruohan; Liu, Weiyu; Liang, Percy; Fei-Fei, Li; Mao, Jiayuan; Wu, Jiajun","Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making","","","","","","We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (EMBODIED AGENT INTERFACE) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics that break down evaluation into error types, such as hallucination errors, affordance errors, and various types of planning errors. Overall, our benchmark offers a comprehensive assessment of LLMs’ performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights into the effective and selective use of LLMs in embodied decision making.","","2025-03-30 16:29:05","2025-03-30 16:29:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KHV2AG2X/Li et al. - Embodied Agent Interface Benchmarking LLMs for Embodied Decision Making.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3XMFNKZ5","journalArticle","","Wei, Boyi; Shi, Weijia; Huang, Yangsibo; Smith, Noah A; Zhang, Chiyuan; Zettlemoyer, Luke; Li, Kai; Henderson, Peter","Evaluating Copyright Takedown Methods for Language Models","","","","","","Language models (LMs) derive their capabilities from extensive training on diverse data, including potentially copyrighted material. These models can memorize and generate content similar to their training data, posing potential concerns. Therefore, model creators are motivated to develop mitigation methods that prevent generating protected content. We term this procedure as copyright takedowns for LMs, noting the conceptual similarity to (but legal distinction from) the Digital Millennium Copyright Act (DMCA) takedown This paper introduces the first evaluation of the feasibility and side effects of copyright takedowns for LMs. We propose COTAEVAL, an evaluation framework to assess the effectiveness of copyright takedown methods, the impact on the model’s ability to retain uncopyrightable factual knowledge from the training data whose recitation is embargoed, and how well the model maintains its general utility and efficiency. We examine several strategies, including adding system prompts, decoding-time filtering interventions, and unlearning approaches. Our findings indicate that no tested method excels across all metrics, showing significant room for research in this unique problem setting and indicating potential unresolved challenges for live policy proposals.","","2025-03-30 16:29:06","2025-03-30 16:29:06","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5EFEAYTZ/Wei et al. - Evaluating Copyright Takedown Methods for Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Z7FZZFZ","journalArticle","","Liu, Chu’nan; Denzler, Lilian; Chen, Yihong; Martin, Andrew; Paige, Brooks","AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction","","","","","","Epitope identification is vital for antibody design yet challenging due to the inherent variability in antibodies. While many deep learning methods have been developed for general protein binding site prediction tasks, whether they work for epitope prediction remains an understudied research question. The challenge is also heightened by the lack of a consistent evaluation pipeline with sufficient dataset size and epitope diversity. We introduce a filtered antibody-antigen complex structure dataset, AsEP (Antibody-specific Epitope Prediction). AsEP is the largest of its kind and provides clustered epitope groups, allowing the community to develop and test novel epitope prediction methods and evaluate their generalisability. AsEP comes with an easy-to-use interface in Python and pre-built graph representations of each antibody-antigen complex while also supporting customizable embedding methods. Using this new dataset, we benchmark several representative general protein-binding site prediction methods and find that their performances fall short of expectations for epitope prediction. To address this, we propose a novel method, WALLE, which leverages both unstructured modeling from protein language models and structural modeling from graph neural networks. WALLE demonstrate up to 3-10X performance improvement over the baseline methods. Our empirical findings suggest that epitope prediction benefits from combining sequential features provided by language models with geometrical information from graph representations. This provides a guideline for future epitope prediction method design. In addition, we reformulate the task as bipartite link prediction, allowing convenient model performance attribution and interpretability. We open source our data and code at https://github.com/biochunan/AsEP-dataset.","","2025-03-30 16:29:07","2025-03-30 16:29:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CCMID7A2/Liu et al. - AsEP Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XAJRSLAJ","journalArticle","","Zi, Chenyi; Zhao, Haihong; Sun, Xiangguo; Lin, Yiqing; Cheng, Hong; Li, Jia","ProG: A Graph Prompt Learning Benchmark","","","","","","Artificial general intelligence on graphs has shown significant advancements across various applications, yet the traditional ‘Pre-train & Fine-tune’ paradigm faces inefficiencies and negative transfer issues, particularly in complex and few-shot settings. Graph prompt learning emerges as a promising alternative, leveraging lightweight prompts to manipulate data and fill the task gap by reformulating downstream tasks to the pretext. However, several critical challenges still remain: how to unify diverse graph prompt models, how to evaluate the quality of graph prompts, and to improve their usability for practical comparisons and selection. In response to these challenges, we introduce the first comprehensive benchmark for graph prompt learning. Our benchmark integrates SIX pre-training methods and FIVE state-of-the-art graph prompt techniques, evaluated across FIFTEEN diverse datasets to assess performance, flexibility, and efficiency. We also present ‘ProG’, an easy-to-use open-source library that streamlines the execution of various graph prompt models, facilitating objective evaluations. Additionally, we propose a unified framework that categorizes existing graph prompt methods into two main approaches: prompts as graphs and prompts as tokens. This framework enhances the applicability and comparison of graph prompt techniques. The code is available at: https://github.com/sheldonresearch/ProG.","","2025-03-30 16:29:08","2025-03-30 16:29:08","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JUZ6IMP3/Zi et al. - ProG A Graph Prompt Learning Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9Q7RLKGM","journalArticle","","Chen, Shan; Gallifant, Jack; Gao, Mingye; Moreira, Pedro; Munch, Nikolaj; Muthukkumar, Ajay; Rajan, Arvind; Kolluri, Jaya; Fiske, Amelia; Hastings, Janna; Aerts, Hugo; Anthony, Brian; Celi, Leo Anthony","Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias","","","","","","Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data. In this study, we introduce Cross-Care, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups. We systematically evaluate how demographic biases embedded in pre-training corpora like T heP ile influence the outputs of LLMs. We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups. Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs. Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models’ representation of disease prevalence across different languages. For further exploration and analysis, we make all data and a data visualization tool available at: www.crosscare.net.","","2025-03-30 16:29:10","2025-03-30 16:29:10","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/3JE2QAVA/Chen et al. - Cross-Care Assessing the Healthcare Implications of Pre-training Data on Language Model Bias.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TAHI6QG5","journalArticle","","Zhou, Jingbo; Chen, Shaorong; Xia, Jun; Liu, Sizhe; Ling, Tianze; Du, Wenjie; Liu, Yue; Yin, Jianwei; Li, Stan Z","NovoBench: Benchmarking Deep Learning-based De Novo Peptide Sequencing Methods in Proteomics","","","","","","Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the high-throughput analysis of protein composition in biological tissues. Many deep learning methods have been developed for de novo peptide sequencing task, i.e., predicting the peptide sequence for the observed mass spectrum. However, two key challenges seriously hinder the further advancement of this important task. Firstly, since there is no consensus for the evaluation datasets, the empirical results in different research papers are often not comparable, leading to unfair comparison. Secondly, the current methods are usually limited to amino acid-level or peptide-level precision and recall metrics. In this work, we present the first unified benchmark NovoBench for de novo peptide sequencing, which comprises diverse mass spectrum data, integrated models, and comprehensive evaluation metrics. Recent impressive methods, including DeepNovo, PointNovo, Casanovo, InstaNovo, AdaNovo and π-HelixNovo are integrated into our framework. In addition to amino acid-level and peptide-level precision and recall, we evaluate the models’ performance in terms of identifying post-tranlational modifications (PTMs), efficiency and robustness to peptide length, noise peaks and missing fragment ratio, which are important influencing factors while seldom be considered. Leveraging this benchmark, we conduct a large-scale study of current methods, report many insightful findings that open up new possibilities for future development. The code is available at https://github.com/Westlake-OmicsAI/NovoBench.","","2025-03-30 16:29:11","2025-03-30 16:29:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4B897ZXH/Zhou et al. - NovoBench Benchmarking Deep Learning-based De Novo Peptide Sequencing Methods in Proteomics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D4I4B7SI","journalArticle","","Huang, Zhen; Wang, Zengzhi; Xia, Shijie; Li, Xuefeng; Zou, Haoyang; Xu, Ruijie; Fan, Run-Ze; Ye, Lyumanshan; Chern, Ethan; Ye, Yixin; Zhang, Yikai; Yang, Yuqing; Wu, Ting; Wang, Binjie; Sun, Shichao; Xiao, Yang; Li, Yiyuan; Zhou, Fan; Chern, Steffi; Qin, Yiwei; Ma, Yan; Su, Jiadi; Liu, Yixiu; Zheng, Yuxiang; Zhang, Shaoting; Lin, Dahua; Qiao, Yu; Liu, Pengfei","OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI","","","","","","","","2025-03-30 16:29:12","2025-03-30 16:29:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/95E2X39V/Huang et al. - OlympicArena Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5FG2HLT5","journalArticle","","Wang, Yubo; Ma, Xueguang; Zhang, Ge; Ni, Yuansheng; Chandra, Abhranil; Guo, Shiguang; Ren, Weiming; Arulraj, Aaran; He, Xuan; Jiang, Ziyan; Li, Tianle; Ku, Max; Wang, Kai; Zhuang, Alex; Fan, Rongqi; Yue, Xiang; Chen, Wenhu","MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark","","","","","","In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.","","2025-03-30 16:29:14","2025-03-30 16:29:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DKXMITWX/Wang et al. - MMLU-Pro A More Robust and Challenging Multi-Task Language Understanding Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LRSCBFI6","journalArticle","","Myung, Junho; Lee, Nayeon; Zhou, Yi; Jin, Jiho; Putri, Rifki Afina; Antypas, Dimosthenis; Borkakoty, Hsuvas; Kim, Eunsu; Perez-Almendros, Carla; Ayele, Abinew Ali; Gutiérrez-Basulto, Víctor; Ibáñez-García, Yazmín; Lee, Hwaran; Muhammad, Shamsuddeen Hassan; Park, Kiwoong; Rzayev, Anar Sabuhi; White, Nina; Yimam, Seid Muhie; Pilehvar, Mohammad Taher; Ousidhoum, Nedjma; Camacho-Collados, Jose; Oh, Alice","BLEND: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages","","","","","","Large language models (LLMs) often lack culture-specific knowledge of daily life, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs’ cultural sensitivities are limited to a single language or collected from online sources such as Wikipedia, which do not reflect the mundane everyday lifestyles of diverse regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is common cultural knowledge but uncommon in easily collected online sources, especially for underrepresented cultures. To address this issue, we introduce BLEND, a hand-crafted benchmark designed to evaluate LLMs’ everyday knowledge across diverse cultures and languages. BLEND comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We construct the benchmark to include two formats of questions: short-answer and multiple-choice. We show that LLMs perform better for cultures that are highly represented online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format. For cultures represented by mid-to-high-resource languages, LLMs perform better in their local languages, but for cultures represented by low-resource languages, LLMs perform better in English than the local languages. We make our dataset publicly available at: https://github.com/nlee0212/BLEnD.","","2025-03-30 16:29:15","2025-03-30 16:29:15","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PMH79RHW/Myung et al. - BLEND A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YTDXACA2","journalArticle","","Kwon, Yeonsu; Kim, Jiho; Lee, Gyubok; Bae, Seongsu; Kyung, Daeun; Cha, Wonchul; Pollard, Tom; Johnson, Alistair; Choi, Edward","EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records","","","","","","Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (e.g., medications) with detailed clinical notes (e.g., physician notes). These elements are essential for straightforward data retrieval and provide deep, contextual insights into patient care. However, they often suffer from discrepancies due to unintuitive EHR system designs and human errors, posing serious risks to patient safety. To address this, we developed EHRCon, a new dataset and task specifically designed to ensure data consistency between structured tables and unstructured notes in EHRs. EHRCon was crafted in collaboration with healthcare professionals using the MIMIC-III EHR dataset, and includes manual annotations of 4,101 entities across 105 clinical notes checked against database entries for consistency. EHRCon has two versions, one using the original MIMICIII schema, and another using the OMOP CDM schema, in order to increase its applicability and generalizability. Furthermore, leveraging the capabilities of large language models, we introduce CheckEHR, a novel framework for verifying the consistency between clinical notes and database tables. CheckEHR utilizes an eight-stage process and shows promising results in both few-shot and zero-shot settings. The code is available at https://github.com/dustn1259/EHRCon.","","2025-03-30 16:29:16","2025-03-30 16:29:16","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SEZB946K/Kwon et al. - EHRCon Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electro.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VKRVFWUE","journalArticle","","Hu, Jiamian; Hong, Yuanyuan; Chen, Yihua; Wang, He; Yasuhara, Moriaki","Noisy Ostracods: A Fine-Grained, Imbalanced Real-World Dataset for Benchmarking Robust Machine Learning and Label Correction Methods","","","","","","We present the Noisy Ostracods, a noisy dataset for genus and species classification of crustacean ostracods with specialists’ annotations. Over the 71466 specimens collected, 5.58% of them are estimated to be noisy (possibly problematic) at genus level. The dataset is created to addressing a real-world challenge: creating a clean fine-grained taxonomy dataset. The Noisy Ostracods dataset has diverse noises from multiple sources. Firstly, the noise is open-set, including new classes discovered during curation that were not part of the original annotation. The dataset has pseudo-classes, where annotators misclassified samples that should belong to an existing class into a new pseudo-class. The Noisy Ostracods dataset is highly imbalanced with a imbalance factor ρ = 22429. This presents a unique challenge for robust machine learning methods, as existing approaches have not been extensively evaluated on fine-grained classification tasks with such diverse real-world noise. Initial experiments using current robust learning techniques have not yielded significant performance improvements on the Noisy Ostracods dataset compared to cross-entropy training on the raw, noisy data. On the other hand, noise detection methods have underperformed in error hit rate compared to naive cross-validation ensembling for identifying problematic labels. These findings suggest that the fine-grained, imbalanced nature, and complex noise characteristics of the dataset present considerable challenges for existing noiserobust algorithms. By openly releasing the Noisy Ostracods dataset, our goal is to encourage further research into the development of noise-resilient machine learning methods capable of effectively handling diverse, real-world noise in finegrained classification tasks. The dataset, along with its evaluation protocols, can be accessed at https://github.com/H-Jamieu/Noisy_ostracods.","","2025-03-30 16:29:18","2025-03-30 16:29:18","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SQXZB92Y/Hu et al. - Noisy Ostracods A Fine-Grained, Imbalanced Real-World Dataset for Benchmarking Robust Machine Learn.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"53RT2LG2","journalArticle","","Udandarao, Vishaal; Roth, Karsten; Dziadzio, Sebastian; Prabhu, Ameya; Cherti, Mehdi; Vinyals, Oriol; Hénaff, Olivier; Albanie, Samuel; Akata, Zeynep; Bethge, Matthias","A Practitioner’s Guide to Continual Multimodal Pretraining","","","","","","Multimodal foundation models serve numerous applications at the intersection of vision and language. Still, despite being pretrained on extensive data, they become outdated over time. To keep models updated, research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate updates on large-scale new data, or (2) frequent, sample-level updates. However, practical model deployment often operates in the gap between these two limit cases, as real-world applications demand adaptation to specific subdomains, tasks or concepts — spread over the entire, varying life cycle of a model. In this work, we complement current perspectives on continual pretraining through a research test bed and offer comprehensive guidance for effective continual model updates in such scenarios. We first introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with realistic compute constraints and practical deployment requirements, constructed over 63 datasets with diverse visual and semantic coverage. Using FoMo-in-Flux, we explore the complex landscape of practical continual pretraining through multiple perspectives: (1) data mixtures and stream orderings that emulate real-world deployment settings, (2) methods ranging from simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model merging, (3) meta-learning-rate schedules and mechanistic design choices, and (4) model and compute scaling. Together, our insights provide a practitioner’s guide to continual multimodal pretraining for real-world deployment. Benchmark and code is provided here: github.com/ExplainableML/fomo_in_flux.","","2025-03-30 16:29:19","2025-03-30 16:29:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SFBPKH8G/Udandarao et al. - A Practitioner’s Guide to Continual Multimodal Pretraining.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F742HUXT","journalArticle","","Rutherford, Alexander; Ellis, Benjamin; Gallici, Matteo; Cook, Jonathan; Lupu, Andrei; Ingvarsson, Garðar; Willi, Timon; Hammond, Ravi; Khan, Akbir; de Witt, Christian Schroeder; Souly, Alexandra; Bandyopadhyay, Saptarashmi; Samvelyan, Mikayel; Jiang, Minqi; Lange, Robert; Whiteson, Shimon; Lacerda, Bruno; Hawes, Nick; Rocktäschel, Tim; Lu, Chris; Foerster, Jakob","JaxMARL: Multi-Agent RL Environments and Algorithms in JAX","","","","","","Benchmarks are crucial in the development of machine learning algorithms, with available environments significantly influencing reinforcement learning (RL) research. Traditionally, RL environments run on the CPU, which limits their scalability with typical academic compute. However, recent advancements in JAX have enabled the wider use of hardware acceleration, enabling massively parallel RL training pipelines and environments. While this has been successfully applied to single-agent RL, it has not yet been widely adopted for multi-agent scenarios. In this paper, we present JaxMARL, the first open-source, Python-based library that combines GPU-enabled efficiency with support for a large number of commonly used MARL environments and popular baseline algorithms. Our experiments show that, in terms of wall clock time, our JAX-based training pipeline is around 14 times faster than existing approaches, and up to 12500x when multiple training runs are vectorized. This enables efficient and thorough evaluations, potentially alleviating the evaluation crisis in the field. We also introduce and benchmark SMAX, a JAXbased approximate reimplementation of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. The code is available at https://github.com/flairox/jaxmarl.","","2025-03-30 16:29:20","2025-03-30 16:29:20","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XGWHS4XC/Rutherford et al. - JaxMARL Multi-Agent RL Environments and Algorithms in JAX.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C85Q459D","journalArticle","","Ma, Yubo; Zang, Yuhang; Chen, Liangyu; Chen, Meiqi; Jiao, Yizhu; Li, Xinze; Lu, Xinyuan; Liu, Ziyu; Ma, Yan; Dong, Xiaoyi; Zhang, Pan; Pan, Liangming; Jiang, Yu-Gang; Wang, Jiaqi; Cao, Yixin; Sun, Aixin","MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations","","","","","","Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLONGBENCH-DOC, a long-context, multimodal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e., page number). Moreover, 33.7% of the questions are cross-page questions requiring evidence across multiple pages. 20.6% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9%, while the second-best, GPT-4V, scores 30.5%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.","","2025-03-30 16:29:22","2025-03-30 16:29:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6TU99IRJ/Ma et al. - MMLONGBENCH-DOC Benchmarking Long-context Document Understanding with Visualizations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPP4Z3IV","journalArticle","","Boisvert, Léo; Thakkar, Megh; Gasse, Maxime; Caccia, Massimo","WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks","","","","","","The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress toward capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena.","","2025-03-30 16:29:23","2025-03-30 16:29:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BLPGLYP5/Boisvert et al. - WorkArena++ Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZFRK2BDI","journalArticle","","Hua, Chenqing; Zhong, Bozitao; Luan, Sitao; Hong, Liang; Wolf, Guy; Precup, Doina; Zheng, Shuangjia","ReactZyme: A Benchmark for Enzyme-Reaction Prediction","","","","","","Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation (https://github.com/WillHua127/ReactZyme).","","2025-03-30 16:29:25","2025-03-30 16:29:25","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XT2MCHYL/Hua et al. - ReactZyme A Benchmark for Enzyme-Reaction Prediction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3TLEI6DD","journalArticle","","Jignasu, Anushrut; Marshall, Kelly O; Mishra, Ankush Kumar; Rillo, Lucas Nerone; Ganapathysubramanian, Baskar; Balu, Aditya; Hegde, Chinmay; Krishnamurthy, Adarsh","Slice-100K: A Multimodal Dataset for Extrusion-based 3D Printing","","","","","","G-code (Geometric code) or RS-274 is the most widely used computer numerical control (CNC) and 3D printing programming language. G-code provides machine instructions for the movement of the 3D printer, especially for the nozzle, stage, and extrusion of material for extrusion-based additive manufacturing. Currently, there does not exist a large repository of curated CAD models along with their corresponding G-code files for additive manufacturing. To address this issue, we present Slice-100K, a first-of-its-kind dataset of over 100,000 G-code files, along with their tessellated CAD model, LVIS (Large Vocabulary Instance Segmentation) categories, geometric properties, and renderings. We build our dataset from triangulated meshes derived from Objaverse-XL and Thingi10K datasets. We demonstrate the utility of this dataset by finetuning GPT-2 on a subset of the dataset for G-code translation from a legacy G-code format (Sailfish) to a more modern, widely used format (Marlin). Our dataset can be found here. Slice-100K will be the first step in developing a multimodal foundation model for digital manufacturing.","","2025-03-30 16:29:26","2025-03-30 16:29:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7ENL49VQ/Jignasu et al. - Slice-100K A Multimodal Dataset for Extrusion-based 3D Printing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T49DG9QK","journalArticle","","Formanek, Claude; Tilbury, Callum Rhys; Beyers, Louise; Shock, Jonathan; Pretorius, Arnu","Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation","","","","","","Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications. Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work. In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work. Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve stateof-the-art (SOTA) results across a wide range of tasks. Specifically, we show that on 35 out of 47 datasets used in prior work (almost 75% of cases), we match or surpass the performance of the current purported SOTA. Strikingly, our baselines often substantially outperform these more sophisticated algorithms. Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work. Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward.","","2025-03-30 16:29:27","2025-03-30 16:29:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8J96UYG5/Formanek et al. - Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N8AHTGIX","journalArticle","","Martinez, Julieta; Kim, Emily; Romero, Javier; Bagautdinov, Timur; Saito, Shunsuke; Yu, Shoou-I; Anderson, Stuart; Zollhöfer, Michael; Wang, Te-Li; Bai, Shaojie; Li, Chenghui; Wei, Shih-En; Joshi, Rohan; Borsos, Wyatt; Simon, Tomas; Saragih, Jason; Theodosis, Paul; Greene, Alexander; Josyula, Anjani; Maeta, Silvio Mano; Jewett, Andrew I; Venshtain, Simon; Heilman, Christopher; Chen, Yueh-Tung; Fu, Sidi; Elshaer, Mohamed Ezzeldin A; Du, Tingfang; Wu, Longhua; Chen, Shen-Chi; Kang, Kai; Wu, Michael; Emad, Youssef; Longay, Steven; Brewer, Ashley; Shah, Hitesh; Booth, James; Koska, Taylor; Haidle, Kayla; Andromalos, Matt; Hsu, Joanna; Dauer, Thomas; Selednik, Peter; Godisart, Tim; Ardisson, Scott; Cipperly, Matthew; Humberston, Ben; Farr, Lon; Hansen, Bob; Guo, Peihong; Braun, Dave; Krenn, Steven; Wen, He; Evans, Lucas; Fadeeva, Natalia; Stewart, Matthew; Schwartz, Gabriel; Gupta, Divam; Moon, Gyeongsik; Guo, Kaiwen; Dong, Yuan; Xu, Yichen; Shiratori, Takaaki; Prada, Fabian; Pires, Bernardo R; Peng, Bo; Buffalini, Julia; Trimble, Autumn; McPhail, Kevyn; Schoeller, Melissa; Sheikh, Yaser","Codec Avatar Studio: Paired Human Captures for Complete, Driveable, and Generalizable Avatars","","","","","","To create photorealistic avatars that users can embody, human modeling must be complete (encompass the full body), driveable (able to reproduce motion of the user from lightweight sensors), and generalizable (i.e., easily adaptable to novel identities). Towards these goals, paired captures, that is, captures of the same subject obtained from systems of diverse quality and availability, are crucial. However, paired captures are rarely available to researchers outside of dedicated industrial labs: Codec Avatar Studio is our proposal to close this gap. Towards generalization and driveability, we introduce a dataset of 256 subjects captured in two modalities: high resolution multi-view scans of their heads, and video from the internal cameras of a headset. Towards completeness, we introduce a dataset of 4 subjects captured in eight modalities: high quality relightable multi-view captures of heads and hands, full body multi-view captures with minimal and regular clothes, and corresponding head, hands and body phone captures. Together with our data, we also provide code and pre-trained models for different state-of-the-art human generation models. Our datasets and code are available at https://github.com/facebookresearch/ava-256 and https://github.com/facebookresearch/goliath.","","2025-03-30 16:29:29","2025-03-30 16:29:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WUXGSWYL/Martinez et al. - Codec Avatar Studio Paired Human Captures for Complete, Driveable, and Generalizable Avatars.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZSHRJYL","journalArticle","","Belharbi, Soufiane; Whitford, Mara KM; Hoang, Phuong; Murtaza, Shakeeb; McCaffrey, Luke; Granger, Eric","SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image Super-Resolution","","","","","","Confocal fluorescence microscopy is one of the most accessible and widely used imaging techniques for the study of biological processes at the cellular and subcellular levels. Scanning confocal microscopy allows the capture of high-quality images from thick three-dimensional (3D) samples, yet suffers from well-known limitations such as photobleaching and phototoxicity of specimens caused by intense light exposure, which limits its use in some applications, especially for living cells. Cellular damage can be alleviated by changing imaging parameters to reduce light exposure, often at the expense of image quality. Machine/deep learning methods for single-image super-resolution (SISR) can be applied to restore image quality by upscaling lower-resolution (LR) images to produce high-resolution images (HR). These SISR methods have been successfully applied to photo-realistic images due partly to the abundance of publicly available datasets. In contrast, the lack of publicly available data partly limits their application and success in scanning confocal microscopy. In this paper, we introduce a large scanning confocal microscopy dataset named SR-CACO-2 that is comprised of low- and high-resolution image pairs marked for three different fluorescent markers. It allows to evaluate the performance of SISR methods on three different upscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured with four resolutions and three markers, that have been translated in the form of 9,937 patches for experiments with SISR methods. Given the new SR-CACO-2 dataset, we also provide benchmarking results for 16 state-of-the-art methods that are representative of the main SISR families. Results show that these methods have limited success in producing highresolution textures, indicating that SR-CACO-2 represents a challenging problem. The dataset is released under a Creative Commons license (CC BY-NC-SA 4.0), and it can be accessed freely. Our dataset, code and pretrained weights for SISR methods are publicly available: https://github.com/sbelharbi/sr-caco-2.","","2025-03-30 16:29:30","2025-03-30 16:29:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/F936H8BI/Belharbi et al. - SR-CACO-2 A Dataset for Confocal Fluorescence Microscopy Image Super-Resolution.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UIAZIF7T","journalArticle","","Boettcher, Wolfgang; Hoyer, Lukas; Unal, Ozan; Lenssen, Jan Eric; Schiele, Bernt","Scribbles for All: Benchmarking Scribble Supervised Segmentation Across Datasets","","","","","","In this work, we introduce Scribbles for All, a label and training data generation algorithm for semantic segmentation trained on scribble labels. Training or fine-tuning semantic segmentation models with weak supervision has become an important topic recently and was subject to significant advances in model quality. In this setting, scribbles are a promising label type to achieve high quality segmentation results while requiring a much lower annotation effort than usual pixel-wise dense semantic segmentation annotations. The main limitation of scribbles as source for weak supervision is the lack of challenging datasets for scribble segmentation, which hinders the development of novel methods and conclusive evaluations. To overcome this limitation, Scribbles for All provides scribble labels for several popular segmentation datasets and provides an algorithm to automatically generate scribble labels for any dataset with dense annotations, paving the way for new insights and model advancements in the field of weakly supervised segmentation. In addition to providing datasets and algorithm, we evaluate state-of-the-art segmentation models on our datasets and show that models trained with our synthetic labels perform competitively with respect to models trained on manual labels. Thus, our datasets enable state-of-the-art research into methods for scribble-labeled semantic segmentation. The datasets, scribble generation algorithm, and baselines are publicly available at https://github.com/wbkit/Scribbles4All.","","2025-03-30 16:29:31","2025-03-30 16:29:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Y29NSV2M/Boettcher et al. - Scribbles for All Benchmarking Scribble Supervised Segmentation Across Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHUW6FL2","journalArticle","","Angeloudi, Eirini; Audenaert, Jeroen; Bowles, Micah; Boyd, Benjamin M; Chemaly, David; Cherinka, Brian; Ciucă, Ioana; Cranmer, Miles; Do, Aaron; Grayling, Matthew; Hayes, Erin E; Hehir, Tom; Ho, Shirley; Huertas-Company, Marc; Iyer, Kartheik G; Jablonska, Maja; Lanusse, Francois; Leung, Henry W; Mandel, Kaisey; Martínez-Galarza, Juan Rafael; Melchior, Peter; Meyer, Lucas; Parker, Liam H; Qu, Helen; Shen, Jeff; Smith, Michael J; Stone, Connor; Walmsley, Mike; Wu, John F","The Multimodal Universe: Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientific Data","","","","","","","","2025-03-30 16:29:33","2025-03-30 16:29:33","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SKEYZRSE/Angeloudi et al. - The Multimodal Universe Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientifi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7YCGRPA8","journalArticle","","Alexos, Antonios; Liu, Junze; Galla, Shashank; Hayes, Sean; Bhardwaj, Kshitij; Schwartz, Alexander; Biener, Monika; Baldi, Pierre; Bukkapatnam, Satish; Bhandarkar, Suhas","Nuclear Fusion Diamond Polishing Dataset","","","","","","In the Inertial Confinement Fusion (ICF) process, roughly a 2mm spherical shell made of high-density carbon is used as a target for laser beams, which compress and heat it to energy levels needed for high fusion yield in nuclear fusion. These shells are polished meticulously to meet the standards for a fusion shot. However, the polishing of these shells involves multiple stages, with each stage taking several hours. To make sure that the polishing process is advancing in the right direction, we are able to measure the shell surface roughness. This measurement, however, is very labor-intensive, time-consuming, and requires a human operator. To help improve the polishing process we have released the first dataset to the public that consists of raw vibration signals with the corresponding polishing surface roughness changes. We show that this dataset can be used with a variety of neural network based methods for prediction of the change of polishing surface roughness, hence eliminating the need for the time-consuming manual process. This is the first dataset of its kind to be released in public and its use will allow the operator to make any necessary changes to the ICF polishing process for optimal results. This dataset contains the raw vibration data of multiple polishing runs with their extracted statistical features and the corresponding surface roughness values. Additionally, to generalize the prediction models to different polishing conditions, we also apply domain adaptation techniques to improve prediction accuracy for conditions unseen by the trained model. The dataset is available in https://junzeliu.github.io/Diamond-Polishing-Dataset/.","","2025-03-30 16:29:34","2025-03-30 16:29:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/VISAT654/Alexos et al. - Nuclear Fusion Diamond Polishing Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JZJFDV6C","dataset","2024","Wu, Yiwei; Li, Hanlin; Li, Hanlin","Annotations for ""A Systematic Review of NeurIPS Dataset Management Practices""","","","","10.18738/T8/HLTRQP","https://dataverse.tdl.org/citation?persistentId=doi:10.18738/T8/HLTRQP","As new machine learning methods demand larger training datasets, researchers and developers face significant challenges in dataset management. Although ethics reviews, documentation, and checklists have been established, it remains uncertain whether consistent dataset management practices exist across the community. This lack of a comprehensive overview hinders our ability to diagnose and address fundamental tensions and ethical issues related to managing large datasets. We present a systematic review of datasets published at the NeurIPS Datasets and Benchmarks track, focusing on four key aspects: provenance, distribution, ethical disclosure, and licensing. Our findings reveal that dataset provenance is often unclear due to ambiguous filtering and curation processes. Additionally, a variety of sites are used for dataset hosting, but only a few offer structured metadata and version control. These inconsistencies underscore the urgent need for standardized data infrastructures for the publication and management of datasets.","2024","2025-03-30 16:29:35","2025-03-30 16:29:35","2025-03-30 16:29:35","","","","","","","","","","","","Texas Data Repository","","en",", Creative Commons Zero v1.0 Universal","","","","DOI.org (Datacite)","","Artwork Size: 91869 Pages: 91869","","/Users/nikolajmosgaardsomod/Zotero/storage/IS2DDQ6X/Wu and Li - 2024 - Annotations for A Systematic Review of NeurIPS Dataset Management Practices.pdf","","","Computer and Information Science","","","","Li, Hanlin","","","","","","","","","","","","","","","","","","","text/tab-separated-values","","","","","","","","","","","","","","","1.1","","","","","","","",""
"SH7MGSUG","journalArticle","","Liu, Ziqiang; Fang, Feiteng; Feng, Xi; Du, Xinrun; Zhang, Chenhao; Wang, Zekun; Bai, Yuelin; Zhao, Qixuan; Fan, Liyang; Gan, Chengguang; Lin, Hongquan; Li, Jiaming; Ni, Yuansheng; Wu, Haihong; Narsupalli, Yaswanth; Zheng, Zhigang; Li, Chengming; Hu, Xiping; Xu, Ruifeng; Chen, Xiaojun; Yang, Min; Liu, Jiaheng; Liu, Ruibo; Huang, Wenhao; Zhang, Ge; Ni, Shiwen","II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models","","","","","","The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks. In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs. However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs. To fill this gap, we propose the Image Implication understanding Benchmark, II-Bench, which aims to evaluate the model’s higher-order perception of images. Through extensive experiments on II-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive 98%. Subsequently, MLLMs perform worse on abstract and complex images, suggesting limitations in their ability to understand high-level semantics and capture image details. Finally, it is observed that most models exhibit enhanced accuracy when image sentiment polarity hints are incorporated into the prompts. This observation underscores a notable deficiency in their inherent understanding of image sentiment. We believe that II-Bench will inspire the community to develop the next generation of MLLMs, advancing the journey towards expert artificial general intelligence (AGI). II-Bench is publicly available at https://huggingface.co/datasets/ m-a-p/II-Bench.","","2025-03-30 16:29:37","2025-03-30 16:29:37","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/2RTFJCHM/Liu et al. - II-Bench An Image Implication Understanding Benchmark for Multimodal Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XYFSBAP8","journalArticle","","Pardawala, Huzaifa; Sukhani, Siddhant; Shah, Agam; Kejriwal, Veer; Pillai, Abhishek; Bhasin, Rohan; DiBiasio, Andrew; Mandapati, Tarun; Adha, Dhruv; Chava, Sudheer","SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts’ QA Through Six-Dimensional Feature Analysis","","","","","","Fact-checking is extensively studied in the context of misinformation and disinformation, addressing objective inaccuracies. However, a softer form of misinformation involves responses that are factually correct but lack certain features such as clarity and relevance. This challenge is prevalent in formal Question-Answer (QA) settings such as press conferences in ﬁnance, politics, sports, and other domains, where subjective answers can obscure transparency. Despite this, there is a lack of manually annotated datasets for subjective features across multiple dimensions. To address this gap, we introduce SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts’ (ECTs) QA sessions as the answers given by company representatives are often open to subjective interpretations and scrutiny. The dataset includes 49, 446 annotations for long-form QA pairs across six features: Assertive, Cautious, Optimistic, Specific, Clear, and Relevant. These features are carefully selected to encompass the key attributes that reﬂect the tone of the answers provided during QA sessions across different domains. Our ﬁndings are that the best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity, such as Relevant and Clear, with a mean difference of 2.17% in their weighted F1 scores. The models perform signiﬁcantly better on features with higher subjectivity, such as Specific and Assertive, with a mean difference of 10.01% in their weighted F1 scores. Furthermore, testing SubjECTive-QA’s generalizability using QAs from White House Press Brieﬁngs and Gaggles yields an average weighted F1 score of 65.97% using our best models for each feature, demonstrating broader applicability beyond the ﬁnancial domain. SubjECTive-QA is publicly available under the CC BY 4.0 license1.","","2025-03-30 16:29:38","2025-03-30 16:29:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FFM6DMJA/Pardawala et al. - SubjECTive-QA Measuring Subjectivity in Earnings Call Transcripts’ QA Through Six-Dimensional Featu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTCM6W8P","journalArticle","","Jin, Ruinan; Xu, Zikang; Zhong, Yuan; Yao, Qingsong; Dou, Qi; Zhou, S Kevin; Li, Xiaoxiao","FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models","","","","","","The advent of foundation models (FMs) in healthcare offers unprecedented opportunities to enhance medical diagnostics through automated classification and segmentation tasks. However, these models also raise significant concerns about their fairness, especially when applied to diverse and underrepresented populations in healthcare applications. Currently, there is a lack of comprehensive benchmarks, standardized pipelines, and easily adaptable libraries to evaluate and understand the fairness performance of FMs in medical imaging, leading to considerable challenges in formulating and implementing solutions that ensure equitable outcomes across diverse patient populations. To fill this gap, we introduce FairMedFM, a fairness benchmark for FM research in medical imaging. FairMedFM integrates with 17 popular medical imaging datasets, encompassing different modalities, dimensionalities, and sensitive attributes. It explores 20 widely used FMs, with various usages such as zero-shot learning, linear probing, parameter-efficient fine-tuning, and prompting in various downstream tasks – classification and segmentation. Our exhaustive analysis evaluates the fairness performance over different evaluation metrics from multiple perspectives, revealing the existence of bias, varied utilityfairness trade-offs on different FMs, consistent disparities on the same datasets regardless FMs, and limited effectiveness of existing unfairness mitigation methods. Checkout FairMedFM’s project page and open-sourced codebase, which supports extendible functionalities and applications as well as inclusive for studies on FMs in medical imaging over the long term.","","2025-03-30 16:29:39","2025-03-30 16:29:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/NDGVESV8/Jin et al. - FairMedFM Fairness Benchmarking for Medical Imaging Foundation Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FUINYZU4","journalArticle","","Yang, Chih-Hsuan; Feuer, Ben; Jubery, Zaki; Deng, Zi K; Nakkab, Andre; Chiranjeevi, Shivani; Marshall, Kelly; Baishnab, Nirmal; Singh, Asheesh K; Sarkar, Soumik; Merchant, Nirav; Hegde, Chinmay","BioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity","","","","","","We introduce BioTrove, the largest publicly accessible dataset designed to advance AI applications in biodiversity. Curated from the iNaturalist platform and vetted to include only research-grade data, BioTrove contains 161.9 million images, offering unprecedented scale and diversity from three primary kingdoms: Animalia (""animals""), Fungi (""fungi""), and Plantae (""plants""), spanning approximately 366.6K species. Each image is annotated with scientific names, taxonomic hierarchies, and common names, providing rich metadata to support accurate AI model development across diverse species and ecosystems.","","2025-03-30 16:29:41","2025-03-30 16:29:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SUBB4DMZ/Yang et al. - BioTrove A Large Curated Image Dataset Enabling AI for Biodiversity.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CF4CLFJ4","journalArticle","","Wang, Siyan; Levy, Bradford","BeanCounter: A low-toxicity, large-scale, and open dataset of business-oriented text","","","","","","Many of the recent breakthroughs in language modeling have resulted from scaling effectively the same model architecture to larger datasets. In this vein, recent work has highlighted performance gains from increasing training dataset size and quality, suggesting a need for novel sources of large-scale datasets. In this work, we introduce BeanCounter, a public dataset consisting of more than 159B tokens extracted from businesses’ disclosures. We show that this data is indeed novel: less than 0.1% of BeanCounter appears in Common Crawl-based datasets and the data is an order of magnitude larger than datasets relying on similar sources. Given the data’s provenance, we hypothesize that BeanCounter is comparatively more factual and less toxic than web-based datasets. Exploring this hypothesis, we find that many demographic identities occur with similar prevalence in BeanCounter but with significantly less toxic context relative to other datasets. To demonstrate the utility of BeanCounter, we evaluate and compare two LLMs continually pre-trained on BeanCounter with their base models. We find an 18-33% reduction in toxic generation and improved performance within the finance domain for the continually pretrained models. Collectively, our work suggests that BeanCounter is a novel source of low-toxicity and high-quality domain-specific data with sufficient scale to train multi-billion parameter LLMs.","","2025-03-30 16:29:42","2025-03-30 16:29:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/E55TREJV/Wang and Levy - BeanCounter A low-toxicity, large-scale, and open dataset of business-oriented text.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LTLRWX7D","journalArticle","","Askari, Farzaneh; Lyu, Lingjuan; Sharma, Vivek","DECO-Bench: Unified Benchmark for Decoupled Task-Agnostic Synthetic Data Release","","","","","","In this work, we tackle the question of how to systematically benchmark taskagnostic decoupling methods for privacy-preserving machine learning (ML). Sharing datasets that include sensitive information often triggers privacy concerns, necessitating robust decoupling methods to separate sensitive and non-sensitive attributes. Despite the development of numerous decoupling techniques, a standard benchmark for systematically comparing these methods remains absent. Our framework integrates various decoupling techniques along with synthetic data generation and evaluation protocols within a unified system. Using our framework, we benchmark various decoupling techniques and evaluate their privacy-utility trade-offs. Finally, we release our source code, pre-trained models, datasets of decoupled representations to foster research in this area.","","2025-03-30 16:29:44","2025-03-30 16:29:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/X2J7WPZM/Askari et al. - DECO-Bench Unified Benchmark for Decoupled Task-Agnostic Synthetic Data Release.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YLLRQQ8","journalArticle","","Wang, Jiaqi; Wang, Xiaochen; Lyu, Lingjuan; Chen, Jinghui; Ma, Fenglong","FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection","","","","","","This study introduces the Federated Medical Knowledge Injection (FEDMEKI) platform, a new benchmark designed to address the unique challenges of integrating medical knowledge into foundation models under privacy constraints. By leveraging a cross-silo federated learning approach, FEDMEKI circumvents the issues associated with centralized data collection, which is often prohibited under health regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the USA. The platform is meticulously designed to handle multi-site, multi-modal, and multi-task medical data, which includes 7 medical modalities, including images, signals, texts, laboratory test results, vital signs, input variables, and output variables. The curated dataset to validate FEDMEKI covers 8 medical tasks, including 6 classification tasks (lung opacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal detection, mortality prediction, sepsis prediction, and enlarged cardiomediastinum detection) and 2 generation tasks (medical visual question answering (MedVQA) and ECG noise clarification). This comprehensive dataset is partitioned across several clients to facilitate the decentralized training process under 16 benchmark approaches. FEDMEKI not only preserves data privacy but also enhances the capability of medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure, thereby setting a new benchmark in the application of foundation models within the healthcare sector.","","2025-03-30 16:29:45","2025-03-30 16:29:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WP2N4I2L/Wang et al. - FEDMEKI A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7G6TMXKP","journalArticle","","Deng, Junwei; Li, Ting-Wei; Zhang, Shiyuan; Liu, Shixuan; Pan, Yijun; Huang, Hao; Wang, Xinhe; Hu, Pingbang; Zhang, Xingjian; Ma, Jiaqi W","dattri: A Library for Efficient Data Attribution","","","","","","Data attribution methods aim to quantify the influence of individual training samples on the prediction of artificial intelligence (AI) models. As training data plays an increasingly crucial role in the modern development of large-scale AI models, data attribution has found broad applications in improving AI performance and safety. However, despite a surge of new data attribution methods being developed recently, there lacks a comprehensive library that facilitates the development, benchmarking, and deployment of different data attribution methods. In this work, we introduce dattri, an open-source data attribution library that addresses the above needs. Specifically, dattri highlights three novel design features. Firstly, dattri proposes a unified and easy-to-use API, allowing users to integrate different data attribution methods into their PyTorch-based machine learning pipeline with a few lines of code changed. Secondly, dattri modularizes low-level utility functions that are commonly used in data attribution methods, such as Hessianvector product, inverse-Hessian-vector product or random projection, making it easier for researchers to develop new data attribution methods. Thirdly, dattri provides a comprehensive benchmark framework with pre-trained models and ground truth annotations for a variety of benchmark settings, including generative AI settings. We have implemented a variety of state-of-the-art efficient data attribution methods that can be applied to large-scale neural network models, and will continuously update the library in the future. Using the developed dattri library, we are able to perform a comprehensive and fair benchmark analysis across a wide range of data attribution methods. The source code of dattri is available at https://github.com/TRAIS-Lab/dattri.","","2025-03-30 16:29:46","2025-03-30 16:29:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PGCVGULP/Deng et al. - dattri A Library for Efficient Data Attribution.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AFN7G6D","journalArticle","","Wu, Cheng-Kuang; Tam, Zhi Rui; Lin, Chieh-Yen; Chen, Yun-Nung; Lee, Hung-yi","StreamBench: Towards Benchmarking Continuous Improvement of Language Agents","","","","","","Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios. Source code: https://github.com/stream-bench/stream-bench. Benchmark website: https://stream-bench.github.io.","","2025-03-30 16:29:48","2025-03-30 16:29:48","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DI62LSVZ/Wu et al. - StreamBench Towards Benchmarking Continuous Improvement of Language Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86RIMHYB","journalArticle","","Jung, Sangwon; Yu, Sumin; Chun, Sanghyuk; Moon, Taesup","Do Counterfactually Fair Image Classiﬁers Satisfy Group Fairness? – A Theoretical and Empirical Study","","","","","","The notion of algorithmic fairness has been actively explored from various aspects of fairness, such as counterfactual fairness (CF) and group fairness (GF). However, the exact relationship between CF and GF remains to be unclear, especially in image classiﬁcation tasks; the reason is because we often cannot collect counterfactual samples regarding a sensitive attribute, essential for evaluating CF, from the existing images (e.g., a photo of the same person but with different secondary sex characteristics). In this paper, we construct new image datasets for evaluating CF by using a high-quality image editing method and carefully labeling with human annotators. Our datasets, CelebA-CF and LFW-CF, build upon the popular image GF benchmarks; hence, we can evaluate CF and GF simultaneously. We empirically observe that CF does not imply GF in image classiﬁcation, whereas previous studies on tabular datasets observed the opposite. We theoretically show that it could be due to the existence of a latent attribute G that is correlated with, but not caused by, the sensitive attribute (e.g., secondary sex characteristics are highly correlated with hair length). From this observation, we propose a simple baseline, Counterfactual Knowledge Distillation (CKD), to mitigate such correlation with the sensitive attributes. Extensive experimental results on CelebA-CF and LFW-CF demonstrate that CF-achieving models satisfy GF if we successfully reduce the reliance on G (e.g., using CKD).","","2025-03-30 16:29:49","2025-03-30 16:29:49","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KB9Z2MB4/Jung et al. - Do Counterfactually Fair Image Classiﬁers Satisfy Group Fairness – A Theoretical and Empirical Stud.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJYTYDAR","journalArticle","","Ru, Dongyu; Qiu, Lin; Hu, Xiangkun; Zhang, Tianhang; Shi, Peng; Chang, Shuaichen; Jiayang, Cheng; Wang, Cunxiang; Sun, Shichao; Li, Huanyu; Zhang, Zizhao; Wang, Binjie; Jiang, Jiarong; He, Tong; Wang, Zhiguo; Liu, Pengfei; Zhang, Yue; Zhang, Zheng","RAGCHECKER: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation","","","","","","Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGCHECKER, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGCHECKER has significantly better correlations with human judgments than other evaluation metrics. Using RAGCHECKER, we evaluate 8 RAG systems and conduct an indepth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGCHECKER can guide researchers and practitioners in developing more effective RAG systems3.","","2025-03-30 16:29:50","2025-03-30 16:29:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/36MT7CB8/Ru et al. - RAGCHECKER A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVQ39DP8","journalArticle","","Ye, Fanghua; Yang, Mingming; Pang, Jianhui; Wang, Longyue; Wong, Derek F; Yilmaz, Emine; Shi, Shuming; Tu, Zhaopeng","Benchmarking LLMs via Uncertainty Quantification","","","","","","The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect – uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty into the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.","","2025-03-30 16:29:52","2025-03-30 16:29:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RSNNA25W/Ye et al. - Benchmarking LLMs via Uncertainty Quantification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YA46QZFK","journalArticle","","Longpre, Shayne; Mahari, Robert; Lee, Ariel; Lund, Campbell; Oderinwale, Hamidah; Brannon, William; Saxena, Nayan; Obeng-Marnu, Naana; South, Tobin; Hunter, Cole; Klamm, Christopher; Schoelkopf, Hailey; Singh, Nikhil; Cherep, Manuel; Anis, Mustafa; Dinh, An; Chitongo, Caroline; Yin, Da; Sileo, Damien; Mataciunas, Deividas; Misra, Diganta; Alghamdi, Emad; Shippole, Enrico; Zhang, Jianguo; Materzynska, Joanna; Qian, Kun; Tiwary, Kush; Miranda, Lester; Dey, Manan; Liang, Minnie; Muennighoff, Niklas; Ye, Seonghyeon; Kim, Seungone; Mohanty, Shrestha; Sharma, Vivek; Chien, Vu Minh; Zhou, Xuhui; Li, Yizhi; Xiong, Caiming; Villa, Luis; Biderman, Stella; Li, Hanlin; Ippolito, Daphne; Hooker, Sara; Kabbara, Jad","Consent in Crisis: The Rapid Decline of the AI Data Commons","","","","","","General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To our knowledge, we conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains underlying AI training corpora. Our audit of 14, 000 web domains provides an expansive view of crawlable web data and how codified data use preferences are changing over time. We observe a proliferation of AIspecific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites’ expressed intentions in their Terms of Service and their robots.txt. We diagnose these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. Our longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. We hope to illustrate the emerging crises in data consent, for both developers and creators. The foreclosure of much of the open web will impact not only commercial AI, but also non-commercial AI and academic research.","","2025-03-30 16:29:54","2025-03-30 16:29:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6DCUC3Z9/Longpre et al. - Consent in Crisis The Rapid Decline of the AI Data Commons.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXUISHL6","journalArticle","","Peng, Liang; Gao, Junyuan; Liu, Xinran; Li, Weihong; Dong, Shaohua; Zhang, Zhipeng; Fan, Heng; Zhang, Libo","VastTrack: Vast Category Visual Object Tracking","","","","","","In this paper, we propose a novel benchmark, named VastTrack, aiming to facilitate the development of general visual tracking via encompassing abundant classes and videos. VastTrack consists of a few attractive properties: (1) Vast Object Category. In particular, it covers targets from 2,115 categories, significantly surpassing object classes of existing popular benchmarks (e.g., GOT-10k with 563 classes and LaSOT with 70 categories). Through providing such vast object classes, we expect to learn more general object tracking. (2) Larger scale. Compared with current benchmarks, VastTrack provides 50,610 videos with 4.2 million frames, which makes it to date the largest dataset in term of the number of videos, and hence could benefit training even more powerful visual trackers in the deep learning era. (3) Rich Annotation. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions with more than 50K sentences for the videos. Such rich annotations of VastTrack enable the development of both vision-only and vision-language tracking. In order to ensure precise annotation, each frame in the videos is manually labeled with multi-stage of careful inspections and refinements. To understand performance of existing trackers and to provide baselines for future comparison, we extensively evaluate 25 representative trackers. The results, not surprisingly, display significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are urgently required to improve general visual tracking. Our VastTrack, the toolkit, and evaluation results are publicly available at https://github.com/HengLan/VastTrack.","","2025-03-30 16:29:55","2025-03-30 16:29:55","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5WB3ILVM/Peng et al. - VastTrack Vast Category Visual Object Tracking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKR9G3D9","journalArticle","","Monroc, Claire Bizon; Bušic, Ana; Dubuc, Donatien; Zhu, Jiamin","WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control","","","","","","The wind farm control problem is challenging, since conventional model-based control strategies require tractable models of complex aerodynamical interactions between the turbines and suffer from the curse of dimension when the number of turbines increases. Recently, model-free and multi-agent reinforcement learning approaches have been used to address this challenge. In this article, we introduce WFCRL (Wind Farm Control with Reinforcement Learning), the first open suite of multi-agent reinforcement learning environments for the wind farm control problem. WFCRL frames a cooperative Multi-Agent Reinforcement Learning (MARL) problem: each turbine is an agent and can learn to adjust its yaw, pitch or torque to maximize the common objective (e.g. the total power production of the farm). WFCRL also offers turbine load observations that will allow to optimize the farm performance while limiting turbine structural damages. Interfaces with two state-of-the-art farm simulators are implemented in WFCRL: a static simulator (FLORIS) and a dynamic simulator (FAST.Farm). For each simulator, 10 wind layouts are provided, including 5 real wind farms. Two state-of-the-art online MARL algorithms are implemented to illustrate the scaling challenges. As learning online on FAST.Farm is highly time-consuming, WFCRL offers the possibility of designing transfer learning strategies from FLORIS to FAST.Farm.","","2025-03-30 16:29:56","2025-03-30 16:29:56","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XF62GDAL/Monroc et al. - WFCRL A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E482C4FD","journalArticle","","Liu, Jiaheng; Ni, Zehao; Que, Haoran; Sun, Tao; Wang, Zekun; Yang, Jian; Wang, Jiakai; Guo, Hongcheng; Peng, Zhongyuan; Zhang, Ge; Tian, Jiayi; Bu, Xingyuan; Xu, Ke; Rong, Wenge; Peng, Junran; Zhang, Zhaoxiang","RoleAgent: Building, Interacting, and Benchmarking High-quality Role-Playing Agents from Scripts","","","","","","Believable agents can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication. Recently, generative agents have been proposed to simulate believable human behavior by using Large Language Models. However, the existing method heavily relies on humanannotated agent profiles (e.g., name, age, personality, relationships with others, and so on) for the initialization of each agent, which cannot be scaled up easily. In this paper, we propose a scalable RoleAgent framework to generate high-quality role-playing agents from raw scripts, which includes building and interacting stages. Specifically, in the building stage, we use a hierarchical memory system to extract and summarize the structure and high-level information of each agent for the raw script. In the interacting stage, we propose a novel innovative mechanism with four steps to achieve a high-quality interaction between agents. Finally, we introduce a systematic and comprehensive evaluation benchmark called RoleAgentBench to evaluate the effectiveness of our RoleAgent, which includes 100 and 28 roles for 20 English and 5 Chinese scripts, respectively. Extensive experimental results on RoleAgentBench demonstrate the effectiveness of RoleAgent.","","2025-03-30 16:29:58","2025-03-30 16:29:58","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JTMRWNYD/Liu et al. - RoleAgent Building, Interacting, and Benchmarking High-quality Role-Playing Agents from Scripts.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5EYIE9MV","journalArticle","","Dauner, Daniel; Hallgarten, Marcel; Li, Tianyu; Weng, Xinshuo; Huang, Zhiyu; Yang, Zetong; Li, Hongyang; Gilitschenski, Igor; Ivanovic, Boris; Pavone, Marco; Geiger, Andreas; Chitta, Kashyap","NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking","","","","","","Benchmarking vision-based driving policies is challenging. On one hand, openloop evaluation with real data is easy, but these results do not reflect closedloop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird’s eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.","","2025-03-30 16:29:59","2025-03-30 16:29:59","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5WU26JQ4/Dauner et al. - NAVSIM Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YSZKJ6UB","journalArticle","","Wu, Haoning; Li, Dongxu; Chen, Bei; Li, Junnan","LONGVIDEOBENCH: A Benchmark for Long-context Interleaved Video-Language Understanding","","","","","","Large multimodal models (LMMs) are processing increasingly longer and richer inputs. Albeit the progress, few public benchmark is available to measure such development. To mitigate this gap, we introduce LONGVIDEOBENCH, a questionanswering benchmark that features video-language interleaved inputs up to an hour long. Our benchmark includes 3,763 varying-length web-collected videos with their subtitles across diverse themes, designed to comprehensively evaluate LMMs on long-term multimodal understanding. To achieve this, we interpret the primary challenge as to accurately retrieve and reason over detailed multimodal information from long inputs. As such, we formulate a novel video question-answering task termed referring reasoning. Specifically, as part of the question, it contains a referring query that references related video contexts, called referred context. The model is then required to reason over relevant video details from the referred context. Following the paradigm of referring reasoning, we curate 6,678 human-annotated multiple-choice questions in 17 fine-grained categories, establishing one of the most comprehensive benchmarks for long-form video understanding. Evaluations suggest that the LONGVIDEOBENCH presents significant challenges even for the most advanced proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their open-source counterparts show an even larger performance gap. In addition, our results indicate that model performance on the benchmark improves only when they are capable of processing more frames, positioning LONGVIDEOBENCH as a valuable benchmark for evaluating future-generation long-context LMMs.","","2025-03-30 16:30:01","2025-03-30 16:30:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/B66ZCWUP/Wu et al. - LONGVIDEOBENCH A Benchmark for Long-context Interleaved Video-Language Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NC88TY6A","journalArticle","","Zhang, Yuwei; Xia, Tong; Han, Jing; Wu, Yu Yvonne; Rizos, Georgios; Liu, Yang; Mosuily, Mohammed; Chauhan, Jagmohan; Mascolo, Cecilia","Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking","","","","","","Respiratory audio, such as coughing and breathing sounds, has predictive power for a wide range of healthcare applications, yet is currently under-explored. The main problem for those applications arises from the difficulty in collecting large labeled task-specific data for model development. Generalizable respiratory acoustic foundation models pretrained with unlabeled data would offer appealing advantages and possibly unlock this impasse. However, given the safety-critical nature of healthcare applications, it is pivotal to also ensure openness and replicability for any proposed foundation model solution. To this end, we introduce OPERA, an OPEn Respiratory Acoustic foundation model pretraining and benchmarking system, as the first approach answering this need. We curate large-scale respiratory audio datasets (∼136K samples, over 400 hours), pretrain three pioneering generalizable acoustic models, and build a benchmark consisting of 19 downstream respiratory health tasks for evaluation. Our pretrained models demonstrate superior performance (against existing acoustic models pretrained with general audio on 16 out of 19 tasks) and generalizability (to unseen datasets and new respiratory audio modalities). This highlights the great promise of respiratory acoustic foundation models and encourages more studies using OPERA as an open resource to accelerate research on respiratory audio for health.","","2025-03-30 16:30:02","2025-03-30 16:30:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HCJSB4BX/Zhang et al. - Towards Open Respiratory Acoustic Foundation Models Pretraining and Benchmarking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K26PU7Y9","journalArticle","","Liu, Shuo; Ying, Kaining; Zhang, Hao; Yang, Yue; Lin, Yuqi; Zhang, Tianle; Li, Chuanhao; Qiao, Yu; Luo, Ping; Shao, Wenqi; Zhang, Kaipeng","ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models","","","","","","Multi-turn visual conversation is an important ability of real-world AI assistants. However, the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities ablation evaluation for Large Vision-Language Models (LVLMs). ConvBench comprises 577 curated multi-turn conversations, encompassing 215 tasks. These tasks are broad and open-ended, which resemble real-world user behaviors. ConvBench progressively examines the LVLMs’ perception, reasoning, and creativity capabilities in each conversation and can decouple these capabilities in evaluations and thus perform reliable error attribution. Besides, considering the diversity of open-ended questions, we introduce an efficient and reliable automatic evaluation framework. Experimental results reveal that ConvBench is a significant challenge for current LVLMs, even for GPT4V, which achieves only a 39.51% score. Besides, we have some insightful findings, such as the weak perception of LVLMs inhibits authentic strengths in reasoning and creation. We believe our design of hierarchical capabilities, decoupling capabilities evaluation, and multi-turn conversation can blaze a new trail in LVLMs evaluation. Code and benchmark are released at https://github.com/shirlyliu64/ConvBench.","","2025-03-30 16:30:03","2025-03-30 16:30:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KGSYPQ63/Liu et al. - ConvBench A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4CGUV6D","preprint","2024","Ren, Yuchen; Chen, Zhiyuan; Qiao, Lifeng; Jing, Hongtai; Cai, Yuchen; Xu, Sheng; Ye, Peng; Ma, Xinzhu; Sun, Siqi; Yan, Hongliang; Yuan, Dong; Ouyang, Wanli; Liu, Xihui","BEACON: Benchmark for Comprehensive RNA Tasks and Language Models","","","","10.1101/2024.06.22.600190","http://biorxiv.org/lookup/doi/10.1101/2024.06.22.600190","RNA plays a pivotal role in translating genetic instructions into functional outcomes, underscoring its importance in biological processes and disease mechanisms. Despite the emergence of numerous deep learning approaches for RNA, particularly universal RNA language models, there remains a significant lack of standardized benchmarks to assess the effectiveness of these methods. In this study, we introduce the first comprehensive RNA benchmark BEACON (BEnchmArk for COmprehensive RNA Task and Language Models). First, BEACON comprises 13 distinct tasks derived from extensive previous work covering structural analysis, functional studies, and engineering applications, enabling a comprehensive assessment of the performance of methods on various RNA understanding tasks. Second, we examine a range of models, including traditional approaches like CNNs, as well as advanced RNA foundation models based on language models, offering valuable insights into the task-specific performances of these models. Third, we investigate the vital RNA language model components from the tokenizer and positional encoding aspects. Notably, our findings emphasize the superiority of single nucleotide tokenization and the effectiveness of Attention with Linear Biases (ALiBi) over traditional positional encoding methods. Based on these insights, a simple yet strong baseline called BEACON-B is proposed, which can achieve outstanding performance with limited data and computational resources. The datasets and source code of our benchmark are available at https://github.com/terry-r123/RNABenchmark.","2024-06-28","2025-03-30 16:30:05","2025-03-30 16:30:06","2025-03-30 16:30:05","","","","","","","BEACON","","","","","","","en","","","","","Bioinformatics","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ITZ4TWHK/Ren et al. - 2024 - BEACON Benchmark for Comprehensive RNA Tasks and Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KRLWVTTX","journalArticle","","Zhou, Yang; Faith, Tan Li Hui; Xu, Yanyu; Leng, Sicong; Xu, Xinxing; Liu, Yong; Goh, Rick Siow Mong","BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays","","","","","","Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.","","2025-03-30 16:30:07","2025-03-30 16:30:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/I6U6PK8C/Zhou et al. - BenchX A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKKSDCE4","journalArticle","","Chen, Zhikai; Mao, Haitao; Liu, Jingzhe; Song, Yu; Li, Bingheng; Jin, Wei; Fatemi, Bahare; Tsitsulin, Anton; Perozzi, Bryan; Liu, Hui; Tang, Jiliang","Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights","","","","","","Given the ubiquity of graph data and its applications in diverse domains, building a Graph Foundation Model (GFM) that can work well across different graphs and tasks with a unified backbone has recently garnered significant interests. A major obstacle to achieving this goal stems from the fact that graphs from different domains often exhibit diverse node features. Inspired by multi-modal models that align different modalities with natural language, the text has recently been adopted to provide a unified feature space for diverse graphs. Despite the great potential of these text-space GFMs, current research in this field is hampered by two problems. First, the absence of a comprehensive benchmark with unified problem settings hinders a clear understanding of the comparative effectiveness and practical value of different text-space GFMs. Second, there is a lack of sufficient datasets to thoroughly explore the methods’ full potential and verify their effectiveness across diverse settings. To address these issues, we conduct a comprehensive benchmark providing novel text-space datasets and comprehensive evaluation under unified problem settings. Empirical results provide new insights and inspire future research directions. Our code and data are publicly available from https://github.com/CurryTang/TSGFM.","","2025-03-30 16:30:08","2025-03-30 16:30:08","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ZTRN6VMA/Chen et al. - Text-space Graph Foundation Models Comprehensive Benchmarks and New Insights.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F4QX38QX","journalArticle","","Mertens, Laurent; Yargholi, Elahe’; de Beeck, Hans Op","FindingEmo: An Image Dataset for Emotion Recognition in the Wild","","","","","","We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.","","2025-03-30 16:30:09","2025-03-30 16:30:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/6UVHS4D7/Mertens et al. - FindingEmo An Image Dataset for Emotion Recognition in the Wild.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RJHI9WAD","journalArticle","","Liu, Yunong; Eyzaguirre, Cristobal; Li, Manling; Khanna, Shubh; Niebles, Juan Carlos; Ravi, Vineeth; Mishra, Saumitra; Liu, Weiyu; Wu, Jiajun","IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos","","","","","","Shape assembly is a ubiquitous task in daily life, integral for constructing complex 3D structures like IKEA furniture. While significant progress has been made in developing autonomous agents for shape assembly, existing datasets have not yet tackled the 4D grounding of assembly instructions in videos, essential for a holistic understanding of assembly in 3D space over time. We introduce IKEA Video Manuals, a dataset that features 3D models of furniture parts, instructional manuals, assembly videos from the Internet, and most importantly, annotations of dense spatio-temporal alignments between these data modalities. To demonstrate the utility of IKEA Video Manuals, we present five applications essential for shape assembly: assembly plan generation, part-conditioned segmentation, partconditioned pose estimation, video object segmentation, and furniture assembly based on instructional video manuals. For each application, we provide evaluation metrics and baseline methods. Through experiments on our annotated data, we highlight many challenges in grounding assembly instructions in videos to improve shape assembly, including handling occlusions, varying viewpoints, and extended assembly sequences.","","2025-03-30 16:30:11","2025-03-30 16:30:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7UXEV5U2/Liu et al. - IKEA Manuals at Work 4D Grounding of Assembly Instructions on Internet Videos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NGZAJ3C7","journalArticle","","Kajic, Ivana; Wiles, Olivia; Albuquerque, Isabela; Bauer, Matthias; Wang, Su; Pont-Tuset, Jordi; Nematzadeh, Aida","Evaluating Numerical Reasoning in Text-to-Image Models","","","","","","Text-to-image generative models are capable of producing high-quality images that often faithfully depict concepts described using natural language. In this work, we comprehensively evaluate a range of text-to-image models on numerical reasoning tasks of varying difficulty, and show that even the most advanced models have only rudimentary numerical skills. Specifically, their ability to correctly generate an exact number of objects in an image is limited to small numbers, it is highly dependent on the context the number term appears in, and it deteriorates quickly with each successive number. We also demonstrate that models have poor understanding of linguistic quantifiers (such as “a few” or “as many as”), the concept of zero, and struggle with more advanced concepts such as partial quantities and fractional representations. We bundle prompts, generated images and human annotations into GECKONUM, a novel benchmark for evaluation of numerical reasoning.","","2025-03-30 16:30:12","2025-03-30 16:30:12","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/A7HEY6K5/Kajic et al. - Evaluating Numerical Reasoning in Text-to-Image Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IIFHEXNH","journalArticle","","Pi, Renjie; Zhang, Jianshu; Zhang, Jipeng; Pan, Rui; Chen, Zhekai; Zhang, Tong","Image Textualization : An Automatic Framework for Creating Accurate and Detailed Image Descriptions","","","","","","Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval. Currently, image description datasets primarily originate from two sources. One source is the scraping of image-text pairs from the web. Despite their abundance, these descriptions are often of low quality and noisy. Another is through human labeling. Datasets such as COCO are generally very short and lack details. Although detailed image descriptions can be annotated by humans, the high annotation cost limits the feasibility. These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions. In this paper, we propose an innovative framework termed Image Textualization (IT), which automatically produces high-quality image descriptions by leveraging existing multi-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner, which maximally convert the visual information into text. To address the current lack of benchmarks for detailed descriptions, we propose several benchmarks for comprehensive evaluation, which verifies the quality of image descriptions created by our framework. Furthermore, we show that LLaVA-7B, benefiting from fine-tuning on IT-curated descriptions, acquire improved capability to generate richer image descriptions, substantially increasing the length and detail of their output with less hallucination.","","2025-03-30 16:30:14","2025-03-30 16:30:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YAY996GC/Pi et al. - Image Textualization  An Automatic Framework for Creating Accurate and Detailed Image Descriptions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CLZ4ZLE9","journalArticle","","Ma, Chang; Zhang, Junlei; Zhu, Zhihao; Yang, Cheng; Yang, Yujiu; Jin, Yaohui; Lan, Zhenzhong; Kong, Lingpeng; He, Junxian","AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents","","","","","","","","2025-03-30 16:30:15","2025-03-30 16:30:15","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Y3S53GK9/Ma et al. - AgentBoard An Analytical Evaluation Board of Multi-turn LLM Agents.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q8L7QZBW","journalArticle","","Estermann, Benjamin; Lanzendörfer, Luca A; Niedermayr, Yannick; Wattenhofer, Roger","PUZZLES: A Benchmark for Neural Algorithmic Reasoning","","","","","","Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal role in problem-solving and decision-making processes. Reinforcement Learning (RL) has demonstrated remarkable proficiency in tasks such as motor control, handling perceptual input, and managing stochastic environments. These advancements have been enabled in part by the availability of benchmarks. In this work we introduce PUZZLES, a benchmark based on Simon Tatham’s Portable Puzzle Collection, aimed at fostering progress in algorithmic and logical reasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizes and varying levels of complexity; many puzzles also feature a diverse set of additional configuration parameters. The 40 puzzles provide detailed information on the strengths and generalization capabilities of RL agents. Furthermore, we evaluate various RL algorithms on PUZZLES, providing baseline comparisons and demonstrating the potential for future research. All the software, including the environment, is available at https://github.com/ETH-DISCO/rlp.","","2025-03-30 16:30:17","2025-03-30 16:30:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8ECWSFUT/Estermann et al. - PUZZLES A Benchmark for Neural Algorithmic Reasoning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRJ87BZG","journalArticle","","Hu, Yubin; Wen, Kairui; Zhou, Heng; Guo, Xiaoyang; Liu, Yong-Jin","SS3DM: Benchmarking Street-View Surface Reconstruction with a Synthetic 3D Mesh Dataset","","","","","","Reconstructing accurate 3D surfaces for street-view scenarios is crucial for applications such as digital entertainment and autonomous driving simulation. However, existing street-view datasets, including KITTI, Waymo, and nuScenes, only offer noisy LiDAR points as ground-truth data for geometric evaluation of reconstructed surfaces. These geometric ground-truths often lack the necessary precision to evaluate surface positions and do not provide data for assessing surface normals. To overcome these challenges, we introduce the SS3DM dataset, comprising precise Synthetic Street-view 3D Mesh models exported from the CARLA simulator. These mesh models facilitate accurate position evaluation and include normal vectors for evaluating surface normal. To simulate the input data in realistic driving scenarios for 3D reconstruction, we virtually drive a vehicle equipped with six RGB cameras and five LiDAR sensors in diverse outdoor scenes. Leveraging this dataset, we establish a benchmark for state-of-the-art surface reconstruction methods, providing a comprehensive evaluation of the associated challenges. For more information, visit our homepage at https://ss3dm.top.","","2025-03-30 16:30:18","2025-03-30 16:30:18","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CQTKNNMC/Hu et al. - SS3DM Benchmarking Street-View Surface Reconstruction with a Synthetic 3D Mesh Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTFK5ZGA","journalArticle","","Lee, Dongwoo; Park, Joonkyu; Lee, Kyoung Mu","GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring","","","","","","","","2025-03-30 16:30:19","2025-03-30 16:30:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HZSPDKZA/Lee et al. - GS-Blur A 3D Scene-Based Dataset for Realistic Image Deblurring.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7N7TVFER","journalArticle","","Patel, Aman; Singhal, Arpita; Wang, Austin; Pampari, Anusri; Kasowski, Maya; Kundaje, Anshul","DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA","","","","","","","","2025-03-30 16:30:20","2025-03-30 16:30:20","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YPJXVYX9/Patel et al. - DART-Eval A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5LRW9VXK","journalArticle","","Xu, Xiaoyue; Ye, Qinyuan; Ren, Xiang","Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack","","","","","","","","2025-03-30 16:30:22","2025-03-30 16:30:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4M5QVFWY/Xu et al. - Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2QFLD89","journalArticle","","Liu, Ye; Ma, Zongyang; Qi, Zhongang; Wu, Yang; Shan, Ying; Chen, Chang Wen","E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding","","","","","","Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our finegrained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.","","2025-03-30 16:30:23","2025-03-30 16:30:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/UFLVXFKR/Liu et al. - E.T. Bench Towards Open-Ended Event-Level Video-Language Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXJAAARP","dataset","2024","Franzen, Jannik; Winklmayr, Claudia; Guarino, Vanessa Emanuela; Karg, Christoph; Yu, Xiaoyan; Koreuber, Nora; Albrecht, Jan Philipp; Bischoff, Philipp; Kainmueller, Dagmar","Arctique - ARtificial Colon Tissue Images for Qualitative Uncertainty Evaluation","","","","10.5281/ZENODO.11635056","https://zenodo.org/doi/10.5281/zenodo.11635056","Uncertainty Quantification (UQ) is crucial for reliable image segmentation. Yet, while the field sees continual development of novel methods, a lack of agreedupon benchmarks limits their systematic comparison and evaluation: Current UQ methods are typically tested either on overly simplistic toy datasets or on complex real-world datasets that do not allow to discern true uncertainty. To unify both controllability and complexity, we introduce Arctique, a procedurally generated dataset modeled after histopathological colon images. We chose histopathological images for two reasons: 1) their complexity in terms of intricate object structures and highly variable appearance, which yields challenging segmentation problems, and 2) their broad prevalence for medical diagnosis and respective relevance of high-quality UQ. To generate Arctique, we established a Blender-based framework for 3D scene creation with intrinsic noise manipulation. Arctique contains up to 50,000 rendered images with precise masks as well as noisy label simulations. We show that by independently controlling the uncertainty in both images and labels, we can effectively study the performance of several commonly used UQ methods. Hence, Arctique serves as a critical resource for benchmarking and advancing UQ techniques and other methodologies in complex, multi-object environments, bridging the gap between realism and controllability. All code is publicly available, allowing re-creation and controlled manipulations of our shipped images as well as creation and rendering of new scenes.","2024-10-31","2025-03-30 16:30:25","2025-03-30 16:30:25","2025-03-30 16:30:25","","","","","","","","","","","","Zenodo","","en","MIT License","","","","DOI.org (Datacite)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/DL4GUIWJ/Franzen et al. - 2024 - Arctique - ARtificial Colon Tissue Images for Qualitative Uncertainty Evaluation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F9PVDNIZ","journalArticle","","Zhu, Haoyi; Wang, Yating; Huang, Di; Ye, Weicai; Ouyang, Wanli; He, Tong","Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning","","","","","","In robot learning, the observation space is crucial due to the distinct characteristics of different modalities, which can potentially become a bottleneck alongside policy design. In this study, we explore the influence of various observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. We introduce OBSBench, a benchmark comprising two simulators and 125 tasks, along with standardized pipelines for various encoders and policy baselines. Extensive experiments on diverse contact-rich manipulation tasks reveal a notable trend: point cloud-based methods, even those with the simplest designs, frequently outperform their RGB and RGB-D counterparts. This trend persists in both scenarios: training from scratch and utilizing pre-training. Furthermore, our findings demonstrate that point cloud observations often yield better policy performance and significantly stronger generalization capabilities across various geometric and visual conditions. These outcomes suggest that the 3D point cloud is a valuable observation modality for intricate robotic tasks. We also suggest that incorporating both appearance and coordinate information can enhance the performance of point cloud methods. We hope our work provides valuable insights and guidance for designing more generalizable and robust robotic models.","","2025-03-30 16:30:26","2025-03-30 16:30:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IP3F5HIW/Zhu et al. - Point Cloud Matters Rethinking the Impact of Different Observation Spaces on Robot Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3UB78F4","journalArticle","","Lyu, Ruiyuan; Lin, Jingli; Wang, Tai; Yang, Shuai; Mao, Xiaohan; Chen, Yilun; Xu, Runsen; Huang, Haifeng; Zhu, Chenming; Lin, Dahua; Pang, Jiangmiao","MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations","","","","","","With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress. However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene. To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan. It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding. The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans’ correction in the loop to ensure the annotations are natural, correct, and comprehensive. Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks. We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.","","2025-03-30 16:30:27","2025-03-30 16:30:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8S53YD88/Lyu et al. - MMScan A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IT9U8Z8B","journalArticle","","Chen, Lin; Wei, Xilin; Li, Jinsong; Dong, Xiaoyi; Zhang, Pan; Zang, Yuhang; Chen, Zehui; Duan, Haodong; Lin, Bin; Tang, Zhenyu; Yuan, Li; Qiao, Yu; Lin, Dahua; Zhao, Feng; Wang, Jiaqi","ShareGPT4Video: Improving Video Understanding and Generation with Better Captions","","","","","","","","2025-03-30 16:30:29","2025-03-30 16:30:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XVSCQ4MY/Chen et al. - ShareGPT4Video Improving Video Understanding and Generation with Better Captions.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XYIKZGUN","journalArticle","","Vu, Hien; Prabhune, Omkar; Raskar, Unmesh; Panditharatne, Dimuth; Chung, Hanwook; Choi, Christopher Y; Kim, Younghyun","MMCOWS: A Multimodal Dataset for Dairy Cattle Monitoring","","","","","","","","2025-03-30 16:30:30","2025-03-30 16:30:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IP3IKMWI/Vu et al. - MMCOWS A Multimodal Dataset for Dairy Cattle Monitoring.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PG3YYQYN","journalArticle","","Ying, Jiahao; Cao, Yixin; Bai, Yushi; Sun, Qianru; Wang, Bo; Tang, Wei; Ding, Zhaojun; Yang, Yizhe; Huang, Xuanjing; Yan, Shuicheng","Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models","","","","","","Large language models (LLMs) have achieved impressive performance across various natural language benchmarks, prompting a continual need to curate more difficult datasets for larger LLMs, which is costly and time-consuming. In this paper, we propose to automate dataset updating and provide systematical analysis regarding its effectiveness in dealing with benchmark leakage issue, difficulty control, and stability. Thus, once current benchmark has been mastered or leaked, we can update it for timely and reliable evaluation. There are two updating strategies: 1) mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence, and 2) extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom’s taxonomy of educational objectives. Extensive experiments on updated MMLU and BIG-Bench demonstrate the stability of the proposed strategies and find that the mimicking strategy can effectively alleviate issues of overestimation from benchmark leakage. In cases where the efficient mimicking strategy fails, our extending strategy still shows promising results. Additionally, by controlling the difficulty, we can better discern the models’ performance and enable fine-grained analysis — neither too difficult nor too easy an exam can fairly judge students’ learning status. To the best of our knowledge, we are the first to automate updating benchmarks for reliable and timely evaluation. Our demo leaderboard can be found at https://yingjiahao14.github.io/Automating-DatasetUpdates/.","","2025-03-30 16:30:32","2025-03-30 16:30:32","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BKLMRWCX/Ying et al. - Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMSWIT28","journalArticle","","Zhu, Liyun; Wang, Lei; Raj, Arjun; Gedeon, Tom; Chen, Chen","Advancing Video Anomaly Detection: A Concise Review and a New Dataset","","","","","","","","2025-03-30 16:30:33","2025-03-30 16:30:33","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/RKVK4KQI/Zhu et al. - Advancing Video Anomaly Detection A Concise Review and a New Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ARYZ66Z","journalArticle","","Vladimirova, Mariia; Diemert, Eustache; Pavone, Federico","FairJob: A Real-World Dataset for Fairness in Online Systems","","","","","","We introduce a fairness-aware dataset for job recommendation in advertising, designed to foster research in algorithmic fairness within real-world scenarios. It was collected and prepared to comply with privacy standards and business confidentiality. An additional challenge is the lack of access to protected user attributes such as gender, for which we propose a solution to obtain a proxy estimate. Despite being anonymized and including a proxy for a sensitive attribute, our dataset preserves predictive power and maintains a realistic and challenging benchmark. This dataset addresses a significant gap in the availability of fairnessfocused resources for high-impact domains like advertising – the actual impact being having access or not to precious employment opportunities, where balancing fairness and utility is a common industrial challenge. We also explore various stages in the advertising process where unfairness can occur and introduce a method to compute a fair utility metric for the job recommendations in online systems case from a biased dataset. Experimental evaluations of bias mitigation techniques on the released dataset demonstrate potential improvements in fairness and the associated trade-offs with utility.","","2025-03-30 16:30:35","2025-03-30 16:30:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9QZBBUEE/Vladimirova et al. - FairJob A Real-World Dataset for Fairness in Online Systems.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2ZZLUQL","journalArticle","","Junczyk, Michał","BIGOS V2 Benchmark for Polish ASR: Curated Datasets and Tools for Reproducible Evaluation","","","","","","","","2025-03-30 16:30:36","2025-03-30 16:30:36","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/C9MXFY9K/Junczyk - BIGOS V2 Benchmark for Polish ASR Curated Datasets and Tools for Reproducible Evaluation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYBSMENP","journalArticle","","Kuratov, Yuri; Bulatov, Aydar; Anokhin, Petr; Rodkin, Ivan; Sorokin, Dmitry; Sorokin, Artyom; Burtsev, Mikhail","BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack","","","","","","In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models’ ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to incontext reasoning, Retrieval-Augmented Generation methods achieve a modest 60% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.","","2025-03-30 16:30:38","2025-03-30 16:30:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CR3ARIXK/Kuratov et al. - BABILong Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W2PBE9XP","journalArticle","","Wang, Xihuai; Zhang, Shao; Zhang, Wenhao; Dong, Wentao; Chen, Jingxiao; Wen, Ying; Zhang, Weinan","ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination","","","","","","Zero-shot coordination (ZSC) is a new cooperative multi-agent reinforcement learning (MARL) challenge that aims to train an ego agent to work with diverse, unseen partners during deployment. The significant difference between the deploymenttime partners’ distribution and the training partners’ distribution determined by the training algorithm makes ZSC a unique out-of-distribution (OOD) generalization challenge. The potential distribution gap between evaluation and deploymenttime partners leads to inadequate evaluation, which is exacerbated by the lack of appropriate evaluation metrics. In this paper, we present ZSC-Eval, the first evaluation toolkit and benchmark for ZSC algorithms. ZSC-Eval consists of: 1) Generation of evaluation partner candidates through behavior-preferring rewards to approximate deployment-time partners’ distribution; 2) Selection of evaluation partners by Best-Response Diversity (BR-Div); 3) Measurement of generalization performance with various evaluation partners via the Best-Response Proximity (BR-Prox) metric. We use ZSC-Eval to benchmark ZSC algorithms in Overcooked and Google Research Football environments and get novel empirical findings. We also conduct a human experiment of current ZSC algorithms to verify the ZSC-Eval’s consistency with human evaluation. ZSC-Eval is now available at https://github.com/sjtu-marl/ZSC-Eval.","","2025-03-30 16:30:40","2025-03-30 16:30:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/EZDSEQQ9/Wang et al. - ZSC-Eval An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABLUNMWT","journalArticle","","Wang, Wenhao; Yang, Yi",": A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models","","","","","","","","2025-03-30 16:30:41","2025-03-30 16:30:41","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KF2YUS22/Wang and Yang -  A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WC3M3QMN","journalArticle","","Jiang, Yifan; Zhang, Jiarui; Sun, Kexuan; Sourati, Zhivar; Ahrabian, Kian; Ma, Kaixin; Ilievski, Filip; Pujara, Jay","MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning","","","","","","While multi-modal large language models (MLLMs) have shown significant progress across popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints on numbers) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix). However, existing AVR benchmarks only consider a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 × 3 matrices). And they fail to capture all abstract reasoning patterns in human cognition necessary for addressing real-world tasks, such as geometric properties and object boundary understanding in realworld navigation. To evaluate MLLMs’ AVR abilities systematically, we introduce MARVEL founded on the core knowledge system in human cognition, a multidimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model performance is grounded in perception or reasoning, MARVEL complements the standard AVR question with perception questions in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with ten representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all MLLMs show near-random performance on MARVEL, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance). Although closed-source MLLMs, such as GPT-4V, show a promising understanding of reasoning patterns (on par with humans) after adding textual descriptions, this advantage is hindered by their weak perception abilities. We release our entire code and dataset at https://github.com/1171-jpg/MARVEL_AVR.","","2025-03-30 16:30:42","2025-03-30 16:30:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MYG4FTCH/Jiang et al. - MARVEL Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZK6KY64S","journalArticle","","Zhang, Hugh; Da, Jeff; Lee, Dean; Robinson, Vaughn; Wu, Catherine; Song, Will; Zhao, Tiffany; Raja, Pranav; Zhuang, Charlotte; Slack, Dylan; Lyu, Qin; Hendryx, Sean; Kaplan, Russell; Yue, Summer","A Careful Examination of Large Language Model Performance on Grade School Arithmetic","","","","","","","","2025-03-30 16:30:44","2025-03-30 16:30:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/Z4R4WF8U/Zhang et al. - A Careful Examination of Large Language Model Performance on Grade School Arithmetic.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9EEVG3W","journalArticle","","Wei, Fangyun; Zhao, Jinjing; Yan, Kun; Zhang, Hongyang; Xu, Chang","A Large-Scale Human-Centric Benchmark for Referring Expression Comprehension in the LMM Era","","","","","","Prior research in human-centric AI has primarily addressed single-modality tasks like pedestrian detection, action recognition, and pose estimation. However, the emergence of large multimodal models (LMMs) such as GPT-4V has redirected attention towards integrating language with visual content. Referring expression comprehension (REC) represents a prime example of this multimodal approach. Current human-centric REC benchmarks, typically sourced from general datasets, fall short in the LMM era due to their limitations, such as insufficient testing samples, overly concise referring expressions, and limited vocabulary, making them inadequate for evaluating the full capabilities of modern REC models. In response, we present HC-RefLoCo (Human-Centric Referring Expression Comprehension with Long Context), a benchmark that includes 13,452 images, 24,129 instances, and 44,738 detailed annotations, encompassing a vocabulary of 18,681 words. Each annotation, meticulously reviewed for accuracy, averages 93.2 words and includes topics such as appearance, human-object interaction, location, action, celebrity, and OCR. HC-RefLoCo provides a wider range of instance scales and diverse evaluation protocols, encompassing accuracy with various IoU criteria, scale-aware evaluation, and subject-specific assessments. Our experiments, which assess 24 models, highlight HC-RefLoCo’s potential to advance human-centric AI by challenging contemporary REC models with comprehensive and varied data. Our benchmark, along with the evaluation code, are available at https: //github.com/ZhaoJingjing713/HC-RefLoCo.","","2025-03-30 16:30:45","2025-03-30 16:30:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/4MY88TRY/Wei et al. - A Large-Scale Human-Centric Benchmark for Referring Expression Comprehension in the LMM Era.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ASC94KU","journalArticle","","Pa, Victor-Alexandru; Singla, Adish","Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming","","","","","","Generative models have demonstrated human-level proficiency in various benchmarks across domains like programming, natural sciences, and general knowledge. Despite these promising results on competitive benchmarks, they still struggle with seemingly simple problem-solving tasks typically carried out by elementary-level students. How do state-of-the-art models perform on standardized programmingrelated tests designed to assess computational thinking and problem-solving skills at schools? In this paper, we curate a novel benchmark involving computational thinking tests grounded in elementary visual programming domains. Our initial results show that state-of-the-art models like GPT-4o and Llama3 barely match the performance of an average school student. To further boost the performance of these models, we fine-tune them using a novel synthetic data generation methodology. The key idea is to develop a comprehensive dataset using symbolic methods that capture different skill levels, ranging from recognition of visual elements to multi-choice quizzes to synthesis-style tasks. We showcase how various aspects of symbolic information in synthetic data help improve fine-tuned models’ performance. We will release the full implementation and datasets to facilitate further research on enhancing computational thinking in generative models.","","2025-03-30 16:30:46","2025-03-30 16:30:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AWZ2X2NR/Pa and Singla - Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JTD9BD7","journalArticle","","Zhang, Jifan; Jain, Lalit; Guo, Yang; Chen, Jiayi; Zhou, Kuan Lok; Suresh, Siddharth; Wagenmaker, Andrew; Sievert, Scott; Rogers, Timothy; Jamieson, Kevin; Mankoff, Robert; Nowak, Robert","Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning","","","","","","We present a novel multimodal preference dataset for creative tasks, consisting of over 250 million human ratings on more than 2.2 million captions, collected through crowdsourcing rating data for The New Yorker’s weekly cartoon caption contest over the past eight years. This unique dataset supports the development and evaluation of multimodal large language models and preference-based fine-tuning algorithms for humorous caption generation. We propose novel benchmarks for judging the quality of model-generated captions, utilizing both GPT4 and human judgments to establish ranking-based evaluation strategies. Our experimental results highlight the limitations of current fine-tuning methods, such as RLHF and DPO, when applied to creative tasks. Furthermore, we demonstrate that even stateof-the-art models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. As we conclude this extensive data collection effort, we release the entire preference dataset to the research community, fostering further advancements in AI humor generation and evaluation.","","2025-03-30 16:30:48","2025-03-30 16:30:48","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/MK2TFPCX/Zhang et al. - Humor in AI Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5JQ8WMAM","journalArticle","","Su, Zhaochen; Zhang, Jun; Qu, Xiaoye; Zhu, Tong; Li, Yanshu; Sun, Jiashuo; Li, Juntao; Zhang, Min; Cheng, Yu","CONFLICTBANK: A Benchmark for Evaluating Knowledge Conﬂicts in Large Language Models","","","","","","Large language models (LLMs) have achieved impressive advancements across numerous disciplines, yet the critical issue of knowledge conﬂicts, a major source of hallucinations, has rarely been studied. While a few research explored the conﬂicts between the inherent knowledge of LLMs and the retrieved contextual knowledge, a comprehensive assessment of knowledge conﬂict in LLMs is still missing. Motivated by this research gap, we ﬁrstly propose CONFLICTBANK, the largest benchmark with 7.45M claim-evidence pairs and 553k QA pairs, addressing conﬂicts from misinformation, temporal discrepancies, and semantic divergences. Using CONFLICTBANK, we conduct the thorough and controlled experiments for a comprehensive understanding of LLM behavior in knowledge conﬂicts, focusing on three key aspects: (i) conﬂicts encountered in retrieved knowledge, (ii) conﬂicts within the models’ encoded knowledge, and (iii) the interplay between these conﬂict forms. Our investigation delves into four model families and twelve LLM instances and provides insights into conﬂict types, model sizes, and the impact at different stages. We believe that knowledge conﬂicts represent a critical bottleneck to achieving trustworthy artiﬁcial intelligence and hope our work will offer valuable guidance for future model training and development. Resources are available at https://github.com/zhaochen0110/conflictbank.","","2025-03-30 16:30:49","2025-03-30 16:30:49","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/27ADLSWA/Su et al. - CONFLICTBANK A Benchmark for Evaluating Knowledge Conﬂicts in Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D45CII3Z","journalArticle","","Naik, Hemal; Yang, Junran; Das, Dipin; Crofoot, Margaret C; Rathore, Akanksha; Sridhar, Vivek Hari","BuckTales: A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes","","","","","","Understanding animal behaviour is central to predicting, understanding, and mitigating impacts of natural and anthropogenic changes on animal populations and ecosystems. However, the challenges of acquiring and processing long-term, ecologically relevant data in wild settings have constrained the scope of behavioural research. The increasing availability of Unmanned Aerial Vehicles (UAVs), coupled with advances in machine learning, has opened new opportunities for wildlife monitoring using aerial tracking. However, limited availability of datasets with wild animals in natural habitats has hindered progress in automated computer vision solutions for long-term animal tracking. Here we introduce BuckTales, the first large-scale UAV dataset designed to solve multi-object tracking (MOT) and re-identification (Re-ID) problem in wild animals, specifically the mating behaviour (or lekking) of blackbuck antelopes. Collected in collaboration with biologists, the MOT dataset includes over 1.2 million annotations including 680 tracks across 12 high-resolution (5.4K) videos, each averaging 66 seconds and featuring 30 to 130 individuals. The Re-ID dataset includes 730 individuals captured with two UAVs simultaneously. The dataset is designed to drive scalable, long-term animal behaviour tracking using multiple camera sensors. By providing baseline performance with two detectors, and benchmarking several state-of-the-art tracking methods, our dataset reflects the real-world challenges of tracking wild animals in socially and ecologically relevant contexts. In making these data widely available, we hope to catalyze progress in MOT and Re-ID for wild animals, fostering insights into animal behaviour, conservation efforts, and ecosystem dynamics through automated, long-term monitoring.","","2025-03-30 16:30:50","2025-03-30 16:30:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YCGG6X6V/Naik et al. - BuckTales A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4MKMH5VR","journalArticle","","Haider, Momin; Yin, Ming; Zhang, Menglei; Gupta, Arpit; Zhu, Jing; Wang, Yu-Xiang","NetworkGym: Reinforcement Learning Environments for Multi-Access Trafﬁc Management in Network Simulation","","","","","","Mobile devices such as smartphones, laptops, and tablets can often connect to multiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously. Recent advancements facilitate seamless integration of these connections below the transport layer, enhancing the experience for apps that lack inherent multi-path support. This optimization hinges on dynamically determining the trafﬁc distribution across networks for each device, a process referred to as multi-access trafﬁc splitting. This paper introduces NetworkGym, a high-ﬁdelity network environment simulator that facilitates generating multiple network trafﬁc ﬂows and multiaccess trafﬁc splitting. This simulator facilitates training and evaluating different RL-based solutions for the multi-access trafﬁc splitting problem. Our initial explorations demonstrate that the majority of existing state-of-the-art ofﬂine RL algorithms (e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average. This illustrates the urgent need to evaluate ofﬂine RL algorithms against a broader range of benchmarks, rather than relying solely on popular ones such as D4RL. We also propose an extension to the TD3+BC algorithm, named Pessimistic TD3 (PTD3), and demonstrate that it outperforms many state-of-the-art ofﬂine RL algorithms. PTD3’s behavioral constraint mechanism, which relies on value-function pessimism, is theoretically motivated and relatively simple to implement.","","2025-03-30 16:30:51","2025-03-30 16:30:51","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/HMT95BI5/Haider et al. - NetworkGym Reinforcement Learning Environments for Multi-Access Trafﬁc Management in Network Simula.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZLZNADUV","journalArticle","","Krojer, Benno; Vattikonda, Dheeraj; Lara, Luis; Jampani, Varun; Portelance, Eva; Pal, Christopher; Reddy, Siva","Learning Action and Reasoning-Centric Image Editing from Videos and Simulations","","","","","","An image editing model should be able to perform diverse edits, ranging from object replacement, changing attributes or style, to performing actions or movement, which require many forms of reasoning. Current general instruction-guided editing models have significant shortcomings with action and reasoning-centric edits. Object, attribute or stylistic changes can be learned from visually static datasets. On the other hand, high-quality data for action and reasoning-centric edits is scarce and has to come from entirely different sources that cover e.g. physical dynamics, temporality and spatial reasoning. To this end, we meticulously curate the AURORA Dataset (Action-Reasoning-Object-Attribute), a collection of highquality training data, human-annotated and curated from videos and simulation engines. We focus on a key aspect of quality training data: triplets (source image, prompt, target image) contain a single meaningful visual change described by the prompt, i.e., truly minimal changes between source and target images. To demonstrate the value of our dataset, we evaluate an AURORA-finetuned model on a new expert-curated benchmark (AURORA-BENCH) covering 8 diverse editing tasks. Our model significantly outperforms previous editing models as judged by human raters. For automatic evaluations, we find important flaws in previous metrics and caution their use for semantically hard editing tasks. Instead, we propose a new automatic metric that focuses on discriminative understanding. We hope that our efforts : (1) curating a quality training dataset and an evaluation benchmark, (2) developing critical evaluations, and (3) releasing a state-of-the-art model1, will fuel further progress on general image editing.","","2025-03-30 16:30:53","2025-03-30 16:30:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BT4LW3F7/Krojer et al. - Learning Action and Reasoning-Centric Image Editing from Videos and Simulations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SHK2GYP","journalArticle","","Jaume, Guillaume; Doucet, Paul; Song, Andrew H; Lu, Ming Y; Almagro-Pérez, Cristina; Wagner, Sophia J; Vaidya, Anurag J; Chen, Richard J; Williamson, Drew F K; Kim, Ahrong; Mahmood, Faisal","HEST-1k: A Dataset for Spatial Transcriptomics and Histology Image Analysis","","","","","","Spatial transcriptomics enables interrogating the molecular composition of tissue with ever-increasing resolution and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods in ST to narrow tasks and small cohorts. In addition, the underlying tissue morphology, as reflected by H&E-stained whole slide images (WSIs), encodes rich information often overlooked in ST studies. Here, we introduce HEST-1k, a collection of 1,229 spatial transcriptomic profiles, each linked to a WSI and extensive metadata. HEST-1k was assembled from 153 public and internal cohorts encompassing 26 organs, two species (Homo Sapiens and Mus Musculus), and 367 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 2.1 million expression–morphology pairs and over 76 million nuclei. To support its development, we additionally introduce the HEST-Library, a Python package designed to perform a range of actions with HEST samples. We test HEST-1k and Library on three use cases: (1) benchmarking foundation models for pathology (HEST-Benchmark), (2) biomarker exploration, and (3) multimodal representation learning. HEST-1k, HEST-Library, and HEST-Benchmark can be freely accessed at https://github.com/mahmoodlab/hest.","","2025-03-30 16:30:54","2025-03-30 16:30:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9Y7EF8VB/Jaume et al. - HEST-1k A Dataset for Spatial Transcriptomics and Histology Image Analysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SMDYYVAU","journalArticle","","Wang, Bowen; Chang, Jiuyang; Qian, Yiming; Chen, Guoxin; Chen, Junhao; Jiang, Zhouqiang; Zhang, Jiahao; Nakashima, Yuta; Nagahara, Hajime","DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models","","","","","","Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 511 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability and that of human doctors, highlighting the critical need for models that can reason effectively in real-world clinical scenarios ‡.","","2025-03-30 16:30:56","2025-03-30 16:30:56","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/YDUSQWHD/Wang et al. - DiReCT Diagnostic Reasoning for Clinical Notes via Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EY2IKJZS","journalArticle","","Wu, Junchao; Zhan, Runzhe; Wong, Derek F; Yang, Shu; Yang, Xinyi; Yuan, Yulin; Chao, Lidia S","DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios","","","","","","Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating various prompts usages, human revisions like word substitutions, and writing noises like spelling mistakes. Our development of DetectRL reveals the strengths and limitations of current SOTA detectors. More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe DetectRL could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors2.","","2025-03-30 16:30:57","2025-03-30 16:30:57","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/AGLQESKX/Wu et al. - DetectRL Benchmarking LLM-Generated Text Detection in Real-World Scenarios.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EYHD7LUZ","journalArticle","","Guo, Kehan; Nan, Bozhao; Zhou, Yujun; Guo, Taicheng; Guo, Zhichun; Surve, Mihir; Liang, Zhenwen; Chawla, Nitesh V; Wiest, Olaf; Zhang, Xiangliang","Can LLMs Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation","","","","","","","","2025-03-30 16:30:59","2025-03-30 16:30:59","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/58DXHSZC/Guo et al. - Can LLMs Solve Molecule Puzzles A Multimodal Benchmark for Molecular Structure Elucidation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HFYQ2Z7E","journalArticle","","Xie, Qianqian; Han, Weiguang; Chen, Zhengyu; Xiang, Ruoyu; Zhang, Xiao; He, Yueru; Xiao, Mengxi; Li, Dong; Dai, Yongfu; Feng, Duanyu; Xu, Yijing; Kang, Haoqiang; Kuang, Ziyan; Yuan, Chenhan; Yang, Kailai; Luo, Zheheng; Zhang, Tianlin; Liu, Zhiwei; Xiong, Guojun; Deng, Zhiyang; Jiang, Yuechen; Yao, Zhiyuan; Li, Haohang; Yu, Yangyang; Hu, Gang; Huang, Jiajia; Liu, Xiao-Yang; Lopez-Lira, Alejandro; Wang, Benyou; Lai, Yanzhao; Wang, Hao; Peng, Min; Ananiadou, Sophia; Huang, Jimin","FinBen: A Holistic Financial Benchmark for Large Language Models","","","","","","LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 42 datasets spanning 24 financial tasks, covering eight critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, decision-making, and bilingual (English and Spanish). FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and two novel datasets for regulations and stock trading. Our evaluation of 21 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen’s potential to drive innovations in financial LLMs. All datasets and code are publicly available for the research community2, with results shared and updated regularly on the Open Financial LLM Leaderboard3.","","2025-03-30 16:31:01","2025-03-30 16:31:01","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/V6K5TKMF/Xie et al. - FinBen A Holistic Financial Benchmark for Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UJIRXRWH","journalArticle","","Jeon, Minkyu; Raghu, Rishwanth; Astore, Miro; Woollard, Geoffrey; Feathers, Ryan; Kaz, Alkin; Hanson, Sonya M; Cossio, Pilar; Zhong, Ellen D","CryoBench: Diverse and challenging datasets for the heterogeneity problem in cryo-EM","","","","","","Cryo-electron microscopy (cryo-EM) is a powerful technique for determining high-resolution 3D biomolecular structures from imaging data. Its unique ability to capture structural variability has spurred the development of heterogeneous reconstruction algorithms that can infer distributions of 3D structures from noisy, unlabeled imaging data. Despite the growing number of advanced methods, progress in the field is hindered by the lack of standardized benchmarks with ground truth information and reliable validation metrics. Here, we introduce CryoBench, a suite of datasets, metrics, and benchmarks for heterogeneous reconstruction in cryo-EM. CryoBench includes five datasets representing different sources of heterogeneity and degrees of difficulty. These include conformational heterogeneity generated from designed motions of antibody complexes or sampled from a molecular dynamics simulation, as well as compositional heterogeneity from mixtures of ribosome assembly states or 100 common complexes present in cells. We then analyze stateof-the-art heterogeneous reconstruction tools, including neural and non-neural methods, assess their sensitivity to noise, and propose new metrics for quantitative evaluation. We hope that CryoBench will be a foundational resource for accelerating algorithmic development and evaluation in the cryo-EM and machine learning communities. Project page: https://cryobench.cs.princeton.edu.","","2025-03-30 16:31:02","2025-03-30 16:31:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CEHCBH2P/Jeon et al. - CryoBench Diverse and challenging datasets for the heterogeneity problem in cryo-EM.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9R4SF5L9","journalArticle","","Melistas, Thomas; Spyrou, Nikos; Gkouti, Nefeli; Sanchez, Pedro; Panagakis, Yannis; Papanastasiou, Giorgos; Tsaftaris, Sotirios A","Benchmarking Counterfactual Image Generation","","","","","","Generative AI has revolutionised visual content editing, empowering users to effortlessly modify images and videos. However, not all edits are equal. To perform realistic edits in domains such as natural image or medical imaging, modifications must respect causal relationships inherent to the data generation process. Such image editing falls into the counterfactual image generation regime. Evaluating counterfactual image generation is substantially complex: not only it lacks observable ground truths, but also requires adherence to causal constraints. Although several counterfactual image generation methods and evaluation metrics exist, a comprehensive comparison within a unified setting is lacking. We present a comparison framework to thoroughly benchmark counterfactual image generation methods. We evaluate the performance of three conditional image generation model families developed within the Structural Causal Model (SCM) framework. We incorporate several metrics that assess diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We integrate all models that have been used for the task at hand and expand them to novel datasets and causal graphs, demonstrating the superiority of Hierarchical VAEs across most datasets and metrics. Our framework is implemented in a userfriendly Python package that can be extended to incorporate additional SCMs, causal methods, generative models, and datasets for the community to build on. Code: https://github.com/gulnazaki/counterfactual-benchmark.","","2025-03-30 16:31:03","2025-03-30 16:31:04","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/X6Q76PVT/Melistas et al. - Benchmarking Counterfactual Image Generation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SCT7UJYA","journalArticle","","Remonda, Adrian; Hansen, Nicklas; Raji, Ayoub; Musiu, Nicola; Bertogna, Marko; Veas, Eduardo E; Wang, Xiaolong","A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data","","","","","","Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: https://assetto-corsa-gym.github.io.","","2025-03-30 16:31:05","2025-03-30 16:31:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/5I5KM6YE/Remonda et al. - A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZXIT6JT","journalArticle","","Weber, Maurice; Fu, Daniel Y; Anthony, Quentin; Oren, Yonatan; Adams, Shane; Alexandrov, Anton; Lyu, Xiaozhong; Nguyen, Huu; Yao, Xiaozhe; Adams, Virginia; Athiwaratkun, Ben; Chalamala, Rahul; Chen, Kezhen; Ryabinin, Max; Dao, Tri; Liang, Percy; Ré, Christopher; Rish, Irina; Zhang, Ce","RedPajama: an Open Dataset for Training Large Language Models","","","","","","Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce’s XGen and AI2’s OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.","","2025-03-30 16:31:06","2025-03-30 16:31:06","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/WJTL2P7G/Weber et al. - RedPajama an Open Dataset for Training Large Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADXMQKYJ","journalArticle","","Kotalwar, Nachiket; Gotovos, Alkis; Singla, Adish","Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation","","","","","","Generative AI and large language models hold great promise in enhancing programming education by generating individualized feedback and hints for learners. Recent works have primarily focused on improving the quality of generated feedback to achieve human tutors’ quality. While quality is an important performance criterion, it is not the only criterion to optimize for real-world educational deployments. In this paper, we benchmark language models for programming feedback generation across several performance criteria, including quality, cost, time, and data privacy. The key idea is to leverage recent advances in the new paradigm of in-browser inference that allow running these models directly in the browser, thereby providing direct benefits across cost and data privacy. To boost the feedback quality of small models compatible with in-browser inference engines, we develop a fine-tuning pipeline based on GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM’s in-browser inference engine on three different Python programming datasets. We also release the full implementation along with a web app and datasets to facilitate further research on in-browser language models.","","2025-03-30 16:31:08","2025-03-30 16:31:08","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/V7TI3AQW/Kotalwar et al. - Hints-In-Browser Benchmarking Language Models for Programming Feedback Generation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2RZDH46","journalArticle","","Blacher, Mark; Staudt, Christoph; Klaus, Julien; Wenig, Maurice; Merk, Niklas; Breuer, Alexander; Engel, Max; Laue, Sören; Giesen, Joachim","Einsum Benchmark: Enabling the Development of Next-Generation Tensor Execution Engines","","","","","","Modern artiﬁcial intelligence and machine learning workﬂows rely on efﬁcient tensor libraries. However, tuning tensor libraries without considering the actual problems they are meant to execute can lead to a mismatch between expected performance and the actual performance. Einsum libraries are tuned to efﬁciently execute tensor expressions with only a few, relatively large, dense, ﬂoating-point tensors. But, practical applications of einsum cover a much broader range of tensor expressions than those that can currently be executed efﬁciently. For this reason, we have created a benchmark dataset that encompasses this broad range of tensor expressions, allowing future implementations of einsum to build upon and be evaluated against. In addition, we also provide generators for einsum expressions and converters to einsum expressions in our repository, so that additional data can be generated as needed. The benchmark dataset, the generators and converters are released openly and are publicly available at https://benchmark.einsum.org.","","2025-03-30 16:31:09","2025-03-30 16:31:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/XF6RL9QG/Blacher et al. - Einsum Benchmark Enabling the Development of Next-Generation Tensor Execution Engines.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFUF8JZG","journalArticle","","Klein, Lukas; Lüth, Carsten; Schlegel, Udo; Bungert, Till; El-Assady, Mennatallah; Jäger, Paul","Navigating the Maze of Explainable AI: A Systematic Approach to Evaluating Methods and Metrics","","","","","","Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed methods as well as metrics aiming to evaluate their efficacy. However, current studies are often of limited scope, examining only a handful of XAI methods and ignoring underlying design parameters for performance, such as the model architecture or the nature of input data. Moreover, they often rely on one or a few metrics and neglect thorough validation, increasing the risk of selection bias and ignoring discrepancies among metrics. These shortcomings leave practitioners confused about which method to choose for their problem. In response, we introduce LATEC, a large-scale benchmark that critically evaluates 17 prominent XAI methods using 20 distinct metrics. We systematically incorporate vital design parameters like varied architectures and diverse input modalities, resulting in 7,560 examined combinations. Through LATEC, we showcase the high risk of conflicting metrics leading to unreliable rankings and consequently propose a more robust evaluation scheme. Further, we comprehensively evaluate various XAI methods to assist practitioners in selecting appropriate methods aligning with their needs. Curiously, the emerging top-performing method, Expected Gradients, is not examined in any relevant related study. LATEC reinforces its role in future XAI research by publicly releasing all 326k saliency maps and 378k metric scores as a (meta-)evaluation dataset. The benchmark is hosted at: https://github.com/IML-DKFZ/latec.","","2025-03-30 16:31:10","2025-03-30 16:31:10","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JDKD9MLJ/Klein et al. - Navigating the Maze of Explainable AI A Systematic Approach to Evaluating Methods and Metrics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCKKWWZK","journalArticle","","Meier, Manuel; Demirel, Berken Utku; Holz, Christian","WildPPG: A Real-World PPG Dataset of Long Continuous Recordings","","","","","","Reflective photoplethysmography (PPG) has become the default sensing technique in wearable devices to monitor cardiac activity via a person’s heart rate (HR). However, PPG-based HR estimates can be substantially impacted by factors such as the wearer’s activities, sensor placement and resulting motion artifacts, as well as environmental characteristics such as temperature and ambient light. These and other factors can significantly impact and decrease HR prediction reliability.","","2025-03-30 16:31:11","2025-03-30 16:31:11","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SJ9XBRPF/Meier et al. - WildPPG A Real-World PPG Dataset of Long Continuous Recordings.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KWXPUU73","journalArticle","","Li, Baiqi; Lin, Zhiqiu; Peng, Wenxuan; Nyandwi, Jean de Dieu; Jiang, Daniel; Ma, Zixian; Khanuja, Simran; Krishna, Ranjay; Neubig, Graham; Ramanan, Deva","NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples","","","","","","Vision-language models (VLMs) have made significant progress in recent visualquestion-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using offthe-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a vision-centric design by pairing each question with two images that yield different answers, preventing “blind” solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can largely be solved with language priors like commonsense knowledge. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like BLIP-3, LLaVA-OneVision, Cambrian-1, InternLM-XC2, Llama3.2-Vision, Molmo, Qwen2-VL, and even the (closed-source) GPT-4o lag 50%-70% behind human performance (which is above 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. We show that debiasing can be crucial for VLM performance. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.","","2025-03-30 16:31:13","2025-03-30 16:31:13","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/7T6VHZSJ/Li et al. - NaturalBench Evaluating Vision-Language Models on Natural Adversarial Samples.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PA92XWJB","journalArticle","","Luo, Bingqiao; Zhang, Zhen; Wang, Qian; He, Bingsheng","Multi-Chain Graphs of Graphs: A New Approach to Analyzing Blockchain Datasets","","","","","","Machine learning applied to blockchain graphs offers significant opportunities for enhanced data analysis and applications. However, the potential of this field is constrained by the lack of a large-scale, cross-chain dataset that includes hierarchical graph-level data. To address this issue, we present novel datasets that provide detailed label information at the token level and integrate interactions between tokens across multiple blockchain platforms. We model transactions within each token as local graphs and the relationships between tokens as global graphs, collectively forming a ""Graphs of Graphs"" (GoG) approach. This innovative approach facilitates a deeper understanding of systemic structures and hierarchical interactions, which are essential for applications such as link prediction, anomaly detection, and token classification. We conduct a series of experiments demonstrating that this dataset delivers new insights and challenges for exploring GoG within the blockchain domain. Our work promotes advancements and opens new avenues for research in both the blockchain and graph communities. Source code and datasets are available at https: //github.com/Xtra-Computing/Cryptocurrency-Graphs-of-graphs.","","2025-03-30 16:31:14","2025-03-30 16:31:14","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PJILPYRV/Luo et al. - Multi-Chain Graphs of Graphs A New Approach to Analyzing Blockchain Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N33YU32G","journalArticle","1954","","University of the Witwatersrand, Johannesburg, South Africa","Public Health","","00333506","10.1016/S0033-3506(54)80136-3","https://linkinghub.elsevier.com/retrieve/pii/S0033350654801363","Large language models (LLMs) have recently demonstrated great success in generating and understanding natural language. While they have also shown potential beyond the domain of natural language, it remains an open question as to what extent and in which way these LLMs can plan. We investigate their planning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark consisting of diverse 2D grid-based game maps. An LLM succeeds if it can traverse through given objectives, with a minimum number of steps and a minimum number of generation errors. We evaluate a number of LLMs on GTB and found that GPT-4-Turbo achieved the highest score of 44.97% on GTB_Score (GTBS), a composite score that combines the three above criteria. Furthermore, we preliminarily test large reasoning models, namely o1, which scores 67.84% on GTBS, indicating that the benchmark remains challenging for current models. Code, data, and documentation are available at https://github.com/umair-nasir14/Game-Traversal-Benchmark.","1954-10","2025-03-30 16:31:15","2025-03-30 16:31:15","2025-03-30 16:31:15","182","","","68","","Public Health","","","","","","","","en","https://www.elsevier.com/tdm/userlicense/1.0/","","","","DOI.org (Crossref)","","","","/Users/nikolajmosgaardsomod/Zotero/storage/J4SUJBWQ/1954 - University of the Witwatersrand, Johannesburg, South Africa.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5WSDYAN","journalArticle","","Naug, Avisek; Guillen, Antonio; Luna, Ricardo; Gundecha, Vineet; Bash, Cullen; Ghorbanpour, Sahand; Mousavi, Sajad; Babu, Ashwin Ramesh; Markovikj, Dejan; Kashyap, Lekhapriya D; Rengarajan, Desik; Sarkar, Soumyendu","SustainDC: Benchmarking for Sustainable Data Center Control","","","","","","Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multiagent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities to improve data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for developing and benchmarking advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.","","2025-03-30 16:31:17","2025-03-30 16:31:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/BVZLR4CP/Naug et al. - SustainDC Benchmarking for Sustainable Data Center Control.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U3D3F2H7","journalArticle","","Zhou, Hangyu; Kao, Chia-Hsiang; Phoo, Cheng Perng; Mall, Utkarsh; Hariharan, Bharath; Bala, Kavita","AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in Satellite Imagery","","","","","","Clouds in satellite imagery pose a significant challenge for downstream applications. A major challenge in current cloud removal research is the absence of a comprehensive benchmark and a sufficiently large and diverse training dataset. To address this problem, we introduce the largest public dataset — AllClear for cloud removal, featuring 23,742 globally distributed regions of interest (ROIs) with diverse land-use patterns, comprising 4 million images in total. Each ROI includes complete temporal captures from the year 2022, with (1) multi-spectral optical imagery from Sentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from Sentinel-1, and (3) auxiliary remote sensing products such as cloud masks and land cover maps. We validate the effectiveness of our dataset by benchmarking performance, demonstrating the scaling law — the PSNR rises from 28.47 to 33.87 with 30× more data, and conducting ablation studies on the temporal length and the importance of individual modalities. This dataset aims to provide comprehensive coverage of the Earth’s surface and promote better cloud removal results.","","2025-03-30 16:31:18","2025-03-30 16:31:18","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/FQ56PCIB/Zhou et al. - AllClear A Comprehensive Dataset and Benchmark for Cloud Removal in Satellite Imagery.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XH3Z8EEK","journalArticle","","Cherian, Anoop; Peng, Kuan-Chuan; Lohit, Suhas; Matthiesen, Joanna; Smith, Kevin; Tenenbaum, Joshua B","Evaluating Large Vision-and-Language Models on Children’s Mathematical Olympiads","","","","","","Recent years have seen a significant progress in the general-purpose problem solving abilities of large vision and language models (LVLMs), such as ChatGPT, Gemini, etc.; some of these breakthroughs even seem to enable AI models to outperform human abilities in varied tasks that demand higher-order cognitive skills. Are the current large AI models indeed capable of generalized problem solving as humans do? A systematic analysis of AI capabilities for joint vision and text reasoning, however, is missing in the current scientific literature. In this paper, we make an effort towards filling this gap, by evaluating state-of-the-art LVLMs on their mathematical and algorithmic reasoning abilities using visuo-linguistic problems from children’s Olympiads. Specifically, we consider problems from the Mathematical Kangaroo (MK) Olympiad, which is a popular international competition targeted at children from grades 1-12, that tests children’s deeper mathematical abilities using puzzles that are appropriately gauged to their age and skills. Using the puzzles from MK, we created a dataset, dubbed SMART-840, consisting of 840 problems from years 2020-2024. With our dataset, we analyze LVLMs power on mathematical reasoning; their responses on our puzzles offer a direct way to compare against that of children. Our results show that modern LVLMs do demonstrate increasingly powerful reasoning skills in solving problems for higher grades, but lack the foundations to correctly answer problems designed for younger children. Further analysis shows that there is no significant correlation between the reasoning capabilities of AI models and that of young children, and their capabilities appear to be based on a different type of reasoning than the cumulative knowledge that underlies children’s mathematics and logic skills.","","2025-03-30 16:31:19","2025-03-30 16:31:19","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IXNP8EJZ/Cherian et al. - Evaluating Large Vision-and-Language Models on Children’s Mathematical Olympiads.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SA9JE3YR","journalArticle","","Lee, Kyungeun; Rhee, Wonjong","A Benchmark Suite for Evaluating Neural Mutual Information Estimators on Unstructured Datasets","","","","","","Mutual Information (MI) is a fundamental metric for quantifying dependency between two random variables. When we can access only the samples, but not the underlying distribution functions, we can evaluate MI using sample-based estimators. Assessment of such MI estimators, however, has almost always relied on analytical datasets including Gaussian multivariates. Such datasets allow analytical calculations of the true MI values, but they are limited in that they do not reflect the complexities of real-world datasets. This study introduces a comprehensive benchmark suite for evaluating neural MI estimators on unstructured datasets, specifically focusing on images and texts. By leveraging same-class sampling for positive pairing and introducing a binary symmetric channel trick, we show that we can accurately manipulate true MI values of real-world datasets. Using the benchmark suite, we investigate seven challenging scenarios, shedding light on the reliability of neural MI estimators for unstructured datasets.","","2025-03-30 16:31:20","2025-03-30 16:31:20","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/8BJGHDIE/Lee and Rhee - A Benchmark Suite for Evaluating Neural Mutual Information Estimators on Unstructured Datasets.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6IBQVQ4V","journalArticle","","Li, Zehui; Subasri, Vallijah; Stan, Guy-Bart; Zhao, Yiren; Wang, Bo","GV-Rep: A Large-Scale Dataset for Genetic Variant Representation Learning","","","","","","Genetic variants (GVs) are defined as differences in the DNA sequences among individuals and play a crucial role in diagnosing and treating genetic diseases. The rapid decrease in next generation sequencing cost, analogous to Moore’s Law, has led to an exponential increase in the availability of patient-level GV data. This growth poses a challenge for clinicians who must efficiently prioritize patientspecific GVs and integrate them with existing genomic databases to inform patient management. To addressing the interpretation of GVs, genomic foundation models (GFMs) have emerged. However, these models lack standardized performance assessments, leading to considerable variability in model evaluations. This poses the question: How effectively do deep learning methods classify unknown GVs and align them with clinically-verified GVs? We argue that representation learning, which transforms raw data into meaningful feature spaces, is an effective approach for addressing both indexing and classification challenges. We introduce a large-scale genetic variant dataset, named GV-Rep, featuring variable-length contexts and detailed annotations, designed for deep learning models to learn GV representations across various traits, diseases, tissue types, and experimental contexts. Our contributions are three-fold: (i) Construction of a comprehensive dataset with 7 million records, each labeled with characteristics of the corresponding variants, alongside additional data from 17,548 gene knockout tests across 1,107 cell types, 1,808 variant combinations, and 156 unique clinically-verified GVs from real-world patients. (ii) Analysis of the structure and properties of the dataset. (iii) Experimentation of the dataset with pre-trained genomic foundation models (GFMs). The results highlight a significant disparity between the current capabilities of GFMs and the accurate representation of GVs. We hope this dataset will advance genomic deep learning to bridge this gap.","","2025-03-30 16:31:22","2025-03-30 16:31:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/V28JF2CF/Li et al. - GV-Rep A Large-Scale Dataset for Genetic Variant Representation Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6CD6HJSI","journalArticle","","Werner, Thorben; Burchert, Johannes; Stubbemann, Maximilian; Schmidt-Thieme, Lars","A Cross-Domain Benchmark for Active Learning","","","","","","Active Learning (AL) deals with identifying the most informative samples for labeling to reduce data annotation costs for supervised learning tasks. AL research suffers from the fact that lifts from literature generalize poorly and that only a small number of repetitions of experiments are conducted. To overcome these obstacles, we propose CDALBench, the first active learning benchmark which includes tasks in computer vision, natural language processing and tabular learning. Furthermore, by providing an efficient, greedy oracle, CDALBench can be evaluated with 50 runs for each experiment. We show, that both the cross-domain character and a large amount of repetitions are crucial for sophisticated evaluation of AL research. Concretely, we show that the superiority of specific methods varies over the different domains, making it important to evaluate Active Learning with a cross-domain benchmark. Additionally, we show that having a large amount of runs is crucial. With only conducting three runs as often done in the literature, the superiority of specific methods can strongly vary with the specific runs. This effect is so strong, that, depending on the seed, even a well-established method’s performance can be significantly better and significantly worse than random for the same dataset.","","2025-03-30 16:31:23","2025-03-30 16:31:23","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/LH43CB3V/Werner et al. - A Cross-Domain Benchmark for Active Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGB54RWM","journalArticle","","Shao, Hao; Qian, Shengju; Xiao, Han; Song, Guanglu; Zong, Zhuofan; Wang, Letian; Liu, Yu; Li, Hongsheng","Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning","","","","","","Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges, we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally, about 98k pairs of them are annotated with detailed reasoning steps. Importantly, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available on this webpage to support further research in this area.","","2025-03-30 16:31:24","2025-03-30 16:31:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/G8X6X8FR/Shao et al. - Visual CoT Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Cha.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFS3ST54","journalArticle","","Sukthanker, Rhea Sanjay; Zela, Arber; Staffler, Benedikt; Klein, Aaron; Purucker, Lennart; Franke, Jörg K H; Hutter, Frank","HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models","","","","","","The increasing size of language models necessitates a thorough analysis across multiple dimensions to assess trade-offs among crucial hardware metrics such as latency, energy consumption, GPU memory usage, and performance. Identifying optimal model configurations under specific hardware constraints is becoming essential but remains challenging due to the computational load of exhaustive training and evaluation on multiple devices. To address this, we introduce HWGPT-Bench, a hardware-aware benchmark that utilizes surrogate predictions to approximate various hardware metrics across 13 devices of architectures in the GPT2 family, with architectures containing up to 1.55B parameters. Our surrogates, via calibrated predictions and reliable uncertainty estimates, faithfully model the heteroscedastic noise inherent in the energy and latency measurements. To estimate perplexity, we employ weight-sharing techniques from Neural Architecture Search (NAS), inheriting pretrained weights from the largest GPT-2 model. Finally, we demonstrate the utility of HW-GPT-Bench by simulating optimization trajectories of various multi-objective optimization algorithms in just a few seconds.","","2025-03-30 16:31:26","2025-03-30 16:31:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/SXP2Q4H6/Sukthanker et al. - HW-GPT-Bench Hardware-Aware Architecture Benchmark for Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HGTXYTQ","journalArticle","","Sun, Qingyun; Chen, Ziying; Yang, Beining; Ji, Cheng; Fu, Xingcheng; Zhou, Sheng; Peng, Hao; Li, Jianxin; Yu, Philip S","GC-Bench: An Open and Unified Benchmark for Graph Condensation","","","","","","Graph condensation (GC) has recently garnered considerable attention due to its ability to reduce large-scale graph datasets while preserving their essential properties. The core concept of GC is to create a smaller, more manageable graph that retains the characteristics of the original graph. Despite the proliferation of graph condensation methods developed in recent years, there is no comprehensive evaluation and in-depth analysis, which creates a great obstacle to understanding the progress in this field. To fill this gap, we develop a comprehensive Graph Condensation Benchmark (GC-Bench) to analyze the performance of graph condensation in different scenarios systematically. Specifically, GC-Bench systematically investigates the characteristics of graph condensation in terms of the following dimensions: effectiveness, transferability, and complexity. We comprehensively evaluate 12 state-of-the-art graph condensation algorithms in node-level and graphlevel tasks and analyze their performance in 12 diverse graph datasets. Further, we have developed an easy-to-use library for training and evaluating different GC methods to facilitate reproducible research. The GC-Bench library is available at https://github.com/RingBDStack/GC-Bench.","","2025-03-30 16:31:27","2025-03-30 16:31:27","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/LUZ8T387/Sun et al. - GC-Bench An Open and Unified Benchmark for Graph Condensation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VLPYXRXB","journalArticle","","Zhao, Haozhe; Ma, Xiaojian; Chen, Liang; Si, Shuzheng; Wu, Rujie; An, Kaikai; Yu, Peiyu; Zhang, Minjia; Li, Qing; Chang, Baobao","UltraEdit: Instruction-based Fine-Grained Image Editing at Scale","","","","","","This paper presents ULTRAEDIT, a large-scale (~4M editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix [10] and MagicBrush [71], and provide a systematic approach to producing massive and highquality image editing samples. ULTRAEDIT offers several distinct advantages: 1) It features a broader range of editing instructions by leveraging the creativity of large language models (LLMs) alongside in-context editing examples from human raters; 2) Its data sources are based on real images, including photographs and artworks, which provide greater diversity and reduced bias compared to datasets solely generated by text-to-image models; 3) It also supports region-based editing, enhanced by high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on ULTRAEDIT set new records on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models are available in github.com/pkunlp-icler/UltraEdit.","","2025-03-30 16:31:29","2025-03-30 16:31:29","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/IWC86T9D/Zhao et al. - UltraEdit Instruction-based Fine-Grained Image Editing at Scale.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M95HSALJ","journalArticle","","Zhang, Dan; Hu, Ziniu; Zhoubian, Sining; Du, Zhengxiao; Yang, Kaiyu; Wang, Zihan; Yue, Yisong; Dong, Yuxiao; Tang, Jie","SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models","","","","","","Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs’ deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.","","2025-03-30 16:31:30","2025-03-30 16:31:30","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/CVVJWT3E/Zhang et al. - SciInstruct a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FLL6HW5L","journalArticle","","Allen, Matt; Dorr, Francisco; Gallego-Mejia, Joseph A; Martínez-Ferrer, Laura; Jungbluth, Anna; Kalaitzis, Freddie; Ramos-Pollán, Raúl","M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and Multispectral Data","","","","","","Satellite-based remote sensing has revolutionised the way we address global challenges in a rapidly evolving world. Huge quantities of Earth Observation (EO) data are generated by satellite sensors daily, but processing these large datasets for use in ML pipelines is technically and computationally challenging. Specifically, different types of EO data are often hosted on a variety of platforms, with differing degrees of availability for Python preprocessing tools. In addition, spatial alignment across data sources and data tiling for easier handling can present significant technical hurdles for novice users. While some preprocessed Earth observation datasets exist, their content is often limited to optical or near-optical wavelength data, which is ineffective at night or in adverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing technique based on microwave length radiation, offers a viable alternative. However, the application of machine learning to SAR has been limited due to a lack of ML-ready data and pipelines, particularly for the full diversity of SAR data, including polarimetry, coherence and interferometry. In this work, we introduce M3LEO, a multi-modal, multi-label Earth observation dataset that includes polarimetric, interferometric, and coherence SAR data derived from Sentinel-1, alongside multispectral Sentinel-2 imagery and a suite of auxiliary data describing terrain properties such as land use. M3LEO spans approximately 17M data chips, each measuring 4x4 km, across six diverse geographic regions. The dataset is complemented by a flexible PyTorch Lightning framework, with configuration management using Hydra, to accommodate its use across diverse ML applications in Earth observation. Additionally, we provide tools to process any dataset available on popular platforms such as Google Earth Engine for seamless integration with our framework. We show that the distribution shift in self-supervised embeddings is substantial across geographic regions, even when controlling for terrain properties. Data is available at huggingface.co/M3LEO, and code at github.com/spaceml-org/M3LEO.","","2025-03-30 16:31:31","2025-03-30 16:31:31","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/R4B5ABSA/Allen et al. - M3LEO A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and Mult.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YVHG9VDL","journalArticle","","Zhang, Jiasheng; Chen, Jialin; Yang, Menglin; Feng, Aosong; Liang, Shuang; Shao, Jie; Ying, Rex","DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs","","","","","","Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DyTAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at https://github.com/zjs123/DTGB.","","2025-03-30 16:31:33","2025-03-30 16:31:33","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/ACCEB6HG/Zhang et al. - DTGB A Comprehensive Benchmark for Dynamic Text-Attributed Graphs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASV6G9S7","journalArticle","","Chen, Houlun; Wang, Xin; Chen, Hong; Zhang, Zeyang; Feng, Wei; Huang, Bin; Jia, Jia; Zhu, Wenwu","VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding","","","","","","Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding, which hinders precise video moment localization when given finegrained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic VidEo-text annotation pipeline to generate captions with RelIable FInE-grained statics and Dynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR. Code and Datasets are in https://github.com/hlchen23/VERIFIED.","","2025-03-30 16:31:34","2025-03-30 16:31:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/TJQFL6Y9/Chen et al. - VERIFIED A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IF62GV8J","journalArticle","","Pal, Anisha; Kruk, Julia; Phute, Mansi; Bhattaram, Manognya; Yang, Diyi; Chau, Duen Horng; Hoffman, Judy","Semi-Truths: A Large-Scale Dataset of AI-Augmented Images for Evaluating Robustness of AI-Generated Image detectors","","","","","","Text-to-image diffusion models have impactful applications in art, design, and entertainment, yet these technologies also pose significant risks by enabling the creation and dissemination of misinformation. Although recent advancements have produced AI-generated image detectors that claim robustness against various augmentations, their true effectiveness remains uncertain. Do these detectors reliably identify images with different levels of augmentation? Are they biased toward specific scenes or data distributions? To investigate, we introduce SEMI-TRUTHS, featuring 27, 600 real images, 223, 400 masks, and 1, 329, 155 AI-augmented images that feature targeted and localized perturbations produced using diverse augmentation techniques, diffusion models, and data distributions. Each augmented image is accompanied by metadata for standardized and targeted evaluation of detector robustness. Our findings suggest that state-of-the-art detectors exhibit varying sensitivities to the types and degrees of perturbations, data distributions, and augmentation methods used, offering new insights into their performance and limitations. The code for the augmentation and evaluation pipeline is available at https://github.com/J-Kruk/SemiTruths.","","2025-03-30 16:31:35","2025-03-30 16:31:35","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/X9D7I78U/Pal et al. - Semi-Truths A Large-Scale Dataset of AI-Augmented Images for Evaluating Robustness of AI-Generated.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZIUQ4HV","journalArticle","","Arodi, Akshatha; Luck, Margaux; Bedwani, Jean-Luc; Zaimi, Aldo; Li, Ge; Pouliot, Nicolas; Beaudry, Julien; Caron, Gaétan Marceau","CableInspect-AD: An Expert-Annotated Anomaly Detection Dataset","","","","","","Machine learning models are increasingly being deployed in real-world contexts. However, systematic studies on their transferability to speciﬁc and critical applications are underrepresented in the research literature. An important example is visual anomaly detection (VAD) for robotic power line inspection. While existing VAD methods perform well in controlled environments, real-world scenarios present diverse and unexpected anomalies that current datasets fail to capture. To address this gap, we introduce CableInspect-AD, a high-quality, publicly available dataset created and annotated by domain experts from Hydro-Québec, a Canadian public utility. This dataset includes high-resolution images with challenging real-world anomalies, covering defects with varying severity levels. To address the challenges of collecting diverse anomalous and nominal examples for setting a detection threshold, we propose an enhancement to the celebrated PatchCore algorithm. This enhancement enables its use in scenarios with limited labeled data. We also present a comprehensive evaluation protocol based on cross-validation to assess models’ performances. We evaluate our Enhanced-PatchCore for few-shot and many-shot detection, and Vision-Language Models for zero-shot detection. While promising, these models struggle to detect all anomalies, highlighting the dataset’s value as a challenging benchmark for the broader research community. Project page: https://mila-iqia.github.io/cableinspect-ad/.","","2025-03-30 16:31:37","2025-03-30 16:31:37","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/THZD3DJN/Arodi et al. - CableInspect-AD An Expert-Annotated Anomaly Detection Dataset.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BA8EN5SW","journalArticle","","Defrance, MaryBeth; Buyl, Maarten; Bie, Tijl De","ABCFair: an Adaptable Benchmark approach for Comparing Fairness methods","","","","","","Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, these subtle differences make it highly complicated to benchmark fairness methods, as their performance can strongly depend on exactly how the bias mitigation problem was originally framed.","","2025-03-30 16:31:38","2025-03-30 16:31:38","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JEQFLIRG/Defrance et al. - ABCFair an Adaptable Benchmark approach for Comparing Fairness methods.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J88CX3VQ","journalArticle","","Leong, Jovin; Koa, Ming Di","SHDocs: A dataset, benchmark, and method to efficiently generate high-quality, real-world specular highlight data with near-perfect alignment","","","","","","","","2025-03-30 16:31:39","2025-03-30 16:31:39","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/PAZFSG2Z/Leong and Koa - SHDocs A dataset, benchmark, and method to efficiently generate high-quality, real-world specular h.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C385T6KG","journalArticle","","Saeed, Mehreen; Chan, Adrian; Mijar, Anupam; Moukarzel, Joseph; Habchi, Georges; Younes, Carlos; Elias, Amin; Wong, Chau-Wai; Khater, Akram","Muharaf: Manuscripts of Handwritten Arabic Dataset for Cursive Text Recognition","","","","","","We present the Manuscripts of Handwritten Arabic (Muharaf) dataset, which is a machine learning dataset consisting of more than 1,600 historic handwritten page images transcribed by experts in archival Arabic. Each document image is accompanied by spatial polygonal coordinates of its text lines as well as basic page elements. This dataset was compiled to advance the state of the art in handwritten text recognition (HTR), not only for Arabic manuscripts but also for cursive text in general. The Muharaf dataset includes diverse handwriting styles and a wide range of document types, including personal letters, diaries, notes, poems, church records, and legal correspondences. In this paper, we describe the data acquisition pipeline, notable dataset features, and statistics. We also provide a preliminary baseline result achieved by training convolutional neural networks using this data.","","2025-03-30 16:31:40","2025-03-30 16:31:40","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9X9A8IIA/Saeed et al. - Muharaf Manuscripts of Handwritten Arabic Dataset for Cursive Text Recognition.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIYWYZM5","journalArticle","","Kirk, Hannah Rose; Whitefield, Alexander; Röttger, Paul; Bean, Andrew; Margatina, Katerina; Ciro, Juan; Mosquera, Rafael; Bartolo, Max; Williams, Adina; He, He; Vidgen, Bertie; Hale, Scott A","The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models","","","","","","","","2025-03-30 16:31:42","2025-03-30 16:31:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/JYXYFTIQ/Kirk et al. - The PRISM Alignment Dataset What Participatory, Representative and Individualised Human Feedback Re.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EJHCXCWN","journalArticle","","Abdelnabi, Sahar; Gomaa, Amr; Sivaprasad, Sarath; Schönherr, Lea; Fritz, Mario","Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation","","","","","","There is a growing interest in using Large Language Models (LLMs) in multi-agent systems to tackle interactive real-world tasks that require effective collaboration and assessment of complex situations. Yet, we have a limited understanding of LLMs’ communication and decision-making abilities in multi-agent setups. The fundamental task of negotiation spans many key features of communication, such as cooperation, competition, and manipulation potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We create a testbed of complex multi-agent, multi-issue, and semantically rich negotiation games. To reach an agreement, agents must have strong arithmetic, inference, exploration, and planning capabilities while integrating them in a dynamic and multi-turn setup. We propose metrics to rigorously quantify agents’ performance and alignment with the assigned role. We provide procedures to create new games and increase the difﬁculty of games to have an evolving benchmark. Importantly, we evaluate critical safety aspects such as the interaction dynamics between agents inﬂuenced by greedy and adversarial players. Our benchmark is highly challenging; GPT-3.5 and small models mostly fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform in reaching agreement in non-cooperative and more difﬁcult games1.","","2025-03-30 16:31:44","2025-03-30 16:31:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/P8MVJ4EN/Abdelnabi et al. - Cooperation, Competition, and Maliciousness LLM-Stakeholders Interactive Negotiation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJ4CNPQS","journalArticle","","Pramanick, Shraman; Chellappa, Rama; Venugopalan, Subhashini","SPIQA: A Dataset for Multimodal Question Answering on Scientiﬁc Papers","","","","","","Seeking answers to questions within long scientiﬁc research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientiﬁc papers are limited in scale and focus solely on textual content. We introduce SPIQA (Scientiﬁc Paper Image Question Answering), the ﬁrst large-scale QA dataset speciﬁcally designed to interpret complex ﬁgures and tables within the context of scientiﬁc research articles across various domains of computer science. Leveraging the breadth of expertise and ability of multimodal large language models (MLLMs) to understand ﬁgures, we employ automatic and manual curation to create the dataset. We craft an information-seeking task on interleaved images and text that involves multiple images covering plots, charts, tables, schematic diagrams, and result visualizations. SPIQA comprises 270K questions divided into training, validation, and three different evaluation splits. Through extensive experiments with 12 prominent foundational models, we evaluate the ability of current multimodal systems to comprehend the nuanced aspects of research articles. Additionally, we propose a Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that allows ﬁne-grained, step-by-step assessment and improves model performance. We further explore the upper bounds of performance enhancement with additional textual information, highlighting its promising potential for future research and the dataset’s impact on revolutionizing how we interact with scientiﬁc literature.","","2025-03-30 16:31:45","2025-03-30 16:31:45","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/9AL6L83J/Pramanick et al. - SPIQA A Dataset for Multimodal Question Answering on Scientiﬁc Papers.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BA3ZHWMA","journalArticle","","Wang, Yuqi; Cheng, Ke; He, Jiawei; Wang, Qitai; Dai, Hengchen; Chen, Yuntao; Xia, Fei; Zhang, Zhaoxiang","DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model","","","","","","Driving world models have gained increasing attention due to their ability to model complex physical dynamics. However, their superb modeling capability is yet to be fully unleashed due to the limited video diversity in current driving datasets. We introduce DrivingDojo, the ﬁrst dataset tailor-made for training interactive world models with complex driving dynamics. Our dataset features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. We further deﬁne an action instruction following (AIF) benchmark for world models and demonstrate the superiority of the proposed dataset for generating action-controlled future predictions.","","2025-03-30 16:31:46","2025-03-30 16:31:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/nikolajmosgaardsomod/Zotero/storage/KSLGWXW6/Wang et al. - DrivingDojo Dataset Advancing Interactive and Knowledge-Enriched Driving World Model.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PXAN75Y4","preprint","2024","Bean, Andrew M.; Hellsten, Simi; Mayne, Harry; Magomere, Jabez; Chi, Ethan A.; Chi, Ryan; Hale, Scott A.; Kirk, Hannah Rose","LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages","","","","10.48550/arXiv.2406.06196","http://arxiv.org/abs/2406.06196","In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.","2024-10-31","2025-03-30 18:07:16","2025-03-30 18:07:16","2025-03-30 18:07:16","","","","","","","LINGOLY","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2406.06196 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/RULLF2W4/Bean et al. - 2024 - LINGOLY A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Lang.pdf","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2406.06196","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LA6KYGKV","preprint","2024","Jin, Zhuoran; Cao, Pengfei; Wang, Chenhao; He, Zhitao; Yuan, Hongbang; Li, Jiachun; Chen, Yubo; Liu, Kang; Zhao, Jun","RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models","","","","10.48550/arXiv.2406.10890","http://arxiv.org/abs/2406.10890","Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.","2024-06-16","2025-03-30 18:07:57","2025-03-30 18:07:57","2025-03-30 18:07:57","","","","","","","RWKU","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2406.10890 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/Y4KGCBP7/Jin et al. - 2024 - RWKU Benchmarking Real-World Knowledge Unlearning for Large Language Models.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2406.10890","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAY2NUD7","preprint","2025","Zhang, Jieyu; Huang, Weikai; Ma, Zixian; Michel, Oscar; He, Dong; Gupta, Tanmay; Ma, Wei-Chiu; Farhadi, Ali; Kembhavi, Aniruddha; Krishna, Ranjay","Task Me Anything","","","","10.48550/arXiv.2406.11775","http://arxiv.org/abs/2406.11775","Benchmarks for large multimodal language models (MLMs) now serve to simultaneously assess the general capabilities of models instead of evaluating for a specific capability. As a result, when a developer wants to identify which models to use for their application, they are overwhelmed by the number of benchmarks and remain uncertain about which benchmark's results are most reflective of their specific use case. This paper introduces Task-Me-Anything, a benchmark generation engine which produces a benchmark tailored to a user's needs. Task-Me-Anything maintains an extendable taxonomy of visual assets and can programmatically generate a vast number of task instances. Additionally, it algorithmically addresses user queries regarding MLM performance efficiently within a computational budget. It contains 113K images, 10K videos, 2K 3D object assets, over 365 object categories, 655 attributes, and 335 relationships. It can generate 750M image/video question-answering pairs, which focus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals critical insights: open-source MLMs excel in object and attribute recognition but lack spatial and temporal understanding; each model exhibits unique strengths and weaknesses; larger models generally perform better, though exceptions exist; and GPT4o demonstrates challenges in recognizing rotating/moving objects and distinguishing colors.","2025-01-27","2025-03-30 18:08:59","2025-03-30 18:08:59","2025-03-30 18:08:59","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2406.11775 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/39FNNN9G/Zhang et al. - 2025 - Task Me Anything.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2406.11775","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JAU7B4GH","preprint","2024","Kim, Hee Jae; Sengupta, Kathakoli; Kuribayashi, Masaki; Kacorri, Hernisa; Ohn-Bar, Eshed","Text to Blind Motion","","","","10.48550/arXiv.2412.05277","http://arxiv.org/abs/2412.05277","People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io.","2024-12-06","2025-03-30 18:09:53","2025-03-30 18:09:53","2025-03-30 18:09:53","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2412.05277 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/7A85BVKW/Kim et al. - 2024 - Text to Blind Motion.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2412.05277","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C29X6MZN","preprint","2025","Bassi, Pedro R. A. S.; Li, Wenxuan; Tang, Yucheng; Isensee, Fabian; Wang, Zifu; Chen, Jieneng; Chou, Yu-Cheng; Kirchhoff, Yannick; Rokuss, Maximilian; Huang, Ziyan; Ye, Jin; He, Junjun; Wald, Tassilo; Ulrich, Constantin; Baumgartner, Michael; Roy, Saikat; Maier-Hein, Klaus H.; Jaeger, Paul; Ye, Yiwen; Xie, Yutong; Zhang, Jianpeng; Chen, Ziyang; Xia, Yong; Xing, Zhaohu; Zhu, Lei; Sadegheih, Yousef; Bozorgpour, Afshin; Kumari, Pratibha; Azad, Reza; Merhof, Dorit; Shi, Pengcheng; Ma, Ting; Du, Yuxin; Bai, Fan; Huang, Tiejun; Zhao, Bo; Wang, Haonan; Li, Xiaomeng; Gu, Hanxue; Dong, Haoyu; Yang, Jichen; Mazurowski, Maciej A.; Gupta, Saumya; Wu, Linshan; Zhuang, Jiaxin; Chen, Hao; Roth, Holger; Xu, Daguang; Blaschko, Matthew B.; Decherchi, Sergio; Cavalli, Andrea; Yuille, Alan L.; Zhou, Zongwei","Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?","","","","10.48550/arXiv.2411.03670","http://arxiv.org/abs/2411.03670","How can we test AI performance? This question seems trivial, but it isn't. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone, a large-scale collaborative segmentation benchmark of 9 types of abdominal organs. This benchmark is based on 5,195 training CT scans from 76 hospitals around the world and 5,903 testing CT scans from 11 additional hospitals. This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across various out-of-distribution scenarios. We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms on three test sets. In addition, we also evaluated pre-existing AI frameworks--which, differing from algorithms, are more flexible and can support different algorithms--including MONAI from NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are committed to expanding this benchmark to encourage more innovation of AI algorithms for the medical domain.","2025-01-20","2025-03-30 18:10:49","2025-03-30 18:10:49","2025-03-30 18:10:49","","","","","","","Touchstone Benchmark","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2411.03670 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/YZGCIXZC/Bassi et al. - 2025 - Touchstone Benchmark Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2411.03670","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UWSSZJTM","preprint","2024","Zhang, Yihua; Fan, Chongyu; Zhang, Yimeng; Yao, Yuguang; Jia, Jinghan; Liu, Jiancheng; Zhang, Gaoyuan; Liu, Gaowen; Kompella, Ramana Rao; Liu, Xiaoming; Liu, Sijia","UnlearnCanvas: Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models","","","","10.48550/arXiv.2402.11846","http://arxiv.org/abs/2402.11846","The technological advancements in diffusion models (DMs) have demonstrated unprecedented capabilities in text-to-image generation and are widely used in diverse applications. However, they have also raised significant societal concerns, such as the generation of harmful content and copyright disputes. Machine unlearning (MU) has emerged as a promising solution, capable of removing undesired generative capabilities from DMs. However, existing MU evaluation systems present several key challenges that can result in incomplete and inaccurate assessments. To address these issues, we propose UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates the evaluation of the unlearning of artistic styles and associated objects. This dataset enables the establishment of a standardized, automated evaluation framework with 7 quantitative metrics assessing various aspects of the unlearning performance for DMs. Through extensive experiments, we benchmark 9 state-of-the-art MU methods for DMs, revealing novel insights into their strengths, weaknesses, and underlying mechanisms. Additionally, we explore challenging unlearning scenarios for DMs to evaluate worst-case performance against adversarial prompts, the unlearning of finer-scale concepts, and sequential unlearning. We hope that this study can pave the way for developing more effective, accurate, and robust DM unlearning methods, ensuring safer and more ethical applications of DMs in the future. The dataset, benchmark, and codes are publicly available at https://unlearn-canvas.netlify.app/.","2024-10-29","2025-03-30 18:11:16","2025-03-30 18:11:16","2025-03-30 18:11:16","","","","","","","UnlearnCanvas","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.11846 [cs]","","/Users/nikolajmosgaardsomod/Zotero/storage/VMSB8A6F/Zhang et al. - 2024 - UnlearnCanvas Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2402.11846","","","","","","","","","","","","","","","","","","","","","","","","","","",""