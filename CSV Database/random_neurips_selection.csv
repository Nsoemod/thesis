Key,Item Type,Publication Year,Author,Title,Publication Title,DOI,Url,Abstract Note,Date,Conference Name,Source File,Field,Novel Database,Representativity Mentions,Similarity Mentions,Diversity Mentions,Notes
52G7S892,journalArticle,,"Nikulin, Alexander; Kurenkov, Vladislav; Zisman, Ilya; Agarkov, Artem; Sinii, Viacheslav; Kolesnikov, Sergey",XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX,,,,"Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging. XLand-MiniGrid is open-source and available at https://github.com/corl-team/xland-minigrid.",,,NeurIPS.csv,,,,,,
QIZLL694,journalArticle,,"Baek, Jae-Yong; Yoo, Yong-Sang; Bae, Seung-Hwan",A New Multi-Source Light Detection Benchmark and Semi-Supervised Focal Light Detection,,,,"This paper addresses a multi-source light detection (LD) problem from vehicles, trafﬁc signals, and streetlights under driving scenarios. Albeit it is crucial for autonomous driving and night vision, this problem has not been yet focused on as much as other object detection (OD). One of the main reasons is the absence of a public available LD benchmark dataset. Therefore, we construct a new large LD dataset consisting of different light sources via heavy annotation:YouTube Driving Light Detection dataset (YDLD). Compared to the existing LD datasets, our dataset has much more images and box annotations for multi-source lights. We also provide rigorous statistical analysis and transfer learning comparison of other well-known detection benchmark datasets to prove the generality of our YDLD.",,,NeurIPS.csv,,,,,,
FQSW9S2F,journalArticle,,"Shao, Minghao; Jancheska, Sofija; Udeshi, Meet; Dolan-Gavitt, Brendan; Xi, Haoran; Milner, Kimberly; Chen, Boyuan; Yin, Max; Garg, Siddharth; Krishnamurthy, Prashanth; Khorrami, Farshad; Karri, Ramesh; Shafique, Muhammad",NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security,,,,"Large Language Models (LLMs) are being deployed across various domains today. However, their capacity to solve Capture the Flag (CTF) challenges in cybersecurity has not been thoroughly evaluated. To address this, we develop a novel method to assess LLMs in solving CTF challenges by creating a scalable, open-source benchmark database specifically designed for these applications. This database includes metadata for LLM testing and adaptive learning, compiling a diverse range of CTF challenges from popular competitions. Utilizing the advanced function calling capabilities of LLMs, we build a fully automated system with an enhanced workflow and support for external tool calls. Our benchmark dataset and automated framework allow us to evaluate the performance of five LLMs, encompassing both black-box and open-source models. This work lays the foundation for future research into improving the efficiency of LLMs in interactive cybersecurity tasks and automated task planning. By providing a specialized benchmark, our project offers an ideal platform for developing, testing, and refining LLM-based approaches to vulnerability detection and resolution. Evaluating LLMs on these challenges and comparing with human performance yields insights into their potential for AI-driven cybersecurity solutions to perform real-world threat management. We make our benchmark dataset open source to public https://github.com/NYU-LLM-CTF/NYU_CTF_Bench along with our playground automated framework https://github.com/NYU-LLM-CTF/llm_ ctf_automation.",,,NeurIPS.csv,,,,,,
8HGTXYTQ,journalArticle,,"Sun, Qingyun; Chen, Ziying; Yang, Beining; Ji, Cheng; Fu, Xingcheng; Zhou, Sheng; Peng, Hao; Li, Jianxin; Yu, Philip S",GC-Bench: An Open and Unified Benchmark for Graph Condensation,,,,"Graph condensation (GC) has recently garnered considerable attention due to its ability to reduce large-scale graph datasets while preserving their essential properties. The core concept of GC is to create a smaller, more manageable graph that retains the characteristics of the original graph. Despite the proliferation of graph condensation methods developed in recent years, there is no comprehensive evaluation and in-depth analysis, which creates a great obstacle to understanding the progress in this field. To fill this gap, we develop a comprehensive Graph Condensation Benchmark (GC-Bench) to analyze the performance of graph condensation in different scenarios systematically. Specifically, GC-Bench systematically investigates the characteristics of graph condensation in terms of the following dimensions: effectiveness, transferability, and complexity. We comprehensively evaluate 12 state-of-the-art graph condensation algorithms in node-level and graphlevel tasks and analyze their performance in 12 diverse graph datasets. Further, we have developed an easy-to-use library for training and evaluating different GC methods to facilitate reproducible research. The GC-Bench library is available at https://github.com/RingBDStack/GC-Bench.",,,NeurIPS.csv,,,,,,
66X94SUX,journalArticle,,"Vendrow, Edward; Pantazis, Omiros; Shepard, Alexander; Brostow, Gabriel; Jones, Kate E; Aodha, Oisin Mac; Beery, Sara; Horn, Grant Van",INQUIRE: A Natural World Text-to-Image Retrieval Benchmark,,,,"We introduce INQUIRE, a text-to-image retrieval benchmark designed to challenge multimodal vision-language models on expert-level queries. INQUIRE includes iNaturalist 2024 (iNat24), a new dataset of ﬁve million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identiﬁcation, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) INQUIRE-FULLRANK, a full dataset ranking task, and (2) INQUIRE-RERANK, a reranking task for reﬁning top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that INQUIRE poses a signiﬁcant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a signiﬁcant margin for improvement. By focusing on scientiﬁcally-motivated ecological challenges, INQUIRE aims to bridge the gap between AI capabilities and the needs of real-world scientiﬁc inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.",,,NeurIPS.csv,,,,,,
J8EMV5YS,journalArticle,,"Wen, Bosi; Ke, Pei; Gu, Xiaotao; Wu, Lindong; Huang, Hao; Zhou, Jinfeng; Li, Wenchuang; Hu, Binxin; Gao, Wendy; Xu, Jiaxin; Liu, Yiming; Tang, Jie; Wang, Hongning; Huang, Minlie",Benchmarking Complex Instruction-Following with Multiple Constraints Composition,,,,"Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose ComplexBench, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition. Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. ComplexBench identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition1.",,,NeurIPS.csv,,,,,,
WC3M3QMN,journalArticle,,"Jiang, Yifan; Zhang, Jiarui; Sun, Kexuan; Sourati, Zhivar; Ahrabian, Kian; Ma, Kaixin; Ilievski, Filip; Pujara, Jay",MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning,,,,"While multi-modal large language models (MLLMs) have shown significant progress across popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints on numbers) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix). However, existing AVR benchmarks only consider a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 × 3 matrices). And they fail to capture all abstract reasoning patterns in human cognition necessary for addressing real-world tasks, such as geometric properties and object boundary understanding in realworld navigation. To evaluate MLLMs’ AVR abilities systematically, we introduce MARVEL founded on the core knowledge system in human cognition, a multidimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model performance is grounded in perception or reasoning, MARVEL complements the standard AVR question with perception questions in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with ten representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all MLLMs show near-random performance on MARVEL, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance). Although closed-source MLLMs, such as GPT-4V, show a promising understanding of reasoning patterns (on par with humans) after adding textual descriptions, this advantage is hindered by their weak perception abilities. We release our entire code and dataset at https://github.com/1171-jpg/MARVEL_AVR.",,,NeurIPS.csv,,,,,,
HLKWZ4RU,journalArticle,,"Shen, Xin; Du, Heming; Sheng, Hongwei; Wang, Shuyun; Chen, Hui; Chen, Huiqiang; Wu, Zhuojie; Du, Xiaobiao; Ying, Jiaying; Lu, Ruihan; Xu, Qingzheng; Yu, Xin",MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset,,,,"Isolated Sign Language Recognition (ISLR) focuses on identifying individual sign language signs. Considering the diversity of sign languages across geographical regions, developing region-specific ISLR datasets is crucial for supporting communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale word-level dataset for the ISLR task. To fill this gap, we curate the first large-scale Multi-view Multi-modal Word-Level Australian Sign Language recognition dataset, dubbed MM-WLAuslan. Compared to other publicly available datasets, MM-WLAuslan exhibits three significant advantages: (1) the largest amount of data, (2) the most extensive vocabulary, and (3) the most diverse of multi-modal camera views. Specifically, we record 282K+ sign videos covering 3,215 commonly used Auslan glosses presented by 73 signers in a studio environment. Moreover, our filming system includes two different types of cameras, i.e., three Kinect-V2 cameras and a RealSense camera. We position cameras hemispherically around the front half of the model and simultaneously record videos using all four cameras. Furthermore, we benchmark results with state-of-the-art methods for various multi-modal ISLR settings on MM-WLAuslan, including multi-view, cross-camera, and cross-view. Experiment results indicate that MM-WLAuslan is a challenging ISLR dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide. All datasets and benchmarks are available at  MM-WLAuslan.",,,NeurIPS.csv,,,,,,
4SZVXNRT,journalArticle,,"Liu, Puze; Günster, Jonas; Funk, Niklas; Gröger, Simon; Chen, Dong; Bou-Ammar, Haitham; Jankowski, Julius; Maric, Ante; Calinon, Sylvain; Orsula, Andrej; Olivares-Mendez, Miguel; Zhou, Hongyi; Lioutikov, Rudolf; Neumann, Gerhard; Likmeta, Amarildo; Zhalehmehrabi, Amirhossein; Bonenfant, Thomas; Restelli, Marcello; Tateo, Davide; Liu, Ziyuan; Peters, Jan","A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics",,,,"Machine learning methods have a groundbreaking impact in many application domains, but their application on real robotic platforms is still limited. Despite the many challenges associated with combining machine learning technology with robotics, robot learning remains one of the most promising directions for enhancing the capabilities of robots. When deploying learning-based approaches on real robots, extra effort is required to address the challenges posed by various real-world factors. To investigate the key factors influencing real-world deployment and to encourage original solutions from different researchers, we organized the Robot Air Hockey Challenge at the NeurIPS 2023 conference. We selected the air hockey task as a benchmark, encompassing low-level robotics problems and high-level tactics. Different from other machine learning-centric benchmarks, participants need to tackle practical challenges in robotics, such as the sim-toreal gap, low-level control issues, safety problems, real-time requirements, and the limited availability of real-world data. Furthermore, we focus on a dynamic environment, removing the typical assumption of quasi-static motions of other real-world benchmarks. The competition’s results show that solutions combining learning-based approaches with prior knowledge outperform those relying solely on data when real-world deployment is challenging. Our ablation study reveals which real-world factors may be overlooked when building a learning-based solution. The successful real-world air hockey deployment of best-performing agents sets the foundation for future competitions and follow-up research directions.",,,NeurIPS.csv,,,,,,
92L5PX3Q,journalArticle,,"Deng, Hexuan; Jiao, Wenxiang; Liu, Xuebo; Zhang, Min; Tu, Zhaopeng",NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates,,,,"Despite their remarkable abilities in various tasks, large language models (LLMs) still struggle with real-time information (e.g., new facts and terms) due to the knowledge cutoff in their development process. However, existing benchmarks focus on outdated content and limited fields, facing difficulties in real-time updating and leaving new terms unexplored. To address this problem, we propose an adaptive benchmark, NewTerm, for real-time evaluation of new terms. We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort, allowing flexible updates for real-time information. Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms. Additionally, while updates to the knowledge cutoff of LLMs can cover some of the new terms, they are unable to generalize to more distant new terms. We also analyze which types of terms are more challenging and why LLMs struggle with new terms, paving the way for future research. Finally, we construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually. The benchmark and codes can be found at https://github.com/hexuandeng/NewTerm.",,,NeurIPS.csv,,,,,,
EJHCXCWN,journalArticle,,"Abdelnabi, Sahar; Gomaa, Amr; Sivaprasad, Sarath; Schönherr, Lea; Fritz, Mario","Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation",,,,"There is a growing interest in using Large Language Models (LLMs) in multi-agent systems to tackle interactive real-world tasks that require effective collaboration and assessment of complex situations. Yet, we have a limited understanding of LLMs’ communication and decision-making abilities in multi-agent setups. The fundamental task of negotiation spans many key features of communication, such as cooperation, competition, and manipulation potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We create a testbed of complex multi-agent, multi-issue, and semantically rich negotiation games. To reach an agreement, agents must have strong arithmetic, inference, exploration, and planning capabilities while integrating them in a dynamic and multi-turn setup. We propose metrics to rigorously quantify agents’ performance and alignment with the assigned role. We provide procedures to create new games and increase the difﬁculty of games to have an evolving benchmark. Importantly, we evaluate critical safety aspects such as the interaction dynamics between agents inﬂuenced by greedy and adversarial players. Our benchmark is highly challenging; GPT-3.5 and small models mostly fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform in reaching agreement in non-cooperative and more difﬁcult games1.",,,NeurIPS.csv,,,,,,
5FG2HLT5,journalArticle,,"Wang, Yubo; Ma, Xueguang; Zhang, Ge; Ni, Yuansheng; Chandra, Abhranil; Guo, Shiguang; Ren, Weiming; Arulraj, Aaran; He, Xuan; Jiang, Ziyan; Li, Tianle; Ku, Max; Wang, Kai; Zhuang, Alex; Fan, Rongqi; Yue, Xiang; Chen, Wenhu",MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,,,,"In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.",,,NeurIPS.csv,,,,,,
D38SLIPD,journalArticle,,"Longjohn, Rachel; Kelly, Markelle; Singh, Sameer; Smyth, Padhraic",Benchmark Data Repositories for Better Benchmarking,,,,"In machine learning research, it is common to evaluate algorithms via their performance on standard benchmark datasets. While a growing body of work establishes guidelines for—and levies criticisms at—data and benchmarking practices in machine learning, comparatively less attention has been paid to the data repositories where these datasets are stored, documented, and shared. In this paper, we analyze the landscape of these benchmark data repositories and the role they can play in improving benchmarking. This role includes addressing issues with both datasets themselves (e.g., representational harms, construct validity) and the manner in which evaluation is carried out using such datasets (e.g., overemphasis on a few datasets and metrics, lack of reproducibility). To this end, we identify and discuss a set of considerations surrounding the design and use of benchmark data repositories, with a focus on improving benchmarking practices in machine learning.",,,NeurIPS.csv,,,,,,
5E7P5EAS,preprint,2024.0,"Salehi, Mohammadreza; Park, Jae Sung; Yadav, Tanush; Kusupati, Aditya; Krishna, Ranjay; Choi, Yejin; Hajishirzi, Hannaneh; Farhadi, Ali",ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition,,10.48550/arXiv.2410.05774,http://arxiv.org/abs/2410.05774,"Our world is full of varied actions and moves across specialized domains that we, as humans, strive to identify and understand. Within any single domain, actions can often appear quite similar, making it challenging for deep models to distinguish them accurately. To evaluate the effectiveness of multimodal foundation models in helping us recognize such actions, we present ActionAtlas v1.0, a multiple-choice video question-answering benchmark featuring short videos across various sports. Each video in the dataset is paired with a question and four or five choices. The question pinpoints specific individuals, asking which choice “best” describes their action within a certain temporal context. Overall, the dataset includes 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices. Unlike most existing video question answering benchmarks that only cover simplistic actions, often identifiable from a single frame, ActionAtlas focuses on intricate movements and rigorously tests the model’s capability to discern subtle differences between moves that look similar within each domain. We evaluate open and proprietary foundation models on this benchmark, finding that the best model, GPT-4o, achieves a maximum accuracy of 45.52%. Meanwhile, Nonexpert crowd workers, provided with action description for each choice, achieve 61.64% accuracy, where random chance is approximately 21%. Our findings with state-of-the-art models indicate that having a high frame sampling rate is important for accurately recognizing actions in ActionAtlas, a feature that some leading proprietary video models, such as Gemini, do not include in their default configurations.",2024-11-11,,NeurIPS.csv,,,,,,
2YGUGXND,journalArticle,,"Wang, Aoran; Tong, Tsz Pan; Mizera, Andrzej; Pang, Jun",Benchmarking Structural Inference Methods for Interacting Dynamical Systems with Synthetic Data,,,,"Understanding complex dynamical systems begins with identifying their topological structures, which expose the organization of the systems. This requires robust structural inference methods that can deduce structure from observed behavior. However, existing methods are often domain-specific and lack a standardized, objective comparison framework. We address this gap by benchmarking 13 structural inference methods from various disciplines on simulations representing two types of dynamics and 11 interaction graph models, supplemented by a biological experimental dataset to mirror real-world application. We evaluated the methods for accuracy, scalability, robustness, and sensitivity to graph properties. Our findings indicate that deep learning methods excel with multi-dimensional data, while classical statistics and information theory based approaches are notably accurate and robust. Additionally, performance correlates positively with the graph’s average shortest path length. This benchmark should aid researchers in selecting suitable methods for their specific needs and stimulate further methodological innovation. Project website: https://structinfer.github.io/.",,,NeurIPS.csv,,,,,,
EY2IKJZS,journalArticle,,"Wu, Junchao; Zhan, Runzhe; Wong, Derek F; Yang, Shu; Yang, Xinyi; Yuan, Yulin; Chao, Lidia S",DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios,,,,"Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating various prompts usages, human revisions like word substitutions, and writing noises like spelling mistakes. Our development of DetectRL reveals the strengths and limitations of current SOTA detectors. More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe DetectRL could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors2.",,,NeurIPS.csv,,,,,,
UZUQP9HF,journalArticle,,"Bukas, Christina; Subramanian, Harshavardhan; See, Fenja; Steinchen, Carina; Ezhov, Ivan; Boosarpu, Gowtham; Asgharpour, Sara; Burgstaller, Gerald; Lehmann, Mareike; Kofler, Florian; Piraud, Marie",MultiOrg: A Multi-rater Organoid-detection Dataset,,,,"High-throughput image analysis in the biomedical domain has gained significant attention in recent years, driving advancements in drug discovery, disease prediction, and personalized medicine. Organoids, specifically, are an active area of research, providing excellent models for human organs and their functions. Automating the quantification of organoids in microscopy images would provide an effective solution to overcome substantial manual quantification bottlenecks, particularly in high-throughput image analysis. However, there is a notable lack of open biomedical datasets, in contrast to other domains, such as autonomous driving, and, notably, only few of them have attempted to quantify annotation uncertainty. In this work, we present MultiOrg a comprehensive organoid dataset tailored for object detection tasks with uncertainty quantification. This dataset comprises over 400 high-resolution 2d microscopy images and curated annotations of more than 60,000 organoids. Most importantly, it includes three label sets for the test data, independently annotated by two experts at distinct time points. We additionally provide a benchmark for organoid detection, and make the best model available through an easily installable, interactive plugin for the popular image visualization tool Napari, to perform organoid quantification.",,,NeurIPS.csv,,,,,,
VSBD3SL6,journalArticle,,"Choi, Yejin; Chung, Jiwan; Shim, Sumin; Oh, Giyeong; Yu, Youngjae",Towards Visual Text Design Transfer Across Languages,,,,"Visual text design plays a critical role in conveying themes, emotions, and atmospheres in multimodal formats such as film posters and album covers. Translating these visual and textual elements across languages extends the concept of translation beyond mere text, requiring the adaptation of aesthetic and stylistic features. To address this, we introduce a novel task of Multimodal Style Translation (MuSTBench), a benchmark designed to evaluate the ability of visual text generation models to perform translation across different writing systems while preserving design intent. Our initial experiments on MuST-Bench reveal that existing visual text generation models struggle with the proposed task due to the inadequacy of textual descriptions in conveying visual design. In response, we introduce SIGIL, a framework for multimodal style translation that eliminates the need for style descriptions. SIGIL enhances image generation models through three innovations: glyph latent for multilingual settings, pre-trained VAEs for stable style guidance, and an OCR model with reinforcement learning feedback for optimizing readable character generation. SIGIL outperforms existing baselines by achieving superior style consistency and legibility while maintaining visual fidelity, setting itself apart from traditional description-based approaches. We release MuST-Bench publicly for broader use and exploration1.",,,NeurIPS.csv,,,,,,
BA8EN5SW,journalArticle,,"Defrance, MaryBeth; Buyl, Maarten; Bie, Tijl De",ABCFair: an Adaptable Benchmark approach for Comparing Fairness methods,,,,"Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, these subtle differences make it highly complicated to benchmark fairness methods, as their performance can strongly depend on exactly how the bias mitigation problem was originally framed.",,,NeurIPS.csv,,,,,,
IHBLLIBV,dataset,2025.0,"Hauser, Jakob Elias",HiST-LLM,,10.5281/ZENODO.14671247,https://zenodo.org/doi/10.5281/zenodo.14671247,"Large Language Models (LLMs) have the potential to transform humanities and social science research, yet their history knowledge and comprehension at a graduate level remains untested. Benchmarking LLMs in history is particularly challenging, given that human knowledge of history is inherently unbalanced, with more information available on Western history and recent periods. We introduce the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over 2,700 scholarly references. This dataset covers every major world region from the Neolithic period to the Industrial Revolution and includes information reviewed and assembled by history experts and graduate research assistants. Using this dataset, we benchmark a total of seven models from the Gemini, OpenAI, and Llama families. We find that, in a four-choice format, LLMs have a balanced accuracy ranging from 33.6% (Llama-3.1-8B) to 46% (GPT-4-Turbo), outperforming random guessing (25%) but falling short of expert comprehension. LLMs perform better on earlier historical periods. Regionally, performance is more even but still better for the Americas and lowest in Oceania and Sub-Saharan Africa for the more advanced models. Our benchmark shows that while LLMs possess some expert-level historical knowledge, there is considerable room for improvement.",2025-01-16,,NeurIPS.csv,,,,,,
E893SYR9,journalArticle,,"Dong, Zibin; Yuan, Yifu; Hao, Jianye; Ni, Fei; Ma, Yi; Li, Pengyi; Zheng, Yan",CleanDiffuser: An Easy-to-use Modularized Library for Diffusion Models in Decision Making,,,,"Leveraging the powerful generative capability of diffusion models (DMs) to build decision-making agents has achieved extensive success. However, there is still a demand for an easy-to-use and modularized open-source library that offers customized and efficient development for DM-based decision-making algorithms. In this work, we introduce CleanDiffuser, the first DM library specifically designed for decision-making algorithms. By revisiting the roles of DMs in the decisionmaking domain, we identify a set of essential sub-modules that constitute the core of CleanDiffuser, allowing for the implementation of various DM algorithms with simple and flexible building blocks. To demonstrate the reliability and flexibility of CleanDiffuser, we conduct comprehensive evaluations of various DM algorithms implemented with CleanDiffuser across an extensive range of tasks. The analytical experiments provide a wealth of valuable design choices and insights, reveal opportunities and challenges, and lay a solid groundwork for future research. CleanDiffuser will provide long-term support to the decision-making community, enhancing reproducibility and fostering the development of more robust solutions. The code and documentation of CleanDiffuser are open-sourced on the project website.",,,NeurIPS.csv,,,,,,
2XIPE3PA,journalArticle,,"Shao, Yijia; Li, Tianshi; Shi, Weiyan; Liu, Yanchen; Yang, Diyi",PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action,,,,"As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents’ actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-ofthe-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.",,,NeurIPS.csv,,,,,,
C29X6MZN,preprint,2025.0,"Bassi, Pedro R. A. S.; Li, Wenxuan; Tang, Yucheng; Isensee, Fabian; Wang, Zifu; Chen, Jieneng; Chou, Yu-Cheng; Kirchhoff, Yannick; Rokuss, Maximilian; Huang, Ziyan; Ye, Jin; He, Junjun; Wald, Tassilo; Ulrich, Constantin; Baumgartner, Michael; Roy, Saikat; Maier-Hein, Klaus H.; Jaeger, Paul; Ye, Yiwen; Xie, Yutong; Zhang, Jianpeng; Chen, Ziyang; Xia, Yong; Xing, Zhaohu; Zhu, Lei; Sadegheih, Yousef; Bozorgpour, Afshin; Kumari, Pratibha; Azad, Reza; Merhof, Dorit; Shi, Pengcheng; Ma, Ting; Du, Yuxin; Bai, Fan; Huang, Tiejun; Zhao, Bo; Wang, Haonan; Li, Xiaomeng; Gu, Hanxue; Dong, Haoyu; Yang, Jichen; Mazurowski, Maciej A.; Gupta, Saumya; Wu, Linshan; Zhuang, Jiaxin; Chen, Hao; Roth, Holger; Xu, Daguang; Blaschko, Matthew B.; Decherchi, Sergio; Cavalli, Andrea; Yuille, Alan L.; Zhou, Zongwei",Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?,,10.48550/arXiv.2411.03670,http://arxiv.org/abs/2411.03670,"How can we test AI performance? This question seems trivial, but it isn't. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone, a large-scale collaborative segmentation benchmark of 9 types of abdominal organs. This benchmark is based on 5,195 training CT scans from 76 hospitals around the world and 5,903 testing CT scans from 11 additional hospitals. This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across various out-of-distribution scenarios. We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms on three test sets. In addition, we also evaluated pre-existing AI frameworks--which, differing from algorithms, are more flexible and can support different algorithms--including MONAI from NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are committed to expanding this benchmark to encourage more innovation of AI algorithms for the medical domain.",2025-01-20,,NeurIPS.csv,,,,,,
HNNF8YBY,journalArticle,,"Liu, Haoxin; Xu, Shangqing; Zhao, Zhiyuan; Kong, Lingkai; Kamarthi, Harshavardhan; Sasanur, Aditya B; Sharma, Megha; Cui, Jiaming; Wen, Qingsong; Zhang, Chao; Prakash, B Aditya",Time-MMD: Multi-Domain Multimodal Dataset for Time Series Analysis,,,,"Time series data are ubiquitous across a wide range of real-world domains. While real-world time series analysis (TSA) requires human experts to integrate numerical series data with multimodal domain-specific knowledge, most existing TSA models rely solely on numerical data, overlooking the significance of information beyond numerical series. This oversight is due to the untapped potential of textual series data and the absence of a comprehensive, high-quality multimodal dataset. To overcome this obstacle, we introduce Time-MMD, the first multi-domain, multimodal time series dataset covering 9 primary data domains. Time-MMD ensures fine-grained modality alignment, eliminates data contamination, and provides high usability. Additionally, we develop MM-TSFlib, the first-cut multimodal time-series forecasting (TSF) library, seamlessly pipelining multimodal TSF evaluations based on Time-MMD for in-depth analyses. Extensive experiments conducted on Time-MMD through MM-TSFlib demonstrate significant performance enhancements by extending unimodal TSF to multimodality, evidenced by over 15% mean squared error reduction in general, and up to 40% in domains with rich textual data. More importantly, our datasets and library revolutionize broader applications, impacts, research topics to advance TSA. The dataset is available at https://github.com/AdityaLab/Time-MMD.",,,NeurIPS.csv,,,,,,
4CR6L6BR,journalArticle,,"Souly, Alexandra; Lu, Qingyuan; Bowen, Dillon; Trinh, Tu; Hsieh, Elvis; Pandey, Sana; Abbeel, Pieter; Svegliato, Justin; Emmons, Scott; Watkins, Olivia; Toyer, Sam",A STRONGREJECT for Empty Jailbreaks,,,,"Most jailbreak papers claim the jailbreaks they propose are highly effective, often boasting near-100% attack success rates. However, it is perhaps more common than not for jailbreak developers to substantially exaggerate the effectiveness of their jailbreaks. We suggest this problem arises because jailbreak researchers lack a standard, high-quality benchmark for evaluating jailbreak performance, leaving researchers to create their own. To create a benchmark, researchers must choose a dataset of forbidden prompts to which a victim model will respond, along with an evaluation method that scores the harmfulness of the victim model’s responses. We show that existing benchmarks suffer from significant shortcomings and introduce the StrongREJECT benchmark to address these issues. StrongREJECT’s dataset contains prompts that victim models must answer with specific, harmful information, while its automated evaluator measures the extent to which a response gives useful information to forbidden prompts. In doing so, the StrongREJECT evaluator achieves state-of-the-art agreement with human judgments of jailbreak effectiveness. Notably, we find that existing evaluation methods significantly overstate jailbreak effectiveness compared to human judgments and the StrongREJECT evaluator. We describe a surprising and novel phenomenon that explains this discrepancy: jailbreaks bypassing a victim model’s safety fine-tuning tend to reduce its capabilities. Together, our findings underscore the need for researchers to use a high-quality benchmark, such as StrongREJECT, when developing new jailbreak attacks. We release the StrongREJECT code and data at https://strong-reject.readthedocs.io/.",,,NeurIPS.csv,,,,,,
B7MIY9Z7,journalArticle,,"Cui, Hejie; Mao, Lingjun; Liang, Xin; Zhang, Jieyu; Ren, Hui; Li, Quanzheng; Li, Xiang; Yang, Carl",Biomedical Visual Instruction Tuning with Clinician Preference Alignment,,,,"Recent advancements in multimodal foundation models have showcased impressive capabilities in understanding and reasoning with visual and textual information. Adapting these foundation models trained for general usage to specialized domains like biomedicine requires large-scale domain-specific instruction datasets. While existing works have explored curating such datasets automatically, the resultant datasets are not explicitly aligned with domain expertise. In this work, we propose a data-centric framework, Biomedical Visual Instruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that incorporates clinician preferences into both stages of generating and selecting instruction data for tuning biomedical multimodal foundation models. First, during the generation stage, we prompt the GPT-4V generator with a diverse set of clinician-selected demonstrations for preference-aligned data candidate generation. Then, during the selection phase, we train a separate selection model, which explicitly distills clinician and policy-guided model preferences into a rating function to select high-quality data for medical instruction tuning. Results show that the model tuned with the instruction data from our method demonstrates a significant improvement in open visual chat (18.5% relatively) and medical VQA (win rate up to 81.73%). Our instruction-following data, models, and code are available at https://BioMed-VITAL.github.io.",,,NeurIPS.csv,,,,,,
WBUY2PVD,journalArticle,,"Robinson, Joshua; Ranjan, Rishabh; Hu, Weihua; Huang, Kexin; Han, Jiaqi; Dobles, Alejandro; Fey, Matthias; Lenssen, Jan E; Yuan, Yiwen; Zhang, Zecheng; He, Xinwei; Leskovec, Jure",RELBENCH: A Benchmark for Deep Learning on Relational Databases,,,,"We present RELBENCH, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RELBENCH provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. We use RELBENCH to conduct the ﬁrst comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a signiﬁcant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, we conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RELBENCH.",,,NeurIPS.csv,,,,,,
MN2JVT9W,journalArticle,,"Khrabrov, Kuzma; Ber, Anton; Tsypin, Artem; Ushenin, Konstantin; Rumiantsev, Egor; Telepov, Alexander; Protasov, Dmitry; Shenbin, Ilya; Alekseev, Anton; Shirokikh, Mikhail; Nikolenko, Sergey; Tutubalina, Elena; Kadurin, Artur",∇2DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials,,,,"Methods of computational quantum chemistry provide accurate approximations of molecular properties crucial for computer-aided drug discovery and other areas of chemical science. However, high computational complexity limits the scalability of their applications. Neural network potentials (NNPs) are a promising alternative to quantum chemistry methods, but they require large and diverse datasets for training. This work presents a new dataset and benchmark called ∇2DFT that is based on the nablaDFT. It contains twice as much molecular structures, three times more conformations, new data types and tasks, and state-of-the-art models. The dataset includes energies, forces, 17 molecular properties, Hamiltonian and overlap matrices, and a wavefunction object. All calculations were performed at the DFT level (ωB97X-D/def2-SVP) for each conformation. Moreover, ∇2DFT is the first dataset that contains relaxation trajectories for a substantial number of drug-like molecules. We also introduce a novel benchmark for evaluating NNPs in molecular property prediction, Hamiltonian prediction, and conformational optimization tasks. Finally, we propose an extendable framework for training NNPs and implement 10 models within it.",,,NeurIPS.csv,,,,,,
SH2723EF,journalArticle,,"Dumpala, Sri Harsha; Jaiswal, Aman; Sastry, Chandramouli; Milios, Evangelos; Oore, Sageev; Sajjad, Hassan",SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations,,,,"Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly to object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We demonstrate that models excelling on compositionality datasets may not perform equally well on SUGARCREPE++. This indicates that compositionality alone might not be sufficient to fully understand semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community. Data and code is available at https://github.com/Sri-Harsha/scpp.",,,NeurIPS.csv,,,,,,
MDWXU772,journalArticle,,"Gong, Biao; Tan, Shuai; Feng, Yutong; Xie, Xiaoying; Li, Yuyuan; Chen, Chaochao; Zheng, Kecheng; Shen, Yujun; Zhao, Deli",UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,,,,"This work presents a unified knowledge protocol, called UKnow, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under UKnow format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following UKnow protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 visionrelated ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on 4 benchmarks demonstrate the potential of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. See Appendix A to download the dataset.",,,NeurIPS.csv,,,,,,
ZTC9QUBJ,journalArticle,,"Panchal, Sunny; Bhattacharyya, Apratim; Berger, Guillaume; Mercier, Antoine; Böhm, Cornelius; Dietrichkeit, Florian; Pourreza, Reza; Li, Xuanlin; Madan, Pulkit; Lee, Mingu; Todorovich, Mark; Bax, Ingo; Memisevic, Roland",What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction,,,,"Vision-language models have shown impressive progress in recent years. However, existing models are largely limited to turn-based interactions, where each turn must be stepped (i.e., prompted) by the user. Open-ended, asynchronous interactions, where an AI model may proactively deliver timely responses or feedback based on the unfolding situation in real-time, are an open challenge. In this work, we present the QEVD benchmark and dataset, which explores human-AI interaction in the challenging, yet controlled, real-world domain of fitness coaching – a task which intrinsically requires monitoring live user activity and providing immediate feedback. The benchmark requires vision-language models to recognize complex human actions, identify possible mistakes, and provide appropriate feedback in real-time. Our experiments reveal the limitations of existing state-of-the-art vision-language models for such asynchronous situated interactions. Motivated by this, we propose a simple end-to-end streaming baseline that can respond asynchronously to human actions with appropriate feedback at the appropriate time.",,,NeurIPS.csv,,,,,,
DRE5R9AI,journalArticle,,"Chen, Pengcheng; Ye, Jin; Wang, Guoan; Li, Yanjun; Deng, Zhongying; Li, Wei; Li, Tianbin; Duan, Haodong; Huang, Ziyan; Su, Yanzhou; Wang, Benyou; Zhang, Shaoting; Fu, Bin; Cai, Jianfei; Zhuang, Bohan; Seibel, Eric J; Qiao, Yu; He, Junjun",GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI,,,,,,,NeurIPS.csv,,,,,,
PXAN75Y4,preprint,2024.0,"Bean, Andrew M.; Hellsten, Simi; Mayne, Harry; Magomere, Jabez; Chi, Ethan A.; Chi, Ryan; Hale, Scott A.; Kirk, Hannah Rose",LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages,,10.48550/arXiv.2406.06196,http://arxiv.org/abs/2406.06196,"In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.",2024-10-31,,NeurIPS.csv,,,,,,
H2YD2CEH,journalArticle,,"Lee, Tony; Tu, Haoqin; Wong, Chi Heem; Zheng, Wenhao; Zhou, Yiyang; Mai, Yifan; Roberts, Josselin Somerville; Yasunaga, Michihiro; Yao, Huaxiu; Xie, Cihang; Liang, Percy",VHELM: A Holistic Evaluation of Vision Language Models,,,,"Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiencyfocused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.1. VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.",,,NeurIPS.csv,,,,,,
R4CGUV6D,preprint,2024.0,"Ren, Yuchen; Chen, Zhiyuan; Qiao, Lifeng; Jing, Hongtai; Cai, Yuchen; Xu, Sheng; Ye, Peng; Ma, Xinzhu; Sun, Siqi; Yan, Hongliang; Yuan, Dong; Ouyang, Wanli; Liu, Xihui",BEACON: Benchmark for Comprehensive RNA Tasks and Language Models,,10.1101/2024.06.22.600190,http://biorxiv.org/lookup/doi/10.1101/2024.06.22.600190,"RNA plays a pivotal role in translating genetic instructions into functional outcomes, underscoring its importance in biological processes and disease mechanisms. Despite the emergence of numerous deep learning approaches for RNA, particularly universal RNA language models, there remains a significant lack of standardized benchmarks to assess the effectiveness of these methods. In this study, we introduce the first comprehensive RNA benchmark BEACON (BEnchmArk for COmprehensive RNA Task and Language Models). First, BEACON comprises 13 distinct tasks derived from extensive previous work covering structural analysis, functional studies, and engineering applications, enabling a comprehensive assessment of the performance of methods on various RNA understanding tasks. Second, we examine a range of models, including traditional approaches like CNNs, as well as advanced RNA foundation models based on language models, offering valuable insights into the task-specific performances of these models. Third, we investigate the vital RNA language model components from the tokenizer and positional encoding aspects. Notably, our findings emphasize the superiority of single nucleotide tokenization and the effectiveness of Attention with Linear Biases (ALiBi) over traditional positional encoding methods. Based on these insights, a simple yet strong baseline called BEACON-B is proposed, which can achieve outstanding performance with limited data and computational resources. The datasets and source code of our benchmark are available at https://github.com/terry-r123/RNABenchmark.",2024-06-28,,NeurIPS.csv,,,,,,
V35WWXH2,journalArticle,,"Liu, Chang; Saul, Rebecca; Sun, Yihao; Raff, Edward; Fuchs, Maya; Pantano, Townsend Southard; Holt, James; Micinski, Kristopher",ASSEMBLAGE: Automatic Binary Dataset Construction for Machine Learning,,,,"Binary code is pervasive, and binary analysis is a key task in reverse engineering, malware classification, and vulnerability discovery. Unfortunately, while there exist large corpora of malicious binaries, obtaining high-quality corpora of benign binaries for modern systems has proven challenging (e.g., due to licensing issues). Consequently, machine learning based pipelines for binary analysis utilize either costly commercial corpora (e.g., VirusTotal) or open-source binaries (e.g., coreutils) available in limited quantities. To address these issues, we present ASSEMBLAGE: an extensible distributed system that crawls, configures, and builds Windows PE binaries to obtain high-quality binary corpora suitable for training state-of-the-art models in binary analysis. We have run ASSEMBLAGE on AWS over the past year, producing 890k Windows PE and 428k Linux ELF binaries across 29 configurations. ASSEMBLAGE is designed to be both reproducible and extensible, enabling users to publish “recipes” for their datasets, and facilitating the extraction of a wide array of features. We evaluated ASSEMBLAGE by using its data to train modern learning-based pipelines for compiler provenance and binary function similarity. Our results illustrate the practical need for robust corpora of high-quality Windows PE binaries in training modern learning-based binary analyses. ASSEMBLAGE code is open sourced under the MIT license, and the dataset can be downloaded from https://assemblage-dataset.net/.",,,NeurIPS.csv,,,,,,
4ASC94KU,journalArticle,,"Pa, Victor-Alexandru; Singla, Adish",Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming,,,,"Generative models have demonstrated human-level proficiency in various benchmarks across domains like programming, natural sciences, and general knowledge. Despite these promising results on competitive benchmarks, they still struggle with seemingly simple problem-solving tasks typically carried out by elementary-level students. How do state-of-the-art models perform on standardized programmingrelated tests designed to assess computational thinking and problem-solving skills at schools? In this paper, we curate a novel benchmark involving computational thinking tests grounded in elementary visual programming domains. Our initial results show that state-of-the-art models like GPT-4o and Llama3 barely match the performance of an average school student. To further boost the performance of these models, we fine-tune them using a novel synthetic data generation methodology. The key idea is to develop a comprehensive dataset using symbolic methods that capture different skill levels, ranging from recognition of visual elements to multi-choice quizzes to synthesis-style tasks. We showcase how various aspects of symbolic information in synthetic data help improve fine-tuned models’ performance. We will release the full implementation and datasets to facilitate further research on enhancing computational thinking in generative models.",,,NeurIPS.csv,,,,,,
C69GNNI3,journalArticle,,"Ohana, Ruben; McCabe, Michael; Meyer, Lucas; Morel, Rudy; Agocs, Fruzsina J; Beneitez, Miguel; Berger, Marsha; Burkhart, Blakesley; Dalziel, Stuart B; Fielding, Drummond B; Fortunato, Daniel; Goldberg, Jared A; Hirashima, Keiya; Jiang, Yan-Fei; Kerswell, Rich R; Maddu, Suryanarayana; Miller, Jonah; Mukhopadhyay, Payel; Nixon, Stefan S; Shen, Jeff; Watteaux, Romain; Blancard, Bruno Régaldo-Saint; Rozet, François; Parker, Liam H; Cranmer, Miles; Ho, Shirley",The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning,,,,"Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.",,,NeurIPS.csv,,,,,,
3AEWWJUV,journalArticle,,"Tschalzev, Andrej; Marton, Sascha; Lüdtke, Stefan; Bartelt, Christian; Stuckenschmidt, Heiner",A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data,,,,"Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing the performance of models typically consist of model-centric evaluation setups with overly standardized data preprocessing. This paper demonstrates that such model-centric evaluations are biased, as real-world modeling pipelines often require dataset-specific preprocessing, which includes feature engineering. Therefore, we propose a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings are: 1. After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. 2. Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. 3. While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics. Our framework is available under: https://github.com/atschalz/dc_tabeval.",,,NeurIPS.csv,,,,,,
XSWTVBC4,journalArticle,,"Wang, Zhecan; Liu, Junzhang; Tang, Chia-Wei; Alomari, Hani; Sivakumar, Anushka; Sun, Rui; Li, Wenhao; Ayyubi, Hammad; You, Haoxuan; Ishmam, Alvi; Chang, Kai-Wei; Chang, Shih-Fu; Thomas, Chris",JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images,,,,,,,NeurIPS.csv,,,,,,
PA92XWJB,journalArticle,,"Luo, Bingqiao; Zhang, Zhen; Wang, Qian; He, Bingsheng",Multi-Chain Graphs of Graphs: A New Approach to Analyzing Blockchain Datasets,,,,"Machine learning applied to blockchain graphs offers significant opportunities for enhanced data analysis and applications. However, the potential of this field is constrained by the lack of a large-scale, cross-chain dataset that includes hierarchical graph-level data. To address this issue, we present novel datasets that provide detailed label information at the token level and integrate interactions between tokens across multiple blockchain platforms. We model transactions within each token as local graphs and the relationships between tokens as global graphs, collectively forming a ""Graphs of Graphs"" (GoG) approach. This innovative approach facilitates a deeper understanding of systemic structures and hierarchical interactions, which are essential for applications such as link prediction, anomaly detection, and token classification. We conduct a series of experiments demonstrating that this dataset delivers new insights and challenges for exploring GoG within the blockchain domain. Our work promotes advancements and opens new avenues for research in both the blockchain and graph communities. Source code and datasets are available at https: //github.com/Xtra-Computing/Cryptocurrency-Graphs-of-graphs.",,,NeurIPS.csv,,,,,,
XTFK5ZGA,journalArticle,,"Lee, Dongwoo; Park, Joonkyu; Lee, Kyoung Mu",GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring,,,,,,,NeurIPS.csv,,,,,,
9VKM3VQ4,journalArticle,,"Chao, Patrick; Debenedetti, Edoardo; Robey, Alexander; Andriushchenko, Maksym; Croce, Francesco; Sehwag, Vikash; Dobriban, Edgar; Flammarion, Nicolas; Pappas, George J; Tramer, Florian; Hassani, Hamed; Wong, Eric",JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,,,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors—both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024)—which align with OpenAI’s usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.",,,NeurIPS.csv,,,,,,
DSAWWSWC,journalArticle,,"Bonnen, Tyler; Fu, Stephanie; Bai, Yutong; O’Connell, Thomas; Friedman, Yoni; Kanwisher, Nancy; Tenenbaum, Joshua B; Efros, Alexei A",Evaluating Multiview Object Consistency in Humans and Image Models,,,,"We introduce a benchmark to directly evaluate the alignment between human observers and vision models on a 3D shape inference task. We leverage an experimental design from the cognitive sciences: given a set of images, participants identify which contain the same/different objects, despite considerable viewpoint variation. We draw from a diverse range of images that include common objects (e.g., chairs) as well as abstract shapes (i.e., procedurally generated ‘nonsense’ objects). After constructing over 2000 unique image sets, we administer these tasks to human participants, collecting 35K trials of behavioral data from over 500 participants. This includes explicit choice behaviors as well as intermediate measures, such as reaction time and gaze data. We then evaluate the performance of common vision models (e.g., DINOv2, MAE, CLIP). We find that humans outperform all models by a wide margin. Using a multi-scale evaluation approach, we identify underlying similarities and differences between models and humans: while human-model performance is correlated, humans allocate more time/processing on challenging trials. All images, data, and code can be accessed via our project page.",,,NeurIPS.csv,,,,,,
QTVRLH5I,journalArticle,,"Sivasubramaniam, Sithursan; Osei-Akoto, Cedric; Zhang, Yi; Stockinger, Kurt; Fürst, Jonathan",SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark,,,,"Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far. In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy—a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs. Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last, SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.",,,NeurIPS.csv,,,,,,
4MKMH5VR,journalArticle,,"Haider, Momin; Yin, Ming; Zhang, Menglei; Gupta, Arpit; Zhu, Jing; Wang, Yu-Xiang",NetworkGym: Reinforcement Learning Environments for Multi-Access Trafﬁc Management in Network Simulation,,,,"Mobile devices such as smartphones, laptops, and tablets can often connect to multiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously. Recent advancements facilitate seamless integration of these connections below the transport layer, enhancing the experience for apps that lack inherent multi-path support. This optimization hinges on dynamically determining the trafﬁc distribution across networks for each device, a process referred to as multi-access trafﬁc splitting. This paper introduces NetworkGym, a high-ﬁdelity network environment simulator that facilitates generating multiple network trafﬁc ﬂows and multiaccess trafﬁc splitting. This simulator facilitates training and evaluating different RL-based solutions for the multi-access trafﬁc splitting problem. Our initial explorations demonstrate that the majority of existing state-of-the-art ofﬂine RL algorithms (e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average. This illustrates the urgent need to evaluate ofﬂine RL algorithms against a broader range of benchmarks, rather than relying solely on popular ones such as D4RL. We also propose an extension to the TD3+BC algorithm, named Pessimistic TD3 (PTD3), and demonstrate that it outperforms many state-of-the-art ofﬂine RL algorithms. PTD3’s behavioral constraint mechanism, which relies on value-function pessimism, is theoretically motivated and relatively simple to implement.",,,NeurIPS.csv,,,,,,
Q9K6CKXP,journalArticle,,"Chen, Chaochao; Zhang, Jiaming; Zhang, Yizhao; Zhang, Li; Lyu, Lingjuan; Li, Yuyuan; Gong, Biao; Yan, Chenggang",CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper Inﬂuence,,,,"With increasing privacy concerns in artiﬁcial intelligence, regulations have mandated the right to be forgotten, granting individuals the right to withdraw their data from models. Machine unlearning has emerged as a potential solution to enable selective forgetting in models, particularly in recommender systems where historical data contains sensitive user information. Despite recent advances in recommendation unlearning, evaluating unlearning methods comprehensively remains challenging due to the absence of a uniﬁed evaluation framework and overlooked aspects of deeper inﬂuence, e.g., fairness. To address these gaps, we propose CURE4Rec, the ﬁrst comprehensive benchmark for recommendation unlearning evaluation. CURE4Rec covers four aspects, i.e., unlearning Completeness, recommendation Utility, unleaRning efﬁciency, and recommendation fairnEss, under three data selection strategies, i.e., core data, edge data, and random data. Speciﬁcally, we consider the deeper inﬂuence of unlearning on recommendation fairness and robustness towards data with varying impact levels. We construct multiple datasets with CURE4Rec evaluation and conduct extensive experiments on existing recommendation unlearning methods. Our code is released at https://github.com/xiye7lai/CURE4Rec.",,,NeurIPS.csv,,,,,,
QQU45MB6,journalArticle,,"Song, Jian; Chen, Hongruixuan; Xuan, Weihao; Xia, Junshi; Yokoya, Naoto",SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding from Monocular Remote Sensing Imagery,,,,"Global semantic 3D understanding from single-view high-resolution remote sensing (RS) imagery is crucial for Earth observation (EO). However, this task faces significant challenges due to the high costs of annotations and data collection, as well as geographically restricted data availability. To address these challenges, synthetic data offer a promising solution by being unrestricted and automatically annotatable, thus enabling the provision of large and diverse datasets. We develop a specialized synthetic data generation pipeline for EO and introduce SynRS3D, the largest synthetic RS dataset. SynRS3D comprises 69,667 high-resolution optical images that cover six different city styles worldwide and feature eight land cover types, precise height information, and building change masks. To further enhance its utility, we develop a novel multi-task unsupervised domain adaptation (UDA) method, RS3DAda, coupled with our synthetic dataset, which facilitates the RS-specific transition from synthetic to real scenarios for land cover mapping and height estimation tasks, ultimately enabling global monocular 3D semantic understanding based on synthetic data. Extensive experiments on various real-world datasets demonstrate the adaptability and effectiveness of our synthetic dataset and the proposed RS3DAda method. SynRS3D and related codes are available at https://github.com/JTRNEO/SynRS3D.",,,NeurIPS.csv,,,,,,
T2RZDH46,journalArticle,,"Blacher, Mark; Staudt, Christoph; Klaus, Julien; Wenig, Maurice; Merk, Niklas; Breuer, Alexander; Engel, Max; Laue, Sören; Giesen, Joachim",Einsum Benchmark: Enabling the Development of Next-Generation Tensor Execution Engines,,,,"Modern artiﬁcial intelligence and machine learning workﬂows rely on efﬁcient tensor libraries. However, tuning tensor libraries without considering the actual problems they are meant to execute can lead to a mismatch between expected performance and the actual performance. Einsum libraries are tuned to efﬁciently execute tensor expressions with only a few, relatively large, dense, ﬂoating-point tensors. But, practical applications of einsum cover a much broader range of tensor expressions than those that can currently be executed efﬁciently. For this reason, we have created a benchmark dataset that encompasses this broad range of tensor expressions, allowing future implementations of einsum to build upon and be evaluated against. In addition, we also provide generators for einsum expressions and converters to einsum expressions in our repository, so that additional data can be generated as needed. The benchmark dataset, the generators and converters are released openly and are publicly available at https://benchmark.einsum.org.",,,NeurIPS.csv,,,,,,
ELCP3KTG,journalArticle,,"Wang, Zengzhi; Li, Xuefeng; Xia, Rui; Liu, Pengfei",MATHPILE : A Billion-Token-Scale Pre-training Corpus for Math,,,,"High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MATHPILE , a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of “less is more”, firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MATHPILE to boost language models’ mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field (available at https://github.com/GAIR-NLP/MathPile/).",,,NeurIPS.csv,,,,,,
