Key,Item Type,Publication Year,Author,Title,Publication Title,DOI,Url,Abstract Note,Date,Conference Name,Source File,Field,Novel Database,Representativity Mentions,Similarity Mentions,Diversity Mentions,Notes
ZLSVM7UD,conferencePaper,2024.0,"Li, Haoran; Guo, Dadi; Li, Donghao; Fan, Wei; Hu, Qi; Liu, Xin; Chan, Chunkit; Yao, Duanyi; Yao, Yuan; Song, Yangqiu",PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.4,https://aclanthology.org/2024.acl-long.4,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
WBN4DR9E,preprint,2024.0,"Hu, Yong; Meng, Fandong; Zhou, Jie",CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers,,10.48550/arXiv.2211.08788,http://arxiv.org/abs/2211.08788,"In this paper, we present CSCD-NS, the first Chinese spelling check (CSC) dataset designed for native speakers, containing 40,000 samples from a Chinese social platform. Compared with existing CSC datasets aimed at Chinese learners, CSCD-NS is ten times larger in scale and exhibits a distinct error distribution, with a significantly higher proportion of word-level errors. To further enhance the data resource, we propose a novel method that simulates the input process through an input method, generating large-scale and high-quality pseudo data that closely resembles the actual error distribution and outperforms existing methods. Moreover, we investigate the performance of various models in this scenario, including large language models (LLMs), such as ChatGPT. The result indicates that generative models underperform BERT-like classification models due to strict length and pronunciation constraints. The high prevalence of word-level errors also makes CSC for native speakers challenging enough, leaving substantial room for improvement.",2024-05-23,,ACL.csv,,,,,,
9XF59PKK,conferencePaper,2024.0,"Poesina, Eduard; Caragea, Cornelia; Ionescu, Radu",A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.15,https://aclanthology.org/2024.acl-long.15,"Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classiﬁcation, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the ﬁrst Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available at https://github.com/Eduard6421/RONLI.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
IVP948X3,conferencePaper,2024.0,"Wang, Xiyao; Zhou, Yuhang; Liu, Xiaoyu; Lu, Hongjin; Xu, Yuancheng; He, Feihong; Yoon, Jaehong; Lu, Taixi; Liu, Fuxiao; Bertasius, Gedas; Bansal, Mohit; Yao, Huaxiu; Huang, Furong",Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.25,https://aclanthology.org/2024.acl-long.25,"Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs’ sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs’ sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of cooccurring behaviors, and the compounding impact of behavioral hallucinations.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
9FJT7KT3,conferencePaper,2024.0,"Xia, Congying; Xing, Chen; Du, Jiangshu; Yang, Xinyi; Feng, Yihao; Xu, Ran; Yin, Wenpeng; Xiong, Caiming",FOFO: A Benchmark to Evaluate LLMs’ Format-Following Capability,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.40,https://aclanthology.org/2024.acl-long.40,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
JLPZF3DP,conferencePaper,2024.0,"Ghosh, Sreyan; Tyagi, Utkarsh; Kumar, Sonal; Evuru, Chandra Kiran; S, Ramaneswaran; Sakshi, S; Manocha, Dinesh",ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.43,https://aclanthology.org/2024.acl-long.43,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
AKN5AHRM,conferencePaper,2024.0,"Bandarkar, Lucas; Liang, Davis; Muller, Benjamin; Artetxe, Mikel; Shukla, Satya Narayan; Husa, Donald; Goyal, Naman; Krishnan, Abhinandan; Zettlemoyer, Luke; Khabsa, Madian",The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.44,https://aclanthology.org/2024.acl-long.44,"We present BELEBELE, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiplechoice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in Englishcentric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, BELEBELE opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
9UH78XFY,conferencePaper,2024.0,"Zhang, Zhihan; Cao, Yixin; Ye, Chenchen; Ma, Yunshan; Liao, Lizi; Chua, Tat-Seng","Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding",Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.87,https://aclanthology.org/2024.acl-long.87,"The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
22LY8PZZ,conferencePaper,2024.0,"Jin, Chuhao; Ren, Kening; Kong, Lingzhen; Wang, Xiting; Song, Ruihua; Chen, Huan",Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.92,https://aclanthology.org/2024.acl-long.92,"Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrievalbased models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of user’s utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
CYVHMULL,conferencePaper,2024.0,"Yang, Qian; Xu, Jin; Liu, Wenrui; Chu, Yunfei; Jiang, Ziyue; Zhou, Xiaohuan; Leng, Yichong; Lv, Yuanjun; Zhao, Zhou; Zhou, Chang; Zhou, Jingren",AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.109,https://aclanthology.org/2024.acl-long.109,"Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as automatic speech recognition, and lack an assessment of the openended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (Audio InstRuction Benchmark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIRBench encompasses two dimensions: foundation and chat benchmarks. The former consists of 19 tasks with approximately 19k singlechoice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended questionand-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
MNNB7VUZ,conferencePaper,2024.0,"Yin, Xunjian; Zhang, Xu; Ruan, Jie; Wan, Xiaojun",Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.124,https://aclanthology.org/2024.acl-long.124,"In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and promptsensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose a projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
WWI9F5VX,conferencePaper,2024.0,"Sadat, Mobashir; Caragea, Cornelia",Co-training for Low Resource Scientific Natural Language Inference,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.139,https://aclanthology.org/2024.acl-long.139,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
TTN9G3QF,conferencePaper,2024.0,"Wei, Xiao; Xu, Qi; Yu, Hang; Liu, Qian; Cambria, Erik",Through the MUD: A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.158,https://aclanthology.org/2024.acl-long.158,"The current charge prediction datasets mostly focus on single-defendant criminal cases. However, real-world criminal cases usually involve multiple defendants whose criminal facts are intertwined. In an early attempt to fill this gap, we introduce a new benchmark that encompasses legal cases involving multiple defendants, where each defendant is labeled with a charge and four types of crime elements, i.e., Object Element, Objective Element, Subject Element, and Subjective Element. Based on the dataset, we further develop an interpretable model called EJudge that incorporates crime elements and legal rules to infer charges. We observe that predicting crime charges while providing corresponding rationales benefits the interpretable AI system. Extensive experiments show that EJudge significantly surpasses state-of-the-art methods, which verify the importance of crime elements and legal rules in multi-defendant charge prediction. Source code and dataset available at https://github.com/welchxu/MCP.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
GJY3647G,conferencePaper,2024.0,"Li, Qintong; Cui, Leyang; Zhao, Xueliang; Kong, Lingpeng; Bi, Wei",GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.163,https://aclanthology.org/2024.acl-long.163,"Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs’ math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-PLUS) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
A4PG2VSB,conferencePaper,2024.0,"Bai, Yushi; Lv, Xin; Zhang, Jiajie; Lyu, Hongchang; Tang, Jiankai; Huang, Zhidian; Du, Zhengxiao; Liu, Xiao; Zeng, Aohan; Hou, Lei; Dong, Yuxiao; Tang, Jie; Li, Juanzi","LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.172,https://aclanthology.org/2024.acl-long.172,"Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs’ long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including singledoc QA, multi-doc QA, summarization, fewshot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other opensourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
DBRYFCLD,conferencePaper,2024.0,"Chen, Yuyan; Wu, Chenwei; Yan, Songzhou; Liu, Panjun; Xiao, Yanghua",Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.173,https://aclanthology.org/2024.acl-long.173,"Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs’ capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored. In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles. Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl’s taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs’ outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
HKCIF2M9,conferencePaper,2024.0,"Pham, Trinh; Le, Khoi; Luu, Anh Tuan",UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.174,https://aclanthology.org/2024.acl-long.174,"In this paper, we introduce UniBridge (CrossLingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
98YFLJ9K,conferencePaper,2024.0,"Park, Chanjun; Kim, Hyeonwoo; Kim, Dahyun; Cho, SeongHwan; Kim, Sanghoon; Lee, Sukyung; Kim, Yungi; Lee, Hwalsuk",Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.177,https://aclanthology.org/2024.acl-long.177,"This paper introduces the Open Ko-LLM Leaderboard1 and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
Z9U7G92S,conferencePaper,2024.0,"Nguyen, Xuan-Phi; Aljunied, Mahani; Joty, Shafiq; Bing, Lidong",Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.192,https://aclanthology.org/2024.acl-long.192,"Large language models (LLMs) are known to perform tasks by simply observing few exemplars. Moreover, competent generative capabilities of LLMs are observed mostly in highresource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs’ ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages. These prompts can directly induce generative capabilities in lowresource languages and serve as intra-lingual exemplars to even improve tasks in these languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 34 Indic and African languages, and surpasses supervised prompting in non-English tasks. The method also significantly improves low-resource performances in many other intra-lingual tasks like summarization (XLSum), question answering (XQUAD & TydiQA) and conversational instruction following (Sea-Bench).",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
T7TXIIYX,conferencePaper,2024.0,"Tong, Xiaoyu; Choenni, Rochelle; Lewis, Martha; Shutova, Ekaterina",Metaphor Understanding Challenge Dataset for LLMs,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.193,https://aclanthology.org/2024.acl-long.193,"Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs. The dataset is freely accessible at https://github.com/xiaoyuisrain/ metaphor-understanding-challenge.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
YBQNF4NU,conferencePaper,2024.0,"Wang, Yuxin; Yang, Ivory; Hassanpour, Saeed; Vosoughi, Soroush",MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.206,https://aclanthology.org/2024.acl-long.206,"Mental manipulation, a significant form of abuse in interpersonal conversations, presents a challenge to identify due to its contextdependent and often subtle nature. The detection of manipulative language is essential for protecting potential victims, yet the field of Natural Language Processing (NLP) currently faces a scarcity of resources and research on this topic. Our study addresses this gap by introducing a new dataset, named MENTALMANIP, which consists of 4, 000 annotated fictional dialogues. This dataset enables a comprehensive analysis of mental manipulation, pinpointing both the techniques utilized for manipulation and the vulnerabilities targeted in victims. Our research further explores the effectiveness of leading-edge models in recognizing manipulative dialogue and its components through a series of experiments with various configurations. The results demonstrate that these models inadequately identify and categorize manipulative content. Attempts to improve their performance by fine-tuning with existing datasets on mental health and toxicity have not overcome these limitations. We anticipate that MENTALMANIP will stimulate further research, leading to progress in both understanding and mitigating the impact of mental manipulation in conversations.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
5PCDI4MS,conferencePaper,2024.0,"He, Chaoqun; Luo, Renjie; Bai, Yuzhuo; Hu, Shengding; Thai, Zhen; Shen, Junhao; Hu, Jinyi; Han, Xu; Huang, Yujie; Zhang, Yuxiang; Liu, Jie; Qi, Lei; Liu, Zhiyuan; Sun, Maosong",OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.211,https://aclanthology.org/2024.acl-long.211,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
U7A4CKDB,conferencePaper,2024.0,"Wang, Yuxia; Mansurov, Jonibek; Ivanov, Petar; Su, Jinyan; Shelmanov, Artem; Tsvigun, Akim; Mohammed Afzal, Osama; Mahmoud, Tarek; Puccetti, Giovanni; Arnold, Thomas; Aji, Alham; Habash, Nizar; Gurevych, Iryna; Nakov, Preslav",M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.218,https://aclanthology.org/2024.acl-long.218,"The advent of Large Language Models (LLMs) has brought an unprecedented surge in machinegenerated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multidomain, and multi-generator corpus of MGTs — M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed humanmachine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github. com/mbzuai-nlp/M4GT-Bench.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
TNJ5W6TH,conferencePaper,2024.0,"Wang, Xiaozhi; Peng, Hao; Guan, Yong; Zeng, Kaisheng; Chen, Jianhui; Hou, Lei; Han, Xu; Lin, Yankai; Liu, Zhiyuan; Xie, Ruobing; Zhou, Jie; Li, Juanzi",MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.224,https://aclanthology.org/2024.acl-long.224,"Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a largescale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-ARG, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-ARG offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98, 591 events and 290, 613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVENARG is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVENARG and codes can be obtained from https: //github.com/THU-KEG/MAVEN-Argument.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
EN5IZDCU,conferencePaper,2024.0,"Fan, Lizhou; Hua, Wenyue; Li, Lingyao; Ling, Haoyang; Zhang, Yongfeng",NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.225,https://aclanthology.org/2024.acl-long.225,"Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting and memorization, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NPHard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting or memorizing the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https:// github.com/casmlab/NPHardEval.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
K5PEQ2ID,conferencePaper,2024.0,"Wang, Kexin; Reimers, Nils; Gurevych, Iryna",DAPR: A Benchmark on Document-Aware Passage Retrieval,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.236,https://aclanthology.org/2024.acl-long.236,"The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. Wikipedia articles, research papers, etc. We propose and name this task Document-Aware Passage Retrieval (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextualized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries, but overall they also perform rather poorly. Our created benchmark enables future research on developing and comparing retrieval systems for the new task. The code and the data are available1.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
HJD7GGA2,conferencePaper,2024.0,"Jacovi, Alon; Bitton, Yonatan; Bohnet, Bernd; Herzig, Jonathan; Honovich, Or; Tseng, Michael; Collins, Michael; Aharoni, Roee; Geva, Mor",A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.254,https://aclanthology.org/2024.acl-long.254,"Prompting language models to provide stepby-step answers (e.g., “Chain-of-Thought”) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model’s answer, across a wide variety of datasets and state-of-the-art language models. Available at reveal-dataset.github.io.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
5VENFC44,conferencePaper,2024.0,"Ruan, Qian; Kuznetsov, Ilia; Gurevych, Iryna",Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.255,https://aclanthology.org/2024.acl-long.255,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
GPT4MEB7,conferencePaper,2024.0,"Jiang, Yuxin; Wang, Yufei; Zeng, Xingshan; Zhong, Wanjun; Li, Liangyou; Mi, Fei; Shang, Lifeng; Jiang, Xin; Liu, Qun; Wang, Wei",FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.257,https://aclanthology.org/2024.acl-long.257,"The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs’ outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and opensource popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github. com/YJiangcm/FollowBench.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
E84KU33B,conferencePaper,2024.0,"Guo, Shiguang; Deng, Ziliang; Lin, Hongyu; Lu, Yaojie; Han, Xianpei; Sun, Le",Open Grounded Planning: Challenges and Benchmark Construction,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.272,https://aclanthology.org/2024.acl-long.272,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
XFTGQCSY,journalArticle,,"Liang, Xun; Song, Shichao; Niu, Simin; Li, Zhiyu; Xiong, Feiyu; Tang, Bo; Wang, Yezhaohui; He, Dawei; Cheng, Peng; Wang, Zhonghao; Deng, Haiying",al UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation,,,,"Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a wellestablished Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, containing hallucinations generated by LLMs with minimal restrictions1. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.",,,ACL.csv,,,,,,
JXPKPRBW,conferencePaper,2024.0,"Jinadu, Uthman; Ding, Yi",Noise Correction on Subjective Datasets,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.294,https://aclanthology.org/2024.acl-long.294,"Incorporating every annotator’s perspective is crucial for unbiased data modeling. Annotator fatigue and changing opinions over time can distort dataset annotations. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, this method provides a controllable way to encourage or discourage disagreement. We demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
5XCTBWKY,conferencePaper,2024.0,"Yan, Weixiang; Liu, Haitian; Wang, Yunkun; Li, Yunzhe; Chen, Qian; Wang, Wen; Lin, Tingyu; Zhao, Weishan; Zhu, Li; Sundaram, Hari; Deng, Shuiguang",CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.301,https://aclanthology.org/2024.acl-long.301,"Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers 43 programming languages and eight coding tasks. It evaluates the coding performance of LLMs from three dimensions (perspectives): length, difficulty, and efficiency. To facilitate execution-based evaluations of code generation, we develop MultiCodeEngine, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/ WeixiangYAN/CodeScope.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
IY3LJ4WT,conferencePaper,2024.0,"Zhang, Yuge; Jiang, Qiyang; XingyuHan, XingyuHan; Chen, Nan; Yang, Yuqing; Ren, Kan",Benchmarking Data Science Agents,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.308,https://aclanthology.org/2024.acl-long.308,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
6TJLDMSK,conferencePaper,2024.0,"Chen, Jian; Zhou, Peilin; Hua, Yining; Xin, Loh; Chen, Kehui; Li, Ziyuan; Zhu, Bing; Liang, Junwei",FinTextQA: A Dataset for Long-form Financial Question Answering,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.328,https://aclanthology.org/2024.acl-long.328,"Accurate evaluation of financial questionanswering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold. The dataset is publicly available 1.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
LWC2Y2NJ,conferencePaper,2024.0,"Zhong, Tianqi; Li, Zhaoyi; Wang, Quan; Song, Linqi; Wei, Ying; Lian, Defu; Mao, Zhendong",Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.351,https://aclanthology.org/2024.acl-long.351,"Compositional generalization, representing the model’s ability to generate text with new attribute combinations obtained by recombining single attributes from the training data, is a crucial property for multi-aspect controllable text generation (MCTG) methods. Nonetheless, a comprehensive compositional generalization evaluation benchmark of MCTG is still lacking. We propose CompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets and a crafted three-dimensional evaluation protocol, to holistically evaluate the compositional generalization of MCTG approaches. We observe that existing MCTG works generally confront a noticeable performance drop in compositional testing. To mitigate this issue, we introduce Meta-MCTG, a training framework incorporating meta-learning, where we enable models to learn how to generalize by simulating compositional generalization scenarios in the training phase. We demonstrate the effectiveness of Meta-MCTG through achieving obvious improvement (by at most 3.64%) for compositional testing performance in 94.4% cases1.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
TWAVSM8B,conferencePaper,2024.0,"Khan, Mohammad Abdullah Matin; Bari, M Saiful; Long, Do; Wang, Weishi; Parvez, Md Rizwan; Joty, Shafiq","XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.367,https://aclanthology.org/2024.acl-long.367,"Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce XCODEEVAL, the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples (16.5B tokens) from about 7.5K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. XCODEEVAL adopts an execution-based evaluation and offers a multilingual code execution engine, ExecEval that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI’s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate XCODEEVAL to be quite challenging as per the current advancements in language models.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
8L4YG3Q7,conferencePaper,2024.0,"D’Arcy, Mike; Ross, Alexis; Bransom, Erin; Kuehl, Bailey; Bragg, Jonathan; Hope, Tom; Downey, Doug",ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.377,https://aclanthology.org/2024.acl-long.377,"We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment—especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4’s ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
3JW5K3SZ,conferencePaper,2024.0,"Bai, Ge; Liu, Jie; Bu, Xingyuan; He, Yancheng; Liu, Jiaheng; Zhou, Zhanhui; Lin, Zhuoran; Su, Wenbo; Ge, Tiezheng; Zheng, Bo; Ouyang, Wanli",MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.401,https://aclanthology.org/2024.acl-long.401,"The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarsegrained and incomplete assessments of multiturn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench101, specifically designed to evaluate the finegrained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at https: //github.com/mtbench101/mt-bench-101.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
82F892AX,conferencePaper,2024.0,"Ai, Qihang; Li, Jiafan; Dai, Jincheng; Zhou, Jianwu; Liu, Lemao; Jiang, Haiyun; Shi, Shuming",Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.404,https://aclanthology.org/2024.acl-long.404,"Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning. Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5%-15% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancement in graph data understanding.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
WHJ5WTSN,conferencePaper,2024.0,"Das, Rocktim; Hristov, Simeon; Li, Haonan; Dimitrov, Dimitar; Koychev, Ivan; Nakov, Preslav",EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.420,https://aclanthology.org/2024.acl-long.420,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
8Y8R3IQM,conferencePaper,2024.0,"Wu, Zongru; Zhang, Zhuosheng; Cheng, Pengzhou; Liu, Gongshen",Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.441,https://aclanthology.org/2024.acl-long.441,"Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with lowrank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively highfrequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes are publicly available at https://github.com/ZrW00/MuScleLoRA.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
ZCTAVVV2,conferencePaper,2024.0,"Lv, Kai; Yang, Yuqing; Liu, Tengxiao; Guo, Qipeng; Qiu, Xipeng",Full Parameter Fine-tuning for Large Language Models with Limited Resources,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.445,https://aclanthology.org/2024.acl-long.445,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
T5KTWLIQ,conferencePaper,2024.0,"Chen, Qiguang; Qin, Libo; Zhang, Jin; Chen, Zhi; Xu, Xiao; Che, Wanxiang",M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.446,https://aclanthology.org/2024.acl-long.446,"Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-bystep reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) Domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M3CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M3CoT and there remains a large gap between existing VLLMs and human performance in M3CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M3CoT can serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
CZG4V3FY,conferencePaper,2024.0,"Krumdick, Michael; Koncel-Kedziorski, Rik; Lai, Viet Dac; Reddy, Varshini; Lovering, Charles; Tanner, Chris",BizBench: A Quantitative Reasoning Benchmark for Business and Finance,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.452,https://aclanthology.org/2024.acl-long.452,"Answering questions within business and finance requires reasoning, precision, and a widebreadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models’ ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on questionanswering (QA) over financial data via program synthesis. We include three financiallythemed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model’s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an indepth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs’ limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
ARZQHE8N,journalArticle,,"Xu, Hainiu; Zhao, Runcong; Zhu, Lixing; Du, Jinhua; He, Yulan",t OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models,,,,,,,ACL.csv,,,,,,
FYJ7EMZ3,conferencePaper,2024.0,"Li, Yinghui; Xu, Zishan; Chen, Shaoshen; Huang, Haojing; Li, Yangning; Ma, Shirong; Jiang, Yong; Li, Zhongli; Zhou, Qingyu; Zheng, Hai-Tao; Shen, Ying",Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.469,https://aclanthology.org/2024.acl-long.469,"Writing assistance aims to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. In the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters that can be represented by computer text encoding systems, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present Visual-C3, a human-annotated Visual Chinese Character Checking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C3 is the first real-world visual and the largest humancrafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C3. Extensive empirical results and analyses show that Visual-C3 is high-quality yet challenging. As the first study focusing on Chinese faked characters, the Visual-C3 dataset and the baseline methods are publicly available at https: //github.com/THUKElab/Visual-C3.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
H33GIIXI,conferencePaper,2024.0,"Deng, Shihan; Xu, Weikai; Sun, Hongda; Liu, Wei; Tan, Tao; Liujianfeng, Liujianfeng; Li, Ang; Luan, Jian; Wang, Bin; Yan, Rui; Shang, Shuo",Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.478,https://aclanthology.org/2024.acl-long.478,"With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform are available at https://github.com/XiaoMi/MobileBench.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
KFWUTWLU,conferencePaper,2024.0,"Thrush, Tristan; Moore, Jared; Monares, Miguel; Potts, Christopher; Kiela, Douwe",I am a Strange Dataset: Metalinguistic Tests for Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.482,https://aclanthology.org/2024.acl-long.482,"Statements involving metalinguistic selfreference (“This paper has six sections.”) are prevalent in many domains. Can current large language models (LLMs) handle such language? In this paper, we present “I am a Strange Dataset”, a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like “The penultimate word in this sentence is” (where a correct continuation is “is”). In verification, models judge the truth of statements like “The penultimate word in this sentence is sentence.” (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89–93% range. The dataset and evaluation toolkit are available at https://github.com/ TristanThrush/i-am-a-strange-dataset.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
ZWZAV2AL,conferencePaper,2024.0,"Chen, Zhe; Liu, Heyang; Yu, Wenyi; Sun, Guangzhi; Liu, Hongcheng; Wu, Ji; Zhang, Chao; Wang, Yu; Wang, Yanfeng","M3AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset",Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.489,https://aclanthology.org/2024.acl-long.489,"Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M3AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the slide text and spoken words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual speech recognition, speech synthesis, and slide and script generation tasks demonstrate that the diversity of M3AV makes it a challenging dataset1.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
PC3B3R2Y,conferencePaper,2024.0,"Wang, Yuhao; Liao, Yusheng; Liu, Heyang; Liu, Hongcheng; Wang, Yanfeng; Wang, Yu",MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.498,https://aclanthology.org/2024.acl-long.498,"Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models’ struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the selfawareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights. The experiment results reveal that current MLLMs possess limited selfawareness capabilities, pointing to a crucial area for future advancement in the development of trustworthy MLLMs. Code and data are available at https://github.com/YHWmz/ MM-SAP.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
SK2JLA2P,conferencePaper,2024.0,"Yehuda, Yakir; Malkiel, Itzik; Barkan, Oren; Weill, Jonathan; Ronen, Royi; Koenigstein, Noam",InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.506,https://aclanthology.org/2024.acl-long.506,"Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 87% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy of 81%, all without relying on external knowledge 1.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
MXBISDFI,conferencePaper,2024.0,"Mahari, Robert; Stammbach, Dominik; Ash, Elliott; Pentland, Alex",LePaRD: A Large-Scale Dataset of Judicial Citations to Precedent,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.532,https://aclanthology.org/2024.acl-long.532,"We present the Legal Passage Retrieval Dataset, LePaRD. LePaRD contains millions of examples of U.S. federal judges citing precedent in context. The dataset aims to facilitate work on legal passage retrieval, a challenging practice-oriented legal retrieval and reasoning task. Legal passage retrieval seeks to predict relevant passages from precedential court decisions given the context of a legal argument. We extensively evaluate various approaches on LePaRD, and find that classification-based retrieval appears to work best. Our best models only achieve a recall of 59% when trained on data corresponding to the 10,000 most-cited passages, underscoring the difficulty of legal passage retrieval. By publishing LePaRD, we provide a large-scale and high quality resource to foster further research on legal passage retrieval. We hope that research on this practiceoriented NLP task will help expand access to justice by reducing the burden associated with legal research via computational assistance. Warning: Extracts from judicial opinions may contain offensive language.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
LCJF9XZT,conferencePaper,2024.0,"Pan, Shilong; Tian, Zhiliang; Ding, Liang; Zheng, Haoqi; Huang, Zhen; Wen, Zhihua; Li, Dongsheng",POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.537,https://aclanthology.org/2024.acl-long.537,"Low-resource languages (LRLs) face challenges in supervised neural machine translation (NMT) due to limited parallel data, prompting research in unsupervised NMT (UNMT). UNMT, without requiring ground truth, provides solutions for LRL translations using synthetic pseudo-parallel data and parallel data from auxiliary language pairs. However, they usually encounter translation errors, including errors from synthetic data and from auxiliary language pairs with linguistic biases. We argue that large language models (LLMs) mitigate UNMT’s translation errors by dynamically organizing auxiliary languages in prompts to improve LRL translations. In this paper, we propose PrObability-driven Meta-graph Prompter (POMP), an approach employing a dynamic graph to organize multiple auxiliary languages, to prompt LLMs in LRL translations. POMP proposes a language-specific meta-graph that dynamically samples multiple translation paths to organize auxiliary languages in constructing prompts. Following the path, POMP prompts LLMs to translate with a mixture of auxiliary languages. We achieve the meta-graph’s evolution by back-propagating evaluation scores to update probabilities on the graph. Our experimental improvements show POMP’s effectiveness on LRLs’ translation.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
PPEVJU8S,conferencePaper,2024.0,"Fernandez, Nigel; Scarlatos, Alexander; Lan, Andrew",SyllabusQA: A Course Logistics Question Answering Dataset,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.557,https://aclanthology.org/2024.acl-long.557,"Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SYLLABUSQA 1, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5, 078 open-ended course logisticsrelated question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We introduce Fact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of predicted answers. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
WWJ9IXRI,conferencePaper,2024.0,"Braun, Daniel; Matthes, Florian",AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.559,https://aclanthology.org/2024.acl-long.559,"Legal tasks and datasets are often used as benchmarks for the capabilities of language models. However, openly available annotated datasets are rare. In this paper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, we present a first baseline for the task of detecting potentially void clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. Our results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. While the fine-tuned models often performed better with regard to precision, GPT-3.5 outperformed the other approaches with regard to recall. An analysis of the errors indicates that one of the main challenges could be the correct interpretation of complex clauses, rather than the decision boundaries of what is permissible and what is not.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
LL9ZGLJY,conferencePaper,2024.0,"Siska, Charlotte; Marazopoulou, Katerina; Ailem, Melissa; Bono, James",Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.560,https://aclanthology.org/2024.acl-long.560,"Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model’s average performance across the test prompts of a benchmark to evaluate the model’s performance. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from a real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. We find that (1) the correlation in model performance across test prompts is non-random, (2) accounting for correlations across test prompts can change model rankings on major benchmarks, (3) explanatory factors for these correlations include semantic similarity and common LLM failure points.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
K7HFR2QS,conferencePaper,2024.0,"Luo, Fuwen; Chen, Chi; Wan, Zihao; Kang, Zhaolu; Yan, Qidong; Li, Yingjie; Wang, Xiaolong; Wang, Siyu; Wang, Ziyue; Mi, Xiaoyue; Li, Peng; Ma, Ning; Sun, Maosong; Liu, Yang",CODIS: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.573,https://aclanthology.org/2024.acl-long.573,"Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a contextdependent manner. View our project website at https://thunlp-mt.github.io/CODIS.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
H2HT2XMT,conferencePaper,2024.0,"Zhang, Tong; Qin, Peixin; Deng, Yang; Huang, Chen; Lei, Wenqiang; Liu, Junhong; Jin, Dingnan; Liang, Hongru; Chua, Tat-Seng",CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.578,https://aclanthology.org/2024.acl-long.578,"Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ∼ 12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-theshelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at https://github.com/SCUNLP/CLAMBER.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
ZJBNUK7H,conferencePaper,2024.0,"Du, Weihong; Liao, Wenrui; Liang, Hongru; Lei, Wenqiang",PAGED: A Benchmark for Procedural Graphs Extraction from Documents,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.583,https://aclanthology.org/2024.acl-long.583,"Automatic extraction of procedural graphs from documents creates a low-cost way for users to easily understand a complex procedure by skimming visual graphs. Despite the progress in recent studies, it remains unanswered: whether the existing studies have well solved this task (Q1) and whether the emerging large language models (LLMs) can bring new opportunities to this task (Q2). To this end, we propose a new benchmark PAGED, equipped with a large high-quality dataset and standard evaluations. It investigates five state-of-the-art baselines, revealing that they fail to extract optimal procedural graphs well because of their heavy reliance on hand-written rules and limited available data. We further involve three advanced LLMs in PAGED and enhance them with a novel self-refine strategy. The results point out the advantages of LLMs in identifying textual elements and their gaps in building logical structures. We hope PAGED can serve as a major landmark for automatic procedural graph extraction and the investigations in PAGED can provide valuable insights into the research on logical reasoning among non-sequential elements. The code and dataset are available in https://github.com/SCUNLP/PAGED.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
PKLTEDMU,conferencePaper,2024.0,"Niu, Cheng; Wu, Yuanhao; Zhu, Juno; Xu, Siliang; Shum, KaShun; Zhong, Randy; Song, Juntong; Zhang, Tong",RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.585,https://aclanthology.org/2024.acl-long.585,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
8NLRAUT2,conferencePaper,2024.0,"Singh, Harman; Gupta, Nitish; Bharadwaj, Shikhar; Tewari, Dinesh; Talukdar, Partha",IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.595,https://aclanthology.org/2024.acl-long.595,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
MVWCGPMK,conferencePaper,2024.0,"Zhang, Yuanchi; Wang, Yile; Liu, Zijun; Wang, Shuo; Wang, Xiaolong; Li, Peng; Sun, Maosong; Liu, Yang",Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.603,https://aclanthology.org/2024.acl-long.603,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
VQNR4AGF,conferencePaper,2024.0,"Sun, Jiaxing; Huang, Weiquan; Wu, Jiang; Gu, Chenya; Li, Wei; Zhang, Songyang; Yan, Hang; He, Conghui",Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.604,https://aclanthology.org/2024.acl-long.604,"We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs’ reasoning ability, such as Chain-of-Thought. Our findings indicated that the LLM’s language orientation and the task’s domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs’ memorizationindependent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs’ strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
JGUTYWCC,conferencePaper,2024.0,"Joshi, Abhinav; Paul, Shounak; Sharma, Akshat; Goyal, Pawan; Ghosh, Saptarshi; Modi, Ashutosh",IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.618,https://aclanthology.org/2024.acl-long.618,"Legal systems worldwide are inundated with exponential growth in cases and documents. There is an imminent need to develop NLP and ML techniques for automatically processing and understanding legal documents to streamline the legal system. However, evaluating and comparing various NLP models designed specifically for the legal domain is challenging. This paper addresses this challenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning. ILTUR contains monolingual (English, Hindi) and multi-lingual (9 Indian languages) domainspecific tasks that address different aspects of the legal system from the point of view of understanding and reasoning over Indian legal documents. We present baseline models (including LLM-based) for each task, outlining the gap between models and the ground truth. To foster further research in the legal domain, we create a leaderboard (available at: https://exploration-lab.github.io/ IL-TUR/) where the research community can upload and compare legal text understanding systems.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
TZIIT9H6,conferencePaper,2024.0,"Singh, Shivalika; Vargus, Freddie; D’souza, Daniel; Karlsson, Börje; Mahendiran, Abinaya; Ko, Wei-Yin; Shandilya, Herumb; Patel, Jay; Mataciunas, Deividas; O’Mahony, Laura; Zhang, Mike; Hettiarachchi, Ramith; Wilson, Joseph; Machado, Marina; Moura, Luisa; Krzemiński, Dominik; Fadaei, Hakimeh; Ergun, Irem; Okoh, Ifeoma; Alaagib, Aisha; Mudannayake, Oshan; Alyafeai, Zaid; Chien, Vu; Ruder, Sebastian; Guthikonda, Surya; Alghamdi, Emad; Gehrmann, Sebastian; Muennighoff, Niklas; Bartolo, Max; Kreutzer, Julia; Üstün, Ahmet; Fadaee, Marzieh; Hooker, Sara",Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.620,https://aclanthology.org/2024.acl-long.620,"Datasets are foundational to many breakthroughs in modern artificial intelligence (AI). Many recent achievements in the space of natural language processing (NLP) can be attributed to the fine-tuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and augmenting existing datasets across 114 languages. In total, we contribute three key resources: we develop and opensource the Aya 1 Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as an important framework for future research collaborations that aim to bridge gaps in resources.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
IPWEI7IL,conferencePaper,2024.0,"Liu, Xiao; Lei, Xuanyu; Wang, Shengyuan; Huang, Yue; Feng, Andrew; Wen, Bosi; Cheng, Jiale; Ke, Pei; Xu, Yifan; Tam, Weng Lam; Zhang, Xiaohan; Sun, Lichao; Gu, Xiaotao; Wang, Hongning; Zhang, Jing; Huang, Minlie; Dong, Yuxiao; Tang, Jie",AlignBench: Benchmarking Chinese Alignment of Large Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.624,https://aclanthology.org/2024.acl-long.624,"Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, the effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce ALIGNBENCH, a comprehensive multidimensional benchmark for evaluating LLMs’ alignment in Chinese. We tailor a humanin-the-loop data curation pipeline, containing 8 main categories, 683 real-scenario rooted queries and corresponding human verified references. To ensure the correctness of references, each knowledge-intensive query is accompanied with evidences collected from reliable web sources (including URLs and quotations) by our annotators. For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge (Zheng et al., 2023) approach with Chain-of-Thought to generate explanations and final ratings, ensuring high reliability and interpretability. All evaluation codes, data, and LLM generations are available at https://github.com/THUDM/ AlignBench.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
CSPK3HTE,conferencePaper,2024.0,"Tu, Quan; Fan, Shilong; Tian, Zihang; Shen, Tianhao; Shang, Shuo; Gao, Xin; Yan, Rui",CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.638,https://aclanthology.org/2024.acl-long.638,"Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in CharacterEval, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source, and reward model will be publicly accessible at https://github.com/ morecry/CharacterEval.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
23EPPBWC,conferencePaper,2024.0,"Wang, Haoyu; Wang, Shuo; Yan, Yukun; Wang, Xujia; Yang, Zhiyu; Xu, Yuzhuang; Liu, Zhenghao; Yang, Liner; Ding, Ning; Han, Xu; Liu, Zhiyuan; Sun, Maosong",UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.644,https://aclanthology.org/2024.acl-long.644,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
7YY6GYUX,conferencePaper,2024.0,"Kasner, Zdeněk; Dusek, Ondrej",Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.651,https://aclanthology.org/2024.acl-long.651,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
Q92UGFAK,conferencePaper,2024.0,"Cao, Qingxing; Cheng, Junhao; Liang, Xiaodan; Lin, Liang",VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.658,https://aclanthology.org/2024.acl-long.658,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
NCZIVY99,conferencePaper,2024.0,"Dugan, Liam; Hwang, Alyssa; Trhlík, Filip; Zhu, Andrew; Ludan, Josh Magnus; Xu, Hainiu; Ippolito, Daphne; Callison-Burch, Chris",RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.674,https://aclanthology.org/2024.acl-long.674,"Many commercial and open-source models claim to detect machine-generated text with extremely high accuracy (99% or more). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging—lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machinegenerated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-ofdomain and adversarial robustness of 8 openand 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our data1 along with a leaderboard2 to encourage future research.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
NGE2NEW2,conferencePaper,2024.0,"Salaün, Olivier; Piedboeuf, Frédéric; Le Berre, Guillaume; Alfonso-Hermelo, David; Langlais, Philippe",EUROPA: A Legal Multilingual Keyphrase Generation Dataset,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.687,https://aclanthology.org/2024.acl-long.687,"Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
UBJN3DFU,conferencePaper,2024.0,"Alwajih, Fakhraddin; Nagoudi, El Moatez Billah; Bhatia, Gagan; Mohamed, Abdelrahman; Abdul-Mageed, Muhammad",Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.689,https://aclanthology.org/2024.acl-long.689,"Multimodal large language models (MLLMs) have proven effective in a wide range of tasks that require complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, even those with large speaker populations, such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed Peacock, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce Henna, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturallyaware Arabic MLLMs. The GitHub repository for the Peacock project is available at https: //github.com/UBC-NLP/peacock.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
FKF6LDTX,journalArticle,,"Zhao, Yilun; Liu, Hongjun; Long, Yitao; Zhang, Rui; Zhao, Chen; Cohan, Arman",KnowledgeFMATH: Knowledge-Intensive Math Reasoning in Finance Domains,,,,"We introduce KnowledgeFMATH, a novel benchmark designed to evaluate LLMs’ capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, KnowledgeFMATH includes 1,259 problems with a hybrid of textual and tabular content. These problems require collegelevel knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a highquality benchmark for LLM assessment. We also construct a finance-domain knowledge bank and investigate various knowledge integration strategies. Finally, we evaluate a wide spectrum of 26 LLMs with different prompting strategies like Chain-of-Thought and Programof-Thought. Our experimental results reveal that the current best-performing system (i.e., GPT-4 with CoT prompting) achieves only 56.6% accuracy, leaving substantial room for improvement. Moreover, while augmenting LLMs with external knowledge can improve their performance (e.g., from 33.5% to 47.1% for GPT-3.5) , their accuracy remains significantly lower than the estimated human expert performance of 92%. We believe that KnowledgeFMATH can advance future research in the area of domain-specific knowledge retrieval and integration, particularly within the context of solving math reasoning problems.",,,ACL.csv,,,,,,
IFPRYTR4,conferencePaper,2024.0,"Basu, Kinjal; Abdelaziz, Ibrahim; Chaudhury, Subhajit; Dan, Soham; Crouse, Maxwell; Munawar, Asim; Austel, Vernon; Kumaravel, Sadhana; Muthusamy, Vinod; Kapanipathi, Pavan; Lastras, Luis",API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.694,https://aclanthology.org/2024.acl-long.694,"There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes1.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
ARFHTF2F,conferencePaper,2024.0,"Wang, Chenhao; Cao, Pengfei; Jin, Zhuoran; Chen, Yubo; Zeng, Daojian; Liu, Kang; Zhao, Jun",MULFE: A Multi-Level Benchmark for Free Text Model Editing,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.732,https://aclanthology.org/2024.acl-long.732,"Adjusting the outdated behaviors of large langugae models (LLMs) after deployment remains a significant challenge. It motivates the model editing research, which is however mainly explored in a restricted task form with triplebased edit requests. Recent works have initiated a transition to a more practical and unified editing task that takes free-form text as edit requests. However, there are gaps in nuanced benchmark designs and re-evaluation of existing methods. To bridge the gaps, we introduce a multi-level benchmark for free text model editing (MULFE). The benchmark categorizes probe queries into three levels of generalization, ranging from basic literal memory to deeper understanding and reasoning. Based on the benchmark, we conduct extensive experiments across various base models, edit sizes, and editing methods, including adaptations of mainstream locate-and-edit and hypernetwork methods. The results highlight the inconsistent behaviors of edited models on different generalization levels. Higherlevel generalization remains a significant challenge. Based on the findings, we propose SIDE, a simple yet effective method based on in-context distillation to enhance the generalization performance. The benchmark dataset and evaluation scripts are publicly available at http://github.com/wchrepo/mulfe.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
MRNJXSEG,conferencePaper,2024.0,"Patil, Vaidehi; Ribeiro, Leonardo; Liu, Mengwen; Bansal, Mohit; Dreyer, Markus",REFINESUMM: Self-Refining MLLM for Generating a Multimodal Summarization Dataset,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.743,https://aclanthology.org/2024.acl-long.743,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
SY4GNXL9,conferencePaper,2024.0,"Alzahrani, Norah; Alyahya, Hisham; Alnumay, Yazeed; AlRashed, Sultan; Alsubaie, Shaykhah; Almushayqih, Yousef; Mirza, Faisal; Alotaibi, Nouf; Al-Twairesh, Nora; Alowisheq, Areeb; Bari, M Saiful; Khan, Haidar",When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.744,https://aclanthology.org/2024.acl-long.744,"Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value — we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at https://github.com/National-Centerfor-AI-Saudi-Arabia/lm-evaluation-harness.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
PEWSYZXF,conferencePaper,2024.0,"Zhang, Enshi; Trujillo, Rafael; Poellabauer, Christian",The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.752,https://aclanthology.org/2024.acl-long.752,"Research in the field of speech emotion recognition (SER) relies on the availability of comprehensive datasets to make it possible to design accurate emotion detection models. This study introduces the Multimodal Emotion Recognition and Sentiment Analysis (MERSA) dataset, which includes both natural and scripted speech recordings, transcribed text, physiological data, and self-reported emotional surveys from 150 participants collected over a two-week period. This work also presents a novel emotion recognition approach that uses a transformer-based model, integrating pre-trained wav2vec 2.0 and BERT for feature extractions and additional LSTM layers to learn hidden representations from fused representations from speech and text. Our model predicts emotions on dimensions of arousal, valence, and dominance. We trained and evaluated the model on the MSP-PODCAST dataset and achieved competitive results from the best-performing model regarding the concordance correlation coefficient (CCC). Further, this paper demonstrates the effectiveness of this model through crossdomain evaluations on both IEMOCAP and MERSA datasets.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
9ZZUWKK2,conferencePaper,2024.0,"Schroeder, Hope; Roy, Deb; Kabbara, Jad",Fora: A corpus and framework for the study of facilitated dialogue,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.754,https://aclanthology.org/2024.acl-long.754,"Facilitated dialogue is increasingly popular as a method of civic engagement and as a method for gathering social insight, but resources for its study are scant. We present Fora, a unique collection of annotated facilitated dialogues. We compile 262 facilitated conversations that were hosted with partner organizations seeking to engage their members and surface insights regarding issues like education, elections, and public health, primarily through the sharing of personal experience. Alongside this corpus of 39,911 speaker turns, we present a framework for the analysis of facilitated dialogue. We taxonomize key personal sharing behaviors and facilitation strategies in the corpus, annotate a 25% sample (10,000+ speaker turns) of the data accordingly, and evaluate and establish baselines on a number of tasks essential to the identification of these phenomena in dialogue. We describe the data, and relate facilitator behavior to turn-taking and participant sharing. We outline how this research can inform future work in understanding and improving facilitated dialogue, parsing spoken conversation, and improving the behavior of dialogue agents.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
YG2BM66A,journalArticle,,"Felkner, Virginia K; Thompson, Jennifer A",GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction,,,,"Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.",,,ACL.csv,,,,,,
NZ4CN3E4,conferencePaper,2024.0,"Li, Lei; Wang, Yuqi; Xu, Runxin; Wang, Peiyi; Feng, Xiachong; Kong, Lingpeng; Liu, Qi",Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.775,https://aclanthology.org/2024.acl-long.775,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
QQBQMSR5,journalArticle,,"Faisal, Fahim; Ahia, Orevaoghene; Srivastava, Aarohi; Ahuja, Kabir; Chiang, David; Tsvetkov, Yulia; Anastasopoulos, Antonios","A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages",,,,,,,ACL.csv,,,,,,
NTP4GF7B,conferencePaper,2024.0,"Arora, Aryaman; Jurafsky, Dan; Potts, Christopher",CausalGym: Benchmarking causal interpretability methods on linguistic tasks,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.785,https://aclanthology.org/2024.acl-long.785,"Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M–6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler–gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
EUC9H37X,conferencePaper,2024.0,"Niklaus, Joel; Matoshi, Veton; Stürmer, Matthias; Chalkidis, Ilias; Ho, Daniel",MultiLegalPile: A 689GB Multilingual Legal Corpus,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.805,https://aclanthology.org/2024.acl-long.805,"Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, few datasets are available for specialized critical domains such as law and the available ones are often small and only in English. To fill this gap, we curate and release MULTILEGALPILE, a 689GB corpus in 24 languages from 17 jurisdictions. MULTILEGALPILE includes diverse legal data sources and allows for pretraining NLP models under fair use, with most of the dataset licensed very permissively. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the languagespecific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, trained models, and all code under the most open licenses possible.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
ZUMXINXM,conferencePaper,2024.0,"Kwan, Wai-Chung; Zeng, Xingshan; Wang, Yufei; Sun, Yusen; Li, Liangyou; Jiang, Yuxin; Shang, Lifeng; Liu, Qun; Wong, Kam-Fai",M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.832,https://aclanthology.org/2024.acl-long.832,"Managing long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMs’ long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length.1 Our evaluation of 11 prominent LLMs reveals that 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area 2.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
UAJEMS94,conferencePaper,2024.0,"Zhao, Chenye; Caragea, Cornelia",EZ-STANCE: A Large Dataset for English Zero-Shot Stance Detection,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.838,https://aclanthology.org/2024.acl-long.838,"Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in favor, against, or neutral toward a target that is unseen during training. In this paper, we present EZ-STANCE, a large English ZSSD dataset with 47,316 annotated text-target pairs. In contrast to VAST (Allaway and McKeown, 2020), which is the only other large existing ZSSD dataset for English, EZ-STANCE is 2.5 times larger, includes both noun-phrase targets and claim targets that cover a wide range of domains, provides two challenging subtasks for ZSSD: target-based ZSSD and domain-based ZSSD, and contains much harder examples for the neutral class. We evaluate EZ-STANCE using state-of-the-art deep learning models. Furthermore, we propose to transform ZSSD into the NLI task by applying simple yet effective prompts to noun-phrase targets. Our experimental results show that EZ-STANCE is a challenging new benchmark, which provides significant research opportunities on English ZSSD. We publicly release our dataset and code at https://github.com/chenyez/EZ-STANCE.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
53XJVXPY,conferencePaper,2024.0,"Soldaini, Luca; Kinney, Rodney; Bhagia, Akshita; Schwenk, Dustin; Atkinson, David; Authur, Russell; Bogin, Ben; Chandu, Khyathi; Dumas, Jennifer; Elazar, Yanai; Hofmann, Valentin; Jha, Ananya; Kumar, Sachin; Lucy, Li; Lyu, Xinxi; Lambert, Nathan; Magnusson, Ian; Morrison, Jacob; Muennighoff, Niklas; Naik, Aakanksha; Nam, Crystal; Peters, Matthew; Ravichander, Abhilasha; Richardson, Kyle; Shen, Zejiang; Strubell, Emma; Subramani, Nishant; Tafjord, Oyvind; Walsh, Evan; Zettlemoyer, Luke; Smith, Noah; Hajishirzi, Hannaneh; Beltagy, Iz; Groeneveld, Dirk; Dodge, Jesse; Lo, Kyle",Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.840,https://aclanthology.org/2024.acl-long.840,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
PNS6Z3RN,conferencePaper,2024.0,"Khan, Mohammed; Mehta, Priyam; Sankar, Ananth; Kumaravelan, Umashankar; Doddapaneni, Sumanth; B, Suriyaprasaad; G, Varun; Jain, Sparsh; Kunchukuttan, Anoop; Kumar, Pratyush; Dabre, Raj; Khapra, Mitesh",IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.843,https://aclanthology.org/2024.acl-long.843,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
DIBX7JFY,journalArticle,,"Chen, Zhuang; Wu, Jincenzi; Zhou, Jinfeng; Wen, Bosi; Bi, Guanqun; Jiang, Gongyao; Cao, Yaru; Hu, Mengting; Lai, Yunghwei; Xiong, Zexuan; Huang, Minlie",T MBENCH: Benchmarking Theory of Mind in Large Language Models,,,,"Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce T MBENCH with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on T MBENCH, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with T MBENCH is to enable an efficient and effective evaluation of LLMs’ ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence.",,,ACL.csv,,,,,,
AJ4XBTPH,conferencePaper,2024.0,"Casola, Silvia; Frenda, Simona; Lo, Soda Marem; Sezerer, Erhan; Uva, Antonio; Basile, Valerio; Bosco, Cristina; Pedrani, Alessandro; Rubagotti, Chiara; Patti, Viviana; Bernardi, Davide",MultiPICo: Multilingual Perspectivist Irony Corpus,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.849,https://aclanthology.org/2024.acl-long.849,"Recently, several scholars have contributed to the growth of a new theoretical framework in NLP called perspectivism. This approach aims to leverage data annotated by different individuals to model diverse perspectives that affect their opinions on subjective phenomena such as irony. In this context, we propose MultiPICo, a multilingual perspectivist corpus of ironic short conversations in different languages and linguistic varieties extracted from Twitter and Reddit. The corpus includes sociodemographic information about its annotators. Our analysis of the annotated corpus shows how different demographic cohorts may significantly disagree on their annotation of irony and how certain cultural factors influence the perception of the phenomenon and the agreement on the annotation. Moreover, we show how disaggregated annotations and rich annotator metadata can be exploited to benchmark the ability of large language models to recognize irony, their positionality with respect to sociodemographic groups, and the efficacy of perspective-taking prompting for irony detection in multiple languages.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
QL3TNCYI,conferencePaper,2024.0,"Trivedi, Harsh; Khot, Tushar; Hartmann, Mareike; Manku, Ruskin; Dong, Vinty; Li, Edward; Gupta, Shashank; Sabharwal, Ashish; Balasubramanian, Niranjan",AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.850,https://aclanthology.org/2024.acl-long.850,"Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine,1 a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our ‘normal’ tasks and ~30% of ‘challenge’ tasks, while other models solve at least 16% fewer. This highlights the benchmark’s difficulty and AppWorld’s potential to push the frontiers of interactive coding agents.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
BGEJAH79,conferencePaper,2024.0,"Liu, Yu Lu; Blodgett, Su Lin; Cheung, Jackie; Liao, Q. Vera; Olteanu, Alexandra; Xiao, Ziang",ECBD: Evidence-Centered Benchmark Design for NLP,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/2024.acl-long.861,https://aclanthology.org/2024.acl-long.861,"Benchmarking is seen as critical to assessing progress in NLP. However, creating a benchmark involves many design decisions (e.g., which datasets to include, which metrics to use) that often rely on tacit, untested assumptions about what the benchmark is intended to measure or is actually measuring. There is currently no principled way of analyzing these decisions and how they impact the validity of the benchmark’s measurements. To address this gap, we draw on evidence-centered design in educational assessments and propose EvidenceCentered Benchmark Design (ECBD), a framework which formalizes the benchmark design process into five modules. ECBD specifies the role each module plays in helping practitioners collect evidence about capabilities of interest. Specifically, each module requires benchmark designers to describe, justify, and support benchmark design choices—e.g., clearly specifying the capabilities the benchmark aims to measure or how evidence about those capabilities is collected from model responses. To demonstrate the use of ECBD, we conduct case studies with three benchmarks: BoolQ, SuperGLUE, and HELM. Our analysis reveals common trends in benchmark design and documentation that could threaten the validity of benchmarks’ measurements.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),ACL.csv,,,,,,
GS6L386S,conferencePaper,2024.0,"Zhu, Andrew; Hwang, Alyssa; Dugan, Liam; Callison-Burch, Chris","FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models",Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.2,https://aclanthology.org/2024.acl-short.2,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
JH9W3ANF,conferencePaper,2024.0,"Liang, Zhenwen; Guo, Kehan; Liu, Gang; Guo, Taicheng; Zhou, Yujun; Yang, Tianyu; Jiao, Jiajun; Pi, Renjie; Zhang, Jipeng; Zhang, Xiangliang",SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.11,https://aclanthology.org/2024.acl-short.11,"The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and freeresponse formats, ensuring a comprehensive evaluation of AI models’ abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both opensource and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
Y65WKZ3G,journalArticle,,"Gui, Honghao; Yuan, Lin; Ye, Hongbin; Zhang, Ningyu; Sun, Mengshu; Liang, Lei; Chen, Huajun",IEPILE: Unearthing Large-Scale Schema-Based Information Extraction Corpus,,,,"Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPILE, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPILE by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimentally, IEPILE enhance the performance of LLMs for IE, with notable improvements in zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.",,,ACL.csv,,,,,,
IFJYYRGM,conferencePaper,2024.0,"Ahmed, Shafiuddin Rehan; Wang, Zhiyong; Baker, George; Stowe, Kevin; Martin, James",Generating Harder Cross-document Event Coreference Resolution Datasets using Metaphoric Paraphrasing,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.27,https://aclanthology.org/2024.acl-short.27,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
GDNCBGF5,conferencePaper,2024.0,"Ho, Gia-Bao; Tan, Chang; Darban, Zahra; Salehi, Mahsa; Haf, Reza; Buntine, Wray",MTP: A Dataset for Multi-Modal Turning Points in Casual Conversations,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.30,https://aclanthology.org/2024.acl-short.30,"Detecting critical moments, such as emotional outbursts or changes in decisions during conversations, is crucial for understanding shifts in human behavior and their consequences. Our work introduces a novel problem setting focusing on these moments as turning points (TPs), accompanied by a meticulously curated, high-consensus, human-annotated multi-modal dataset. We provide precise timestamps, descriptions, and visual-textual evidence highlighting changes in emotions, behaviors, perspectives, and decisions at these turning points. We also propose a framework, TPMaven, utilizing state-of-the-art vision-language models to construct a narrative from the videos and large language models to classify and detect turning points in our multi-modal dataset. Evaluation results show that TPMaven achieves an F1-score of 0.88 in classification and 0.61 in detection, with additional explanations aligning with human expectations.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
DAJBZTRX,conferencePaper,2024.0,"Du, Mengfei; Wu, Binhao; Li, Zejun; Huang, Xuanjing; Wei, Zhongyu",EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.33,https://aclanthology.org/2024.acl-short.33,"The recent rapid development of Large VisionLanguage Models (LVLMs) has indicated their potential for embodied tasks. However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs. The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective. Experiments expose the insufficient capacity of current LVLMs (even GPT4V). We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs’ embodied spatial understanding.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
7FJ6QRW2,conferencePaper,2024.0,"Wretblad, Niklas; Riseby, Fredrik; Biswas, Rahul; Ahmadi, Amin; Holmström, Oskar",Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.34,https://aclanthology.org/2024.acl-short.34,"Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of ‘noise,’ such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRDBench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold SQL queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the benchmark’s reliability. Surprisingly, when evaluating models on corrected SQL queries, zero-shot baselines surpassed the performance of state-of-the-art prompting methods. We conclude that informative noise labels and reliable benchmarks are crucial to developing new Textto-SQL methods that can handle varying types of noise. All datasets, annotations, and code are available at this URL.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
DFJKIR3Z,conferencePaper,2024.0,"Reddy, Varshini; Koncel-Kedziorski, Rik; Lai, Viet; Krumdick, Michael; Lovering, Charles; Tanner, Chris",DocFinQA: A Long-Context Financial Reasoning Dataset,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.42,https://aclanthology.org/2024.acl-short.42,"For large language models (LLMs) to be effective in the financial domain – where each decision can have a significant impact – it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents spanning hundreds of pages, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. Based on our experiments, DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case study on a subset of the longest documents in DocFinQA and find that models particularly struggle with these documents. Addressing these challenges may have a wide-reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis. DocFinQA dataset is publicly accessible1.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
QIDKHMT9,conferencePaper,2024.0,"Haq, Saiful; Sharma, Ashutosh; Khattab, Omar; Chhaya, Niyati; Bhattacharyya, Pushpak",IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian Languages,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.46,https://aclanthology.org/2024.acl-short.46,"In this paper, we introduce Neural Information Retrieval resources for 11 widely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian language families (Indo-Aryan and Dravidian). These resources include (a) INDICMARCO, a multilingual version of the MS MARCO dataset in 11 Indian Languages created using Machine Translation, and (b) IndicColBERT, a collection of 11 distinct Monolingual Neural Information Retrieval models, each trained on one of the 11 languages in the INDIC-MARCO dataset. To the best of our knowledge, IndicIRSuite is the first attempt at building large-scale Neural Information Retrieval resources for a large number of Indian languages, and we hope that it will help accelerate research in Neural IR for Indian Languages. Experiments demonstrate that Indic-ColBERT achieves 47.47% improvement in the MRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian languages except Oriya, 12.26% improvement in the NDCG@10 score averaged over the MIRACL Bengali and Hindi Language baselines, and 20% improvement in the MRR@100 Score over the Mr. Tydi Bengali Language baseline. IndicIRSuite is available at github.com/saifulhaq95/IndicIRSuite.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
DHX4SDUG,conferencePaper,2024.0,"Sun, Bin; Li, Jianfeng; Zhou, Hao; Meng, Fandong; Li, Kan; Zhou, Jie",Exploring Conditional Variational Mechanism to Pinyin Input Method for Addressing One-to-Many Mappings in Low-Resource Scenarios,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.56,https://aclanthology.org/2024.acl-short.56,"Pinyin input method engine (IME) refers to the transformation tool from pinyin sequence to Chinese characters, which is widely used on mobile phone applications. Due to the homophones, Pinyin IME suffers from the oneto-many mapping problem in the process of pinyin sequences to Chinese characters. To solve the above issue, this paper makes the first exploration to leverage an effective conditional variational mechanism (CVM) for pinyin IME. However, to ensure the stable and smooth operation of Pinyin IME under low-resource conditions (e.g., on offline mobile devices), we should balance diversity, accuracy, and efficiency with CVM, which is still challenging. To this end, we employ a novel strategy that simplifies the complexity of semantic encoding by facilitating the interaction between pinyin and the Chinese character information during the construction of continuous latent variables. Concurrently, the accuracy of the outcomes is enhanced by capitalizing on the discrete latent variables. Experimental results demonstrate the superior performance of our method.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
8RDCA3QU,conferencePaper,2024.0,"Singh, Anushka; Sai, Ananya; Dabre, Raj; Puduppully, Ratish; Kunchukuttan, Anoop; Khapra, Mitesh",How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.58,https://aclanthology.org/2024.acl-short.58,,2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
G5EPV3AY,conferencePaper,2024.0,"Adeyemi, Mofetoluwa; Oladipo, Akintunde; Pradeep, Ronak; Lin, Jimmy",Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.59,https://aclanthology.org/2024.acl-short.59,"Large language models (LLMs) as listwise rerankers have shown impressive zero-shot capabilities in various passage ranking tasks. Despite their success, there is still a gap in existing literature on their effectiveness in reranking low-resource languages. To address this, we investigate how LLMs function as listwise rerankers in cross-lingual information retrieval (CLIR) systems with queries in English and passages in four African languages: Hausa, Somali, Swahili, and Yoruba. We analyze and compare the effectiveness of monolingual reranking using either query or document translations. We also evaluate the effectiveness of LLMs when leveraging their own generated translations. To grasp the general picture, we examine the effectiveness of multiple LLMs—the proprietary models RankGPT4 and RankGPT3.5, along with the open-source model RankZephyr. While the document translation setting, i.e., both queries and documents are in English, leads to the best reranking effectiveness, our results indicate that for specific LLMs, reranking in the African language setting achieves competitive effectiveness with the cross-lingual setting, and even performs better when using the LLM’s own translations.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
G3VKVXVZ,journalArticle,,"Keleg, Amr; Magdy, Walid; Goldwater, Sharon",Estimating the Level of Dialectness Predicts Interannotator Agreement in Multi-dialect Arabic Datasets,,,,"On annotating multi-dialect Arabic datasets, it is common to randomly assign the samples across a pool of native Arabic speakers. Recent analyses recommended routing dialectal samples to native speakers of their respective dialects to build higher-quality datasets. However, automatically identifying the dialect of samples is hard. Moreover, the pool of annotators who are native speakers of specific Arabic dialects might be scarce. Arabic Level of Dialectness (ALDi) was recently introduced as a quantitative variable that measures how sentences diverge from Standard Arabic. On randomly assigning samples to annotators, we hypothesize that samples of higher ALDi scores are harder to label especially if they are written in dialects that the annotators do not speak. We test this by analyzing the relation between ALDi scores and the annotators’ agreement, on 15 public datasets having raw individual sample annotations for various sentence-classification tasks. We find strong evidence supporting our hypothesis for 11 of them. Consequently, we recommend prioritizing routing samples of high ALDi scores to native speakers of each sample’s dialect, for which the dialect could be automatically identified at higher accuracies.",,,ACL.csv,,,,,,
NEYH88TM,conferencePaper,2024.0,"Xu, Zhipeng; Liu, Zhenghao; Yan, Yukun; Liu, Zhiyuan; Yu, Ge; Xiong, Chenyan",Cleaner Pretraining Corpus Curation with Neural Web Scraping,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.72,https://aclanthology.org/2024.acl-short.72,"The web contains large-scale, diverse, and abundant information to satisfy the informationseeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https: //github.com/OpenMatch/NeuScraper.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
KAFRL7N7,conferencePaper,2024.0,"Bhutani, Mukul; Robinson, Kevin; Prabhakaran, Vinodkumar; Dave, Shachi; Dev, Sunipa",SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.75,https://aclanthology.org/2024.acl-short.75,"While generative multilingual models are rapidly being deployed, their safety and fairness evaluations are largely limited to resources collected in English. This is especially problematic for evaluations targeting inherently socio-cultural phenomena such as stereotyping, where it is important to build multilingual resources that reflect the stereotypes prevalent in respective language communities. However, gathering these resources, at scale, in varied languages and regions pose a significant challenge as it requires broad socio-cultural knowledge and can also be prohibitively expensive. To overcome this critical gap, we employ a recently introduced approach that couples LLM generations for scale with culturally situated validations for reliability, and build SeeGULL Multilingual, a global-scale multilingual dataset of social stereotypes, containing over 25K stereotypes, spanning 23 pairs of languages and regions they are common in,1 with human annotations, and demonstrate its utility in identifying gaps in model evaluations. Content warning: Stereotypes shared in this paper can be offensive.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
ABRVLGIL,conferencePaper,2024.0,"Horvitz, Zachary; Chen, Jingru; Aditya, Rahul; Srivastava, Harshvardhan; West, Robert; Yu, Zhou; McKeown, Kathleen",Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/2024.acl-short.76,https://aclanthology.org/2024.acl-short.76,"Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. We investigate whether large language models (LLMs) can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to “unfun” jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset where we find that GPT-4’s synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.",2024,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),ACL.csv,,,,,,
IHBLLIBV,dataset,2025.0,"Hauser, Jakob Elias",HiST-LLM,,10.5281/ZENODO.14671247,https://zenodo.org/doi/10.5281/zenodo.14671247,"Large Language Models (LLMs) have the potential to transform humanities and social science research, yet their history knowledge and comprehension at a graduate level remains untested. Benchmarking LLMs in history is particularly challenging, given that human knowledge of history is inherently unbalanced, with more information available on Western history and recent periods. We introduce the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over 2,700 scholarly references. This dataset covers every major world region from the Neolithic period to the Industrial Revolution and includes information reviewed and assembled by history experts and graduate research assistants. Using this dataset, we benchmark a total of seven models from the Gemini, OpenAI, and Llama families. We find that, in a four-choice format, LLMs have a balanced accuracy ranging from 33.6% (Llama-3.1-8B) to 46% (GPT-4-Turbo), outperforming random guessing (25%) but falling short of expert comprehension. LLMs perform better on earlier historical periods. Regionally, performance is more even but still better for the Americas and lowest in Oceania and Sub-Saharan Africa for the more advanced models. Our benchmark shows that while LLMs possess some expert-level historical knowledge, there is considerable room for improvement.",2025-01-16,,NeurIPS.csv,,,,,,
58JV2V3K,journalArticle,,"Dong, Linfeng; Wang, Wei; Qiao, Yu; Sun, Xiao",LucidAction: A Hierarchical and Multi-model Dataset for Comprehensive Action Quality Assessment,,,,,,,NeurIPS.csv,,,,,,
VRL2G4I9,journalArticle,,"Zhang, Hang; Sun, Jiawei; Chen, Renqi; Liu, Wei; Yuan, Zhonghang; Zheng, Xinzhe; Wang, Zhefan; Yang, Zhiyuan; Yan, Hang; Zhong, Hansen; Wang, Xiqing; Ouyang, Wanli; Yang, Fan; Dong, Nanqing",Empowering and Assessing the Utility of Large Language Models in Crop Science,,,,"Large language models (LLMs) have demonstrated remarkable efficacy across knowledge-intensive tasks. Nevertheless, their untapped potential in crop science presents an opportunity for advancement. To narrow this gap, we introduce CROP3, which includes a novel instruction tuning dataset specifically designed to enhance LLMs’ professional capabilities in the crop science sector, along with a benchmark that serves as a comprehensive evaluation of LLMs’ understanding of the domain knowledge. The CROP dataset is curated through a task-oriented and LLM-human integrated pipeline, comprising 210,038 single-turn and 1,871 multi-turn dialogues related to crop science scenarios. The CROP benchmark includes 5,045 multiplechoice questions covering three difficulty levels. Our experiments based on the CROP benchmark demonstrate notable enhancements in crop science-related tasks when LLMs are fine-tuned with the CROP dataset. To the best of our knowledge, CROP dataset is the first-ever instruction tuning dataset in the crop science domain. We anticipate that CROP will accelerate the adoption of LLMs in the domain of crop science, ultimately contributing to global food production.",,,NeurIPS.csv,,,,,,
IZFMDATR,journalArticle,,"Luo, Jialin; Wang, Yuanzhi; Gu, Ziqi; Qiu, Yide; Yao, Shuaizhen; Wang, Fuyun; Xu, Chunyan; Zhang, Wenhua; Wang, Dan; Cui, Zhen","MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation",,,,"Recently, the diffusion-based generative paradigm has achieved impressive general image generation capabilities with text prompts due to its accurate distribution modeling and stable training process. However, generating diverse remote sensing (RS) images that are tremendously different from general images in terms of scale and perspective remains a formidable challenge due to the lack of a comprehensive remote sensing image generation dataset with various modalities, ground sample distances (GSD), and scenes. In this paper, we propose a Multi-modal, MultiGSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for textto-image generation in diverse remote sensing scenarios. Specifically, we first collect nine publicly available RS datasets and conduct standardization for all samples. To bridge RS images to textual semantic information, we utilize a largescale pretrained vision-language model to automatically output text prompts and perform hand-crafted rectification, resulting in information-rich text-image pairs (including multi-modal images). In particular, we design some methods to obtain the images with different GSD and various environments (e.g., low-light, foggy) in a single sample. With extensive manual screening and refining annotations, we ultimately obtain a MMM-RS dataset that comprises approximately 2.1 million text-image pairs. Extensive experimental results verify that our proposed MMMRS dataset allows off-the-shelf diffusion models to generate diverse RS images across various modalities, scenes, weather conditions, and GSD. The dataset is available at https://github.com/ljl5261/MMM-RS.",,,NeurIPS.csv,,,,,,
I249SJTS,journalArticle,,"Chen, Cheng; Zhu, Junchen; Luo, Xu; Shen, Heng Tao; Song, Jingkuan; Gao, Lianli",CoIN: A Benchmark of Continual Instruction Tuning for Multimodal Large Language Models,,,,"Instruction tuning demonstrates impressive performance in adapting Multimodal Large Language Models (MLLMs) to follow task instructions and improve generalization ability. By extending tuning across diverse tasks, MLLMs can further enhance their understanding of world knowledge and instruction intent. However, continual instruction tuning has been largely overlooked and there are no public benchmarks available. In this paper, we present CoIN, a comprehensive benchmark tailored for assessing the behavior of existing MLLMs under continual instruction tuning. CoIN comprises 10 meticulously crafted datasets spanning 8 tasks, ensuring diversity and serving as a robust evaluation framework to assess crucial aspects of continual instruction tuning, such as task order, instruction diversity and volume. Additionally, apart from traditional evaluation, we design another LLM-based metric to assess the knowledge preserved within MLLMs for reasoning. Following an in-depth evaluation of several MLLMs, we demonstrate that they still suffer catastrophic forgetting, and the failure in instruction alignment assumes the main responsibility, instead of reasoning knowledge forgetting. To this end, we introduce MoELoRA which is effective in retaining the previous instruction alignment. Codes and datasets are publicly available https://github.com/zackschen/CoIN.",,,NeurIPS.csv,,,,,,
NG7SICZV,journalArticle,,"Mou, Yutao; Zhang, Shikun; Ye, Wei",SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types,,,,"Ensuring the safety of large language model (LLM) applications is essential for developing trustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First, they focus solely on either discriminative or generative evaluation paradigms while ignoring their interconnection. Second, they rely on standardized inputs, overlooking the effects of widespread prompting techniques, such as system prompts, few-shot demonstrations, and chain-of-thought prompting. To overcome these issues, we developed SG-Bench, a novel benchmark to assess the generalization of LLM safety across various tasks and prompt types. This benchmark integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak on LLM safety. Our assessment of 3 advanced proprietary LLMs and 10 opensource LLMs with the benchmark reveals that most LLMs perform worse on discriminative tasks than generative ones, and are highly susceptible to prompts, indicating poor generalization in safety alignment. We also explain these findings quantitatively and qualitatively to provide insights for future research.2 Warning: this paper includes examples that may be offensive or harmful.",,,NeurIPS.csv,,,,,,
N2NIA4VB,journalArticle,,"Yan, Zhiyuan; Yao, Taiping; Chen, Shen; Zhao, Yandan; Fu, Xinghe; Zhu, Junwei; Luo, Donghao; Wang, Chengjie; Ding, Shouhong; Wu, Yunsheng; Yuan, Li",DF40: Toward Next-Generation Deepfake Detection,,,,"We propose a new comprehensive benchmark to revolutionize the current deepfake detection field to the next generation. Predominantly, existing works identify top-notch detection algorithms and models by adhering to the common practice: training detectors on one specific dataset (e.g., FF++ [62]) and testing them on other prevalent deepfake datasets. This protocol is often regarded as a ""golden compass"" for navigating SoTA detectors. But can these stand-out ""winners"" be truly applied to tackle the myriad of realistic and diverse deepfakes lurking in the real world? If not, what underlying factors contribute to this gap? In this work, we found the dataset (both train and test) can be the ""primary culprit"" due to the following: (1) forgery diversity: Deepfake techniques are commonly referred to as both face forgery (face-swapping and face-reenactment) and entire face synthesis (especially face). Most existing datasets only contain partial types of them, with limited forgery methods implemented (e.g., 2 swapping and 2 reenactment methods in FF++); (2) forgery realism: The dominated training dataset, FF++, contains out-of-date forgery techniques from the past four years. ""Honing skills"" on these forgeries makes it difficult to guarantee effective detection generalization toward nowadays’ SoTA deepfakes; (3) evaluation protocol: Most detection works perform evaluations on one type, e.g., training and testing on face-swapping types only, which hinders the development of universal deepfake detectors. To address this dilemma, we construct a highly diverse and large-scale deepfake detection dataset called DF40, which comprises 40 distinct deepfake techniques (10 times larger than FF++). We then conduct comprehensive evaluations using 4 standard evaluation protocols and 8 representative detection methods, resulting in over 2,000 evaluations. Through these evaluations, we provide an extensive analysis from various perspectives, leading to 7 new insightful findings contributing to the field. We also open up 4 valuable yet previously underexplored research questions to inspire future works. We release our dataset, code, and checkpoints at https://github.com/YZY-stack/DF40.",,,NeurIPS.csv,,,,,,
ASZDEX82,journalArticle,,"Hui, Yulong; Lu, Yao; Zhang, Huanchen",UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis,,,,"The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval. We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",,,NeurIPS.csv,,,,,,
CYUKMDR3,journalArticle,,"Cao, Jiahuan; Liu, Yang; Shi, Yongxin; Ding, Kai; Jin, Lianwen",WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts,,,,"Large Language Models (LLMs) have made significant advancements across numerous domains, but their capabilities in Chinese Classical Literature and Language Arts (CCLLA) remain largely unexplored due to the limited scope and tasks of existing benchmarks. To fill this gap, we propose WenMind, a comprehensive benchmark dedicated for evaluating LLMs in CCLLA. WenMind covers the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, comprising 4,875 question-answer pairs, spanning 42 fine-grained tasks, 3 question formats, and 2 evaluation scenarios: domain-oriented and capability-oriented. Based on WenMind, we conduct a thorough evaluation of 31 representative LLMs, including general-purpose models and ancient Chinese LLMs. The results reveal that even the best-performing model, ERNIE-4.0, only achieves a total score of 64.3, indicating significant room for improvement of LLMs in the CCLLA domain. We also provide insights into the strengths and weaknesses of different LLMs and highlight the importance of pre-training data in achieving better results. Overall, WenMind serves as a standardized and comprehensive baseline, providing valuable insights for future CCLLA research. Our benchmark and related code are available at https://github.com/SCUT-DLVCLab/WenMind.",,,NeurIPS.csv,,,,,,
MDWXU772,journalArticle,,"Gong, Biao; Tan, Shuai; Feng, Yutong; Xie, Xiaoying; Li, Yuyuan; Chen, Chaochao; Zheng, Kecheng; Shen, Yujun; Zhao, Deli",UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,,,,"This work presents a unified knowledge protocol, called UKnow, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under UKnow format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following UKnow protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 visionrelated ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on 4 benchmarks demonstrate the potential of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. See Appendix A to download the dataset.",,,NeurIPS.csv,,,,,,
86HQJEK3,journalArticle,,"Lin, Shuyi; He, Haoyu; Wei, Tianhao; Xu, Kaidi; Zhang, Huan; Singh, Gagandeep; Liu, Changliu; Tan, Cheng",NN4SysBench: Characterizing Neural Network Verification for Computer Systems,,,,"We present NN4SysBench, a benchmark suite for neural network verification that is composed of applications from the domain of computer systems. We call these neural networks for computer systems or NN4Sys. NN4Sys is booming: there are many proposals for using neural networks in computer systems—for example, databases, OSes, and networked systems—many of which are safetycritical. Neural network verification is a technique to formally verify whether neural networks satisfy safety properties. We however observe that NN4Sys has some unique characteristics that today’s verification tools overlook and have limited support. Therefore, this benchmark suite aims at bridging the gap between NN4Sys and the verification by using impactful NN4Sys applications as benchmarks to illustrate computer systems’ unique challenges. We also build a compatible version of NN4SysBench, so that today’s verifiers can also work on these benchmarks with approximately the same verification difficulties. The code is available here: https://github.com/Khoury-srg/NN4SysBench.",,,NeurIPS.csv,,,,,,
SAHJD6BL,dataset,2024.0,"Li, Ruohan; Xie, Yiqun; Wang, Dongdong",SolarCube,,10.5281/ZENODO.11498739,https://zenodo.org/doi/10.5281/zenodo.11498739,"Solar power is a critical source of renewable energy, offering significant potential to lower greenhouse gas emissions and mitigate climate change. However, the cloud induced-variability of solar radiation reaching the earth’s surface presents a challenge for integrating solar power into the grid (e.g., storage and backup management). The new generation of geostationary satellites such as GOES-16 has become an important data source for large-scale and high temporal frequency solar radiation forecasting. However, no machine-learning-ready dataset has integrated geostationary satellite data with fine-grained solar radiation information to support forecasting model development and benchmarking with consistent metrics. We present SolarCube, a new ML-ready benchmark dataset for solar radiation forecasting. SolarCube covers 19 study areas distributed over multiple continents: North America, South America, Asia, and Oceania. The dataset supports short (i.e., 30 minutes to 6 hours) and long-term (i.e., day-ahead or longer) solar radiation forecasting at both point-level (i.e., specific locations of monitoring stations) and area-level, by processing and integrating data from multiple sources, including geostationary satellite images, physics-derived solar radiation, and ground station observations from different monitoring networks over the globe. We also evaluated a set of forecasting models for point- and image-based time-series data to develop performance benchmarks under different testing scenarios. The dataset is available at https://doi.org/10.5281/zenodo.11498739. A Python library is available to conveniently generate different variations of the dataset based on user needs, along with baseline models at https://github.com/Ruohan-Li/SolarCube.",2024-06-12,,NeurIPS.csv,,,,,,
K4MGCCB2,journalArticle,,"Chen, Zijian; Sun, Wei; Tian, Yuan; Jia, Jun; Zhang, Zicheng; Wang, Jiarui; Huang, Ru; Min, Xiongkuo; Zhai, Guangtao; Zhang, Wenjun",GAIA: Rethinking Action Quality Assessment for AI-Generated Videos,,,,"Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV). Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs. To address these problems, we construct GAIA, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions. We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods. Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods perform poorly with an average SRCC of 0.454, 0.191, and 0.519, respectively, indicating a sizable gap between current models and human action perception patterns in AIGVs. Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs.",,,NeurIPS.csv,,,,,,
ZSIQZ8UH,journalArticle,,"Yang, Bing; Quan, Changsheng; Wang, Yabo; Wang, Pengyu; Yang, Yujie; Fang, Ying; Shao, Nian; Bu, Hui; Xu, Xin; Li, Xiaofei",RealMAN: A Real-Recorded and Annotated Microphone Array Dataset for Dynamic Speech Enhancement and Localization,,,,"The training of deep learning-based multichannel speech enhancement and source localization systems relies heavily on the simulation of room impulse response and multichannel diffuse noise, due to the lack of large-scale real-recorded datasets. However, the acoustic mismatch between simulated and real-world data could degrade the model performance when applying in real-world scenarios. To bridge this simulation-to-real gap, this paper presents a new relatively large-scale Realrecorded and annotated Microphone Array speech&Noise (RealMAN) dataset2. The proposed dataset is valuable in two aspects: 1) benchmarking speech enhancement and localization algorithms in real scenarios; 2) offering a substantial amount of real-world training data for potentially improving the performance of real-world applications. Specifically, a 32-channel array with high-fidelity microphones is used for recording. A loudspeaker is used for playing source speech signals (about 35 hours of Mandarin speech). A total of 83.7 hours of speech signals (about 48.3 hours for static speaker and 35.4 hours for moving speaker) are recorded in 32 different scenes, and 144.5 hours of background noise are recorded in 31 different scenes. Both speech and noise recording scenes cover various common indoor, outdoor, semi-outdoor and transportation environments, which enables the training of general-purpose speech enhancement and source localization networks. To obtain the task-specific annotations, speaker location is annotated with an omni-directional fisheye camera by automatically detecting the loudspeaker. The direct-path signal is set as the target clean speech for speech enhancement, which is obtained by filtering the source speech signal with an estimated direct-path propagation filter. Baseline experiments demonstrate that i) compared to using simulated data, the proposed dataset is indeed able to train better speech enhancement and source localization networks; ii) using various sub-arrays of the proposed 32-channel microphone array can successfully train variable-array networks that can be directly used to unseen arrays.",,,NeurIPS.csv,,,,,,
8MIVEBFE,journalArticle,,"Lu, Junyu; Xu, Bo; Zhang, Xiaokun; Wang, Hongbo; Zhu, Haohao; Zhang, Dongyu; Yang, Liang; Lin, Hongfei",Towards Comprehensive Detection of Chinese Harmful Memes,,,,"Harmful memes have proliferated on the Chinese Internet, while research on detecting Chinese harmful memes significantly lags behind due to the absence of reliable datasets and effective detectors. To this end, we focus on the comprehensive detection of Chinese harmful memes. We construct TOXICN MM, the first Chinese harmful meme dataset, which consists of 12,000 samples with finegrained annotations for various meme types. Additionally, we propose a baseline detector, Multimodal Knowledge Enhancement (MKE), incorporating contextual information of meme content generated by the LLM to enhance the understanding of Chinese memes. During the evaluation phase, we conduct extensive quantitative experiments and qualitative analyses on multiple baselines, including LLMs and our MKE. The experimental results indicate that detecting Chinese harmful memes is challenging for existing models while demonstrating the effectiveness of MKE.1 Disclaimer: The samples presented by this paper may be considered offensive.",,,NeurIPS.csv,,,,,,
SJCA6MKR,journalArticle,,"Lin, Xiaohan; Cao, Qingxing; Huang, Yinya; Wang, Haiming; Lu, Jianqiao; Liu, Zhengying; Song, Linqi; Liang, Xiaodan",FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving,,,,"Formal verification (FV) has witnessed growing significance with emerging program synthesis by the evolving large language models (LLMs). However, current formal verification mainly resorts to symbolic verifiers or hand-craft rules, resulting in limitations for extensive and flexible verification. On the other hand, formal systems for automated theorem proving, such as Isabelle, serve as another line of rigorous verification, upheld by extensive rules and theorems. In this paper, we propose FVEL3, an interactive Formal Verification Environment with LLMs. Specifically, FVEL transforms a given code to be verified into Isabelle, and then conducts verification via neural automated theorem proving with an LLM. The joined paradigm leverages the rigorous yet abundant formulated and organized rules in Isabelle and is also convenient for introducing and adjusting cutting-edge LLMs. To achieve this goal, we extract a large-scale dataset for automated formal verification named FVELER3. The FVELER dataset includes code dependencies and verification processes that are formulated in Isabelle, containing 758 theories, 29,304 lemmas, and 201,498 proof steps with in-depth dependencies. We benchmark FVELER in the FVEL environment by fine-tuning LLMs with FVELER and then evaluating them on Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned Llama3-8B solves 17.39% (69→81) more problems, and Mistral-7B 12% (75→84) more problems in SV-COMP. And the proportion of proof errors is reduced. Project page: https://fveler.github.io/.",,,NeurIPS.csv,,,,,,
2NNYCNYS,journalArticle,,"Li, Yiwei; Shi, Jiayi; Feng, Shaoxiong; Yuan, Peiwen; Wang, Xinglin; Pan, Boyuan; Wang, Heda; Hu, Yao; Li, Kan",Instruction Embedding: Latent Representations of Instructions Towards Task Identification,,,,"Instruction data is crucial for improving the capability of Large Language Models (LLMs) to align with human-level performance. Recent research LIMA demonstrates that alignment is essentially a process where the model adapts instructions’ interaction style or format to solve various tasks, leveraging pre-trained knowledge and skills. Therefore, for instructional data, the most important aspect is the task it represents, rather than the specific semantics and knowledge information. The latent representations of instructions play roles for some instruction-related tasks like data selection and demonstrations retrieval. However, they are always derived from text embeddings, encompass overall semantic information that influences the representation of task categories. In this work, we introduce a new concept, instruction embedding, and construct Instruction Embedding Benchmark (IEB) for its training and evaluation. Then, we propose a baseline Prompt-based Instruction Embedding (PIE) method to make the representations more attention on tasks. The evaluation of PIE, alongside other embedding methods on IEB with two designed tasks, demonstrates its superior performance in accurately identifying task categories. Moreover, the application of instruction embeddings in four downstream tasks showcases its effectiveness and suitability for instruction-related tasks1.",,,NeurIPS.csv,,,,,,
PG2VLHAV,journalArticle,,"Liu, Hongbin; Guo, Moyang; Jiang, Zhengyuan; Wang, Lun; Gong, Neil Zhenqiang",AudioMarkBench: Benchmarking Robustness of Audio Watermarking,,,,"The increasing realism of synthetic speech, driven by advancements in text-tospeech models, raises ethical concerns regarding impersonation and disinformation. Audio watermarking offers a promising solution via embedding humanimperceptible watermarks into AI-generated audios. However, the robustness of audio watermarking against common/adversarial perturbations remains understudied. We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against watermark removal and watermark forgery. AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations. We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings. Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions. Our dataset and code are publicly available at https://github.com/moyangkuo/AudioMarkBench.",,,NeurIPS.csv,,,,,,
INUH5EEP,journalArticle,,"Yao, Jing; Yi, Xiaoyuan; Xie, Xing",CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses,,,,"The rapid progress in Large Language Models (LLMs) poses potential risks such as generating unethical content. Assessing the values embedded in LLMs’ generated responses can help expose their misalignment, but this relies on reference-free value evaluators, e.g. fine-tuned LLMs or closed-source models like GPT-4. Nevertheless, two key challenges emerge in open-ended value evaluation: the evaluator should adapt to changing human value definitions with minimal annotation, against their own bias (adaptability); and remain robust across varying value expressions and scenarios (generalizability). To handle these challenges, we introduce CLAVE, a novel framework that integrates two complementary LLMs: a large model to extract high-level value concepts from diverse responses, leveraging its extensive knowledge and generalizability, and a small model fine-tuned on these concepts to adapt to human value annotations. This dual-model framework enables adaptation to any value system using <100 human-labeled samples per value type. We also present ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples across diverse domains, covering three major value systems. We benchmark the performance of 15+ popular LLM evaluators and fully analyze their strengths and weaknesses. Our findings reveal that CLAVE combining a large prompt-based model and a small fine-tuned one serves as an optimal balance in value evaluation.",,,NeurIPS.csv,,,,,,
Y79SCJCW,journalArticle,,"Chen, Xi; Qin, Chuan; Fang, Chuyu; Wang, Chao; Zhu, Chen; Zhuang, Fuzhen; Zhu, Hengshu; Xiong, Hui",Job-SDF: A Multi-Granularity Dataset for Job Skill Demand Forecasting and Benchmarking,,,,"In a rapidly evolving job market, skill demand forecasting is crucial as it enables policymakers and businesses to anticipate and adapt to changes, ensuring that workforce skills align with market needs, thereby enhancing productivity and competitiveness. Additionally, by identifying emerging skill requirements, it directs individuals towards relevant training and education opportunities, promoting continuous self-learning and development. However, the absence of comprehensive datasets presents a significant challenge, impeding research and the advancement of this field. To bridge this gap, we present Job-SDF, a dataset designed to train and benchmark job-skill demand forecasting models. Based on millions of public job advertisements collected from online recruitment platforms, this dataset encompasses monthly recruitment demand. Our dataset uniquely enables evaluating skill demand forecasting models at various granularities, including occupation, company, and regional levels. We benchmark a range of models on this dataset, evaluating their performance in standard scenarios, in predictions focused on lower value ranges, and in the presence of structural breaks, providing new insights for further research. Our code and dataset are publicly accessible via the https://github.com/Job-SDF/benchmark.",,,NeurIPS.csv,,,,,,
BYDGLYUW,journalArticle,,"Jin, Xin; Qiao, Qianqian; Lu, Yi; Wang, Huaye; Huang, Heng; Gao, Shan; Liu, Jianfei; Li, Rui",APDDv2: Aesthetics of Paintings and Drawings Dataset with Artist Labeled Scores and Comments,,,,"Datasets play a pivotal role in training visual models, facilitating the development of abstract understandings of visual features through diverse image samples and multidimensional attributes. However, in the realm of aesthetic evaluation of artistic images, datasets remain relatively scarce. Existing painting datasets are often characterized by limited scoring dimensions and insufficient annotations, thereby constraining the advancement and application of automatic aesthetic evaluation methods in the domain of painting. To bridge this gap, we introduce the Aesthetics Paintings and Drawings Dataset (APDD), the first comprehensive collection of paintings encompassing 24 distinct artistic categories and 10 aesthetic attributes. Building upon the initial release of APDDv1[Jin et al., 2024], our ongoing research has identified opportunities for enhancement in data scale and annotation precision. Consequently, APDDv2 boasts an expanded image corpus and improved annotation quality, featuring detailed language comments to better cater to the needs of both researchers and practitioners seeking high-quality painting datasets. Furthermore, we present an updated version of the Art Assessment Network for Specific Painting Styles, denoted as ArtCLIP. Experimental validation demonstrates the superior performance of this revised model in the realm of aesthetic evaluation, surpassing its predecessor in accuracy and efficacy. The dataset and model are available at https://github.com/BestiVictory/APDDv2.git.",,,NeurIPS.csv,,,,,,
E5CZZLQA,journalArticle,,"Khandekar, Nikhil; Jin, Qiao; Xiong, Guangzhi; Dunn, Soren; Applebaum, Serina S; Anwar, Zain; Sarfo-Gyamfi, Maame; Safranek, Conrad W; Anwar, Abid A; Zhang, Andrew; Gilson, Aidan; Singer, Maxwell B; Dave, Amisha; Taylor, Andrew; Zhang, Aidong; Chen, Qingyu; Lu, Zhiyong",MEDCALC-BENCH: Evaluating Large Language Models for Medical Calculations,,,,,,,NeurIPS.csv,,,,,,
ENX794ZS,journalArticle,,"Zhong, Ming; Lyu, Fang; Wang, Lulin; Geng, Hongna; Qiu, Lei; Cui, Huimin; Feng, Xiaobing",ComBack: A Versatile Dataset for Enhancing Compiler Backend Development Efficiency,,,,"Compiler backends are tasked with generating executable machine code for processors. With the proliferation of diverse processors, it is imperative for programmers to tailor specific compiler backends to accommodate each one. Meanwhile, compiler backend development is a laborious and time-consuming task, lacking effective automation methods. Although language models have demonstrated strong abilities in code related tasks, the lack of appropriate datasets for compiler backend development limits the application of language models in this field.",,,NeurIPS.csv,,,,,,
DC4W2JFN,journalArticle,,"Ma, Jiefeng; Wang, Yan; Liu, Chenyu; Du, Jun; Hu, Yu; Zhang, Zhenrong; Hu, Pengfei; Wang, Qing; Zhang, Jianshu",SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding,,,,"Accurate identification and organizing of textual content is crucial for the automation of document processing in the field of form understanding. Existing datasets, such as FUNSD and XFUND, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. This limitation overlooks the hierarchically structured representation of documents, constraining a comprehensive understanding of complex forms. To address this issue, we present the SRFUND, a hierarchically structured multi-task form understanding benchmark. SRFUND provides refined annotations on top of the original FUNSD and XFUND datasets, encompassing five tasks: (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery. We meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. Additionally, we introduce global hierarchical structure dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. The SRFUND dataset includes eight languages including English, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese, making it a powerful tool for understanding cross-lingual forms. Extensive experimental results demonstrate that the SRFUND dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understanding. The original data set and implementations of the baseline methods are available at https://sprateam-ustc.github.io/SRFUND.",,,NeurIPS.csv,,,,,,
3PS8XFCG,journalArticle,,"Silcock, Emily; Arora, Abhishek; D’Amico-Wong, Luca",Newswire: A Large-Scale Structured Database of a Century of Historical News,,,,"In the U.S. historically, local newspapers drew their content largely from newswires like the Associated Press. Historians argue that newswires played a pivotal role in creating a national identity and shared understanding of the world, but there is no comprehensive archive of the content sent over newswires. We reconstruct such an archive by applying a customized deep learning pipeline to hundreds of terabytes of raw image scans from thousands of local newspapers. The resulting dataset contains 2.7 million unique public domain U.S. newswire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model. To construct the Newswire dataset, we first recognize newspaper layouts and transcribe around 138 million structured article texts from raw image scans. We then use a customized neural bi-encoder model to de-duplicate reproduced articles, in the presence of considerable abridgement and noise, quantifying how widely each article was reproduced. A text classifier is used to ensure that we only include newswire articles, which historically are in the public domain. The structured data that accompany the texts provide rich information about the who (disambiguated individuals), what (topics), and where (georeferencing) of the news that millions of Americans read over the course of a century. We also include Library of Congress metadata information about the newspapers that ran the articles on their front pages. The Newswire dataset is useful both for large language modeling - expanding training data beyond what is available from modern web texts - and for studying a diversity of questions in computational linguistics, social science, and the digital humanities.",,,NeurIPS.csv,,,,,,
KLLVQQXR,journalArticle,,"Li, Xiaotong; Zhang, Fan; Diao, Haiwen; Wang, Yueze; Wang, Xinlong; Duan, Ling-Yu",DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception,,,,"Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, and spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions. Specifically, Perceptual Fusion integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs’ perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs. The dataset and code are publicly available at https://github.com/baaivision/DenseFusion.",,,NeurIPS.csv,,,,,,
I9MAISVX,journalArticle,,"Al-Tahan, Haider; Garrido, Quentin; Balestriero, Randall; Bouchacourt, Diane; Hazirbas, Caner; Ibrahim, Mark",UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling,,,,,,,NeurIPS.csv,,,,,,
5NT5NLPL,journalArticle,,"Xia, Peng; Chen, Ze; Tian, Juanxi; Gong, Yangrui; Hou, Ruibo; Xu, Yue; Wu, Zhenbang; Fan, Zhiyuan; Zhou, Yiyang; Zhu, Kangyu; Zheng, Wenhao; Wang, Zhaoyang; Wang, Xiao; Zhang, Xuchao; Bansal, Chetan; Niethammer, Marc; Huang, Junzhou; Zhu, Hongtu; Li, Yun; Sun, Jimeng; Ge, Zongyuan; Li, Gang; Zou, James; Yao, Huaxiu",CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models,,,,"Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to Comprehensively evAluate the tRustworthinESs of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://cares-ai.github.io/.",,,NeurIPS.csv,,,,,,
83WEKFCW,journalArticle,,"Yang, Ge; He, Changyi; Guo, Jinyang; Wu, Jianyu; Ding, Yifu; Liu, Aishan; Qin, Haotong; Ji, Pengliang; Liu, Xianglong",LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment,,,,"Although large language models (LLMs) have demonstrated their strong intelligence ability, the high demand for computation and storage hinders their practical application. To this end, many model compression techniques are proposed to increase the efficiency of LLMs. However, current researches only validate their methods on limited models, datasets, metrics, etc, and still lack a comprehensive evaluation under more general scenarios. So it is still a question of which model compression approach we should use under a specific case. To mitigate this gap, we present the Large Language Model Compression Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth analysis for LLM compression algorithms. We first analyze the actual model production requirements and carefully design evaluation tracks and metrics. Then, we conduct extensive experiments and comparison using multiple mainstream LLM compression approaches. Finally, we perform an in-depth analysis based on the evaluation and provide useful insight for LLM compression design. We hope our LLMCBench can contribute insightful suggestions for LLM compression algorithm design and serve as a foundation for future research. Our code is available at https://github.com/AboveParadise/LLMCBench.",,,NeurIPS.csv,,,,,,
TEG6KVS5,journalArticle,,"Eyzaguirre, Cristóbal; Tang, Eric; Buch, Shyamal",Streaming Detection of Queried Event Start,,,,"Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding—Streaming Detection of Queried Event Start (SDQES). The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency. We introduce a new benchmark based on the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting. Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling. We evaluate four vision-language backbones and three adapter architectures in both short-clip and untrimmed video settings.",,,NeurIPS.csv,,,,,,
QIZLL694,journalArticle,,"Baek, Jae-Yong; Yoo, Yong-Sang; Bae, Seung-Hwan",A New Multi-Source Light Detection Benchmark and Semi-Supervised Focal Light Detection,,,,"This paper addresses a multi-source light detection (LD) problem from vehicles, trafﬁc signals, and streetlights under driving scenarios. Albeit it is crucial for autonomous driving and night vision, this problem has not been yet focused on as much as other object detection (OD). One of the main reasons is the absence of a public available LD benchmark dataset. Therefore, we construct a new large LD dataset consisting of different light sources via heavy annotation:YouTube Driving Light Detection dataset (YDLD). Compared to the existing LD datasets, our dataset has much more images and box annotations for multi-source lights. We also provide rigorous statistical analysis and transfer learning comparison of other well-known detection benchmark datasets to prove the generality of our YDLD.",,,NeurIPS.csv,,,,,,
GIUBUE93,conferencePaper,2024.0,"Akhtar, Mubashara; Benjelloun, Omar; Conforti, Costanza; Gijsbers, Pieter; Giner-Miguelez, Joan; Jain, Nitisha; Kuchnik, Michael; Lhoest, Quentin; Marcenac, Pierre; Maskey, Manil; Mattson, Peter; Oala, Luis; Ruyssen, Pierre; Shinde, Rajat; Simperl, Elena; Thomas, Goeffry; Tykhonov, Slava; Vanschoren, Joaquin; Van Der Velde, Jos; Vogler, Steffen; Wu, Carole-Jean",Croissant: A Metadata Format for ML-Ready Datasets,Proceedings of the Eighth Workshop on Data Management for End-to-End Machine Learning,10.1145/3650203.3663326,https://dl.acm.org/doi/10.1145/3650203.3663326,"Data is a critical resource for machine learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that creates a shared representation across ML tools, frameworks, and platforms. Croissant makes datasets more discoverable, portable, and interoperable, thereby addressing signiﬁcant challenges in ML data management. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, enabling easy loading into the most commonly-used ML frameworks, regardless of where the data is stored. Our initial evaluation by human raters shows that Croissant metadata is readable, understandable, complete, yet concise.",2024-06-09,SIGMOD/PODS '24: International Conference on Management of Data,NeurIPS.csv,,,,,,
KCJ8EXH3,journalArticle,,"Wu, Shirley; Zhao, Shiyu; Yasunaga, Michihiro; Huang, Kexin; Cao, Kaidi; Huang, Qian; Ioannidis, Vassilis N; Subbian, Karthik; Zou, James; Leskovec, Jure",STARK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases,,,,"Answering real-world complex queries, such as complex product search, often requires accurate retrieval from semi-structured knowledge bases that involve blend of unstructured (e.g., textual descriptions of products) and structured (e.g., entity relations of products) information. However, many previous works studied textual and relational retrieval tasks as separate topics. To address the gap, we develop STARK, a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases. Our benchmark covers three domains: product search, academic paper search, and queries in precision medicine. We design a novel pipeline to synthesize realistic user queries that integrate diverse relational information and complex textual properties, together with their ground-truth answers (items). We conduct rigorous human evaluation to validate the quality of our synthesized queries. We further enhance the benchmark with high-quality human-generated queries to provide an authentic reference. STARK serves as a comprehensive testbed for evaluating the performance of retrieval systems driven by large language models (LLMs). Our experiments suggest that STARK presents significant challenges to the current retrieval and LLM systems, highlighting the need for more capable semi-structured retrieval systems.",,,NeurIPS.csv,,,,,,
3XZVKD8R,journalArticle,,"Li, Jeffrey; Fang, Alex; Smyrnis, Georgios; Ivgi, Maor; Jordan, Matt; Gadre, Samir; Bansal, Hritik; Guha, Etash; Keh, Sedrick; Arora, Kushal; Garg, Saurabh; Xin, Rui; Muennighoff, Niklas; Heckel, Reinhard; Mercat, Jean; Gururangan, Suchin; Wortsman, Mitchell; Albalak, Alon; Bitton, Yonatan; Nezhurina, Marianna; Abbas, Amro; Hsieh, Cheng-Yu; Ghosh, Dhruba; Gardner, Josh; Kilian, Maciej; Zhang, Hanlin; Shao, Rulin; Pratt, Sarah; Sanyal, Sunny; Ilharco, Gabriel; Daras, Giannis; Marathe, Kalyani; Gokaslan, Aaron; Zhang, Jieyu; Chandu, Khyathi; Nguyen, Thao; Vasiljevic, Igor; Kakade, Sham; Song, Shuran; Sanghavi, Sujay; Oh, Sewoong; Zettlemoyer, Luke; Lo, Kyle; El-Nouby, Alaaeldin; Pouransari, Hadi; Toshev, Alexander; Wang, Stephanie; Groeneveld, Dirk; Soldaini, Luca; Koh, Pang Wei; Jitsev, Jenia; Kollar, Thomas; Dimakis, Alexandros G; Carmon, Yair; Dave, Achal; Schmidt, Ludwig; Shankar, Vaishaal",DataComp-LM: In search of the next generation of training sets for language models,,,,"We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLMBASELINE, enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-BASELINE represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6× less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation. We release the DCLM benchmark, framework, models, and datasets at https://datacomp.ai/dclm.",,,NeurIPS.csv,,,,,,
7KVMVLT6,journalArticle,,"Shangguan, Zhongkai; Huang, Zanming; Ohn-Bar, Eshed; Ozernov-Palchik, Ola; Kosty, Derek; Stoolmiller, Michael; Fien, Hank",Scalable Early Childhood Reading Performance Prediction,,,,"Models for student reading performance can empower educators and institutions to proactively identify at-risk students, thereby enabling early and tailored instructional interventions. However, there are no suitable publicly available educational datasets for modeling and predicting future reading performance. In this work, we introduce the Enhanced Core Reading Instruction (ECRI) dataset, a novel largescale longitudinal tabular dataset collected across 44 schools with 6,916 students and 172 teachers. We leverage the dataset to empirically evaluate the ability of state-of-the-art machine learning models to recognize early childhood educational patterns in multivariate and partial measurements. Specifically, we demonstrate a simple self-supervised strategy in which a Multi-Layer Perception (MLP) network is pre-trained over masked inputs to outperform several strong baselines while generalizing over diverse educational settings. To facilitate future developments in precise modeling and responsible use of models for individualized and early intervention strategies, our data and code are available at https://ecri-data.github.io/.",,,NeurIPS.csv,,,,,,
PH6ZDI9I,journalArticle,,"Liu, Ziyu; Chu, Tao; Zang, Yuhang; Wei, Xilin; Dong, Xiaoyi; Zhang, Pan; Liang, Zijian; Xiong, Yuanjun; Qiao, Yu; Lin, Dahua; Wang, Jiaqi",MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs,,,,"Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simplified scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately assess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce MMDU, a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs’ abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5× longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data. We demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly addresses this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA: +1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. This project is available at https://github.com/Liuziyu77/MMDU.",,,NeurIPS.csv,,,,,,
FEWIIZG7,journalArticle,,"Xue, Chuanyi; Liu, Qihan; Ma, Xiaoteng; Qi, Yang; Qin, Xinyao; Jiang, Yuhua; Gui, Ning; Ren, Jinsheng; Liang, Bin; Yang, Jun",NeuralPlane: An Efficiently Parallelizable Platform for Fixed-wing Aircraft Control with Reinforcement Learning,,,,"Reinforcement learning (RL) demonstrates superior potential over traditional flight control methods for fixed-wing aircraft, particularly under extreme operational conditions. However, the high demand for training samples and the lack of efficient computation in existing simulators hinder its further application. In this paper, we introduce NeuralPlane, the first benchmark platform for large-scale parallel simulations of fixed-wing aircraft. NeuralPlane significantly boosts high-fidelity simulation via GPU-accelerated Flight Dynamics Model (FDM) computation, achieving a single-step simulation time of just 0.2 seconds at a parallel scale of 106 aircraft, far exceeding current platforms. We also provide clear code templates, comprehensive evaluation and visualization tools, and hierarchical frameworks for integrating RL and traditional control methods. We believe that NeuralPlane can accelerate the development of RL-based fixed-wing flight control and serve as a new challenging benchmark for the RL community. Our NeuralPlane is open-source and accessible at https://github.com/xuecy22/NeuralPlane.",,,NeurIPS.csv,,,,,,
PCQB4TA3,journalArticle,,"Geng, Haoyu; Ruan, Hang; Wang, Runzhong; Li, Yang; Wang, Yang; Chen, Lei; Yan, Junchi",Benchmarking PtO and PnO Methods in the Predictive Combinatorial Optimization Regime,,,,"Predictive combinatorial optimization, where the parameters of combinatorial optimization (CO) are unknown at the decision-making time, is the precise modeling of many real-world applications, including energy cost-aware scheduling and budget allocation on advertising. Tackling such a problem usually involves a prediction model and a CO solver. These two modules are integrated into the predictive CO pipeline following two design principles: “Predict-then-Optimize (PtO)”, which learns predictions by supervised training and subsequently solves CO using predicted coefficients, while the other, named “Predict-and-Optimize (PnO)”, directly optimizes towards the ultimate decision quality and claims to yield better decisions than traditional PtO approaches. However, there lacks a systematic benchmark of both approaches, including the specific design choices at the module level, as well as an evaluation dataset that covers representative real-world scenarios. To this end, we develop a modular framework to benchmark 11 existing PtO/PnO methods on 8 problems, including a new industrial dataset for combinatorial advertising that will be released. Our study shows that PnO approaches are better than PtO on 7 out of 8 benchmarks, but there is no silver bullet found for the specific design choices of PnO. A comprehensive categorization of current approaches and integration of typical scenarios are provided under a unified benchmark. Therefore, this paper could serve as a comprehensive benchmark for future PnO approach development and also offer fast prototyping for application-focused development. The code is available at https://github.com/Thinklab-SJTU/PredictiveCO-Benchmark.",,,NeurIPS.csv,,,,,,
XEQR2JEP,journalArticle,,"Wang, Ke; Pan, Junting; Shi, Weikang; Lu, Zimu; Ren, Houxing; Zhou, Aojun; Zhan, Mingjie; Li, Hongsheng",Measuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset,,,,,,,NeurIPS.csv,,,,,,
HNNF8YBY,journalArticle,,"Liu, Haoxin; Xu, Shangqing; Zhao, Zhiyuan; Kong, Lingkai; Kamarthi, Harshavardhan; Sasanur, Aditya B; Sharma, Megha; Cui, Jiaming; Wen, Qingsong; Zhang, Chao; Prakash, B Aditya",Time-MMD: Multi-Domain Multimodal Dataset for Time Series Analysis,,,,"Time series data are ubiquitous across a wide range of real-world domains. While real-world time series analysis (TSA) requires human experts to integrate numerical series data with multimodal domain-specific knowledge, most existing TSA models rely solely on numerical data, overlooking the significance of information beyond numerical series. This oversight is due to the untapped potential of textual series data and the absence of a comprehensive, high-quality multimodal dataset. To overcome this obstacle, we introduce Time-MMD, the first multi-domain, multimodal time series dataset covering 9 primary data domains. Time-MMD ensures fine-grained modality alignment, eliminates data contamination, and provides high usability. Additionally, we develop MM-TSFlib, the first-cut multimodal time-series forecasting (TSF) library, seamlessly pipelining multimodal TSF evaluations based on Time-MMD for in-depth analyses. Extensive experiments conducted on Time-MMD through MM-TSFlib demonstrate significant performance enhancements by extending unimodal TSF to multimodality, evidenced by over 15% mean squared error reduction in general, and up to 40% in domains with rich textual data. More importantly, our datasets and library revolutionize broader applications, impacts, research topics to advance TSA. The dataset is available at https://github.com/AdityaLab/Time-MMD.",,,NeurIPS.csv,,,,,,
PKXHW2MD,journalArticle,,"Debenedetti, Edoardo; Rando, Javier; Paleka, Daniel; Fritz, Mario; Tramèr, Florian; Abdelnabi, Sahar; Schönherr, Lea",Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition,,,,"Large language model systems face significant security risks from maliciously crafted messages that aim to overwrite the system’s original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.",,,NeurIPS.csv,,,,,,
CWBEMNCP,journalArticle,,"Kil, Jihyung; Mai, Zheda; Lee, Justin; Chowdhury, Arpita; Wang, Zihe; Cheng, Kerrie; Wang, Lemeng; Liu, Ye; Chao, Wei-Lun",MLLM-COMPBENCH: A Comparative Reasoning Benchmark for Multimodal LLMs,,,,"The ability to compare objects, scenes, or situations is crucial for effective decisionmaking and problem-solving in everyday life. For instance, comparing the freshness of apples enables better choices during grocery shopping, while comparing sofa designs helps optimize the aesthetics of our living space. Despite its significance, the comparative capability is largely unexplored in artificial general intelligence (AGI). In this paper, we introduce MLLM-COMPBENCH, a benchmark designed to evaluate the comparative reasoning capability of multimodal large language models (MLLMs). MLLM-COMPBENCH mines and pairs images through visually oriented questions covering eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. We curate a collection of around 40K image pairs using metadata from diverse vision datasets and CLIP similarity scores. These image pairs span a broad array of visual domains, including animals, fashion, sports, and both outdoor and indoor scenes. The questions are carefully crafted to discern relative characteristics between two images and are labeled by human annotators for accuracy and relevance. We use MLLM-COMPBENCH to evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable shortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only sheds light on these limitations but also establishes a solid foundation for future enhancements in the comparative capability of MLLMs.",,,NeurIPS.csv,,,,,,
LIX5C7ZZ,journalArticle,,"Laine, Rudolf; Chughtai, Bilal; Betley, Jan; Hariharan, Kaivalya; Scheurer, Jérémy; Balesni, Mikita; Hobbhahn, Marius; Meinke, Alexander; Evans, Owain","Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",,,,"AI assistants such as ChatGPT are trained to respond to users by saying, “I am a large language model”. This raises questions. Do such models know that they are LLMs and reliably act on this knowledge? Are they aware of their current circumstances, such as being deployed to the public? We refer to a model’s knowledge of itself and its circumstances as situational awareness. To quantify situational awareness in LLMs, we introduce a range of behavioral tests, based on question answering and instruction following. These tests form the Situational Awareness Dataset (SAD), a benchmark comprising 7 task categories and over 13,000 questions. The benchmark tests numerous abilities, including the capacity of LLMs to (i) recognize their own generated text, (ii) predict their own behavior, (iii) determine whether a prompt is from internal evaluation or real-world deployment, and (iv) follow instructions that depend on self-knowledge.",,,NeurIPS.csv,,,,,,
XPP6TEE4,journalArticle,,"Hesse, Robin; Schaub-Meyer, Simone; Roth, Stefan",Benchmarking the Attribution Quality of Vision Models,,,,,,,NeurIPS.csv,,,,,,
QW6VNL4Q,journalArticle,,"Dou, Yutao; Yu, Huimin; Li, Wei; Li, Jingyang; Xia, Fei; Xiao, Jian",PEACE: A Dataset of Pharmaceutical Care for Cancer Pain Analgesia Evaluation and Medication Decision,,,,,,,NeurIPS.csv,,,,,,
M9AQ7X63,journalArticle,,"Long, Xingming; Zhang, Jie; Shan, Shiguang; Chen, Xilin",Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites Paradox,,,,"Most existing out-of-distribution (OOD) detection benchmarks classify samples with novel labels as the OOD data. However, some marginal OOD samples actually have close semantic contents to the in-distribution (ID) sample, which makes determining the OOD sample a Sorites Paradox. In this paper, we construct a benchmark named Incremental Shift OOD (IS-OOD) to address the issue, in which we divide the test samples into subsets with different semantic and covariate shift degrees relative to the ID dataset. The data division is achieved through a shift measuring method based on our proposed Language Aligned Image feature Decomposition (LAID). Moreover, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains high-quality generated images with more diverse covariate contents to complement the IS-OOD benchmark. We evaluate current OOD detection methods on our benchmark and find several important insights: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods. Our code and data are released in https://github.com/qqwsad5/IS-OOD.",,,NeurIPS.csv,,,,,,
AELVJCT6,journalArticle,,"Zhang, Chunhui; Liu, Li; Huang, Guanjie; Wen, Hao; Zhou, Xi; Wang, Yanfeng",WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark,,,,"Underwater Object Tracking (UOT) is essential for identifying and tracking submerged objects in underwater videos, but existing datasets are limited in scale, diversity of target categories and scenarios covered, impeding the development of advanced tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, i.e., the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, e.g., UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, e.g., underwater vision-language tracking. Given that most existing trackers are designed for open-air conditions and perform poorly in underwater environments due to domain gaps, we propose a novel framework that uses omni-knowledge distillation to train a student Transformer model effectively. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. We have thoroughly tested WebUOT-1M with 30 deep trackers, showcasing its potential as a benchmark for future UOT research. The complete dataset, along with codes and tracking results, are publicly accessible at here.",,,NeurIPS.csv,,,,,,
6RC9WEV5,journalArticle,,"Cheng, Guangzhao; Fu, Chengbo; Cheng, Lu",NanoBaseLib: A Multi-Task Benchmark Dataset for Nanopore Sequencing,,,,"Nanopore sequencing is the third-generation sequencing technology with capabilities of generating long-read sequences and directly measuring modifications on DNA/RNA molecules, which makes it ideal for biological applications such as human Telomere-to-Telomere (T2T) genome assembly, Ebola virus surveillance and COVID-19 mRNA vaccine development. However, accuracies of computational methods in various tasks of Nanopore sequencing data analysis are far from satisfactory. For instance, the base calling accuracy of Nanopore RNA sequencing is ∼90%, while the aim is ∼99.9%. This highlights an urgent need of contributions from the machine learning community. A bottleneck that prevents machine learning researchers from entering this field is the lack of a large integrated benchmark dataset. To this end, we present NanoBaseLib, a comprehensive multi-task benchmark dataset. It integrates 16 public datasets with over 30 million reads for four critical tasks in Nanopore data analysis. To facilitate method development, we have preprocessed all the raw data using a uniform workflow, stored all the intermediate results in uniform formats, analysed test datasets with various baseline methods for four benchmark tasks, and developed a software package to easily access these results. NanoBaseLib is available at https://nanobaselib.github.io.",,,NeurIPS.csv,,,,,,
N6JPAGCB,journalArticle,,"Zhang, Jiawen; Wen, Xumeng; Zhang, Zhenwei; Zheng, Shun; Li, Jia; Bian, Jiang",ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons,,,,,,,NeurIPS.csv,,,,,,
KVF2AJ8E,journalArticle,,"Li, Wei; Bishop, William; Li, Alice; Rawles, Chris; Campbell-Ajala, Folawiyo; Tyamagundlu, Divya; Riva, Oriana",On the Effects of Data Scale on UI Control Agents,,,,"Autonomous agents that control user interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless ﬁne-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether ﬁne-tuning alone is a viable approach for building real-world UI control agents. To this end we collect and release a new dataset, ANDROIDCONTROL, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each ANDROIDCONTROL task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, ANDROIDCONTROL is the most diverse UI control dataset to date, including 14,548 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we ﬁnd that when tested in domain ﬁne-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales signiﬁcantly more slowly and suggests that in particular for high-level tasks, ﬁne-tuning on more data alone may be insufﬁcient for achieving robust out-of-domain performance.",,,NeurIPS.csv,,,,,,
EIIZK9MQ,journalArticle,,"Wang, Yuxin; Feng, Duanyu; Dai, Yongfu; Chen, Zhengyu; Huang, Jimin; Ananiadou, Sophia; Xie, Qianqian; Wang, Hao",HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection,,,,"Data serves as the fundamental basis for advancing deep learning. The tabular data presented in a structured format is highly valuable for modeling and training. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Therefore, exploring the methods for effectively using models like LLMs to generate synthetic tabular data, which is privacy-preserving but similar to original one, is urgent. In this paper, we introduce a new framework HARMONIC for tabular data generation and evaluation by LLMs. In the data generation part of our framework, we employ fine-tuning to generate tabular data and enhance privacy rather than continued pre-training which is often used by previous small-scale LLM-based methods. In particular, we construct an instruction fine-tuning dataset based on the idea of the k-nearest neighbors algorithm to inspire LLMs to discover inter-row relationships. By such fine-tuning, LLMs are trained to remember the format and connections of the data rather than the data itself, which reduces the risk of privacy leakage. The experiments find that our tabular data generation achieves equivalent performance as existing methods but with better privacy by the metric of MLE, DCR, etc. In the evaluation part of our framework, we develop a specific privacy risk metric DLT for LLM synthetic data generation, which quantifies the extent to which the generator itself leaks data. We also developed LLE, a performance evaluation metric for downstream LLM tasks, which is more practical and credible than previous metrics. The experiments show that our data generation method outperform the previous methods in the metrics DLT and LLE. ∗Co-Corresponding Author.",,,NeurIPS.csv,,,,,,
IYAIW46P,journalArticle,,"Wu, Nemin; Cao, Qian; Wang, Zhangyu; Liu, Zeping; Qi, Yanlin; Zhang, Jielu; Ni, Joshua; Yao, Xiaobai; Ma, Hongxu; Mu, Lan; Ermon, Stefano; Ganu, Tanuja; Nambi, Akshay; Lao, Ni; Mai, Gengchen",TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning,,,,"Spatial representation learning (SRL) aims at learning general-purpose neural network representations from various types of spatial data (e.g., points, polylines, polygons, networks, images, etc.) in their native formats. Learning good spatial representations is a fundamental problem for various downstream applications such as species distribution modeling, weather forecasting, trajectory generation, geographic question answering, etc. Even though SRL has become the foundation of almost all geospatial artificial intelligence (GeoAI) research, we have not yet seen significant efforts to develop an extensive deep learning framework and benchmark to support SRL model development and evaluation. To fill this gap, we propose TorchSpatial, a learning framework and benchmark for location (point) encoding, which is one of the most fundamental data types of spatial representation learning. TorchSpatial contains three key components: 1) a unified location encoding framework that consolidates 15 commonly recognized location encoders, ensuring scalability and reproducibility of the implementations; 2) the LocBench benchmark tasks encompassing 7 geo-aware image classification and 10 geo-aware image regression datasets; 3) a comprehensive suite of evaluation metrics to quantify geo-aware models’ overall performance as well as their geographic bias, with a novel Geo-Bias Score metric. Finally, we provide a detailed analysis and insights into the model performance and geographic bias of different location encoders. We believe TorchSpatial will foster future advancement of spatial representation learning and spatial fairness in GeoAI research. The TorchSpatial model framework and LocBench benchmark are available at https://github.com/seai-lab/ TorchSpatial, and the Geo-Bias Score evaluation framework is available at https://github.com/seai-lab/PyGBS.",,,NeurIPS.csv,,,,,,
AYR9IB7A,journalArticle,,"Tang, Xiaojuan; Li, Jiaqi; Liang, Yitao; Zhu, Song-chun; Zhang, Muhan; Zheng, Zilong",Mars: Situated Inductive Reasoning in an Open-World Environment,,,,"Large Language Models (LLMs) trained on massive corpora have shown remarkable success in knowledge-intensive tasks. Yet, most of them rely on pre-stored knowledge. Inducing new general knowledge from a speciﬁc environment and performing reasoning with the acquired knowledge—situated inductive reasoning, is crucial and challenging for machine intelligence. In this paper, we design Mars, an interactive environment devised for situated inductive reasoning. It introduces counter-commonsense game mechanisms by modifying terrain, survival setting and task dependency while adhering to certain principles. In Mars, agents need to actively interact with their surroundings, derive useful rules and perform decision-making tasks in speciﬁc contexts. We conduct experiments on various RL-based and LLM-based methods, ﬁnding that they all struggle on this challenging situated inductive reasoning benchmark. Furthermore, we explore Induction from Reﬂection, where we instruct agents to perform inductive reasoning from history trajectory. The superior performance underscores the importance of inductive reasoning in Mars. Through Mars, we aim to galvanize advancements in situated inductive reasoning and set the stage for developing the next generation of AI systems that can reason in an adaptive and context-sensitive way.",,,NeurIPS.csv,,,,,,
X2DRVVQW,journalArticle,,"Magnusson, Ian; Bhagia, Akshita; Hofmann, Valentin; Soldaini, Luca; Jha, Ananya Harsh; Tafjord, Oyvind; Schwenk, Dustin; Walsh, Evan Pete; Elazar, Yanai; Lo, Kyle; Groeneveld, Dirk; Beltagy, Iz; Hajishirzi, Hannaneh; Smith, Noah A; Richardson, Kyle; Dodge, Jesse",Paloma : A Benchmark for Evaluating Language Model Fit,,,,"Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains—varying distributions of language. We introduce PERPLEXITY ANALYSIS FOR LANGUAGE MODEL ASSESSMENT (PALOMA)1, a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., r/depression on Reddit) and programming languages (e.g., Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from PALOMA surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.",,,NeurIPS.csv,,,,,,
TN9CYH65,journalArticle,,"Su, Kefan; Huo, Yusen; Zhang, Zhilin; Dou, Shuai; Yu, Chuan; Xu, Jian; Lu, Zongqing; Zheng, Bo",AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games,,,,"Decision-making in large-scale games is an essential research area in artificial intelligence (AI) with significant real-world impact. However, the limited access to realistic large-scale game environments has hindered research progress in this area. In this paper, we present AuctionNet, a benchmark for bid decision-making in largescale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of several baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions through the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse autobidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic Generalized Second Price (GSP) auction but also allows for customization of auction mechanisms as needed. To facilitate research and provide insights into the environment, we have also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet. AuctionNet has powered the NeurIPS 2024 Auto-Bidding in Large-Scale Auctions competition, providing competition environments for over 1,500 teams. We believe that AuctionNet is applicable not only to research on bid decision-making in ad auctions but also to the general area of decision-making in large-scale games. Code3: https://github.com/alimama-tech/AuctionNet.",,,NeurIPS.csv,,,,,,
ZTC9QUBJ,journalArticle,,"Panchal, Sunny; Bhattacharyya, Apratim; Berger, Guillaume; Mercier, Antoine; Böhm, Cornelius; Dietrichkeit, Florian; Pourreza, Reza; Li, Xuanlin; Madan, Pulkit; Lee, Mingu; Todorovich, Mark; Bax, Ingo; Memisevic, Roland",What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction,,,,"Vision-language models have shown impressive progress in recent years. However, existing models are largely limited to turn-based interactions, where each turn must be stepped (i.e., prompted) by the user. Open-ended, asynchronous interactions, where an AI model may proactively deliver timely responses or feedback based on the unfolding situation in real-time, are an open challenge. In this work, we present the QEVD benchmark and dataset, which explores human-AI interaction in the challenging, yet controlled, real-world domain of fitness coaching – a task which intrinsically requires monitoring live user activity and providing immediate feedback. The benchmark requires vision-language models to recognize complex human actions, identify possible mistakes, and provide appropriate feedback in real-time. Our experiments reveal the limitations of existing state-of-the-art vision-language models for such asynchronous situated interactions. Motivated by this, we propose a simple end-to-end streaming baseline that can respond asynchronously to human actions with appropriate feedback at the appropriate time.",,,NeurIPS.csv,,,,,,
AA9XQBWC,journalArticle,,"Wang, Weiyun; Zhang, Shuibo; Ren, Yiming; Duan, Yuchen; Li, Tiantong; Liu, Shuo; Hu, Mengkang; Chen, Zhe; Zhang, Kaipeng; Lu, Lewei; Zhu, Xizhou; Luo, Ping; Qiao, Yu; Dai, Jifeng; Shao, Wenqi; Wang, Wenhai",Needle In A Multimodal Haystack,,,,"With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.",,,NeurIPS.csv,,,,,,
I2HD9LV3,journalArticle,,"Xiong, Tianwei; Wang, Yuqing; Zhou, Daquan; Lin, Zhijie; Feng, Jiashi; Liu, Xihui",LVD-2M: A Long-take Video Dataset with Temporally Dense Captions,,,,,,,NeurIPS.csv,,,,,,
P34I6Z4P,journalArticle,,"Nathaniel, Juan; Qu, Yongquan; Nguyen, Tung; Yu, Sungduk; Busecke, Julius; Grover, Aditya; Gentine, Pierre","ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction",,,,"Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster preparedness and robust decision making amidst climate change. Yet, forecasting beyond the weather timescale is challenging because it deals with problems other than initial condition, including boundary interaction, butterfly effect, and our inherent lack of physical understanding. At present, existing benchmarks tend to have shorter forecasting range of up-to 15 days, do not include a wide range of operational baselines, and lack physics-based constraints for explainability. Thus, we propose ChaosBench, a challenging benchmark to extend the predictability range of data-driven weather emulators to S2S timescale. First, ChaosBench is comprised of variables beyond the typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that span over 45 years to allow for full Earth system emulation that respects boundary conditions. We also propose physics-based, in addition to deterministic and probabilistic metrics, to ensure a physically-consistent ensemble that accounts for butterfly effect. Furthermore, we evaluate on a diverse set of physics-based forecasts from four national weather agencies as baselines to our data-driven counterpart such as ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find methods originally developed for weather-scale applications fail on S2S task: their performance simply collapse to an unskilled climatology. Nonetheless, we outline and demonstrate several strategies that can extend the predictability range of existing weather emulators, including the use of ensembles, robust control of error propagation, and the use of physics-informed models. Our benchmark, datasets, and instructions are available at https://leap-stc.github.io/ChaosBench.",,,NeurIPS.csv,,,,,,
W2VFNPR9,journalArticle,,"Miao, Yibo; Zhu, Yifan; Yu, Lijia; Zhu, Jun; Gao, Xiao-Shan; Dong, Yinpeng",T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models,,,,"The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its safety risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover limited aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, the first comprehensive benchmark for conducting safety-critical assessments of textto-video models. We define 4 primary categories with 14 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts, and jailbreak attack-based prompts. We then conduct a thorough safety evaluation on 9 recently released T2V models. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AIs. Our code is publicly available at https://github.com/yibo-miao/T2VSafetyBench.",,,NeurIPS.csv,,,,,,
PTQ9I677,journalArticle,,"Ahmed, Mahmoud; Li, Xiang; Prajapati, Arpit; Elhoseiny, Mohamed",3DCoMPaT200: Language-Grounded Compositional Understanding of Parts and Materials of 3D Shapes,,,,,,,NeurIPS.csv,,,,,,
Q77BJSK3,journalArticle,,"Zhao, Dora; Scheuerman, Morgan Klaus; Chitre, Pooja; Andrews, Jerone T A",A Taxonomy of Challenges to Curating Fair Datasets,,,,"Despite extensive efforts to create fairer machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation. Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle. Our findings underscore overarching issues within the broader fairness landscape that impact data curation. We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices.",,,NeurIPS.csv,,,,,,
DBQNSYZL,journalArticle,,"Madan, Anish; Peri, Neehar; Kong, Shu; Ramanan, Deva",Revisiting Few-Shot Object Detection with Vision-Language Models,,,,"The era of vision-language models (VLMs) trained on web-scale datasets challenges conventional formulations of “open-world"" perception. In this work, we revisit the task of few-shot object detection (FSOD) in the context of recent foundational VLMs. First, we point out that zero-shot predictions from VLMs such as GroundingDINO signiﬁcantly outperform state-of-the-art few-shot detectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance, such foundation models may still be sub-optimal. For example, trucks on the web may be deﬁned differently from trucks for a target application such as autonomous vehicle perception. We argue that the task of few-shot recognition can be reformulated as aligning foundation models to target concepts using a few examples. Interestingly, such examples can be multi-modal, using both text and visual cues, mimicking instructions that are often given to human annotators when deﬁning a target concept of interest. Concretely, we propose Foundational FSOD, a new benchmark protocol that evaluates detectors pre-trained on any external data and ﬁne-tuned on multi-modal (text and visual) K-shot examples per target class. We repurpose nuImages for Foundational FSOD, benchmark several popular open-source VLMs, and provide an empirical analysis of state-of-the-art methods. Lastly, we discuss our recent CVPR 2024 Foundational FSOD competition and share insights from the community. Notably, the winning team signiﬁcantly outperforms our baseline by 23.3 mAP! Our code and dataset splits are available on GitHub and HuggingFace.",,,NeurIPS.csv,,,,,,
5ZYL2ZAC,journalArticle,,"Tsuruta, Hirofumi; Yamazaki, Hiroyuki; Maeda, Ryota; Tamura, Ryotaro; Imura, Akihiro",A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models,,,,"Antibodies are crucial proteins produced by the immune system to eliminate harmful foreign substances and have become pivotal therapeutic agents for treating human diseases. To accelerate the discovery of antibody therapeutics, there is growing interest in constructing language models using antibody sequences. However, the applicability of pre-trained language models for antibody discovery has not been thoroughly evaluated due to the scarcity of labeled datasets. To overcome these limitations, we introduce AVIDa-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins. AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARSCoV-2 mutants, such as the Delta and Omicron variants. Furthermore, we release VHHCorpus-2M, a pre-training dataset for antibody language models, containing over two million VHH sequences. We report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-speciﬁc pre-trained language models. These results conﬁrm that AVIDa-SARS-CoV-2 provides valuable benchmarks for evaluating the representation capabilities of antibody language models for binding prediction, thereby facilitating the development of AI-driven antibody discovery. The datasets are available at https://datasets.cognanous.com.",,,NeurIPS.csv,,,,,,
ALSQWKXT,journalArticle,,"Yang, Xiao; Sun, Kai; Xin, Hao; Sun, Yushi; Bhalla, Nikita; Chen, Xiangsen; Gui, Rongze Daniel; Jiang, Ziran Will; Jiang, Ziyu; Kong, Lingkun; Moran, Brian; Wang, Jiaqi; Xu, Yifan Ethan; Yan, An; Yang, Chenyu; Yuan, Eting; Zha, Hanwen; Tang, Nan; Chen, Lei; Scheffer, Nicolas; Liu, Yue; Shah, Nirav; Wanga, Rakesh; Kumar, Anuj; Yih, Wen-tau; Dong, Xin Luna",CRAG – Comprehensive RAG Benchmark,,,,"Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation of this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve ď 34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% of questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of participants and submissions. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions. CRAG is available at https://github.com/facebookresearch/CRAG/.",,,NeurIPS.csv,,,,,,
TPJFSUC4,journalArticle,,"Hollidt, Dominik; Streli, Paul; Jiang, Jiaxi; Haghighi, Yasaman; Qian, Changlin; Liu, Xintong; Holz, Christian",EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity,,,,"Research on egocentric tasks in computer vision has mostly focused on headmounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets. We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations. This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition—particularly for the lower body, which is typically occluded.",,,NeurIPS.csv,,,,,,
4ZFCKKZM,journalArticle,,"Ao, Junyi; Wang, Yuancheng; Tian, Xiaohai; Chen, Dekun; Zhang, Jun; Lu, Lu; Wang, Yuxuan; Li, Haizhou; Wu, Zhizheng",SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words,,,,"Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information. This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction. Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech. Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses. We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation. SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound. To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a process similar to that of SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. We also conduct a comprehensive evaluation using objective evaluation methods (e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses. Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures. Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics. We open-source SD-Eval at https://github.com/amphionspace/SD-Eval.",,,NeurIPS.csv,,,,,,
GM58P5RW,journalArticle,,"Wang, Christopher; Yaari, Adam; Singh, Aaditya K; Subramaniam, Vighnesh; Rosenfarb, Dana; DeWitt, Jan; Misra, Pranav; Madsen, Joseph R; Stone, Scellig; Kreiman, Gabriel; Katz, Boris; Cases, Ignacio; Barbu, Andrei",Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli,,,,,,,NeurIPS.csv,,,,,,
YNMBKDRF,journalArticle,,"Dai, Juntao; Chen, Tianle; Wang, Xuyao; Yang, Ziran; Chen, Taiye; Ji, Jiaming; Yang, Yaodong",SAFESORA: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset,,,,"To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the SAFESORA dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The SAFESORA dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the SAFESORA dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms. Our project is available at https://sites.google.com/view/safe-sora.",,,NeurIPS.csv,,,,,,
QAKN4YIK,journalArticle,,"Ashton, Neil; Angel, Jordan B; Ghate, Aditya S; Kenway, Gaetan K W",WindsorML: High-Fidelity Computational Fluid Dynamics Dataset For Automotive Aerodynamics,,,,"This paper presents a new open-source high-fidelity dataset for Machine Learning (ML) containing 355 geometric variants of the Windsor body, to help the development and testing of ML surrogate models for external automotive aerodynamics. Each Computational Fluid Dynamics (CFD) simulation was run with a GPU-native high-fidelity Wall-Modeled Large-Eddy Simulations (WMLES) using a Cartesian immersed-boundary method using more than 280M cells to ensure the greatest possible accuracy. The dataset contains geometry variants that exhibits a wide range of flow characteristics that are representative of those observed on road-cars. The dataset itself contains the 3D time-averaged volume & boundary data as well as the geometry and force & moment coefficients. This paper discusses the validation of the underlying CFD methods as well as contents and structure of the dataset. To the authors knowledge, this represents the first, large-scale high-fidelity CFD dataset for the Windsor body with a permissive open-source license (CC-BY-SA).",,,NeurIPS.csv,,,,,,
E893SYR9,journalArticle,,"Dong, Zibin; Yuan, Yifu; Hao, Jianye; Ni, Fei; Ma, Yi; Li, Pengyi; Zheng, Yan",CleanDiffuser: An Easy-to-use Modularized Library for Diffusion Models in Decision Making,,,,"Leveraging the powerful generative capability of diffusion models (DMs) to build decision-making agents has achieved extensive success. However, there is still a demand for an easy-to-use and modularized open-source library that offers customized and efficient development for DM-based decision-making algorithms. In this work, we introduce CleanDiffuser, the first DM library specifically designed for decision-making algorithms. By revisiting the roles of DMs in the decisionmaking domain, we identify a set of essential sub-modules that constitute the core of CleanDiffuser, allowing for the implementation of various DM algorithms with simple and flexible building blocks. To demonstrate the reliability and flexibility of CleanDiffuser, we conduct comprehensive evaluations of various DM algorithms implemented with CleanDiffuser across an extensive range of tasks. The analytical experiments provide a wealth of valuable design choices and insights, reveal opportunities and challenges, and lay a solid groundwork for future research. CleanDiffuser will provide long-term support to the decision-making community, enhancing reproducibility and fostering the development of more robust solutions. The code and documentation of CleanDiffuser are open-sourced on the project website.",,,NeurIPS.csv,,,,,,
9687XZ9A,journalArticle,,"Lekkala, Kiran; Cai, Peixu; Lim, Wei Zer; Liu, Chen; Itti, Laurent","USCILab3D: A Large-scale, Long-term, Semantically Annotated Outdoor Dataset",,,,"In this paper, we introduce the USCILab3D dataset, a large-scale, annotated outdoor dataset designed for versatile applications across multiple domains, including computer vision, robotics, and machine learning. The dataset was acquired using a mobile robot equipped with 5 cameras and a 32-beam, 360◦ scanning LIDAR. The robot was teleoperated, over the course of a year and under a variety of weather and lighting conditions, through a rich variety of paths within the USC campus (229 acres = ∼ 92.7 hectares). The raw data was annotated using state-of-theart large foundation models, and processed to provide multi-view imagery, 3D reconstructions, semantically-annotated images and point clouds (267 semantic categories), and text descriptions of images and objects within. The dataset also offers a diverse array of complex analyses using pose-stamping and trajectory data. In sum, the dataset offers 1.4M point clouds and 10M images (∼ 6TB of data). Despite covering a narrower geographical scope compared to a whole-city dataset, our dataset prioritizes intricate intersections along with denser multi-view scene images and semantic point clouds, enabling more precise 3D labelling and facilitating a broader spectrum of 3D vision tasks. For data, code and more details, please visit our website.",,,NeurIPS.csv,,,,,,
WBUY2PVD,journalArticle,,"Robinson, Joshua; Ranjan, Rishabh; Hu, Weihua; Huang, Kexin; Han, Jiaqi; Dobles, Alejandro; Fey, Matthias; Lenssen, Jan E; Yuan, Yiwen; Zhang, Zecheng; He, Xinwei; Leskovec, Jure",RELBENCH: A Benchmark for Deep Learning on Relational Databases,,,,"We present RELBENCH, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RELBENCH provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. We use RELBENCH to conduct the ﬁrst comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a signiﬁcant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, we conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RELBENCH.",,,NeurIPS.csv,,,,,,
ELCP3KTG,journalArticle,,"Wang, Zengzhi; Li, Xuefeng; Xia, Rui; Liu, Pengfei",MATHPILE : A Billion-Token-Scale Pre-training Corpus for Math,,,,"High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MATHPILE , a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of “less is more”, firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MATHPILE to boost language models’ mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field (available at https://github.com/GAIR-NLP/MathPile/).",,,NeurIPS.csv,,,,,,
BTTIAM4G,journalArticle,,"Arevalo, John; Su, Ellen; Carpenter, Anne E; Singh, Shantanu",MOTIVE: A Drug-Target Interaction Graph For Inductive Link Prediction,,,,"Drug-target interaction (DTI) prediction is crucial for identifying new therapeutics and detecting mechanisms of action. While structure-based methods accurately model physical interactions between a drug and its protein target, cell-based assays such as Cell Painting can better capture complex DTI interactions. This paper introduces MOTIVE, a Morphological cOmpound Target Interaction Graph dataset comprising Cell Painting features for 11, 000 genes and 3, 600 compounds, along with their relationships extracted from seven publicly available databases. We provide random, cold-source (new drugs), and cold-target (new genes) data splits to enable rigorous evaluation under realistic use cases. Our benchmark results show that graph neural networks that use Cell Painting features consistently outperform those that learn from graph structure alone, feature-based models, and topological heuristics. MOTIVE accelerates both graph ML research and drug discovery by promoting the development of more reliable DTI prediction models. MOTIVE resources are available at https://github.com/carpenter-singh-lab/motive.",,,NeurIPS.csv,,,,,,
92L5PX3Q,journalArticle,,"Deng, Hexuan; Jiao, Wenxiang; Liu, Xuebo; Zhang, Min; Tu, Zhaopeng",NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates,,,,"Despite their remarkable abilities in various tasks, large language models (LLMs) still struggle with real-time information (e.g., new facts and terms) due to the knowledge cutoff in their development process. However, existing benchmarks focus on outdated content and limited fields, facing difficulties in real-time updating and leaving new terms unexplored. To address this problem, we propose an adaptive benchmark, NewTerm, for real-time evaluation of new terms. We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort, allowing flexible updates for real-time information. Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms. Additionally, while updates to the knowledge cutoff of LLMs can cover some of the new terms, they are unable to generalize to more distant new terms. We also analyze which types of terms are more challenging and why LLMs struggle with new terms, paving the way for future research. Finally, we construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually. The benchmark and codes can be found at https://github.com/hexuandeng/NewTerm.",,,NeurIPS.csv,,,,,,
Q9K6CKXP,journalArticle,,"Chen, Chaochao; Zhang, Jiaming; Zhang, Yizhao; Zhang, Li; Lyu, Lingjuan; Li, Yuyuan; Gong, Biao; Yan, Chenggang",CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper Inﬂuence,,,,"With increasing privacy concerns in artiﬁcial intelligence, regulations have mandated the right to be forgotten, granting individuals the right to withdraw their data from models. Machine unlearning has emerged as a potential solution to enable selective forgetting in models, particularly in recommender systems where historical data contains sensitive user information. Despite recent advances in recommendation unlearning, evaluating unlearning methods comprehensively remains challenging due to the absence of a uniﬁed evaluation framework and overlooked aspects of deeper inﬂuence, e.g., fairness. To address these gaps, we propose CURE4Rec, the ﬁrst comprehensive benchmark for recommendation unlearning evaluation. CURE4Rec covers four aspects, i.e., unlearning Completeness, recommendation Utility, unleaRning efﬁciency, and recommendation fairnEss, under three data selection strategies, i.e., core data, edge data, and random data. Speciﬁcally, we consider the deeper inﬂuence of unlearning on recommendation fairness and robustness towards data with varying impact levels. We construct multiple datasets with CURE4Rec evaluation and conduct extensive experiments on existing recommendation unlearning methods. Our code is released at https://github.com/xiye7lai/CURE4Rec.",,,NeurIPS.csv,,,,,,
3AEWWJUV,journalArticle,,"Tschalzev, Andrej; Marton, Sascha; Lüdtke, Stefan; Bartelt, Christian; Stuckenschmidt, Heiner",A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data,,,,"Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing the performance of models typically consist of model-centric evaluation setups with overly standardized data preprocessing. This paper demonstrates that such model-centric evaluations are biased, as real-world modeling pipelines often require dataset-specific preprocessing, which includes feature engineering. Therefore, we propose a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings are: 1. After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. 2. Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. 3. While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics. Our framework is available under: https://github.com/atschalz/dc_tabeval.",,,NeurIPS.csv,,,,,,
D38SLIPD,journalArticle,,"Longjohn, Rachel; Kelly, Markelle; Singh, Sameer; Smyth, Padhraic",Benchmark Data Repositories for Better Benchmarking,,,,"In machine learning research, it is common to evaluate algorithms via their performance on standard benchmark datasets. While a growing body of work establishes guidelines for—and levies criticisms at—data and benchmarking practices in machine learning, comparatively less attention has been paid to the data repositories where these datasets are stored, documented, and shared. In this paper, we analyze the landscape of these benchmark data repositories and the role they can play in improving benchmarking. This role includes addressing issues with both datasets themselves (e.g., representational harms, construct validity) and the manner in which evaluation is carried out using such datasets (e.g., overemphasis on a few datasets and metrics, lack of reproducibility). To this end, we identify and discuss a set of considerations surrounding the design and use of benchmark data repositories, with a focus on improving benchmarking practices in machine learning.",,,NeurIPS.csv,,,,,,
DSAWWSWC,journalArticle,,"Bonnen, Tyler; Fu, Stephanie; Bai, Yutong; O’Connell, Thomas; Friedman, Yoni; Kanwisher, Nancy; Tenenbaum, Joshua B; Efros, Alexei A",Evaluating Multiview Object Consistency in Humans and Image Models,,,,"We introduce a benchmark to directly evaluate the alignment between human observers and vision models on a 3D shape inference task. We leverage an experimental design from the cognitive sciences: given a set of images, participants identify which contain the same/different objects, despite considerable viewpoint variation. We draw from a diverse range of images that include common objects (e.g., chairs) as well as abstract shapes (i.e., procedurally generated ‘nonsense’ objects). After constructing over 2000 unique image sets, we administer these tasks to human participants, collecting 35K trials of behavioral data from over 500 participants. This includes explicit choice behaviors as well as intermediate measures, such as reaction time and gaze data. We then evaluate the performance of common vision models (e.g., DINOv2, MAE, CLIP). We find that humans outperform all models by a wide margin. Using a multi-scale evaluation approach, we identify underlying similarities and differences between models and humans: while human-model performance is correlated, humans allocate more time/processing on challenging trials. All images, data, and code can be accessed via our project page.",,,NeurIPS.csv,,,,,,
PGK3D3KT,journalArticle,,"Barsellotti, Luca; Bigazzi, Roberto; Cornia, Marcella; Baraldi, Lorenzo; Cucchiara, Rita",Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments,,,,"In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.",,,NeurIPS.csv,,,,,,
PAFZ4943,journalArticle,,"Tsesmelis, Theodore; Palmieri, Luca; Khoroshiltseva, Marina; Islam, Adeela; Elkin, Gur; Alali, Nadav; Aslan, Sinem; Morerio, Pietro; Vascon, Sebastiano; Gravina, Elena; Napolitano, Maria Cristina; Scarpati, Giuseppe; Zuchtriegel, Gabriel; Spühler, Alexandra; Fuchs, Michel E; James, Stuart; Ben-Shahar, Ohad; Pelillo, Marcello; Bue, Alessio Del",Re-assembling the past: The RePAIR dataset and benchmark for real world 2D and 3D puzzle solving,,,,"This paper proposes the RePAIR dataset that represents a challenging benchmark to test modern computational and data driven methods for puzzle-solving and reassembly tasks. Our dataset has unique properties that are uncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and fractures are realistic, caused by a collapse of a fresco during a World War II bombing at the Pompeii archaeological park. The fragments are also eroded and have missing pieces with irregular shapes and different dimensions, challenging further the reassembly algorithms. The dataset is multi-modal providing high resolution images with characteristic pictorial elements, detailed 3D scans of the fragments and metadata annotated by the archaeologists. Ground truth has been generated through several years of unceasing eldwork, including the excavation and cleaning of each fragment, followed by manual puzzle solving by archaeologists of a subset of approx. 1000 pieces among the 16000 available. After digitizing all the fragments in 3D, a benchmark was prepared to challenge current reassembly and puzzlesolving methods that often solve more simplistic synthetic scenarios. The tested baselines show that there clearly exists a gap to ll in solving this computationally complex problem.",,,NeurIPS.csv,,,,,,
MN2JVT9W,journalArticle,,"Khrabrov, Kuzma; Ber, Anton; Tsypin, Artem; Ushenin, Konstantin; Rumiantsev, Egor; Telepov, Alexander; Protasov, Dmitry; Shenbin, Ilya; Alekseev, Anton; Shirokikh, Mikhail; Nikolenko, Sergey; Tutubalina, Elena; Kadurin, Artur",∇2DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials,,,,"Methods of computational quantum chemistry provide accurate approximations of molecular properties crucial for computer-aided drug discovery and other areas of chemical science. However, high computational complexity limits the scalability of their applications. Neural network potentials (NNPs) are a promising alternative to quantum chemistry methods, but they require large and diverse datasets for training. This work presents a new dataset and benchmark called ∇2DFT that is based on the nablaDFT. It contains twice as much molecular structures, three times more conformations, new data types and tasks, and state-of-the-art models. The dataset includes energies, forces, 17 molecular properties, Hamiltonian and overlap matrices, and a wavefunction object. All calculations were performed at the DFT level (ωB97X-D/def2-SVP) for each conformation. Moreover, ∇2DFT is the first dataset that contains relaxation trajectories for a substantial number of drug-like molecules. We also introduce a novel benchmark for evaluating NNPs in molecular property prediction, Hamiltonian prediction, and conformational optimization tasks. Finally, we propose an extendable framework for training NNPs and implement 10 models within it.",,,NeurIPS.csv,,,,,,
8MD568B6,journalArticle,,"Yun, Sukmin; Lin, Haokun; Thushara, Rusiru; Bhat, Mohammad Qazim; Wang, Yongxin; Jiang, Zutao; Deng, Mingkai; Wang, Jinhong; Tao, Tianhua; Li, Junbo; Li, Haonan; Nakov, Preslav; Baldwin, Timothy; Liu, Zhengzhong; Xing, Eric P; Liang, Xiaodan; Shen, Zhiqiang",Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs,,,,"Multimodal large language models (MLLMs) have shown impressive success across modalities such as image, video, and audio in a variety of understanding and generation tasks. However, current MLLMs are surprisingly poor at understanding webpage screenshots and generating their corresponding HTML code. To address this problem, we propose Web2Code, a benchmark consisting of a new large-scale webpage-to-code dataset for instruction tuning and an evaluation framework for the webpage understanding and HTML code translation abilities of MLLMs. For dataset construction, we leverage pretrained LLMs to enhance existing webpageto-code datasets as well as generate a diverse pool of new webpages rendered into images. Specifically, the inputs are webpage images and instructions, while the responses are the webpage’s HTML code. We further include diverse natural language QA pairs about the webpage content in the responses to enable a more comprehensive understanding of the web content. To evaluate model performance in these tasks, we develop an evaluation framework for testing MLLMs’ abilities in webpage understanding and web-to-code generation. Extensive experiments show that our proposed dataset is beneficial not only to our proposed tasks but also in the general visual domain. We hope our work will contribute to the development of general MLLMs suitable for web-based content generation and task automation. Our data and code are available at https://github.com/MBZUAI-LLM/web2code.",,,NeurIPS.csv,,,,,,
HDAWF2E7,journalArticle,,"Wang, Yiheng; Wang, Tianyu; Zhang, Yuying; Zhang, Hongji; Zheng, Haoyu; Zheng, Guanjie; Kong, Linghe",UrbanDataLayer: A Unified Data Pipeline for Urban Science,,,,,,,NeurIPS.csv,,,,,,
RCQPKDDH,journalArticle,,"Wu, Xian; Zhao, Yutian; Zhang, Yunyan; Wu, Jiageng; Zhu, Zhihong; Zhang, Yingying; Ouyang, Yi; Zhang, Ziheng; Wang, Huimin; Lin, Zhenxi; Yang, Jie; Zhao, Shuang; Zheng, Yefeng",MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey,,,,"Large language models (LLMs) have demonstrated remarkable capabilities in language understanding and generation, leading to their widespread adoption across various fields. Among these, the medical field is particularly well-suited for LLM applications, as many medical tasks can be enhanced by LLMs. Despite the existence of benchmarks for evaluating LLMs in medical question-answering and exams, there remains a notable gap in assessing LLMs’ performance in supporting patients throughout their entire hospital visit journey in real-world clinical practice. In this paper, we address this gap by dividing a typical patient’s clinical journey into four stages: planning, access, delivery and ongoing care. For each stage, we introduce multiple tasks and corresponding datasets, resulting in a comprehensive benchmark comprising 12 datasets, of which five are newly introduced, and seven are constructed from existing datasets. This proposed benchmark facilitates a thorough evaluation of LLMs’ effectiveness across the entire patient journey, providing insights into their practical application in clinical settings. Additionally, we evaluate three categories of LLMs against this benchmark: 1) proprietary LLM services such as GPT-4; 2) public LLMs like QWen; and 3) specialized medical LLMs, like HuatuoGPT2. Through this extensive evaluation, we aim to provide a better understanding of LLMs’ performance in the medical domain, ultimately contributing to their more effective deployment in healthcare settings.",,,NeurIPS.csv,,,,,,
INQ2ZU57,journalArticle,,"Bortolotti, Samuele; Marconato, Emanuele; Carraro, Tommaso; Morettin, Paolo; van Krieken, Emile; Vergari, Antonio; Teso, Stefano; Passerini, Andrea",A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts,,,,"The advent of powerful neural classiﬁers has increased interest in problems that require both learning and reasoning. These problems are critical for understanding important properties of models, such as trustworthiness, generalization, interpretability, and compliance to safety and structural constraints. However, recent research observed that tasks requiring both learning and reasoning on background knowledge often suffer from reasoning shortcuts (RSs): predictors can solve the downstream reasoning task without associating the correct concepts to the highdimensional data. To address this issue, we introduce rsbench, a comprehensive benchmark suite designed to systematically evaluate the impact of RSs on models by providing easy access to highly customizable tasks affected by RSs. Furthermore, rsbench implements common metrics for evaluating concept quality and introduces novel formal veriﬁcation procedures for assessing the presence of RSs in learning tasks. Using rsbench, we highlight that obtaining high quality concepts in both purely neural and neuro-symbolic models is a far-from-solved problem. rsbench is available at: https://unitn-sml.github.io/rsbench. neurosymbolicisbooljuice.",,,NeurIPS.csv,,,,,,
P3QN4YMX,journalArticle,,"Lin, Wang; Feng, Yueying; Han, Wenkang; Jin, Tao; Zhao, Zhou; Wu, Fei; Yao, Chang; Chen, Jingyuan",E3: Exploring Embodied Emotion Through A Large-Scale Egocentric Video Dataset,,,,"Understanding human emotions is fundamental to enhancing human-computer interaction, especially for embodied agents that mimic human behavior. Traditional emotion analysis often takes a third-person perspective, limiting the ability of agents to interact naturally and empathetically. To address this gap, this paper presents E3 for Exploring Embodied Emotion, the ﬁrst massive ﬁrst-person view video dataset. E3 contains more than 50 hours of video, capturing 8 different emotion types in diverse scenarios and languages. The dataset features videos recorded by individuals in their daily lives, capturing a wide range of real-world emotions conveyed through visual, acoustic, and textual modalities. By leveraging this dataset, we deﬁne 4 core benchmark tasks - emotion recognition, emotion classiﬁcation, emotion localization, and emotion reasoning - supported by more than 80k manually crafted annotations, providing a comprehensive resource for training and evaluating emotion analysis models. We further present Emotion-LlaMa, which complements visual modality with acoustic modality to enhance the understanding of emotion in ﬁrst-person videos. The results of comparison experiments with a large number of baselines demonstrate the superiority of Emotion-LlaMa and set a new benchmark for embodied emotion analysis. We expect that E3 can promote advances in multimodal understanding, robotics, and augmented reality, and provide a solid foundation for the development of more empathetic and context-aware embodied agents. Project page: https://exploring-embodied-emotion-ofﬁcial.github.io.",,,NeurIPS.csv,,,,,,
F6L5BYH3,journalArticle,,"Li, Jia; Li, Ge; Zhang, Xuanming; Zhao, Yunfei; Dong, Yihong; Jin, Zhi; Li, Binhua; Huang, Fei; Li, Yongbin",EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations,,,,"How to evaluate Large Language Models (LLMs) in code generation remains an open question. Many benchmarks have been proposed, but they have two limitations, i.e., data leakage and lack of domain-specific evaluation. The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains.",,,NeurIPS.csv,,,,,,
ZUHQD693,journalArticle,,"Liu, Qinghua; Paparrizos, John",The Elephant in the Room: Towards A Reliable Time-Series Anomaly Detection Benchmark,,,,"Time-series anomaly detection is a fundamental task across scientific fields and industries. However, the field has long faced the “elephant in the room:” critical issues including flawed datasets, biased evaluation measures, and inconsistent benchmarking practices that have remained largely ignored and unaddressed. We introduce the TSB-AD to systematically tackle these issues in the following three aspects: (i) Dataset Integrity: with 1070 high-quality time series from a diverse collection of 40 datasets (doubling the size of the largest collection and four times the number of existing curated datasets), we provide the first large-scale, heterogeneous, meticulously curated dataset that combines the effort of human perception and model interpretation; (ii) Measure Reliability: by revealing issues and biases in evaluation measures, we identify the most reliable and accurate measure, namely, VUS-PR for anomaly detection in time series to address concerns from the community; and (iii) Comprehensive Benchmarking: with a broad spectrum of 40 detection algorithms, from statistical methods to the latest foundation models, we perform a comprehensive evaluation that includes a thorough hyperparameter tuning and a unified setup for a fair and reproducible comparison. Our findings challenge the conventional wisdom regarding the superiority of advanced neural network architectures, revealing that simpler architectures and statistical methods often yield better performance. The promising performance of neural networks on multivariate cases and foundation models on point anomalies highlights the need for further advancements in these methods. We open-source the benchmark at https://github.com/TheDatumOrg/TSB-AD to promote further research.",,,NeurIPS.csv,,,,,,
TWRSJPWK,journalArticle,,"Ju, Xuan; Gao, Yiming; Zhang, Zhaoyang; Yuan, Ziyang; Wang, Xintao; Zeng, Ailing; Xiong, Yu; Xu, Qiang; Shan, Ying",MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions,,,,,,,NeurIPS.csv,,,,,,
K8A9N9GE,journalArticle,,"Repasky, Boris; Dick, Anthony; Abbasnejad, Ehsan",BLURD: Benchmarking and Learning using a Unified Rendering and Diffusion Model,,,,"Recent advancements in pre-trained vision models have made them pivotal in computer vision, emphasizing the need for their thorough evaluation and benchmarking. This evaluation needs to consider various factors of variation, their potential biases, shortcuts, and inaccuracies that ultimately lead to disparate performance in models. Such evaluations are conventionally done using either synthetic data from 2D or 3D rendering software or real-world images in controlled settings. Synthetic methods offer full control and flexibility, while real-world methods are limited by high costs and less adaptability. Moreover, 3D rendering can’t yet fully replicate real photography, creating a realism gap. In this paper, we introduce BLURD–Benchmarking and Learning using a Unified Rendering and Diffusion Model–a novel method combining 3D rendering and Stable Diffusion to bridge this gap in representation learning. With BLURD we create a new family of datasets that allow for the creation of both 3D rendered and photo-realistic images with identical factors. BLURD, therefore, provides deeper insights into the representations learned by various CLIP backbones. The source code for creating the BLURD datasets is available at https://github.com/squaringTheCircle/BLURD.",,,NeurIPS.csv,,,,,,
ER9RPDLU,journalArticle,,"Huang, Yizhe; Wang, Xingbo; Liu, Hao; Kong, Fanqi; Qin, Aoyang; Tang, Min; Zhu, Song-Chun; Bi, Mingjie; Qi, Siyuan; Feng, Xue",AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making,,,,"Traditional interactive environments limit agents’ intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.",,,NeurIPS.csv,,,,,,
MLC49LWG,journalArticle,,"Wei, Chenrui; Sun, Mengzhou; Wang, Wei",Proving Olympiad Algebraic Inequalities without Human Demonstrations,,,,"Solving Olympiad-level mathematical problems represents a significant advancement in machine intelligence and automated reasoning. Current machine learning methods, however, struggle to solve Olympiad-level problems beyond Euclidean plane geometry due to a lack of large-scale, high-quality datasets. The challenge is even greater in algebraic systems, which involve infinite reasoning spaces within finite conditions. To address these issues, we propose AIPS, an Algebraic Inequality Proving System capable of autonomously generating complex inequality theorems and effectively solving Olympiad-level inequality problems without requiring human demonstrations. During proof search in a mixed reasoning manner, a value curriculum learning strategy on generated datasets is implemented to improve proving performance, demonstrating strong mathematical intuitions. On a test set of 20 International Mathematical Olympiad-level inequality problems, AIPS successfully solved 10, outperforming state-of-the-art methods. Furthermore, AIPS automatically generated a vast array of non-trivial theorems without human intervention, some of which have been evaluated by professional contestants and deemed to reach the level of the International Mathematical Olympiad. Notably, one theorem was selected as a competition problem in a major city’s 2024 Mathematical Olympiad. All the materials are available at sites.google.com/view/aips2.",,,NeurIPS.csv,,,,,,
ILWCDUNG,journalArticle,,"Bushuiev, Roman; Bushuiev, Anton; de Jonge, Niek F; Young, Adamo; Kretschmer, Fleming; Samusevich, Raman; Heirman, Janne; Wang, Fei; Zhang, Luke; Dührkop, Kai; Ludwig, Marcus; Haupt, Nils A; Kalia, Apurva; Brungs, Corinna; Schmid, Robin; Greiner, Russell; Wang, Bo; Wishart, David S; Liu, Li-Ping; Rousu, Juho; Bittremieux, Wout; Rost, Hannes; Mak, Tytus D; Hassoun, Soha; Huber, Florian; Böcker, Sebastian; Sivic, Josef; Pluskal, Tomáš",MassSpecGym: A benchmark for the discovery and identification of molecules,,,,"The discovery and identification of molecules in biological and environmental samples is crucial for advancing biomedical and chemical sciences. Tandem mass spectrometry (MS/MS) is the leading technique for high-throughput elucidation of molecular structures. However, decoding a molecular structure from its mass spectrum is exceptionally challenging, even when performed by human experts. As a result, the vast majority of acquired MS/MS spectra remain uninterpreted, thereby limiting our understanding of the underlying (bio)chemical processes. Despite decades of progress in machine learning applications for predicting molecular structures from MS/MS spectra, the development of new methods is severely hindered by the lack of standard datasets and evaluation protocols. To address this problem, we propose MassSpecGym – the first comprehensive benchmark for the discovery and identification of molecules from MS/MS data. Our benchmark comprises the largest publicly available collection of high-quality labeled MS/MS spectra and defines three MS/MS annotation challenges: de novo molecular structure generation, molecule retrieval, and spectrum simulation. It includes new evaluation metrics and a generalization-demanding data split, therefore standardizing the MS/MS annotation tasks and rendering the problem accessible to the broad machine learning community. MassSpecGym is publicly available at https://github.com/pluskal-lab/MassSpecGym.",,,NeurIPS.csv,,,,,,
MZQ3IJMX,journalArticle,,"Ying, Huaiyuan; Wu, Zijian; Geng, Yihan; Wang, Jiayu; Lin, Dahua; Chen, Kai",Lean Workbook: A large-scale Lean problem set formalized from natural language math problems,,,,"Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github. com/InternLM/InternLM-Math and our data at https://huggingface.co/ datasets/InternLM/Lean-Workbook.",,,NeurIPS.csv,,,,,,
GLIIYINJ,journalArticle,,"Wu, Zhenbang; Dadu, Anant; Nalls, Mike; Faghri, Faraz; Sun, Jimeng",Instruction Tuning Large Language Models to Understand Electronic Health Records,,,,"Large language models (LLMs) have shown impressive capabilities in solving a wide range of tasks based on human instructions. However, developing a conversational AI assistant for electronic health record (EHR) data remains challenging due to (1) the lack of large-scale instruction-following datasets and (2) the limitations of existing model architectures in handling complex and heterogeneous EHR data. In this paper, we introduce MIMIC-Instr, a dataset comprising over 400K open-ended instruction-following examples derived from the MIMIC-IV EHR database. This dataset covers various topics and is suitable for instructiontuning general-purpose LLMs for diverse clinical use cases. Additionally, we propose Llemr, a general framework that enables LLMs to process and interpret EHRs with complex data structures. Llemr demonstrates competitive performance in answering a wide range of patient-related questions based on EHR data. Furthermore, our evaluations on clinical predictive modeling benchmarks reveal that the fine-tuned Llemr achieves performance comparable to state-of-the-art (SOTA) baselines using curated features. The dataset and code are available at https://github.com/zzachw/llemr.",,,NeurIPS.csv,,,,,,
HWA8PUPY,journalArticle,,"Dong, Ha; Wang, Xin; Moretti, Rocco; Wang, Yu; Su, Zhaoqian; Gu, Jiawei; Bodenheimer, Bobby; Weaver, Charles David; Meiler, Jens; Derr, Tyler",WelQrate: Defining the Gold Standard in Small Molecule Drug Discovery Benchmarking,,,,"While deep learning has revolutionized computer-aided drug discovery, the AI community has predominantly focused on model innovation and placed less emphasis on establishing best benchmarking practices. We posit that without a sound model evaluation framework, the AI community’s efforts cannot reach their full potential, thereby slowing the progress and transfer of innovation into real-world drug discovery. Thus, in this paper, we seek to establish a new gold standard for small molecule drug discovery benchmarking, WelQrate. Specifically, our contributions are threefold: WelQrate dataset collection - we introduce a meticulously curated collection of 9 datasets spanning 5 therapeutic target classes. Our hierarchical curation pipelines, designed by drug discovery experts, go beyond the primary high-throughput screen by leveraging additional confirmatory and counter screens along with rigorous domain-driven preprocessing, such as PanAssay Interference Compounds (PAINS) filtering, to ensure the high-quality data in the datasets; WelQrate Evaluation Framework - we propose a standardized model evaluation framework considering high-quality datasets, featurization, 3D conformation generation, evaluation metrics, and data splits, which provides a reliable benchmarking for drug discovery experts conducting real-world virtual screening; Benchmarking - we evaluate model performance through various research questions using the WelQrate dataset collection, exploring the effects of different models, dataset quality, featurization methods, and data splitting strategies on the results. In summary, we recommend adopting our proposed WelQrate as the gold standard in small molecule drug discovery benchmarking. The WelQrate dataset collection, along with the curation codes, and experimental scripts are all publicly available at WelQrate.org.",,,NeurIPS.csv,,,,,,
96S9687M,journalArticle,,"Nikitin, Alexander; Iannucci, Letizia; Kaski, Samuel",TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series,,,,,,,NeurIPS.csv,,,,,,
JET3ZCN4,journalArticle,,"Qiu, Tianyi; Zhang, Yang; Huang, Xuchuan; Li, Jasmine Xinze; Ji, Jiaming; Yang, Yaodong",ProgressGym: Alignment with a Millennium of Moral Progress,,,,"Frontier AI systems, including large language models (LLMs), hold increasing inﬂuence over the epistemology of human users. Such inﬂuence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. We introduce progress alignment as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. To empower research in progress alignment, we introduce ProgressGym,4 an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 historical LLMs,5 ProgressGym enables codiﬁcation of real-world progress alignment challenges into concrete benchmarks. Speciﬁcally, we introduce three core challenges: tracking evolving values (PG-Follow), preemptively anticipating moral progress (PG-Predict), and regulating the feedback loop between human and AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension are inapplicable to these tasks. In response, we present lifelong and extrapolative algorithms as baseline methods of progress alignment, and build an open leaderboard6 soliciting novel algorithms and challenges.",,,NeurIPS.csv,,,,,,
LF22YML3,journalArticle,,"Castillo-Bolado, David; Davidson, Joseph; Gray, Finlay; Rosa, Marek",Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models,,,,"We introduce a dynamic benchmarking system for conversational agents that evaluates their performance through a single, simulated, and lengthy user↔agent interaction. The interaction is a conversation between the user and agent, where multiple tasks are introduced and then undertaken concurrently. We context switch regularly to interleave the tasks, which constructs a realistic testing scenario in which we assess the Long-Term Memory, Continual Learning, and Information Integration capabilities of the agents. Results from both proprietary and open-source Large-Language Models show that LLMs in general perform well on single-task interactions, but they struggle on the same tasks when they are interleaved. Notably, short-context LLMs supplemented with an LTM system perform as well as or better than those with larger contexts. Our benchmark suggests that there are other challenges for LLMs responding to more natural interactions that contemporary benchmarks have heretofore not been able to capture.",,,NeurIPS.csv,,,,,,
3QQMQ95K,journalArticle,,"Hu, Dapeng; Luo, Mi; Liang, Jian; Foo, Chuan-Sheng",Towards Reliable Model Selection for Unsupervised Domain Adaptation: An Empirical Study and A Certified Baseline,,,,"Selecting appropriate hyperparameters is crucial for unlocking the full potential of advanced unsupervised domain adaptation (UDA) methods in unlabeled target domains. Although this challenge remains under-explored, it has recently garnered increasing attention with the proposals of various model selection methods. Reliable model selection should maintain performance across diverse UDA methods and scenarios, especially avoiding highly risky worst-case selections—selecting the model or hyperparameter with the worst performance in the pool. Are existing model selection methods reliable and versatile enough for different UDA tasks? In this paper, we provide a comprehensive empirical study involving 8 existing model selection approaches to answer this question. Our evaluation spans 12 UDA methods across 5 diverse UDA benchmarks and 5 popular UDA scenarios. Surprisingly, we find that none of these approaches can effectively avoid the worst-case selection. In contrast, a simple but overlooked ensemble-based selection approach, which we call EnsV, is both theoretically and empirically certified to avoid the worst-case selection, ensuring high reliability. Additionally, EnsV is versatile for various practical but challenging UDA scenarios, including validation of open-partial-set UDA and source-free UDA. Finally, we call for more attention to the reliability of model selection in UDA: avoiding the worst-case is as significant as achieving peak selection performance and should not be overlooked when developing new model selection methods. Code is available at https://github.com/LHXXHB/EnsV.",,,NeurIPS.csv,,,,,,
7IB97QJF,journalArticle,2014.0,"Universitat Politècnica De València, Editorial",Universitat Politècnica de València,Ingeniería del agua,10.4995/ia.2014.3293,http://polipapers.upv.es/index.php/IA/article/view/3293,,2014-09-29,,NeurIPS.csv,,,,,,
8YXD3SNG,journalArticle,,"Wang, Zhonghao; Sun, Danyu; Zhou, Sheng; Wang, Haobo; Fan, Jiapei; Huang, Longtao; Bu, Jiajun",NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise,,,,"Graph Neural Networks (GNNs) exhibit strong potential in node classification tasks through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.",,,NeurIPS.csv,,,,,,
N9WL5SQL,journalArticle,,"Gupta, Rohan; Arcuschin, Iván; Kwa, Thomas; Garriga-Alonso, Adrià",INTERPBENCH: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques,,,,"Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents INTERPBENCH, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model’s output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr’s original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.",,,NeurIPS.csv,,,,,,
IRWM8ASC,journalArticle,,"Roberts, Josselin Somerville; Lee, Tony; Wong, Chi Heem",Image2Struct: Benchmarking Structure Extraction for Vision-Language Models,,,,"We introduce Image2Struct, a benchmark to evaluate vision-language models (VLMs) on extracting structure from images. Our benchmark 1) captures realworld use cases, 2) is fully automatic and does not require human judgment, and 3) is based on a renewable stream of fresh data. In Image2Struct, VLMs are prompted to generate the underlying structure (e.g., LaTeX code or HTML) from an input image (e.g., webpage screenshot). The structure is then rendered to produce an output image (e.g., rendered webpage), which is compared against the input image to produce a similarity score. This round-trip evaluation allows us to quantitatively evaluate VLMs on tasks with multiple valid structures. We create a pipeline that downloads fresh data from active online communities upon execution and evaluates the VLMs without human intervention. We introduce three domains (Webpages, LaTeX, and Musical Scores) and use five image metrics (pixel similarity, cosine similarity between the Inception vectors, learned perceptual image patch similarity, structural similarity index measure, and earth mover similarity) that allow efficient and automatic comparison between pairs of images. We evaluate Image2Struct on 14 prominent VLMs and find that scores vary widely, indicating that Image2Struct can differentiate between the performances of different VLMs. Additionally, the best score varies considerably across domains (e.g., 0.402 on sheet music vs. 0.830 on LaTeX equations), indicating that Image2Struct contains tasks of varying difficulty. For transparency, we release the full results at https: //crfm.stanford.edu/helm/image2struct/v1.0.1/.",,,NeurIPS.csv,,,,,,
QN2FII9Z,journalArticle,,"Madan, Spandan; Xiao, Will; Cao, Mingran",Benchmarking Out-of-Distribution Generalization Capabilities of DNN-based Encoding Models for the Ventral Visual Cortex.,,,,"We characterized the generalization capabilities of deep neural network encoding models when predicting neuronal responses from the visual cortex to flashed images. We collected MacaqueITBench, a large-scale dataset of neuronal population responses from the macaque inferior temporal (IT) cortex to over 300, 000 images, comprising 8, 233 unique natural images presented to seven monkeys over 109 sessions. Using MacaqueITBench, we investigated the impact of distribution shifts on models predicting neuronal activity by dividing the images into OutOf-Distribution (OOD) train and test splits. The OOD splits included variations in image contrast, hue, intensity, temperature, and saturation. Compared to the performance on in-distribution test images—the conventional way in which these models have been evaluated—models performed worse at predicting neuronal responses to out-of-distribution images, retaining as little as 20% of the performance on in-distribution test images. Additionally, the relative ranking of different models in terms of their ability to predict neuronal responses changed drastically across OOD shifts. The generalization performance under OOD shifts can be well accounted by a simple image similarity metric—the cosine distance between image representations extracted from a pre-trained object recognition model is a strong predictor of neuronal predictivity under different distribution shifts. The dataset of images, neuronal firing rate recordings, and computational benchmarks are hosted publicly at: MacaqueITBench Link.",,,NeurIPS.csv,,,,,,
UN9EXMM3,journalArticle,,"Press, Ori; Hochlehnert, Andreas; Prabhu, Ameya; Udandarao, Vishaal; Press, Ofir; Bethge, Matthias",CiteME: Can Language Models Accurately Cite Scientific Claims?,,,,"Thousands of new scientific papers are published each month. Such information overload complicates researcher efforts to stay current with the state-of-the-art as well as to verify and correctly attribute claims. We pose the following research question: Given a text excerpt referencing a paper, could an LM act as a research assistant to correctly identify the referenced paper? We advance efforts to answer this question by building a benchmark that evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME, consists of text excerpts from recent machine learning papers, each referencing a single other paper. CiteME use reveals a large gap between frontier LMs and human performance, with LMs achieving only 4.218.5% accuracy and humans 69.7%. We close this gap by introducing CiteAgent, an autonomous system built on the GPT-4o LM that can also search and read papers, which achieves an accuracy of 35.3% on CiteME. Overall, CiteME serves as a challenging testbed for open-ended claim attribution, driving the research community towards a future where any claim made by an LM can be automatically verified and discarded if found to be incorrect.",,,NeurIPS.csv,,,,,,
9XW4RF49,journalArticle,,"Jansen, Peter; Côté, Marc-Alexandre; Khot, Tushar; Bransom, Erin; Mishra, Bhavana Dalvi; Majumder, Bodhisattwa Prasad; Tafjord, Oyvind; Clark, Peter",DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents,,,,,,,NeurIPS.csv,,,,,,
IKFT77ZD,journalArticle,,"Haresh, Sanjay; Dijkman, Daniel; Bhattacharyya, Apratim; Memisevic, Roland",ClevrSkills: Compositional Language and Visual Reasoning in Robotics,,,,"Robotics tasks are highly compositional by nature. For example, to perform a high-level task like cleaning the table a robot must employ low-level capabilities of moving the effectors to the objects on the table, pick them up and then move them off the table one-by-one, while re-evaluating the consequently dynamic scenario in the process. Given that large vision language models (VLMs) have shown progress on many tasks that require high level, human-like reasoning, we ask the question: if the models are taught the requisite low-level capabilities, can they compose them in novel ways to achieve interesting high-level tasks like cleaning the table without having to be explicitly taught so? To this end, we present ClevrSkills - a benchmark suite for compositional reasoning in robotics. ClevrSkills is an environment suite developed on top of the ManiSkill2 [16] simulator and an accompanying dataset. The dataset contains trajectories generated on a range of robotics tasks with language and visual annotations as well as multi-modal prompts as task specification. The suite includes a curriculum of tasks with three levels of compositional understanding, starting with simple tasks requiring basic motor skills. We benchmark multiple different VLM baselines on ClevrSkills and show that even after being pre-trained on large numbers of tasks, these models fail on compositional reasoning in robotics tasks.",,,NeurIPS.csv,,,,,,
43WXZ2EG,preprint,2024.0,"Peterson, Ralph E; Tanelus, Aramis; Ick, Christopher; Mimica, Bartul; Francis, Niegil; Ivan, Violet J; Choudhri, Aman; Falkner, Annegret L; Murthy, Mala; Schneider, David M; Sanes, Dan H; Williams, Alex H",Vocal Call Locator Benchmark (VCL) for localizing rodent vocalizations from multi-channel audio,,10.1101/2024.09.20.613758,http://biorxiv.org/lookup/doi/10.1101/2024.09.20.613758,"Understanding the behavioral and neural dynamics of social interactions is a goal of contemporary neuroscience. Many machine learning methods have emerged in recent years to make sense of complex video and neurophysiological data that result from these experiments. Less focus has been placed on understanding how animals process acoustic information, including social vocalizations. A critical step to bridge this gap is determining the senders and receivers of acoustic information in social interactions. While sound source localization (SSL) is a classic problem in signal processing, existing approaches are limited in their ability to localize animal-generated sounds in standard laboratory environments. Advances in deep learning methods for SSL are likely to help address these limitations, however there are currently no publicly available models, datasets, or benchmarks to systematically evaluate SSL algorithms in the domain of bioacoustics. Here, we present the VCL Benchmark: the ﬁrst large-scale dataset for benchmarking SSL algorithms in rodents. We acquired synchronized video and multi-channel audio recordings of 767,295 sounds with annotated ground truth sources across 9 conditions. The dataset provides benchmarks which evaluate SSL performance on real data, simulated acoustic data, and a mixture of real and simulated data. We intend for this benchmark to facilitate knowledge transfer between the neuroscience and acoustic machine learning communities, which have had limited overlap.",2024-09-21,,NeurIPS.csv,,,,,,
FPTAM2Z4,journalArticle,,"Kazemi, Mehran; Dikkala, Nishanth; Anand, Ankit; Devic, Petar; Dasgupta, Ishita; Liu, Fangyu; Fatemi, Bahare; Awasthi, Pranjal; Guo, Dee; Gollapudi, Sreenivas; Qureshi, Ahmed",ReMI: A Dataset for Reasoning with Multiple Images,,,,"With the continuous advancement of large language models (LLMs), it is essential to create new benchmarks to effectively evaluate their expanding capabilities and identify areas for improvement. This work focuses on multi-image reasoning, an emerging capability in state-of-the-art LLMs. We introduce ReMI, a dataset designed to assess LLMs’ ability to Reason with Multiple Images. This dataset encompasses a diverse range of tasks, spanning various reasoning domains such as math, physics, logic, code, table/chart understanding, and spatial and temporal reasoning. It also covers a broad spectrum of characteristics found in multi-image reasoning scenarios. We have benchmarked several cutting-edge LLMs using ReMI and found a substantial gap between their performance and human-level proﬁciency. This highlights the challenges in multi-image reasoning and the need for further research. Our analysis also reveals the strengths and weaknesses of different models, shedding light on the types of reasoning that are currently attainable and areas where future models require improvement. To foster further research in this area, we are open-sourcing ReMI: https://huggingface.co/datasets/ mehrankazemi/ReMI.",,,NeurIPS.csv,,,,,,
VP443UBI,journalArticle,,"Turishcheva, Polina; Fahey, Paul G; Vystrcˇilová, Michaela; Hansel, Laura; Froebe, Rachel; Ponder, Kayla; Qiu, Yongrong; Willeke, Konstantin F; Bashiri, Mohammad; Baikulov, Ruslan; Zhu, Yu; Ma, Lei; Yu, Shan; Huang, Tiejun; Li, Bryan M; Wulf, Wolf De; Kudryashova, Nina; Hennig, Matthias H; Rochefort, Nathalie L; Onken, Arno; Wang, Eric; Ding, Zhiwei; Tolias, Andreas S; Sinz, Fabian H; Ecker, Alexander S",Retrospective for the Dynamic Sensorium Competition for predicting large-scale mouse primary visual cortex activity from videos,,,,"Understanding how biological visual systems process information is challenging because of the nonlinear relationship between visual input and neuronal responses. Artiﬁcial neural networks allow computational neuroscientists to create predictive models that connect biological and machine vision. Machine learning has beneﬁted tremendously from benchmarks that compare different models on the same task under standardized conditions. However, there was no standardized benchmark to identify state-of-the-art dynamic models of the mouse visual system. To address this gap, we established the SENSORIUM 2023 Benchmark Competition with dynamic input, featuring a new large-scale dataset from the primary visual cortex of ten mice. This dataset includes responses from 78,853 neurons to 2 hours of dynamic stimuli per neuron, together with behavioral measurements such as running speed, pupil dilation, and eye movements. The competition ranked models in two tracks based on predictive performance for neuronal responses on a held-out test set: one focusing on predicting in-domain natural stimuli and another on out-of-distribution (OOD) stimuli to assess model generalization. As part of the NeurIPS 2023 Competition Track, we received more than 160 model submissions from 22 teams. Several new architectures for predictive models were proposed, and the winning teams improved the previous state-of-the-art model by 50%. Access to the dataset as well as the benchmarking infrastructure will remain online at www.sensorium-competition.net.",,,NeurIPS.csv,,,,,,
M3NQX3ZJ,journalArticle,,"Akhbari, Bardiya; Gawali, Manish; Dronen, Nicholas A",SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models,,,,"Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs’ algorithmic abilities under simple lexical or semantic variations. To this end, we present the SETLEXSEM CHALLENGE, a synthetic benchmark that evaluates the performance of LLMs on set operations. SETLEXSEM assesses the robustness of LLMs’ instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SETLEXSEM, we find that they exhibit poor robustness to variation in both operation and operands. We show – via the framework’s systematic sampling of set members along lexical and semantic dimensions – that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of ""deceptive"" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them independently. The code for reproducing the results of this paper, and for generating the SETLEXSEM CHALLENGE dataset, is available at https://github.com/amazon-science/SetLexSem-Challenge.",,,NeurIPS.csv,,,,,,
M5VH8EBM,journalArticle,,"Wang, Yuli; Peng, Jian; Dai, Yuwei; Jones, Craig; Sair, Haris; Shen, Jinglai; Loizou, Nicolas; Wu, Jing; Hsu, Wen-Chi; Imami, Maliha; Jiao, Zhicheng; Zhang, Paul; Bai, Harrison",Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection,,,,"Recent approaches to vision-language tasks are built on the remarkable capabilities of large vision-language models (VLMs). These models excel in zero-shot and few-shot learning, enabling them to learn new tasks without parameter updates. However, their primary challenge lies in their design, which primarily accommodates 2D input, thus limiting their effectiveness for medical images, particularly radiological images like MRI and CT, which are typically 3D. To bridge the gap between state-of-the-art 2D VLMs and 3D medical image data, we developed an innovative, one-pass, unsupervised representative slice selection method called Vote-MI, which selects representative 2D slices from 3D medical imaging. To evaluate the effectiveness of Vote-MI when implemented with VLMs, we introduce BrainMD, a robust, multimodal dataset comprising 2,453 annotated 3D MRI brain scans with corresponding textual radiology reports and electronic health records. Based on BrainMD, we further develop two benchmarks, BrainMD-select (including the most representative 2D slice of a 3D image) and BrainBench (including various vision-language downstream tasks). Extensive experiments on the BrainMD dataset and its two corresponding benchmarks demonstrate that our representative selection method signiﬁcantly improves performance in zero-shot and few-shot learning tasks. On average, Vote-MI achieves a 14.6% and 16.6% absolute gain for zero-shot and few-shot learning, respectively, compared to randomly selecting examples. Our studies represent a signiﬁcant step toward integrating AI in medical imaging to enhance patient care and facilitate medical research. We hope this work will serve as a foundation for data selection as vision-language models are increasingly applied to new tasks. Code and data examples are available at Github: https://github.com/YuliWanghust/BrainMD.",,,NeurIPS.csv,,,,,,
RSJHHCE3,journalArticle,,"Shah, Nidhish; Genc, Zulkuf; Araci, Dogu",StackEval: Benchmarking LLMs in Coding Assistance,,,,"We present two comprehensive benchmarks to evaluate the performance of language models in coding assistance tasks, covering code writing, debugging, code review, and conceptual understanding. Our main contribution includes two curated datasets: StackEval, a large-scale benchmark derived from Stack Overflow questions, and StackUnseen, a dynamic benchmark featuring the most recent Stack Overflow content. These benchmarks offer novel insights into the capabilities and limitations of LLMs, particularly in handling new and emerging content. Additionally, we assess LLMs’ proficiency as judges for coding tasks using a curated, human-annotated dataset, exploring their evaluation capabilities and potential biases, including whether they favor their own generated solutions. Our findings underscore the potential of these benchmarks to advance LLM development and application in coding assistance. To ensure reproducibility, we publicly share our datasets and evaluation code at https://github.com/ProsusAI/stack-eval.",,,NeurIPS.csv,,,,,,
KVGQK4R2,journalArticle,,"Antonov, Anton; Moskalenko, Andrey; Shepelev, Denis; Krapukhin, Alexander; Soshin, Konstantin; Konushin, Anton; Shakhuro, Vlad",RClicks: Realistic Click Simulation for Benchmarking Interactive Segmentation,,,,"The emergence of Segment Anything (SAM) sparked research interest in the field of interactive segmentation, especially in the context of image editing tasks and speeding up data annotation. Unlike common semantic segmentation, interactive segmentation methods allow users to directly influence their output through prompts (e.g. clicks). However, click patterns in real-world interactive segmentation scenarios remain largely unexplored. Most methods rely on the assumption that users would click in the center of the largest erroneous area. Nevertheless, recent studies show that this is not always the case. Thus, methods may have poor performance in real-world deployment despite high metrics in a baseline benchmark. To accurately simulate real-user clicks, we conducted a large crowdsourcing study of click patterns in an interactive segmentation scenario and collected 475K real-user clicks. Drawing on ideas from saliency tasks, we develop a clickability model that enables sampling clicks, which closely resemble actual user inputs. Using our model and dataset, we propose RClicks benchmark for a comprehensive comparison of existing interactive segmentation methods on realistic clicks. Specifically, we evaluate not only the average quality of methods, but also the robustness w.r.t. click patterns. According to our benchmark, in real-world usage interactive segmentation models may perform worse than it has been reported in the baseline benchmark, and most of the methods are not robust. We believe that RClicks is a significant step towards creating interactive segmentation methods that provide the best user experience in real-world cases.",,,NeurIPS.csv,,,,,,
FLFQ8G5A,journalArticle,,"Coursey, Austin; Ji, Junyi; Quinones-Grueiro, Marcos; Barbour, William; Zhang, Yuhang; Derr, Tyler; Biswas, Gautam; Work, Daniel B",FT-AED: Benchmark Dataset for Early Freeway Traffic Anomalous Event Detection,,,,"Early and accurate detection of anomalous events on the freeway, such as accidents, can improve emergency response and clearance. However, existing delays and mistakes from manual crash reporting records make it a difficult problem to solve. Current large-scale freeway traffic datasets are not designed for anomaly detection and ignore these challenges. In this paper, we introduce the first large-scale lanelevel freeway traffic dataset for anomaly detection. Our dataset consists of a month of weekday radar detection sensor data collected in 4 lanes along an 18-mile stretch of Interstate 24 heading toward Nashville, TN, comprising over 3.7 million sensor measurements. We also collect official crash reports from the Tennessee Department of Transportation Traffic Management Center and manually label all other potential anomalies in the dataset. To show the potential for our dataset to be used in future machine learning and traffic research, we benchmark numerous deep learning anomaly detection models on our dataset. We find that unsupervised graph neural network autoencoders are a promising solution for this problem and that ignoring spatial relationships leads to decreased performance. We demonstrate that our methods can reduce reporting delays by over 10 minutes on average while detecting 75% of crashes. Our dataset and all preprocessing code needed to get started are publicly released at https://vu.edu/ft-aed/ to facilitate future research.",,,NeurIPS.csv,,,,,,
F5FU3269,preprint,2024.0,"Silberg, Jake; Swanson, Kyle; Simon, Elana; Zhang, Angela; Ghazizadeh, Zaniar; Ogden, Scott; Hamadeh, Hisham; Zou, James",UniTox: Leveraging LLMs to Curate a Unified Dataset of Drug-Induced Toxicity from FDA Labels,,10.1101/2024.06.21.24309315,http://medrxiv.org/lookup/doi/10.1101/2024.06.21.24309315,"Drug-induced toxicity is one of the leading reasons new drugs fail clinical trials. Machine learning models that predict drug toxicity from molecular structure could help researchers prioritize less toxic drug candidates. However, current toxicity datasets are typically small and limited to a single organ system (e.g., cardio, renal, or liver). Creating these datasets often involved time-intensive expert curation by parsing drug labelling documents that can exceed 100 pages per drug. Here, we introduce UniTox1, a unified dataset of 2,418 FDA-approved drugs with druginduced toxicity summaries and ratings created by using GPT-4o to process FDA drug labels. UniTox spans eight types of toxicity: cardiotoxicity, liver toxicity, renal toxicity, pulmonary toxicity, hematological toxicity, dermatological toxicity, ototoxicity, and infertility. This is, to the best of our knowledge, the largest such systematic human in vivo database by number of drugs and toxicities, and the first covering nearly all non-combination FDA-approved medications for several of these toxicities. We recruited clinicians to validate a random sample of our GPT-4o annotated toxicities, and UniTox’s toxicity ratings concord with clinician labelers 85–96% of the time. Finally, we benchmark several machine learning models trained on UniTox to demonstrate the utility of this dataset for building molecular toxicity prediction models.",2024-06-22,,NeurIPS.csv,,,,,,
YFY4TN4I,journalArticle,,"Oh, Jio; Kim, Soyeon; Seo, Junseok; Wang, Jindong; Xu, Ruochen; Xie, Xing; Whang, Steven Euijong",ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models,,,,"Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have integrity constraints that can be used to better construct complex in-depth questions and verify answers: (1) functional dependencies can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) foreign key constraints can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.",,,NeurIPS.csv,,,,,,
YZZB9L6Y,journalArticle,,"Duncan, Suzanne; Leoni, Gianna; Steven, Lee; Mahelona, Keoni; Jones, Peter-Lucas","Fit for our purpose, not yours: Benchmark for a low-resource, Indigenous language",,,,"Influential and popular benchmarks in AI are largely irrelevant to developing NLP tools for low-resource, Indigenous languages. With the primary goal of measuring the performance of general-purpose AI systems, these benchmarks fail to give due consideration and care to individual language communities, especially low-resource languages. The datasets contain numerous grammatical and orthographic errors, poor pronunciation, limited vocabulary, and the content lacks cultural relevance to the language community. To overcome the issues with these benchmarks, we have created a dataset for the M¯aori language (the Indigenous language of Aotearoa/New Zealand) to pursue NLP tools that are ‘fit-for-our-purpose’. This paper demonstrates how low-resourced, Indigenous languages can develop tailored, high-quality benchmarks that; i. Reflect the unique characteristics of their language, ii. Reflect the diversity of speakers in the language community, iii. Support the aspirations for the tools they are developing and their language revitalisation efforts. All of which sit within a broader understanding of the impact of colonisation on their language.",,,NeurIPS.csv,,,,,,
S5ZDTAE9,journalArticle,,"Miranda, Imanol; Salaberria, Ander; Agirre, Eneko; Azkune, Gorka",BIVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval,,,,"Existing Vision-Language Compositionality (VLC) benchmarks like SUGARCREPE are formulated as image-to-text retrieval problems, where, given an image, the models need to select between the correct textual description and a synthetic hard negative text. In this work, we present the Bidirectional Vision-Language Compositionality (BIVLC) dataset. The novelty of BIVLC is to add a synthetic hard negative image generated from the synthetic text, resulting in two image-to-text retrieval examples (one for each image) and, more importantly, two text-to-image retrieval examples (one for each text). Human annotators filter out ill-formed examples ensuring the validity of the benchmark. The experiments on BIVLC uncover a weakness of current multimodal models, as they perform poorly in the text-toimage direction. In fact, when considering both retrieval directions, the conclusions obtained in previous works change significantly. In addition to the benchmark, we show that a contrastive model trained using synthetic images and texts significantly improves over the base model in SUGARCREPE and in BIVLC for both retrieval directions. The gap to human performance in BIVLC confirms that Vision-Language Compositionality is still a challenging problem. BIVLC and code are available at https://imirandam.github.io/BiVLC_project_page.",,,NeurIPS.csv,,,,,,
493GYHEK,dataset,2024.0,"Herde, Marek; Huseljic, Denis; Rauch, Lukas; Sick, Bernhard",dopanim: A Dataset of Doppelganger Animals with Noisy Annotations from Multiple Humans,,10.5281/ZENODO.11479589,https://zenodo.org/doi/10.5281/zenodo.11479589,"Human annotators typically provide annotated data for training machine learning models, such as neural networks. Yet, human annotations are subject to noise, impairing generalization performances. Methodological research on approaches counteracting noisy annotations requires corresponding datasets for a meaningful empirical evaluation. Consequently, we introduce a novel benchmark dataset, dopanim, consisting of about 15,750 animal images of 15 classes with ground truth labels. For approximately 10,500 of these images, 20 humans provided over 52,000 annotations with an accuracy of circa 67%. Its key attributes include (1) the challenging task of classifying doppelganger animals, (2) human-estimated likelihoods as annotations, and (3) annotator metadata. We benchmark well-known multi-annotator learning approaches using seven variants of this dataset and outline further evaluation use cases such as learning beyond hard class labels and active learning. Our dataset and a comprehensive codebase are publicly available to emulate the data collection process and to reproduce all empirical results.",2024-10-31,,NeurIPS.csv,,,,,,
C6EHKAVZ,journalArticle,,"Zhu, Yizhang; Du, Shiyin; Li, Boyan; Luo, Yuyu; Tang, Nan",Are Large Language Models Good Statisticians?,,,,"Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry. Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored. To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks. StatQA comprises 11,623 examples tailored to evaluate LLMs’ proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement. Notably, while open-source LLMs (e.g., LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g., GPT-4o). Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential. Our source code and data are available at https://statqa.github.io/.",,,NeurIPS.csv,,,,,,
52G7S892,journalArticle,,"Nikulin, Alexander; Kurenkov, Vladislav; Zisman, Ilya; Agarkov, Artem; Sinii, Viacheslav; Kolesnikov, Sergey",XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX,,,,"Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging. XLand-MiniGrid is open-source and available at https://github.com/corl-team/xland-minigrid.",,,NeurIPS.csv,,,,,,
FU7226SX,journalArticle,,"Sivakumar, Viswanath; Seely, Jeffrey; Du, Alan; Bittner, Sean R; Berenzweig, Adam; Bolarinwa, Anuoluwapo; Gramfort, Alexandre; Mandel, Michael I",emg2qwerty: A Large Dataset with Baselines for Touch Typing using Surface Electromyography,,,,"Surface electromyography (sEMG) non-invasively measures signals generated by muscle activity with sufficient sensitivity to detect individual spinal neurons and richness to identify dozens of gestures and their nuances. Wearable wrist-based sEMG sensors have the potential to offer low friction, subtle, information rich, always available human-computer inputs. To this end, we introduce emg2qwerty, a large-scale dataset of non-invasive electromyographic signals recorded at the wrists while touch typing on a QWERTY keyboard, together with ground-truth annotations and reproducible baselines1. With 1,135 sessions spanning 108 users and 346 hours of recording, this is the largest such public dataset to date. These data demonstrate non-trivial, but well defined hierarchical relationships both in terms of the generative process, from neurons to muscles and muscle combinations, as well as in terms of domain shift across users and user sessions. Applying standard modeling techniques from the closely related field of Automatic Speech Recognition (ASR), we show strong baseline performance on predicting keypresses using sEMG signals alone. We believe the richness of this task and dataset will facilitate progress in several problems of interest to both the machine learning and neuroscientific communities.",,,NeurIPS.csv,,,,,,
SH2723EF,journalArticle,,"Dumpala, Sri Harsha; Jaiswal, Aman; Sastry, Chandramouli; Milios, Evangelos; Oore, Sageev; Sajjad, Hassan",SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations,,,,"Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly to object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We demonstrate that models excelling on compositionality datasets may not perform equally well on SUGARCREPE++. This indicates that compositionality alone might not be sufficient to fully understand semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community. Data and code is available at https://github.com/Sri-Harsha/scpp.",,,NeurIPS.csv,,,,,,
RNDKV7SF,journalArticle,,"Gharaee, Zahra; Lowe, Scott C; Gong, ZeMing; Arias, Pablo Millan; Pellegrino, Nicholas; Wang, Austin T; Haurum, Joakim Bruslund; Zarubiieva, Iuliia; Kari, Lila; Steinke, Dirk; Taylor, Graham W; Fieguth, Paul; Chang, Angel X",BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity,,,,"As part of an ongoing worldwide effort to comprehend and monitor insect biodiversity, this paper presents the BIOSCAN-5M Insect dataset to the machine learning community and establish several benchmark tasks. BIOSCAN-5M is a comprehensive dataset containing multi-modal information for over 5 million insect specimens, and it significantly expands existing image-based biological datasets by including taxonomic labels, raw nucleotide barcode sequences, assigned barcode index numbers, geographical, and size information. We propose three benchmark experiments to demonstrate the impact of the multi-modal data types on the classification and clustering accuracy. First, we pretrain a masked language model on the DNA barcode sequences of the BIOSCAN-5M dataset, and demonstrate the impact of using this large reference library on species- and genus-level classification performance. Second, we propose a zero-shot transfer learning task applied to images and DNA barcodes to cluster feature embeddings obtained from self-supervised learning, to investigate whether meaningful clusters can be derived from these representation embeddings. Third, we benchmark multi-modality by performing contrastive learning on DNA barcodes, image data, and taxonomic information. This yields a general shared embedding space enabling taxonomic classification using multiple types of information and modalities. The code repository of the BIOSCAN-5M Insect dataset is available at https://github.com/bioscan-ml/BIOSCAN-5M.",,,NeurIPS.csv,,,,,,
8IMLAVIE,journalArticle,,"Salter, Sasha; Warren, Richard; Schlager, Collin; Spurr, Adrian; Han, Shangchen; Cai, Yujun; Walkington, Peter; Bolarinwa, Anuoluwapo; Wang, Robert; Merel, Josh; Pnevmatikakis, Eftychios; Marshall, Jesse",emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation,,,,"Hands are the primary means through which humans interact with the world. Reliable and always-available hand pose inference could yield new and intuitive control schemes for human-computer interactions, particularly in virtual and augmented reality. Computer vision is effective but requires one or multiple cameras and can struggle with occlusions, limited field of view, and poor lighting. Wearable wrist-based surface electromyography (sEMG) presents a promising alternative as an always-available modality sensing muscle activities that drive hand motion. However, sEMG signals are strongly dependent on user anatomy and sensor placement; existing sEMG models have thus required hundreds of users and device placements to effectively generalize for tasks other than pose inference. To facilitate progress on sEMG pose inference, we introduce the emg2pose benchmark, which is to our knowledge the first publicly available dataset of high-quality hand pose labels and wrist sEMG recordings. emg2pose contains 2kHz, 16 channel sEMG and pose labels from a 26-camera motion capture rig for 193 users, 370 hours, and 29 stages with diverse gestures - a scale comparable to vision-based hand pose datasets. We provide competitive baselines and challenging tasks evaluating real-world generalization scenarios: held-out users, sensor placements, and stages. This benchmark provides the machine learning community a platform for exploring complex generalization problems, holding potential to significantly enhance the development of sEMG-based human-computer interactions.",,,NeurIPS.csv,,,,,,
UGBGDTQE,journalArticle,,"Saul, Rebecca; Liu, Chang; Fleischmann, Noah; Zak, Richard; Micinski, Kristopher; Raff, Edward; Holt, James",Is Function Similarity Over-Engineered? Building a Benchmark,,,,"Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive preprocessing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFUSE-BENCH, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple baseline —one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing — is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value.",,,NeurIPS.csv,,,,,,
L9B8KG8Q,journalArticle,,"Chen, Jiawen; Zhou, Muqing; Wu, Wenrong; Zhang, Jinwei; Li, Yun; Li, Didong",STimage-1K4M: A histopathology image-gene expression dataset for spatial transcriptomics,,,,,,,NeurIPS.csv,,,,,,
TXFN8GGP,journalArticle,,"Fang, Xinyu; Mao, Kangrui; Duan, Haodong; Zhao, Xiangyu; Li, Yining; Lin, Dahua; Chen, Kai",MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding,,,,"The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models’ temporal comprehension. To address these limitations, we introduce MMBenchVideo, a quantitative benchmark designed to rigorously evaluate LVLMs’ proficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models’ temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding. The evalutation code of MMBench-Video will be integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",,,NeurIPS.csv,,,,,,
SGIAPDQS,journalArticle,,"Liang, Haiji; Han, Ruize",OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking,,,,"Open-vocabulary object perception has become an important topic in artificial intelligence, which aims to identify objects with novel classes that have not been seen during training. Under this setting, open-vocabulary object detection (OVD) in a single image has been studied in many literature. However, open-vocabulary object tracking (OVT) from a video has been studied less, and one reason is the shortage of benchmarks. In this work, we have built a new large-scale benchmark for open-vocabulary multi-object tracking namely OVT-B. OVT-B contains 1,048 categories of objects and 1,973 videos with 637,608 bounding box annotations, which is much larger than the sole open-vocabulary tracking dataset, i.e., OVTAO-val dataset (200+ categories, 900+ videos). The proposed OVT-B can be used as a new benchmark to pave the way for OVT research. We also develop a simple yet effective baseline method for OVT. It integrates the motion features for object tracking, which is an important feature for MOT but is ignored in previous OVT methods. Experimental results have verified the usefulness of the proposed benchmark and the effectiveness of our method. We have released the benchmark to the public at https://github.com/Coo1Sea/OVT-B-Dataset.",,,NeurIPS.csv,,,,,,
9S9ERQQ7,journalArticle,,"Peddi, Rohith; Arya, Shivvrat; Challa, Bharath; Pallapothula, Likhitha; Vyas, Akshay; Gouripeddi, Bhavya; Zhang, Qifan; Wang, Jikai; Komaragiri, Vasundhara; Ragan, Eric; Ruozzi, Nicholas; Xiang, Yu; Gogate, Vibhav",CaptainCook4D: A Dataset for Understanding Errors in Procedural Activities,,,,"Following step-by-step procedures is an essential component of various activities carried out by individuals in their daily lives. These procedures serve as a guiding framework that helps to achieve goals efficiently, whether it is assembling furniture or preparing a recipe. However, the complexity and duration of procedural activities inherently increase the likelihood of making errors. Understanding such procedural activities from a sequence of frames is a challenging task that demands an accurate interpretation of visual information and the ability to reason about the structure of the activity. To this end, we collect a new egocentric 4D dataset CaptainCook4D comprising 384 recordings (94.5 hours) of people performing recipes in real kitchen environments. This dataset consists of two distinct types of activities: one in which participants adhere to the provided recipe instructions and another in which they deviate and induce errors. We provide 5.3K step annotations and 10K finegrained action annotations and benchmark the dataset for the following tasks: error recognition, multi-step localization and procedure learning2.",,,NeurIPS.csv,,,,,,
H7ZVHYCU,journalArticle,,"Jiménez-Sánchez, Amelia; Avlona, Natalia-Rozalia; Juodelyte, Dovile; Sourget, Théo; Vang-Larsen, Caroline; Rogers, Anna; Zaja, Hubert Dariusz",Copycats: the many lives of a publicly available medical imaging dataset,,,,"Medical Imaging (MI) datasets are fundamental to artificial intelligence in healthcare. The accuracy, robustness, and fairness of diagnostic algorithms depend on the data (and its quality) used to train and evaluate the models. MI datasets used to be proprietary, but have become increasingly available to the public, including on community-contributed platforms (CCPs) like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data’s public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper, we conduct an analysis of publicly available machine learning datasets on CCPs, discussing datasets’ context, and identifying limitations and gaps in the current CCP landscape. We highlight differences between MI and computer vision datasets, particularly in the potentially harmful downstream effects from poor adoption of recommended dataset management practices. We compare the analyzed datasets across several dimensions, including data sharing, data documentation, and maintenance. We find vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata, with differences between the platforms. Our research contributes to efforts in responsible data curation and AI algorithms for healthcare.",,,NeurIPS.csv,,,,,,
XL4RPNAZ,journalArticle,,"Wang, Jize; Ma, Zerun; Li, Yining; Zhang, Songyang; Chen, Cailian; Chen, Kai; Le, Xinyi",GTA: A Benchmark for General Tool Agents,,,,"Significant focus has been placed on integrating large language models (LLMs) with various tools in developing general-purpose agents. This poses a challenge to LLMs’ tool-use capabilities. However, there are evident gaps between existing tool-use evaluations and real-world scenarios. Current evaluations often use AIgenerated queries, single-step tasks, dummy tools, and text-only interactions, failing to effectively reveal the agents’ real-world problem-solving abilities. To address this, we propose GTA, a benchmark for General Tool Agents, featuring three main aspects: (i) Real user queries: human-written queries with simple real-world objectives but implicit tool-use, requiring the LLM to reason the suitable tools and plan the solution steps. (ii) Real deployed tools: an evaluation platform equipped with tools across perception, operation, logic, and creativity categories to evaluate the agents’ actual task execution performance. (iii) Real multimodal inputs: authentic image files, such as spatial scenes, web page screenshots, tables, code snippets, and printed/handwritten materials, used as the query contexts to align with real-world scenarios closely. We design 229 real-world tasks and executable tool chains to evaluate mainstream LLMs. Our findings show that real-world user queries are challenging for existing LLMs, with GPT-4 completing less than 50% of the tasks and most LLMs achieving below 25%. This evaluation reveals the bottlenecks in the tool-use capabilities of current LLMs in real-world scenarios, which provides future direction for advancing general-purpose tool agents. Dataset and code are available at https://github.com/open-compass/GTA.",,,NeurIPS.csv,,,,,,
UG3NRHRE,journalArticle,,"Elrefaie, Mohamed; Morar, Florin; Dai, Angela; Ahmed, Faez",DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks,,,,,,,NeurIPS.csv,,,,,,
744VH5HH,journalArticle,,"Vivoli, Emanuele; Bertini, Marco; Karatzas, Dimosthenis",CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding,,,,"The comic domain is rapidly advancing with the development of single-page analysis and synthesis models. However, evaluation metrics and datasets lag behind, often limited to small-scale or single-style test sets. We introduce a novel benchmark, CoMix, designed to evaluate the multi-task capabilities of models in comic analysis. Unlike existing benchmarks that focus on isolated tasks such as object detection or text recognition, CoMix addresses a broader range of tasks including object detection, speaker identification, character re-identification, reading order, and multi-modal reasoning tasks like character naming and dialogue generation. Our benchmark comprises three existing datasets with expanded annotations to support multi-task evaluation. To mitigate the over-representation of manga-style data, we have incorporated a new dataset of carefully selected American comicstyle books, thereby enriching the diversity of comic styles. CoMix is designed to assess pre-trained models in zero-shot and limited fine-tuning settings, probing their transfer capabilities across different comic styles and tasks. The validation split of the benchmark is publicly available for research purposes, and an evaluation server for the held-out test split is also provided. Comparative results between human performance and state-of-the-art models reveal a significant performance gap, highlighting substantial opportunities for advancements in comic understanding. The dataset, baseline models, and code are accessible at the repository link. This initiative sets a new standard for comprehensive comic analysis, providing the community with a common benchmark for evaluation on a large and varied set.",,,NeurIPS.csv,,,,,,
9F4SHRXA,journalArticle,,"Jiang, Dongfu; Ku, Max; Li, Tianle; Ni, Yuansheng; Sun, Shizhuo; Fan, Rongqi; Chen, Wenhu",GenAI Arena: An Open Evaluation Platform for Generative Models,,,,"Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GENAI-ARENA to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GENAI-ARENA aims to provide a more democratic and accurate measure of model performance. It covers three arenas for text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 35 open-source generative models. GENAI-ARENA has been operating for seven months, amassing over 9000 votes from the community. We describe our platform, analyze the data, and explain the statistical methods for ranking the models. To further promote the research in building model-based evaluation metrics, we release a cleaned version of our preference data for the three tasks, namely GenAI-Bench. We prompt the existing multi-modal models like Gemini, GPT-4o to mimic human voting. We compute the accuracy by comparing the model voting with the human voting to understand their juding abilities. Our results show existing multimodal models are still lagging in assessing the generated visual content, even the best model GPT-4o only achieves an average accuracy of 49.19% across the three generative tasks. Open-source MLLMs perform even worse due to the lack of instruction-following and reasoning ability in the complex vision scenarios.",,,NeurIPS.csv,,,,,,
U4G6LN2L,journalArticle,,"Chib, Pranav Singh; Singh, Pravendra","Pedestrian Trajectory Prediction with Missing Data: Datasets, Imputation, and Benchmarking",,,,"Pedestrian trajectory prediction is crucial for several applications such as robotics and self-driving vehicles. Significant progress has been made in the past decade thanks to the availability of pedestrian trajectory datasets, which enable trajectory prediction methods to learn from pedestrians’ past movements and predict future trajectories. However, these datasets and methods typically assume that the observed trajectory sequence is complete, ignoring real-world issues such as sensor failure, occlusion, and limited fields of view that can result in missing values in observed trajectories. To address this challenge, we present TrajImpute, a pedestrian trajectory prediction dataset that simulates missing coordinates in the observed trajectory, enhancing real-world applicability. TrajImpute maintains a uniform distribution of missing data within the observed trajectories. In this work, we comprehensively examine several imputation methods to reconstruct the missing coordinates and benchmark them for imputing pedestrian trajectories. Furthermore, we provide a thorough analysis of recent trajectory prediction methods and evaluate the performance of these models on the imputed trajectories. Our experimental evaluation of the imputation and trajectory prediction methods offers several valuable insights. Our dataset provides a foundational resource for future research on imputation-aware pedestrian trajectory prediction, potentially accelerating the deployment of these methods in real-world applications. Publicly accessible links to the datasets and code files are available at https://github.com/Pranav-chib/TrajImpute.",,,NeurIPS.csv,,,,,,
N7YZYT2L,journalArticle,,"Chen, Wei; Hao, Xixuan; Wu, Yuankai; Liang, Yuxuan",Terra: A Multimodal Spatio-Temporal Dataset Spanning the Earth,,,,"Since the inception of our planet, the meteorological environment, as reflected through spatio-temporal data, has always been a fundamental factor influencing human life, socio-economic progress, and ecological conservation. A comprehensive exploration of this data is thus imperative to gain a deeper understanding and more accurate forecasting of these environmental shifts. Despite the success of deep learning techniques within the realm of spatio-temporal data and earth science, existing public datasets are beset with limitations in terms of spatial scale, temporal coverage, and reliance on limited time series data. These constraints hinder their optimal utilization in practical applications. To address these issues, we introduce Terra, a multimodal spatio-temporal dataset spanning the earth. This dataset encompasses hourly time series data from 6,480,000 grid areas worldwide over the past 45 years, while also incorporating multimodal spatial supplementary information including geo-images and explanatory text. Through a detailed data analysis and evaluation of existing deep learning models within earth sciences, utilizing our constructed dataset. we aim to provide valuable opportunities for enhancing future research in spatio-temporal data mining, thereby advancing towards more spatio-temporal general intelligence. Our source code and data can be accessed at https://github.com/CityMind-Lab/NeurIPS24-Terra.",,,NeurIPS.csv,,,,,,
PPRUH7NK,journalArticle,,"Li, Haitao; Chen, You",LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models,,,,"Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain. However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice. To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval. This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application. We evaluated 38 open-source and commercial LLMs and obtained some interesting findings. The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at https://github.com/CSHaitao/LexEval and will be continuously updated.",,,NeurIPS.csv,,,,,,
VWCCAHZI,journalArticle,,"Karmakar, Prasenjit; Pradhan, Swadhin; Chakraborty, Sandip",Indoor Air Quality Dataset with Activities of Daily Living in Low to Middle-income Communities,,,,"In recent years, indoor air pollution has posed a significant threat to our society, claiming over 3.2 million lives annually. Developing nations, such as India, are most affected since lack of knowledge, inadequate regulation, and outdoor air pollution lead to severe daily exposure to pollutants. However, only a limited number of studies have attempted to understand how indoor air pollution affects developing countries like India. To address this gap, we present spatiotemporal measurements of air quality from 30 indoor sites over six months during summer and winter seasons. The sites are geographically located across four regions of type: rural, suburban, and urban, covering the typical low to middle-income population in India. The dataset1 contains various types of indoor environments (e.g., studio apartments, classrooms, research laboratories, food canteens, and residential households), and can provide the basis for data-driven learning model research aimed at coping with unique pollution patterns in developing countries. This unique dataset demands advanced data cleaning and imputation techniques for handling missing data due to power failure or network outages during data collection. Furthermore, through a simple speech-to-text application, we provide real-time indoor activity labels annotated by occupants. Therefore, environmentalists and ML enthusiasts can utilize this dataset to understand the complex patterns of the pollutants under different indoor activities, identify recurring sources of pollution, forecast exposure, improve floor plans and room structures of modern indoor designs, develop pollution-aware recommender systems, etc.",,,NeurIPS.csv,,,,,,
DJJLZL3Q,journalArticle,,"Zou, Deyu; Liu, Shikun; Miao, Siqi; Fung, Victor; Chang, Shiyu; Li, Pan",GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts,,,,"Geometric deep learning (GDL) has gained significant attention in scientific fields, for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many applications. To bridge this gap, we propose GeSS, a comprehensive benchmark designed for evaluating the performance of GDL models in scientific scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics, materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) test data, including no OOD information, only unlabeled OOD data, and OOD data with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 GDL backbones and 11 learning algorithms in each setting. A thorough analysis of the evaluation results is provided, poised to illuminate insights for GDL researchers and domain practitioners who are to use GDL in their applications.",,,NeurIPS.csv,,,,,,
L6B6TJ68,journalArticle,,"Linghu, Xiongkun; Huang, Jiangyong; Niu, Xuesong; Ma, Xiaojian; Jia, Baoxiong; Huang, Siyuan",Multi-modal Situated Reasoning in 3D Scenes,,,,"Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models’ situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.",,,NeurIPS.csv,,,,,,
CEKC445A,journalArticle,,"Nguyen, Kien X; Qiao, Fengchun; Trembanis, Arthur; Peng, Xi",SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey,,,,"A major obstacle to the advancements of machine learning models in marine science, particularly in sonar imagery analysis, is the scarcity of AI-ready datasets. While there have been efforts to make AI-ready sonar image dataset publicly available, they suffer from limitations in terms of environment setting and scale. To bridge this gap, we introduce SeafloorAI, the first extensive AI-ready datasets for seafloor mapping across 5 geological layers that is curated in collaboration with marine scientists. We further extend the dataset to SeafloorGenAI by incorporating the language component in order to facilitate the development of both visionand language-capable machine learning models for sonar imagery. The dataset consists of 62 geo-distributed data surveys spanning 17,300 square kilometers, with 696K sonar images, 827K annotated segmentation masks, 696K detailed language descriptions and approximately 7M question-answer pairs. By making our data processing source code publicly available, we aim to engage the marine science community to enrich the data pool and inspire the machine learning community to develop more robust models. This collaborative approach will enhance the capabilities and applications of our datasets within both fields. Our code repository are available 1 under the CC-BY-4.0 license.",,,NeurIPS.csv,,,,,,
Q9EV9CUH,journalArticle,,"Hargrave, Mason; Spaeth, Alex; Grosenick, Logan",EpiCare: A Reinforcement Learning Benchmark for Dynamic Treatment Regimes,,,,"Healthcare applications pose significant challenges to existing reinforcement learning (RL) methods due to implementation risks, limited data availability, short treatment episodes, sparse rewards, partial observations, and heterogeneous treatment effects. Despite significant interest in using RL to generate dynamic treatment regimes for longitudinal patient care scenarios, no standardized benchmark has yet been developed. To fill this need we introduce Episodes of Care (EpiCare), a benchmark designed to mimic the challenges associated with applying RL to longitudinal healthcare settings. We leverage this benchmark to test five stateof-the-art offline RL models as well as five common off-policy evaluation (OPE) techniques. Our results suggest that while offline RL may be capable of improving upon existing standards of care given sufficient data, its applicability does not appear to extend to the moderate to low data regimes typical of current healthcare settings. Additionally, we demonstrate that several OPE techniques standard in the the medical RL literature fail to perform adequately on our benchmark. These results suggest that the performance of RL models in dynamic treatment regimes may be difficult to meaningfully evaluate using current OPE methods, indicating that RL for this application domain may still be in its early stages. We hope that these results along with the benchmark will facilitate better comparison of existing methods and inspire further research into techniques that increase the practical applicability of medical RL.",,,NeurIPS.csv,,,,,,
8I9TN7P2,journalArticle,,"Jin, Yilun; Li, Zheng; Zhang, Chenwei; Cao, Tianyu; Gao, Yifan; Jayarao, Pratik; Li, Mao; Liu, Xin; Sarkhel, Ritesh; Tang, Xianfeng; Wang, Haodong; Wang, Zhengyang; Xu, Wenju; Yang, Jingfeng; Yin, Qingyu; Li, Xian; Nigam, Priyanka; Xu, Yi; Chen, Kai; Yang, Qiang; Jiang, Meng; Yin, Bing",Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models,,,,"Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full complexity of online shopping. Large Language Models (LLMs), with their multi-task and few-shot learning abilities, have the potential to profoundly transform online shopping by alleviating task-specific engineering efforts and by providing users with interactive conversations. Despite the potential, LLMs face unique challenges in online shopping, such as domain-specific concepts, implicit knowledge, and heterogeneous user behaviors. Motivated by the potential and challenges, we propose Shopping MMLU, a diverse multi-task online shopping benchmark derived from real-world Amazon data. Shopping MMLU consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality, and can thus comprehensively evaluate the abilities of LLMs as general shop assistants. With Shopping MMLU, we benchmark over 20 existing LLMs and uncover valuable insights about practices and prospects of building versatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at https://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we host a competition in KDD Cup 2024 2 with over 500 participating teams. The winning solutions and the associated workshop can be accessed at our website https://amazon-kddcup24.github.io/.",,,NeurIPS.csv,,,,,,
BUXRXSTA,journalArticle,,"Wang, Zeyu; Zhang, Xiyuxing; Yu, Ruotong",DreamCatcher: A Wearer-aware Sleep Event Dataset Based on Earables in Non-restrictive Environments,,,,"Poor quality sleep can be characterized by the occurrence of events ranging from body movement to breathing impairment. Widely available earbuds equipped with sensors (also known as earables) can be combined with a sleep event detection algorithm to offer a convenient alternative to laborious clinical tests for individuals suffering from sleep disorders. Although various solutions utilizing such devices have been proposed to detect sleep events, they ignore the fact that individuals often share sleeping spaces with roommates or couples. To address this issue, we introduce DreamCatcher, the first publicly available dataset for wearer-aware sleep event algorithm development on earables. DreamCatcher encompasses eight distinct sleep events, including synchronous dual-channel audio and motion data collected from 12 pairs (24 participants) totaling 210 hours (420 hour.person) with fine-grained label. We tested multiple benchmark models on three tasks related to sleep event detection, demonstrating the usability and unique challenge of DreamCatcher. We hope that the proposed DreamCatcher can inspire other researchers to further explore efficient wearer-aware human vocal activity sensing on earables. DreamCatcher is publicly available at https://github.com/thuhci/DreamCatcher.",,,NeurIPS.csv,,,,,,
LCYG8UQ9,journalArticle,,"Guille-Escuret, Charles; Noël, Pierre-André; Mitliagkas, Ioannis; Vazquez, David; Monteiro, Joao",Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection,,,,"Deployed machine learning systems require some mechanism to detect out-ofdistribution (OOD) inputs. Existing research mainly focuses on one type of distribution shift: detecting samples from novel classes, absent from the training set. However, real-world systems encounter a broad variety of anomalous inputs, and the OOD literature neglects this diversity. This work categorizes five distinct types of distribution shifts and critically evaluates the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). We find that while these methods excel in detecting novel classes, their performances are inconsistent across other types of distribution shifts. In other words, they can only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a Gaussian mixture generative model for existing detection scores, enabling an ensemble detection approach that is more consistent and comprehensive for broad OOD detection, with improved performances over existing methods. We release code to build BROAD to facilitate a more comprehensive evaluation of novel OOD detectors.1.",,,NeurIPS.csv,,,,,,
9MVE9XQR,journalArticle,,"Chen, Pin; Peng, Luoxuan; Jiao, Rui; Mo, Qing; Wang, Zhen; Huang, Wenbing; Liu, Yang; Lu, Yutong",Learning Superconductivity from Ordered and Disordered Material Structures,,,,"Superconductivity is a fascinating phenomenon observed in certain materials under certain conditions. However, some critical aspects of it, such as the relationship between superconductivity and materials’ chemical/structural features, still need to be understood. Recent successes of data-driven approaches in material science strongly inspire researchers to study this relationship with them, but a corresponding dataset is still lacking. Hence, we present a new dataset for datadriven approaches, namely SuperCon3D, containing both 3D crystal structures and experimental superconducting transition temperature (Tc) for the first time. Based on SuperCon3D, we propose two deep learning methods for designing high Tc superconductors. The first is SODNet, a novel equivariant graph attention model for screening known structures, which differs from existing models in incorporating both ordered and disordered geometric content. The second is a diffusion generative model DiffCSP-SC for creating new structures, which enables high Tc-targeted generation. Extensive experiments demonstrate that both our proposed dataset and models are advantageous for designing new high Tc superconducting candidates.",,,NeurIPS.csv,,,,,,
LEX28XF5,journalArticle,,"Chasmai, Mustafa; Shepard, Alexander; Maji, Subhransu; Horn, Grant Van",The iNaturalist Sounds Dataset,,,,"We present the iNaturalist Sounds Dataset (iNatSounds), a collection of 230,000 audio files capturing sounds from over 5,500 species, contributed by more than 27,000 recordists worldwide. The dataset encompasses sounds from birds, mammals, insects, reptiles, and amphibians, with audio and species labels derived from observations submitted to iNaturalist, a global citizen science platform. Each recording in the dataset varies in length and includes a single species annotation. We benchmark multiple backbone architectures, comparing multiclass classification objectives with multilabel objectives. Despite weak labeling, we demonstrate that iNatSounds serves as a useful pretraining resource by benchmarking it on strongly labeled downstream evaluation datasets. The dataset is available as a single, freely accessible archive, promoting accessibility and research in this important domain. We envision models trained on this data powering next-generation public engagement applications, and assisting biologists, ecologists, and land use managers in processing large audio collections, thereby contributing to the understanding of species compositions in diverse soundscapes.",,,NeurIPS.csv,,,,,,
IBUPPRQC,journalArticle,,"Hao, Xiaoshuai; Wei, Mengchuan; Yang, Yifan; Zhao, Haimei; Zhang, Hui; Zhou, Yi; Wang, Qiang; Li, Weiming; Kong, Lingdong; Zhang, Jing",Is Your HD Map Constructor Reliable under Sensor Corruptions?,,,,"Driving systems often rely on high-definition (HD) maps for precise environmental information, which is crucial for planning and navigation. While current HD map constructors perform well under ideal conditions, their resilience to real-world challenges, e.g., adverse weather and sensor failures, is not well understood, raising safety concerns. This work introduces MapBench, the first comprehensive benchmark designed to evaluate the robustness of HD map construction methods against various sensor corruptions. Our benchmark encompasses a total of 29 types of corruptions that occur from cameras and LiDAR sensors. Extensive evaluations across 31 HD map constructors reveal significant performance degradation of existing methods under adverse weather conditions and sensor failures, underscoring critical safety concerns. We identify effective strategies for enhancing robustness, including innovative approaches that leverage multi-modal fusion, advanced data augmentation, and architectural techniques. These insights provide a pathway for developing more reliable HD map construction methods, which are essential for the advancement of autonomous driving technology. The benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible.",,,NeurIPS.csv,,,,,,
WNBHHIAL,journalArticle,,"Huang, Irene; Lin, Wei; Mirza, M Jehanzeb; Hansen, Jacob A; Doveh, Sivan; Butoi, Victor Ion; Herzig, Roei; Arbelle, Assaf; Kuehne, Hilde; Darrell, Trevor; Gan, Chuang; Oliva, Aude; Feris, Rogerio; Karlinsky, Leonid",ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs,,,,"Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs’ LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe1 – a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce ‘hard CR Q&A’. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.",,,NeurIPS.csv,,,,,,
66X94SUX,journalArticle,,"Vendrow, Edward; Pantazis, Omiros; Shepard, Alexander; Brostow, Gabriel; Jones, Kate E; Aodha, Oisin Mac; Beery, Sara; Horn, Grant Van",INQUIRE: A Natural World Text-to-Image Retrieval Benchmark,,,,"We introduce INQUIRE, a text-to-image retrieval benchmark designed to challenge multimodal vision-language models on expert-level queries. INQUIRE includes iNaturalist 2024 (iNat24), a new dataset of ﬁve million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identiﬁcation, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) INQUIRE-FULLRANK, a full dataset ranking task, and (2) INQUIRE-RERANK, a reranking task for reﬁning top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that INQUIRE poses a signiﬁcant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a signiﬁcant margin for improvement. By focusing on scientiﬁcally-motivated ecological challenges, INQUIRE aims to bridge the gap between AI capabilities and the needs of real-world scientiﬁc inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.",,,NeurIPS.csv,,,,,,
H2E7WGHL,journalArticle,,"Wang, Zirui; Xia, Mengzhou; He, Luxi; Chen, Howard; Liu, Yitao; Zhu, Richard; Liang, Kaiqu; Wu, Xindi; Liu, Haotian; Malladi, Sadhika; Chevalier, Alexis; Arora, Sanjeev; Chen, Danqi",CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs,,,,"Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an overly optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope that CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress.",,,NeurIPS.csv,,,,,,
6ZQIHDWV,journalArticle,,"Prabowo, Arian; Lin, Xiachong; Razzak, Imran; Xue, Hao; Yap, Emily W; Amos, Matthew; Salim, Flora D",BTS: Building Timeseries Dataset: Empowering Large-Scale Building Analytics,,,,,,,NeurIPS.csv,,,,,,
XWRUSKGL,journalArticle,,"Kon, Patrick Tser Jern; Liu, Jiachen; Qiu, Yiming; Fan, Weijun; He, Ting; Lin, Lei; Zhang, Haoran; Park, Owen M; Elengikal, George S; Kang, Yuxin; Chen, Ang; Chowdhury, Mosharaf; Lee, Myungjin; Wang, Xinyu",IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs,,,,"Infrastructure-as-Code (IaC), an important component of cloud computing, allows the definition of cloud infrastructure in high-level programs. However, developing IaC programs is challenging, complicated by factors that include the burgeoning complexity of the cloud ecosystem (e.g., diversity of cloud services and workloads), and the relative scarcity of IaC-specific code examples and public repositories. While large language models (LLMs) have shown promise in general code generation and could potentially aid in IaC development, no benchmarks currently exist for evaluating their ability to generate IaC code. We present IaC-Eval, a first step in this research direction. IaC-Eval’s dataset includes 458 human-curated scenarios covering a wide range of popular AWS services, at varying difficulty levels. Each scenario mainly comprises a natural language IaC problem description and an infrastructure intent specification. The former is fed as user input to the LLM, while the latter is a general notion used to verify if the generated IaC program conforms to the user’s intent; by making explicit the problem’s requirements that can encompass various cloud services, resources and internal infrastructure details. Our in-depth evaluation shows that contemporary LLMs perform poorly on IaC-Eval, with the top-performing model, GPT-4, obtaining a pass@1 accuracy of 19.36%. In contrast, it scores 86.6% on EvalPlus, a popular Python code generation benchmark, highlighting a need for advancements in this domain. We open-source the IaC-Eval dataset and evaluation framework at https://github.com/autoiac-project/iac-eval to enable future research on LLM-based IaC code generation.",,,NeurIPS.csv,,,,,,
CAQRMHK5,journalArticle,,"Zhang, Yichi; Huang, Yao; Sun, Yitong; Liu, Chang; Zhao, Zhe; Fang, Zhengwei; Wang, Yifan; Chen, Huanran; Yang, Xiao; Wei, Xingxing; Su, Hang; Dong, Yinpeng; Zhu, Jun",MULTITRUST: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models,,,,"Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish MultiTrust, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: https://multi-trust.github.io/.",,,NeurIPS.csv,,,,,,
3SELXSRZ,journalArticle,,"Zhao, Hongbo; Fan, Lue; Chen, Yuntao; Wang, Haochen; Yang, Yuran; Jin, Xiaojuan; Zhang, Yixin; Meng, Gaofeng; Zhang, Zhaoxiang",OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction,,,,"In this paper, we propose OpenSatMap, a fine-grained, high-resolution satellite dataset for large-scale map construction. Map construction is one of the foundations of the transportation industry, such as navigation and autonomous driving. Extracting road structures from satellite images is an efficient way to construct large-scale maps. However, existing satellite datasets provide only coarse semantic-level labels with a relatively low resolution (up to level 19), impeding the advancement of this field. In contrast, the proposed OpenSatMap (1) has fine-grained instance-level annotations; (2) consists of high-resolution images (level 20); (3) is currently the largest one of its kind; (4) collects data with high diversity. Moreover, OpenSatMap covers and aligns with the popular nuScenes dataset and Argoverse 2 dataset to potentially advance autonomous driving technologies. By publishing and maintaining the dataset, we provide a high-quality benchmark for satellite-based map construction and downstream tasks like autonomous driving.",,,NeurIPS.csv,,,,,,
8VDXN6C3,journalArticle,,"Liang, Paul Pu; Goindani, Akshay; Chafekar, Talha; Mathur, Leena; Yu, Haofei; Salakhutdinov, Ruslan; Morency, Louis-Philippe",HEMM: Holistic Evaluation of Multimodal Foundation Models,,,,"Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of realworld applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today’s models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.",,,NeurIPS.csv,,,,,,
4ZIEEPHH,journalArticle,,"Etxaniz, Julen; Azkune, Gorka; Soroa, Aitor; de Lacalle, Oier Lopez; Artetxe, Mikel",BERTAQA: How Much Do Language Models Know About Local Culture?,,,,"Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BERTAQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models’ performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a lowresource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.",,,NeurIPS.csv,,,,,,
QGE4U3RU,journalArticle,,"Sankar, Ashwin; Anand, Srija; Varadhan, Praveen Srinivasa; Thomas, Sherry; Singal, Mehak; Kumar, Shridhar; Mehendale, Deovrat; Krishana, Aditi; Raju, Giri",INDICVOICES-R: Unlocking a Massive Multilingual Multi-speaker Speech Corpus for Scaling Indian TTS,,,,"Recent advancements in text-to-speech (TTS) synthesis show that large-scale models trained with extensive web data produce highly natural-sounding output. However, such data is scarce for Indian languages due to the lack of high-quality, manually subtitled data on platforms like LibriVox or YouTube. To address this gap, we enhance existing large-scale ASR datasets containing natural conversations collected in low-quality environments to generate high-quality TTS training data. Our pipeline leverages the cross-lingual generalization of denoising and speech enhancement models trained on English and applied to Indian languages. This results in IndicVoices-R (IV-R), the largest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704 hours of high-quality speech from 10,496 speakers across 22 Indian languages. IV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS, and IndicTTS. We also introduce the IV-R Benchmark, the first to assess zero-shot, few-shot, and many-shot speaker generalization capabilities of TTS models on Indian voices, ensuring diversity in age, gender, and style. We demonstrate that fine-tuning an English pre-trained model on a combined dataset of high-quality IndicTTS and our IV-R dataset results in better zero-shot speaker generalization compared to fine-tuning on the IndicTTS dataset alone. Further, our evaluation reveals limited zero-shot generalization for Indian voices in TTS models trained on prior datasets, which we improve by fine-tuning the model on our data containing diverse set of speakers across language families. We open-source code and data3 for all 22 official Indian languages.",,,NeurIPS.csv,,,,,,
WNUAGU4M,journalArticle,,"Cruz, André F; Hardt, Moritz; Mendler-Dünner, Celestine",Evaluating language models as risk scores,,,,"Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks. Conditioned on a question and answer-key, does the most likely token match the ground truth? Such benchmarks necessarily fail to evaluate LLMs’ ability to quantify ground-truth outcome uncertainty. In this work, we focus on the use of LLMs as risk scores for unrealizable prediction tasks. We introduce folktexts, a software package to systematically generate risk scores using LLMs, and evaluate them against US Census data products. A flexible API enables the use of different prompting schemes, local or web-hosted models, and diverse census columns that can be used to compose custom prediction tasks. We evaluate 17 recent LLMs across five proposed benchmark tasks. We find that zero-shot risk scores produced by multiple-choice question-answering have high predictive signal but are wildly miscalibrated. Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores. In fact, instruction-tuning polarizes answer distribution regardless of true underlying data uncertainty. This reveals a general inability of instruction-tuned models to express data uncertainty using multiple-choice answers. A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models. These differences in ability to quantify data uncertainty cannot be revealed in realizable settings, and highlight a blind-spot in the current evaluation ecosystem that folktexts covers.",,,NeurIPS.csv,,,,,,
B8LV9JX8,journalArticle,,"Li, Linyi; Geng, Shijie; Li, Zhenwen; He, Yibo; Yu, Hao; Hua, Ziyue; Ning, Guanghan; Wang, Siwei; Xie, Tao; Yang, Hongxia",InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models,,,,"Large Language Models for code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose InfiBench, the first large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. InfiBench uses four types of model-free automatic metrics to evaluate response correctness where domain experts carefully concretize the criterion for each question. We conduct a systematic evaluation for over 100 latest code LLMs on InfiBench, leading to a series of novel and insightful findings. Our detailed analyses showcase potential directions for further advancement of code LLMs. InfiBench is fully open source at https://infi-coder.github.io/infibench and continuously expanding to foster more scientific and systematic practices for code LLM evaluation.",,,NeurIPS.csv,,,,,,
PQCSFPCA,journalArticle,,"Yang, Jason; Mora, Ariane; Liu, Shengchao; Wittmann, Bruce J",CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes,,,,"Enzymes are important proteins that catalyze chemical reactions. In recent years, machine learning methods have emerged to predict enzyme function from sequence; however, there are no standardized benchmarks to evaluate these methods. We introduce CARE, a benchmark and dataset suite for the Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1) classification of a protein sequence by its enzyme commission (EC) number and (2) retrieval of an EC number given a chemical reaction. For each task, we design train-test splits to evaluate different kinds of out-of-distribution generalization that are relevant to real use cases. For the classification task, we provide baselines for state-of-the-art methods. Because the retrieval task has not been previously formalized, we propose a method called Contrastive Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task and compare it to the recent method, CLIPZyme. CARE is available at https://github.com/jsunn-y/CARE/.",,,NeurIPS.csv,,,,,,
6QZ9P362,journalArticle,,"Zhang, Lemei; Liu, Peng; Henriksboe, Marcus Tiedemann Oekland; Lauvrak, Even W; Gulla, Jon Atle; Ramampiaro, Heri",PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models,,,,"With the rapid advancement of Natural Language Processing in recent years, numerous studies have shown that generic summaries generated by Large Language Models (LLMs) can sometimes surpass those annotated by experts, such as journalists, according to human evaluations. However, there is limited research on whether these generic summaries meet the individual needs of ordinary people. The biggest obstacle is the lack of human-annotated datasets from the general public. Existing work on personalized summarization often relies on pseudo datasets created from generic summarization datasets or controllable tasks that focus on specific named entities or other aspects, such as the length and specificity of generated summaries, collected from hypothetical tasks without the annotators’ initiative. To bridge this gap, we propose a high-quality, personalized, manually annotated abstractive summarization dataset called PersonalSum. This dataset is the first to investigate whether the focus of public readers differs from the generic summaries generated by LLMs. It includes user profiles, personalized summaries accompanied by source sentences from given articles, and machine-generated generic summaries along with their sources. We investigate several personal signals — entities/topics, plot, and structure of articles—that may affect the generation of personalized summaries using LLMs in a few-shot in-context learning scenario. Our preliminary results and analysis indicate that entities/topics are merely one of the key factors that impact the diverse preferences of users, and personalized summarization remains a significant challenge for existing LLMs. Our dataset and code are available at https://github.com/SmartmediaAI/PersonalSum.",,,NeurIPS.csv,,,,,,
LN39V8DT,journalArticle,,"Ortiz, Joseph; Dedieu, Antoine; Lehrach, Wolfgang; Guntupalli, J Swaroop; Wendelken, Carter; Humayun, Ahmad; Zhou, Guangyao; Swaminathan, Sivaramakrishnan; Lázaro-Gredilla, Miguel; Murphy, Kevin",DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors,,,,"Learning from previously collected data via behavioral cloning or offline reinforcement learning (RL) is a powerful recipe for scaling generalist agents by avoiding the need for expensive online learning. Despite strong generalization in some respects, agents are often remarkably brittle to minor visual variations in control-irrelevant factors such as the background or camera viewpoint. In this paper, we present the DeepMind Control Vision Benchmark (DMC-VB), a dataset collected in the DeepMind Control Suite to evaluate the robustness of offline RL agents for solving continuous control tasks from visual input in the presence of visual distractors. In contrast to prior works, our dataset (a) combines locomotion and navigation tasks of varying difficulties, (b) includes static and dynamic visual variations, (c) considers data generated by policies with different skill levels, (d) systematically returns pairs of state and pixel observation, (e) is an order of magnitude larger, and (f) includes tasks with hidden goals. Accompanying our dataset, we propose three benchmarks to evaluate representation learning methods for pretraining, and carry out experiments on several recently proposed methods. First, we find that pretrained representations do not help policy learning on DMC-VB, and we highlight a large representation gap between policies learned on pixel observations and on states. Second, we demonstrate when expert data is limited, policy learning can benefit from representations pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals. Our dataset and benchmark code to train and evaluate agents are available at https://github.com/google-deepmind/dmc_vision_benchmark.",,,NeurIPS.csv,,,,,,
9VKM3VQ4,journalArticle,,"Chao, Patrick; Debenedetti, Edoardo; Robey, Alexander; Andriushchenko, Maksym; Croce, Francesco; Sehwag, Vikash; Dobriban, Edgar; Flammarion, Nicolas; Pappas, George J; Tramer, Florian; Hassani, Hamed; Wong, Eric",JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,,,,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors—both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024)—which align with OpenAI’s usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.",,,NeurIPS.csv,,,,,,
7XMJMG4W,journalArticle,,"Zeng, Xianzhi; Jiang, Wenchao; Zhang, Shuhao",LibAMM: Empirical Insights into Approximate Computing for Accelerating Matrix Multiplication,,,,"Matrix multiplication (MM) is pivotal in fields from deep learning to scientific computing, driving the quest for improved computational efficiency. Accelerating MM encompasses strategies like complexity reduction, parallel and distributed computing, hardware acceleration, and approximate computing techniques, namely AMM algorithms. Amidst growing concerns over the resource demands of large language models (LLMs), AMM has garnered renewed focus. However, understanding the nuances that govern AMM’s effectiveness remains incomplete. This study delves into AMM by examining algorithmic strategies, operational specifics, dataset characteristics, and their application in real-world tasks. Through comprehensive testing across diverse datasets and scenarios, we analyze how these factors affect AMM’s performance, uncovering that the selection of AMM approaches significantly influences the balance between efficiency and accuracy, with factors like memory access playing a pivotal role. Additionally, dataset attributes are shown to be vital for the success of AMM in applications. Our results advocate for tailored algorithmic approaches and careful strategy selection to enhance AMM’s effectiveness. To aid in the practical application and ongoing research of AMM, we introduce LibAMM —a toolkit offering a wide range of AMM algorithms, benchmarks, and tools for experiment management. LibAMM aims to facilitate research and application in AMM, guiding future developments towards more adaptive and context-aware computational solutions.",,,NeurIPS.csv,,,,,,
ZN36BBBY,journalArticle,,"Zhu, Derui; Chen, Dingfan; Wu, Xiongfei; Geng, Jiahui; Li, Zhuo; Grossklags, Jens; Ma, Lei",PrivAuditor: Benchmarking Privacy Vulnerabilities in LLM Adaptation Techniques,,,,"Large Language Models (LLMs) are recognized for their potential to be an important building block toward achieving artificial general intelligence due to their unprecedented capability for solving diverse tasks. Despite these achievements, LLMs often underperform in domain-specific tasks without training on relevant domain data. This phenomenon, which is often attributed to distribution shifts, makes adapting pre-trained LLMs with domain-specific data crucial. However, this adaptation raises significant privacy concerns, especially when the data involved come from sensitive domains. In this work, we extensively investigate the privacy vulnerabilities of adapted (fine-tuned) LLMs and benchmark privacy leakage across a wide range of data modalities, state-of-the-art privacy attack methods, adaptation techniques, and model architectures. We systematically evaluate and pinpoint critical factors related to privacy leakage. With our organized codebase and actionable insights, we aim to provide a standardized auditing tool for practitioners seeking to deploy customized LLM applications with faithful privacy assessments.",,,NeurIPS.csv,,,,,,
DR4R8NEG,journalArticle,,"Hao, Zhongkai; Yao, Jiachen; Su, Chang; Su, Hang; Wang, Ziao; Lu, Fanzhi; Xia, Zeyu; Zhang, Yichi; Liu, Songming; Lu, Lu; Zhu, Jun",PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs,,,,"While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains, including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research such as domain decomposition methods and loss reweighting for handling multi-scale problems. To the best of our knowledge, it is the largest benchmark with a diverse and comprehensive evaluation that will undoubtedly foster further research in PINNs.",,,NeurIPS.csv,,,,,,
5JWVW8M2,journalArticle,,"Kannen, Nithish; Ahmad, Arif; Andreetto, Marco; Prabhakaran, Vinodkumar; Prabhu, Utsav; Dieng, Adji Bousso; Bhattacharyya, Pushpak; Dave, Shachi",Beyond Aesthetics: Cultural Competence in Text-to-Image Models,,,,"Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a ﬁrst-of-its-kind benchmark to evaluate cultural competence of T2I models.2 CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of high-quality prompts that enable the evaluation of cultural awareness, and 2) CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to evaluate cultural diversity. We also introduce cultural diversity as a novel T2I evaluation component, leveraging quality-weighted Vendi score. Our evaluations reveal signiﬁcant gaps in the cultural awareness of existing models across countries and provide valuable insights into the cultural diversity of T2I outputs for under-speciﬁed prompts. Our methodology is extendable to other cultural regions and concepts, and can facilitate the development of T2I models that better cater to the global population.",,,NeurIPS.csv,,,,,,
ZKCUPFM7,journalArticle,,"Tsoukalas, George; Lee, Jasper; Jennings, John; Xin, Jimmy; Ding, Michelle; Jennings, Michael; Thakur, Amitayush; Chaudhuri, Swarat",PUTNAMBENCH: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition,,,,"We present PUTNAMBENCH, a new multi-language benchmark for evaluating the ability of neural theorem-provers to solve competition mathematics problems. PUTNAMBENCH consists of 1692 hand-constructed formalizations of 640 theorems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problems have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq formalizations. PUTNAMBENCH requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We use PUTNAMBENCH to evaluate several established neural and symbolic theorem-provers. These approaches can only solve a handful of the PUTNAMBENCH problems, establishing the benchmark as a difficult open challenge for research on neural theorem-proving. PUTNAMBENCH is available at https://github.com/trishullab/PutnamBench.",,,NeurIPS.csv,,,,,,
IA384QXX,journalArticle,,"Li, Bowen; Li, Zhaoyu; Du, Qiwei; Luo, Jinqi; Wang, Wenshan; Xie, Yaqi; Stepputtis, Simon; Wang, Chen; Sycara, Katia; Ravikumar, Pradeep; Gray, Alexander; Si, Xujie; Scherer, Sebastian",LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation,,,,,,,NeurIPS.csv,,,,,,
H5MNC8JL,journalArticle,,"Ye, Hui; Sunderraman, Rajshekhar; Ji, Shihao",UAV3D: A Large-scale 3D Perception Benchmark for Unmanned Aerial Vehicles,,,,"Unmanned Aerial Vehicles (UAVs), equipped with cameras, are employed in numerous applications, including aerial photography, surveillance, and agriculture. In these applications, robust object detection and tracking are essential for the effective deployment of UAVs. However, existing benchmarks for UAV applications are mainly designed for traditional 2D perception tasks, restricting the development of real-world applications that require a 3D understanding of the environment. Furthermore, despite recent advancements in single-UAV perception, limited views of a single UAV platform significantly constrain its perception capabilities over long distances or in occluded areas. To address these challenges, we introduce UAV3D – a benchmark designed to advance research in both 3D and collaborative 3D perception tasks with UAVs. UAV3D comprises 1,000 scenes, each of which has 20 frames with fully annotated 3D bounding boxes on vehicles. We provide the benchmark for four 3D perception tasks: single-UAV 3D object detection, single-UAV object tracking, collaborative-UAV 3D object detection, and collaborative-UAV object tracking. Our dataset and code are available at https://huiyegit.github.io/UAV3D_Benchmark/.",,,NeurIPS.csv,,,,,,
SEG8CTSR,journalArticle,,"Wu, Xindi; Yu, Dingli; Huang, Yangsibo; Russakovsky, Olga; Arora, Sanjeev",ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty,,,,"Compositionality is a critical capability in Text-to-Image (T2I) models, as it reflects their ability to understand and combine multiple concepts from text descriptions. Existing evaluations of compositional capability rely heavily on human-designed text prompts or fixed templates, limiting their diversity and complexity, and yielding low discriminative power. We propose CONCEPTMIX, a scalable, controllable, and customizable benchmark which automatically evaluates compositional generation ability of T2I models. This is done in two stages. First, CONCEPTMIX generates the text prompts: concretely, using categories of visual concepts (e.g., objects, colors, shapes, spatial relationships), it randomly samples an object and k-tuples of visual concepts, then uses GPT-4o to generate text prompts for image generation based on these sampled concepts. Second, CONCEPTMIX evaluates the images generated in response to these prompts: concretely, it checks how many of the k concepts actually appeared in the image by generating one question per visual concept and using a strong VLM to answer them. Through administering CONCEPTMIX to a diverse set of T2I models (proprietary as well as open ones) using increasing values of k, we show that our CONCEPTMIX has higher discrimination power than earlier benchmarks. Specifically, CONCEPTMIX reveals that the performance of several models, especially open models, drops dramatically with increased k. Importantly, it also provides insight into the lack of prompt diversity in widely-used training datasets. Additionally, we conduct extensive human studies to validate the design of CONCEPTMIX and compare our automatic grading with human judgement. We hope it will guide future T2I model development.",,,NeurIPS.csv,,,,,,
9736K86W,journalArticle,,"Simonetto, Thibault; Ghamizi, Salah; Cordy, Maxime",TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases,,,,"While adversarial robustness in computer vision is a mature research field, fewer researchers have tackled the evasion attacks against tabular deep learning, and even fewer investigated robustification mechanisms and reliable defenses. We hypothesize that this lag in the research on tabular adversarial attacks is in part due to the lack of standardized benchmarks. To fill this gap, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness with CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model. In addition to our open benchmark https://github.com/serval-uni-lu/tabularbench where we welcome submissions of new models and defenses, we implement 7 robustification mechanisms inspired by state-of-the-art defenses in computer vision and propose the largest benchmark of robust tabular deep-learning over 200 models across five critical scenarios in finance, healthcare, and security. We curated real datasets for each use case, augmented with hundreds of thousands of realistic synthetic inputs, and trained and assessed our models with and without data augmentations. We open-source our library that provides API access to all our pre-trained robust tabular models, and the largest datasets of real and synthetic tabular inputs. Finally, we analyze the impact of various defenses on the robustness and provide actionable insights to design new defenses and robustification mechanisms.",,,NeurIPS.csv,,,,,,
9JEDAE87,journalArticle,,"Luo, Yuankai; Shi, Lei; Wu, Xiao-Ming",Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification,,,,"Graph Transformers (GTs) have recently emerged as popular alternatives to traditional message-passing Graph Neural Networks (GNNs), due to their theoretically superior expressiveness and impressive performance reported on standard node classification benchmarks, often significantly outperforming GNNs. In this paper, we conduct a thorough empirical analysis to reevaluate the performance of three classic GNN models (GCN, GAT, and GraphSAGE) against GTs. Our findings suggest that the previously reported superiority of GTs may have been overstated due to suboptimal hyperparameter configurations in GNNs. Remarkably, with slight hyperparameter tuning, these classic GNN models achieve state-of-the-art performance, matching or even exceeding that of recent GTs across 17 out of the 18 diverse datasets examined. Additionally, we conduct detailed ablation studies to investigate the influence of various GNN configurations—such as normalization, dropout, residual connections, and network depth—on node classification performance. Our study aims to promote a higher standard of empirical rigor in the field of graph machine learning, encouraging more accurate comparisons and evaluations of model capabilities. Our implementation is available at https://github.com/LUOyk1999/tunedGNN.",,,NeurIPS.csv,,,,,,
FFJ459YN,dataset,2024.0,"European Vegetation Survey, The IAVS Working Group",EVA project # 196 2023-11-22 MAMBO: Modern approaches to the monitoring of biodiversity - L. Picek: SELECTION 2024-01-04,,10.58060/QE37-TK48,https://euroveg.org/eva-database/project-selection/29,"The difficulty of monitoring biodiversity at fine scales and over large areas limits ecological knowledge and conservation efforts. To fill this gap, Species Distribution Models (SDMs) predict species across space from spatially explicit features. Yet, they face the challenge of integrating the rich but heterogeneous data made available over the past decade, notably millions of opportunistic species observations and standardized surveys, as well as multimodal remote sensing data. In light of that, we have designed and developed a new European-scale dataset for SDMs at high spatial resolution (10–50m), including more than 10k species (i.e., most of the European flora). The dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive Presence-Absence survey records, all accompanied by diverse environmental rasters (e.g., elevation, human footprint, and soil) traditionally used in SDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10 m resolution, a 20-year time series of climatic variables, and satellite time series from the Landsat program. In addition to the data, we provide an openly accessible SDM benchmark (hosted on Kaggle), which has already attracted an active community and a set of strong baselines for single predictor/modality and multimodal approaches. All resources, e.g., the dataset, pre-trained models, and baseline methods (in the form of notebooks), are available on Kaggle, allowing one to start with our dataset literally with two mouse clicks.",2024,,NeurIPS.csv,,,,,,
5YT9AK3P,journalArticle,,"Foteinopoulou, Niki Maria; Ghorbel, Enjie; Aouada, Djamila",A Hitchhiker’s Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning,,,,,,,NeurIPS.csv,,,,,,
L9J493XD,journalArticle,,"Jia, Xiaosong; Yang, Zhenjie; Li, Qifeng; Zhang, Zhiyuan; Yan, Junchi",Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving,,,,"In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the datadriven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible. To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems’ multiple abilities in a closed-loop manner. Bench2Drive’s official training data consists of 2 million fully annotated frames, collected from 13638 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.",,,NeurIPS.csv,,,,,,
E9SCT3A4,journalArticle,,"Yuan, Shenghai; Huang, Jinfa; Xu, Yongqi; Liu, Yaoyang; Zhang, Shaofeng; Shi, Yujun; Zhu, Ruijie; Cheng, Xinhua; Luo, Jiebo; Yuan, Li",ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation,,,,,,,NeurIPS.csv,,,,,,
FY7SGQCC,journalArticle,,"Liu, Meihan; Zhang, Zhen; Tang, Jiachen; Bu, Jiajun; He, Bingsheng; Zhou, Sheng","Revisiting, Benchmarking and Understanding Unsupervised Graph Domain Adaptation",,,,"Unsupervised Graph Domain Adaptation (UGDA) involves the transfer of knowledge from a label-rich source graph to an unlabeled target graph under domain discrepancies. Despite the proliferation of methods designed for this emerging task, the lack of standard experimental settings and fair performance comparisons makes it challenging to understand which and when models perform well across different scenarios. To fill this gap, we present the first comprehensive benchmark for unsupervised graph domain adaptation named GDABench, which encompasses 16 algorithms across diverse adaptation tasks. Through extensive experiments, we observe that the performance of current UGDA models varies significantly across different datasets and adaptation scenarios. Specifically, we recognize that when the source and target graphs face significant distribution shifts, it is imperative to formulate strategies to effectively address and mitigate graph structural shifts. We also find that with appropriate neighbourhood aggregation mechanisms, simple GNN variants can even surpass state-of-the-art UGDA baselines. To facilitate reproducibility, we have developed an easy-to-use library PyGDA for training and evaluating existing UGDA methods, providing a standardized platform in this community. Our source codes and datasets can be found at https://github.com/pygda-team/pygda.",,,NeurIPS.csv,,,,,,
MYWREGM2,journalArticle,,"Wang, Zhilin; Dong, Yi; Delalleau, Olivier; Zeng, Jiaqi; Shen, Gerald; Egert, Daniel; Zhang, Jimmy J; Sreedhar, Makesh Narsimhan; Kuchaiev, Oleksii",HelpSteer2: Open-source dataset for training top-performing reward models,,,,"High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences. As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HHRLHF, and HelpSteer need to be updated to remain effective for reward modeling. Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers. To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0). Using a powerful Nemotron4-340B base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench’s primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024. Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. Additionally, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multiattribute score predicted by our reward models. HelpSteer2 is available at https: //huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner.",,,NeurIPS.csv,,,,,,
SFERVXAD,journalArticle,,"Fan, Yaran; Pool, Jamie",Topic-Conversation Relevance (TCR) Dataset and Benchmarks,,,,"Workplace meetings are vital to organizational collaboration, yet a large percentage of meetings are rated as ineffective [1]. To help improve meeting effectiveness by understanding if the conversation is on topic, we create a comprehensive TopicConversation Relevance (TCR) dataset that covers a variety of domains and meeting styles. The TCR dataset includes 1,500 unique meetings, 22 million words in transcripts, and over 15,000 meeting topics, sourced from both newly collected Speech Interruption Meeting (SIM) data and existing public datasets. Along with the text data, we also open source scripts to generate synthetic meetings or create augmented meetings from the TCR dataset to enhance data diversity. For each data source, benchmarks are created using GPT-41 to evaluate the model accuracy in understanding transcription-topic relevance.",,,NeurIPS.csv,,,,,,
P8EKR3A9,journalArticle,,"Roberts, Jonathan; Han, Kai; Houlsby, Neil",SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation,,,,"Large multimodal models (LMMs) have proven flexible and generalisable across many tasks and fields. Although they have strong potential to aid scientific research, their capabilities in this domain are not well characterised. A key aspect of scientific research is the ability to understand and interpret figures, which serve as a rich, compressed source of complex information. In this work, we present SciFIBench, a scientific figure interpretation benchmark consisting of 2000 questions split between two tasks across 8 categories. The questions are curated from arXiv paper figures and captions, using adversarial filtering to find hard negatives and human verification for quality control. We evaluate 28 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we investigate the alignment and reasoning faithfulness of the LMMs on augmented question sets from our benchmark. We release SciFIBench to encourage progress in this domain: https://SciFIBench.github.io/.",,,NeurIPS.csv,,,,,,
J85G5U5S,journalArticle,,"Han, Seungju; Rao, Kavel; Ettinger, Allyson; Jiang, Liwei; Lin, Bill Yuchen; Lambert, Nathan; Choi, Yejin; Dziri, Nouha","WILDGUARD: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs",,,,"We introduce WILDGUARD—an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WILDGUARD serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 [16] score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models’ refusals, a key measure for evaluating safety behaviors in model responses.",,,NeurIPS.csv,,,,,,
VPRPVJTF,journalArticle,,"Zhang, Tianyi; Cai, Linrong; Li, Jeffrey; Roberts, Nicholas; Guha, Neel; Sala, Frederic",Stronger Than You Think: Benchmarking Weak Supervision on Realistic Tasks,,,,"Weak supervision (WS) is a popular approach for label-efficient learning, leveraging diverse sources of noisy but inexpensive weak labels to automatically annotate training data. Despite its wide usage, WS and its practical value are challenging to benchmark due to the many knobs in its setup, including: data sources, labeling functions (LFs), aggregation techniques (called label models), and end model pipelines. Existing evaluation suites tend to be limited, focusing on particular components or specialized use cases. Moreover, they often involve simplistic benchmark tasks or de-facto LF sets that are suboptimally written, producing insights that may not generalize to real-world settings. We address these limitations by introducing a new benchmark, BOXWRENCH,2 designed to more accurately reflect real-world usages of WS. This benchmark features tasks with (1) higher class cardinality and imbalance, (2) notable domain expertise requirements, and (3) opportunities to re-use LFs across parallel multilingual corpora. For all tasks, LFs are written using a careful procedure aimed at mimicking real-world settings. In contrast to existing WS benchmarks, we show that supervised learning requires substantial amounts (1000+) of labeled examples to match WS in many settings.",,,NeurIPS.csv,,,,,,
ZSHZ9XCM,journalArticle,,"Yang, Jehan; Soh, Maxwell; Lieu, Vivianna",EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography,,,,"This paper introduces the first generalization and adaptation benchmark using machine learning for evaluating out-of-distribution performance of electromyography (EMG) classification algorithms. The ability of an EMG classifier to handle inputs drawn from a different distribution than the training distribution is critical for real-world deployment as a control interface. By predicting the user’s intended gesture using EMG signals, we can create a wearable solution to control assistive technologies, such as computers, prosthetics, and mobile manipulator robots. This new out-of-distribution benchmark consists of two major tasks that have utility for building robust and adaptable control interfaces: 1) intersubject classification, and 2) adaptation using train-test splits for time-series. This benchmark spans nine datasets, the largest collection of EMG datasets in a benchmark. Among these, a new dataset is introduced, featuring a novel, easy-to-wear high-density EMG wearable for data collection. The lack of open-source benchmarks has made comparing accuracy results between papers challenging for the EMG research community. This new benchmark provides researchers with a valuable resource for analyzing practical measures of out-of-distribution performance for EMG datasets. Our code and data from our new dataset can be found at emgbench.github.io.",,,NeurIPS.csv,,,,,,
3VLU8HDS,journalArticle,,"Farebrother, Jesse; Castro, Pablo Samuel",CALE: Continuous Arcade Learning Environment,,,,"We introduce the Continuous Arcade Learning Environment (CALE), an extension of the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013]. The CALE uses the same underlying emulator of the Atari 2600 gaming system (Stella), but adds support for continuous actions. This enables the benchmarking and evaluation of continuous-control agents (such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]) and value-based agents (such as DQN [Mnih et al., 2015] and Rainbow [Hessel et al., 2018]) on the same environment suite. We provide a series of open questions and research directions that CALE enables, as well as initial baseline results using Soft Actor-Critic. CALE is available as part of the ALE at https://github.com/Farama-Foundation/ Arcade-Learning-Environment.",,,NeurIPS.csv,,,,,,
GME3JDID,journalArticle,,"Yuan, Shuai; Lin, Guancong; Zhang, Lixian; Dong, Runmin; Zhang, Jinxiao; Chen, Shuang; Zheng, Juepeng; Wang, Jie; Fu, Haohuan",FUSU: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding,,,,"Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions in urban areas. Although there have been advances in high-quality land cover datasets that reveal the physical features of urban landscapes, the lack of fine-grained land use datasets hinders a deeper understanding of how human activities are distributed across the landscape and the impact of these activities on the environment, thus constraining proper technique development. To address this, we introduce FUSU, the first finegrained land use change segmentation dataset for Fine-grained Urban Semantic Understanding. FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations. It includes bi-temporal high-resolution satellite images with 0.2-0.5 m ground sample distance and monthly optical and radar satellite time series, covering 847 km2 across five urban areas in the southern and northern of China with different geographical features. The fine-grained land use pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for developing proper deep learning models to provide contextual insights on human activities and urbanization. To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation. We benchmark FUSU on various methods for several tasks. Dataset and code are available at: https://github.com/yuanshuai0914/FUSU.",,,NeurIPS.csv,,,,,,
PJACP4GY,journalArticle,,"Ye, Rui; Ge, Rui; Zhu, Xinyu; Chai, Jingyi; Du, Yaxin; Liu, Yang; Wang, Yanfeng; Chen, Siheng",FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models,,,,"Federated learning could enable multiple parties to collaboratively fine-tune large language models without directly sharing their data (FedLLM). Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy. However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works often rely on artificially constructed datasets, failing to capture properties in real-world scenarios. Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community. FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747. Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios. Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration). We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons. Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.",,,NeurIPS.csv,,,,,,
F4LJ337W,journalArticle,,"Zhang, Yu; Pan, Changhao; Guo, Wenxiang; Li, Ruiqi; Zhu, Zhiyuan; Wang, Jialei; Xu, Wenhao; Lu, Jingyu; Hong, Zhiqing; Wang, Chuxin; Zhang, LiChao; He, Jinzheng; Jiang, Ziyue; Chen, Yuxin; Yang, Chen; Cheng, Jiecheng Zhou Xinyu; Zhao, Zhou",GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks,,,,"The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large Global, multi-Technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at http://gtsinger.github.io. We provide the dataset and the code for processing data and conducting benchmarks at https://huggingface.co/datasets/GTSinger/GTSinger and https://github.com/GTSinger/GTSinger.",,,NeurIPS.csv,,,,,,
38RLDVBV,journalArticle,,"Jung, HyunJun; Li, Weihang; Wu, Shun-Cheng; Bittner, William; Brasch, Nikolas; Song, Jifei; Pérez-Pellitero, Eduardo; Zhang, Zhensong; Moreau, Arthur; Navab, Nassir; Busam, Benjamin","SCRREAM : SCan, Register, REnder And Map: A Framework for Annotating Accurate and Dense 3D Indoor Scenes with a Benchmark",,,,,,,NeurIPS.csv,,,,,,
MDCP546P,journalArticle,,"Toshniwal, Shubham; Moshkov, Ivan; Narenthiran, Sean; Gitman, Daria; Jia, Fei; Gitman, Igor",OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset,,,,,,,NeurIPS.csv,,,,,,
I5YY9TUS,journalArticle,,"Penedo, Guilherme; Kydlícˇek, Hynek; Mitchell, Margaret; Raffel, Colin",The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale,,,,"The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-ofthe-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including indepth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.",,,NeurIPS.csv,,,,,,
UBAISJW3,journalArticle,,"Guo, Chengquan; Liu, Xun; Xie, Chulin; Zhou, Andy; Zeng, Yi; Lin, Zinan; Song, Dawn; Li, Bo",RedCode: Risky Code Execution and Generation Benchmark for Code Agents,,,,"With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, an evaluation platform with benchmarks grounded in four key principles: real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and highquality safety scenarios and tests. RedCode consists of two parts to evaluate agents’ safety in unsafe code execution and generation: (1) RedCode-Exec provides challenging code prompts in Python as inputs, aiming to evaluate code agents’ ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances. We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems. We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents’ vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing unsafe operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Unsafe operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen reveal that more capable base models and agents with stronger overall coding abilities, such as GPT4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are publicly available at https://github.com/AI-secure/RedCode.",,,NeurIPS.csv,,,,,,
9QT4PT3R,journalArticle,,"Wang, Haohui; Guan, Weijie; Chen, Jianpeng; Wang, Zi; Zhou, Dawei","Towards Heterogeneous Long-tailed Learning: Benchmarking, Metrics, and Toolbox",,,,"Long-tailed data distributions pose challenges for a variety of domains like ecommerce, finance, biomedical science, and cyber security, where the performance of machine learning models is often dominated by head categories while tail categories are inadequately learned. This work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks. We develop HEROLT, a comprehensive longtailed learning benchmark integrating 18 state-of-the-art algorithms, 10 evaluation metrics, and 17 real-world datasets across 6 tasks and 4 data modalities. HEROLT with novel angles and extensive experiments (315 in total) enables effective and fair evaluation of newly proposed methods compared with existing baselines on varying dataset types. Finally, we conclude by highlighting the significant applications of long-tailed learning and identifying several promising future directions. For accessibility and reproducibility, we open-source our benchmark HEROLT and corresponding results at https://github.com/SSSKJ/HeroLT.",,,NeurIPS.csv,,,,,,
FQSW9S2F,journalArticle,,"Shao, Minghao; Jancheska, Sofija; Udeshi, Meet; Dolan-Gavitt, Brendan; Xi, Haoran; Milner, Kimberly; Chen, Boyuan; Yin, Max; Garg, Siddharth; Krishnamurthy, Prashanth; Khorrami, Farshad; Karri, Ramesh; Shafique, Muhammad",NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security,,,,"Large Language Models (LLMs) are being deployed across various domains today. However, their capacity to solve Capture the Flag (CTF) challenges in cybersecurity has not been thoroughly evaluated. To address this, we develop a novel method to assess LLMs in solving CTF challenges by creating a scalable, open-source benchmark database specifically designed for these applications. This database includes metadata for LLM testing and adaptive learning, compiling a diverse range of CTF challenges from popular competitions. Utilizing the advanced function calling capabilities of LLMs, we build a fully automated system with an enhanced workflow and support for external tool calls. Our benchmark dataset and automated framework allow us to evaluate the performance of five LLMs, encompassing both black-box and open-source models. This work lays the foundation for future research into improving the efficiency of LLMs in interactive cybersecurity tasks and automated task planning. By providing a specialized benchmark, our project offers an ideal platform for developing, testing, and refining LLM-based approaches to vulnerability detection and resolution. Evaluating LLMs on these challenges and comparing with human performance yields insights into their potential for AI-driven cybersecurity solutions to perform real-world threat management. We make our benchmark dataset open source to public https://github.com/NYU-LLM-CTF/NYU_CTF_Bench along with our playground automated framework https://github.com/NYU-LLM-CTF/llm_ ctf_automation.",,,NeurIPS.csv,,,,,,
KSNZPTH2,journalArticle,,"Zhao, Xinyu; Sun, Guoheng; Cai, Ruisi; Zhou, Yukun; Li, Pingzhi; Wang, Peihao; Tan, Bowen; He, Yexiao; Chen, Li; Liang, Yi; Chen, Beidi; Yuan, Binhang; Wang, Hongyi; Li, Ang; Wang, Zhangyang; Chen, Tianlong; Ch, Unc; Austin, UT",Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild,,,,"As Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has gained significant attention, which is challenged by potential performance drop when combining disparate models. Various techniques have been proposed to aggregate pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed. In light of this research gap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. Utilizing the insights from the benchmark results, we formulate a strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization. Our methodology involves clustering mergeable models, selecting a merging strategy, and integrating model clusters through model-level mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, Model-GLUE shows an average performance enhancement of 5.61%, achieved without additional training. Codes are available at https://github.com/Model-GLUE/Model-GLUE.",,,NeurIPS.csv,,,,,,
574MP3SL,journalArticle,,"Chandrasegaran, Keshigeyan; Gupta, Agrim; Hadzic, Lea M; Kota, Taran; He, Jimming; Eyzaguirre, Cristobal; Durante, Zane; Li, Manling; Fei-Fei, Jiajun Wu Li",HourVideo: 1-Hour Video-Language Understanding,,,,"We present HourVideo, a benchmark dataset for hour-long video-language understanding. Our dataset consists of a novel task suite comprising summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. HourVideo includes 500 manually curated egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features 12,976 high-quality, five-way multiple-choice questions. Benchmarking results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve marginal improvements over random chance. In stark contrast, human experts significantly outperform the state-of-the-art long-context multimodal model, Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are available at hourvideo.stanford.edu.",,,NeurIPS.csv,,,,,,
GG6CQS9R,journalArticle,,"Johansson, Fredrik D",IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark,,,,"Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.",,,NeurIPS.csv,,,,,,
TK6984J3,journalArticle,,"Stergiou, Alexandros",LAVIB: A Large-scale Video Interpolation Benchmark,,,,,,,NeurIPS.csv,,,,,,
EF7JIQEG,journalArticle,,"Li, Pengxiang; Gao, Zhi; Zhang, Bofei; Yuan, Tao; Wu, Yuwei; Harandi, Mehrtash; Jia, Yunde; Zhu, Song-Chun; Li, Qing",FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models,,,,"Vision language models (VLMs) have achieved impressive progress in diverse applications, becoming a prevalent research direction. In this paper, we build FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn conversations that are derived from 27 source datasets, empowering VLMs to spontaneously refine their responses based on user feedback across diverse tasks. To scale up the data collection, FIRE is collected in two components: FIRE-100K and FIRE1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is freely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a benchmark to comprehensively evaluate the feedback-refining capability of VLMs, which contains 11K feedback-refinement conversations as the test data, two evaluation settings, and a model to provide feedback for VLMs. We develop the FIRELLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows remarkable feedback-refining capability on FIRE-Bench and outperforms untrained VLMs by 50%, making more efficient user-agent interactions and underscoring the significance of the FIRE dataset.",,,NeurIPS.csv,,,,,,
AG23IHCK,journalArticle,,"Zhang, Kaiyan; Zeng, Sihang; Hua, Ermo; Ding, Ning; Chen, Zhang-Ren; Ma, Zhiyuan; Li, Haoxin; Cui, Ganqu; Qi, Biqing; Zhu, Xuekai; Lv, Xingtai; Hu, Jin-Fang; Liu, Zhiyuan; Zhou, Bowen",UltraMedical: Building Specialized Generalists in Biomedicine,,,,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.",,,NeurIPS.csv,,,,,,
UFSC5YLQ,journalArticle,,"Ma, Xiaochen; Zhu, Xuekang; Su, Lei; Du, Bo; Jiang, Zhuohang; Tong, Bingkui; Lei, Zeyu; Yang, Xinyu; Pun, Chi-Man; Lv, Jiancheng; Zhou, Jizhe",IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection & Localization,,,,"A comprehensive benchmark is yet to be established in the Image Manipulation Detection & Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading model evaluations, severely undermining the development of this field. However, the scarcity of open-sourced baseline models and inconsistent training and evaluation protocols make conducting rigorous experiments and faithful comparisons among IMDL models challenging. To address these challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase. IMDL-BenCo: i) decomposes the IMDL framework into standardized, reusable components and revises the model construction pipeline, improving coding efficiency and customization flexibility; ii) fully implements or incorporates training code for state-of-the-art models to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the established benchmark and codebase, offering new insights into IMDL model architecture, dataset characteristics, and evaluation standards. Specifically, IMDL-BenCo includes common processing algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of robustness evaluation. This benchmark and codebase represent a significant leap forward in calibrating the current progress in the IMDL field and inspiring future breakthroughs. Code is available at: https://github.com/scu-zjz/IMDLBenCo.",,,NeurIPS.csv,,,,,,
8YZ9ZCVJ,journalArticle,,"Ho, Cherie; Zou, Jiaye; Alama, Omar; Kumar, Sai Mitheran Jagadesh; Chiang, Benjamin; Gupta, Taneesh; Wang, Chen; Keetha, Nikhil; Sycara, Katia; Scherer, Sebastian",Map It Anywhere (MIA): Empowering Bird’s Eye View Mapping using Large-scale Public Data,,,,"Top-down Bird’s Eye View (BEV) maps are a popular perceptual representation for ground robot navigation due to their richness and flexibility for downstream tasks. While recent methods have shown promise for predicting BEV maps from First-Person View (FPV) images, their generalizability is limited to small regions captured by current autonomous vehicle-based datasets. In this context, we show that a more scalable approach towards generalizable map prediction can be enabled by using two large-scale crowd-sourced mapping platforms, Mapillary for FPV images and OpenStreetMap for BEV semantic maps. We introduce Map It Anywhere (MIA), a data engine that enables seamless curation and modeling of labeled map prediction data from existing open-source map platforms. Using our MIA data engine, we display the ease of automatically collecting a dataset of 1.2 million pairs of FPV images & BEV maps encompassing diverse geographies, landscapes, environmental factors, camera models & capture scenarios. We further train a simple camera model-agnostic model on this data for BEV map prediction. Extensive evaluations using established benchmarks and our dataset show that the data curated by MIA enables effective pretraining for generalizable BEV map prediction, with zero-shot performance far exceeding baselines trained on existing datasets by 35%. Our analysis highlights the promise of using large-scale public maps for developing & testing generalizable BEV perception, paving the way for more robust autonomous navigation. Website: mapitanywhere.github.io ∗Equal contribution.",,,NeurIPS.csv,,,,,,
4CR6L6BR,journalArticle,,"Souly, Alexandra; Lu, Qingyuan; Bowen, Dillon; Trinh, Tu; Hsieh, Elvis; Pandey, Sana; Abbeel, Pieter; Svegliato, Justin; Emmons, Scott; Watkins, Olivia; Toyer, Sam",A STRONGREJECT for Empty Jailbreaks,,,,"Most jailbreak papers claim the jailbreaks they propose are highly effective, often boasting near-100% attack success rates. However, it is perhaps more common than not for jailbreak developers to substantially exaggerate the effectiveness of their jailbreaks. We suggest this problem arises because jailbreak researchers lack a standard, high-quality benchmark for evaluating jailbreak performance, leaving researchers to create their own. To create a benchmark, researchers must choose a dataset of forbidden prompts to which a victim model will respond, along with an evaluation method that scores the harmfulness of the victim model’s responses. We show that existing benchmarks suffer from significant shortcomings and introduce the StrongREJECT benchmark to address these issues. StrongREJECT’s dataset contains prompts that victim models must answer with specific, harmful information, while its automated evaluator measures the extent to which a response gives useful information to forbidden prompts. In doing so, the StrongREJECT evaluator achieves state-of-the-art agreement with human judgments of jailbreak effectiveness. Notably, we find that existing evaluation methods significantly overstate jailbreak effectiveness compared to human judgments and the StrongREJECT evaluator. We describe a surprising and novel phenomenon that explains this discrepancy: jailbreaks bypassing a victim model’s safety fine-tuning tend to reduce its capabilities. Together, our findings underscore the need for researchers to use a high-quality benchmark, such as StrongREJECT, when developing new jailbreak attacks. We release the StrongREJECT code and data at https://strong-reject.readthedocs.io/.",,,NeurIPS.csv,,,,,,
F8XMVGG2,journalArticle,,"Huang, Han; Zhong, Haitian; Yu, Tao; Liu, Qiang; Wu, Shu; Wang, Liang; Tan, Tieniu",VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark,,,,"Recently, knowledge editing on large language models (LLMs) has received considerable attention. Compared to this, editing Large Vision-Language Models (LVLMs) faces extra challenges from diverse data modalities and complicated model components, and data for LVLMs editing are limited. The existing LVLM editing benchmark, which comprises three metrics (Reliability, Locality, and Generality), falls short in the quality of synthesized evaluation images and cannot assess whether models apply edited knowledge in relevant content. Therefore, we employ more reliable data collection methods to construct a new Large Vision-Language Model Knowledge Editing Benchmark, VLKEB, and extend the Portability metric for more comprehensive evaluation. Leveraging a multi-modal knowledge graph, our image data are bound with knowledge entities. This can be further used to extract entity-related knowledge, which constitutes the base of editing data. We conduct experiments of different editing methods on five LVLMs, and thoroughly analyze how do they impact the models. The results reveal strengths and deficiencies of these methods and hopefully provide insights for future research. The codes and dataset are available at: https://github.com/VLKEB/VLKEB.",,,NeurIPS.csv,,,,,,
UZUQP9HF,journalArticle,,"Bukas, Christina; Subramanian, Harshavardhan; See, Fenja; Steinchen, Carina; Ezhov, Ivan; Boosarpu, Gowtham; Asgharpour, Sara; Burgstaller, Gerald; Lehmann, Mareike; Kofler, Florian; Piraud, Marie",MultiOrg: A Multi-rater Organoid-detection Dataset,,,,"High-throughput image analysis in the biomedical domain has gained significant attention in recent years, driving advancements in drug discovery, disease prediction, and personalized medicine. Organoids, specifically, are an active area of research, providing excellent models for human organs and their functions. Automating the quantification of organoids in microscopy images would provide an effective solution to overcome substantial manual quantification bottlenecks, particularly in high-throughput image analysis. However, there is a notable lack of open biomedical datasets, in contrast to other domains, such as autonomous driving, and, notably, only few of them have attempted to quantify annotation uncertainty. In this work, we present MultiOrg a comprehensive organoid dataset tailored for object detection tasks with uncertainty quantification. This dataset comprises over 400 high-resolution 2d microscopy images and curated annotations of more than 60,000 organoids. Most importantly, it includes three label sets for the test data, independently annotated by two experts at distinct time points. We additionally provide a benchmark for organoid detection, and make the best model available through an easily installable, interactive plugin for the popular image visualization tool Napari, to perform organoid quantification.",,,NeurIPS.csv,,,,,,
565ELTVK,dataset,,"Kweon, Sunjun; Kim, Jiyoun; Kwak, Heeyoung; Cha, Dongchul; Yoon, Hangyul; Kim, Kwang Hyun; Yang, Jeewon; Won, Seunghyun; Choi, Edward",EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries,,10.13026/ACGA-HT95,https://physionet.org/content/ehr-notes-qa-llms/1.0.1/,"Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs’ capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients’ discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers ten diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (e.g., the length and number of discharge summaries). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman(ρ): 0.78, Kendall(τ ): 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings. EHRNoteQA is publicly available under PhysioNet credential access at https://doi.org/10.13026/acga-ht95, and the code is available at https://github.com/ji-youn-kim/EHRNoteQA.",,,NeurIPS.csv,,,,,,
7ICRTRHB,journalArticle,,"Sun, Jianhua; Li, Yuxuan; Xu, Longfei; Wang, Nange; Wei, Jiude; Zhang, Yining; Lu, Cewu",ConceptFactory: Facilitate 3D Object Knowledge Annotation with Object Conceptualization,,,,"We present ConceptFactory, a novel scope to facilitate more efficient annotation of 3D object knowledge by recognizing 3D objects through generalized concepts (i.e. object conceptualization), aiming at promoting machine intelligence to learn comprehensive object knowledge from both vision and robotics aspects. This idea originates from the findings in human cognition research that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components (e.g. cuboids and cylinders). ConceptFactory consists of two critical parts: i) ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization, and ii) ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite. Our approach enables researchers to effortlessly acquire or customize extensive varieties of object knowledge to comprehensively study different object understanding tasks. We validate our idea on a wide range of benchmark tasks from both vision and robotics aspects with state-of-the-art algorithms, demonstrating the high quality and versatility of annotations provided by our approach. Our website is available at https://apeirony.github.io/ConceptFactory.",,,NeurIPS.csv,,,,,,
4PR9HW7Q,journalArticle,,"Li, Xiang; Ding, Jian; Elhoseiny, Mohamed",VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding,,,,"We introduce a new benchmark designed to advance the development of generalpurpose, large-scale vision-language models for remote sensing images. Although several vision-language datasets in remote sensing have been proposed to pursue this goal, existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. Exploring these improvement opportunities, we present a Versatile vision-language Benchmark for Remote Sensing image understanding, termed VRSBench. This benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 123,221 question-answer pairs. It facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. We further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. Our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing. The data and code can be accessed at https://vrsbench.github.io.",,,NeurIPS.csv,,,,,,
4SXYPW9I,journalArticle,,"Bountos, Nikolaos Ioannis; Sdraka, Maria; Zavras, Angelos; Karasante, Ilektra; Karavias, Andreas; Herekakis, Themistocles; Thanasou, Angeliki; Michail, Dimitrios; Papoutsis, Ioannis",Kuro Siwo: 33 billion m2 under the water A global multi-temporal satellite dataset for rapid flood mapping,,,,"Global flash floods, exacerbated by climate change, pose severe threats to human life, infrastructure, and the environment. Recent catastrophic events in Pakistan and New Zealand underscore the urgent need for precise flood mapping to guide restoration efforts, understand vulnerabilities, and prepare for future occurrences. While Synthetic Aperture Radar (SAR) remote sensing offers day-and-night, all-weather imaging capabilities, its application in deep learning for flood segmentation is limited by the lack of large annotated datasets. To address this, we introduce Kuro Siwo, a manually annotated multi-temporal dataset, spanning 43 flood events globally. Our dataset maps more than 338 billion m2 of land, with 33 billion designated as either flooded areas or permanent water bodies. Kuro Siwo includes a highly processed product optimized for flash flood mapping based on SAR Ground Range Detected, and a primal SAR Single Look Complex product with minimal preprocessing, designed to promote research on the exploitation of both the phase and amplitude information and to offer maximum flexibility for downstream task preprocessing. To leverage advances in large scale self-supervised pretraining methods for remote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR samples. Finally, we provide an extensive benchmark, namely BlackBench, offering strong baselines for a diverse set of flood events globally. All data and code are published in our Github repository: https://github.com/Orion-AI-Lab/KuroSiwo.",,,NeurIPS.csv,,,,,,
SFV2YKCT,journalArticle,,"Li, Ruosen; Wang, Zimu; Tran, Son Quoc; Xia, Lei; Du, Xinya",MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations,,,,"Existing benchmarks for multi-hop question answering (QA) primarily evaluate models based on their ability to reason about entities and the relationships between them. However, there’s a lack of insight into how these models perform in terms of both events and entities. In this paper, we introduce a novel semi-automatic question generation strategy by composing event structures from information extraction (IE) datasets and present the first Multi-hop Event-centric Question Answering (MEQA) benchmark1. It contains (1) 2,243 challenging questions that require a diverse range of complex reasoning over entity-entity, entity-event, and event-event relations; (2) corresponding multi-step QA-format event reasoning chain (explanation) which leads to the answer for each question. We also introduce two metrics for evaluating explanations: completeness and logical consistency. We conduct comprehensive benchmarking and analysis, which shows that MEQA is challenging for the latest state-of-the-art models encompassing large language models (LLMs); and how they fall short of providing faithful explanations of the event-centric reasoning process.",,,NeurIPS.csv,,,,,,
VUHF2L2P,journalArticle,,"Mathai, Alex; Huang, Chenxi; Maniatis, Petros; Nogikh, Aleksandr; Ivancˇic, Franjo; Yang, Junfeng; Ray, Baishakhi",KGYM: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution,,,,"Large Language Models (LLMs) are consistently improving at increasingly realistic software engineering (SE) tasks. In real-world software stacks, significant SE effort is spent developing foundational system software like the Linux kernel. Unlike application-level software, a systems codebase like Linux is multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines); critical (impacting billions of devices worldwide), and highly concurrent (involving complex multi-threading). To evaluate if machine learning (ML) models are useful while developing such large-scale systems-level software, we introduce KGYM (a platform) and KBENCHSYZ (a dataset). The KGYM2 platform provides a SE environment for large-scale experiments on the Linux kernel, including compiling and running kernels in parallel across several virtual machines, detecting operations and crashes, inspecting logs, and querying and patching the code base. We use KGYM to facilitate evaluation on KBENCHSYZ, a crash resolution benchmark drawn from real-world Linux kernel bugs. An example bug in KBENCHSYZ contains crashing stack traces, a bug-reproducer file, a developer-written fix, and other associated data. To understand current performance, we conduct baseline experiments by prompting LLMs to resolve Linux kernel crashes. Our initial evaluations reveal that the best performing LLM achieves 0.72% and 5.38% in the unassisted and assisted (i.e., buggy files disclosed to the model) settings, respectively. These results highlight the need for further research to enhance model performance in SE tasks. Improving performance on KBENCHSYZ requires models to master new learning skills, including understanding the cause of crashes and repairing faults, writing memory-safe and hardware-aware code, and understanding concurrency. As a result, this work opens up multiple avenues of research at the intersection of machine learning and systems software.",,,NeurIPS.csv,,,,,,
BLMMLIN4,journalArticle,,"Feuer, Benjamin; Xu, Jiawei; Cohen, Niv; Yubeaton, Patrick; Mittal, Govind; Hegde, Chinmay",SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification,,,,"Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT , the first large-scale benchmark of curation strategies for image classification.",,,NeurIPS.csv,,,,,,
W76LEVXH,journalArticle,,"Maruf, M; Daw, Arka; Mehrab, Kazi Sajeed; Manogaran, Harish Babu; Neog, Abhilash; Sawhney, Medha; Khurana, Mridul; Balhoff, James P; Bakıs, Yasin; Altintas, Bahadir; Thompson, Matthew J; Campolongo, Elizabeth G; Uyeda, Josef C; Lapp, Hilmar; Jr, Henry L Bart; Mabee, Paula M; Su, Yu; Chao, Wei-Lun; Stewart, Charles; Berger-Wolf, Tanya; Dahdul, Wasila; Karpatne, Anuj",VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images,,,,,,,NeurIPS.csv,,,,,,
XV6SSUCE,journalArticle,,"Chen, Haozhe; Li, Ang; Che, Ethan; Peng, Tianyi; Dong, Jing; Namkoong, Hongseok",QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers,,,,"Queuing network control determines the allocation of scarce resources to manage congestion, a fundamental problem in manufacturing, communications, and healthcare. Compared to standard RL problems, queueing problems are distinguished by unique challenges: i) a system operating in continuous time, ii) high stochasticity, and iii) long horizons over which the system can become unstable (exploding delays). To spur methodological progress tackling these challenges, we present an open-sourced queueing simulation framework, QGym, that benchmark queueing policies across realistic problem instances. Our modular framework allows the researchers to build on our initial instances, which provide a wide range of environments including parallel servers, criss-cross, tandem, and re-entrant networks, as well as a realistically calibrated hospital queuing system. QGym makes it easy to compare multiple policies, including both model-free RL methods and classical queuing policies. Our testbed complements the traditional focus on evaluating algorithms based on mathematical guarantees in idealized settings, and significantly expands the scope of empirical benchmarking in prior work. QGym code is open-sourced at https://github.com/namkoong-lab/QGym.",,,NeurIPS.csv,,,,,,
S8F2IGMK,journalArticle,,"Alam, Tanvirul; Bhusal, Dipkamal",CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence,,,,"Cyber threat intelligence (CTI) is crucial in today’s cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs’ performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI. Code and dataset available at https://github.com/aiforsec/cti-bench.",,,NeurIPS.csv,,,,,,
J763CBXE,journalArticle,,"Huang, Dong; Qing, Yuhao; Shang, Weiyi; Cui, Heming; Zhang, Jie M",EFFIBENCH: Benchmarking the Efficiency of Automatically Generated Code,,,,"Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in green computing and sustainability efforts — the efficiency of the generated code — has often been neglected. This paper presents EFFIBENCH, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EFFIBENCH contains a diverse set of LeetCode coding problems. Each problem is paired with an executable humanwritten canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EFFIBENCH, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) in generating efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average 3.12 times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 generated code are 13.89 and 43.92 times that of the canonical solutions. The source code of EffiBench is released on https: //github.com/huangd1999/EffiBench. We also provide the LeaderBoard in https://huggingface.co/spaces/EffiBench/effibench-leaderboard.",,,NeurIPS.csv,,,,,,
4SZVXNRT,journalArticle,,"Liu, Puze; Günster, Jonas; Funk, Niklas; Gröger, Simon; Chen, Dong; Bou-Ammar, Haitham; Jankowski, Julius; Maric, Ante; Calinon, Sylvain; Orsula, Andrej; Olivares-Mendez, Miguel; Zhou, Hongyi; Lioutikov, Rudolf; Neumann, Gerhard; Likmeta, Amarildo; Zhalehmehrabi, Amirhossein; Bonenfant, Thomas; Restelli, Marcello; Tateo, Davide; Liu, Ziyuan; Peters, Jan","A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics",,,,"Machine learning methods have a groundbreaking impact in many application domains, but their application on real robotic platforms is still limited. Despite the many challenges associated with combining machine learning technology with robotics, robot learning remains one of the most promising directions for enhancing the capabilities of robots. When deploying learning-based approaches on real robots, extra effort is required to address the challenges posed by various real-world factors. To investigate the key factors influencing real-world deployment and to encourage original solutions from different researchers, we organized the Robot Air Hockey Challenge at the NeurIPS 2023 conference. We selected the air hockey task as a benchmark, encompassing low-level robotics problems and high-level tactics. Different from other machine learning-centric benchmarks, participants need to tackle practical challenges in robotics, such as the sim-toreal gap, low-level control issues, safety problems, real-time requirements, and the limited availability of real-world data. Furthermore, we focus on a dynamic environment, removing the typical assumption of quasi-static motions of other real-world benchmarks. The competition’s results show that solutions combining learning-based approaches with prior knowledge outperform those relying solely on data when real-world deployment is challenging. Our ablation study reveals which real-world factors may be overlooked when building a learning-based solution. The successful real-world air hockey deployment of best-performing agents sets the foundation for future competitions and follow-up research directions.",,,NeurIPS.csv,,,,,,
KXZI4XGU,journalArticle,,"Hemmat, Arshia; Davies, Adam; Lamb, Tom A; Yuan, Jianhao; Torr, Philip; Khakzar, Ashkan; Pinto, Francesco",Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models,,,,,,,NeurIPS.csv,,,,,,
FWP7L65Q,journalArticle,,"Li, Yuhan; Wang, Peisong; Zhu, Xiao; Chen, Aochuan; Jiang, Haiyun; Cai, Deng; Chan, Victor Wai Kin; Li, Jia",GLBench: A Comprehensive Benchmark for Graph with Large Language Models,,,,"The emergence of large language models (LLMs) has revolutionized the way we interact with graphs, leading to a new paradigm called GraphLLM. Despite the rapid development of GraphLLM methods in recent years, the progress and understanding of this field remain unclear due to the lack of a benchmark with consistent experimental protocols. To bridge this gap, we introduce GLBench, the first comprehensive benchmark for evaluating GraphLLM methods in both supervised and zero-shot scenarios. GLBench provides a fair and thorough evaluation of different categories of GraphLLM methods, along with traditional baselines such as graph neural networks. Through extensive experiments on a collection of real-world datasets with consistent data processing and splitting strategies, we have uncovered several key findings. Firstly, GraphLLM methods outperform traditional baselines in supervised settings, with LLM-as-enhancers showing the most robust performance. However, using LLMs as predictors is less effective and often leads to uncontrollable output issues. We also notice that no clear scaling laws exist for current GraphLLM methods. In addition, both structures and semantics are crucial for effective zero-shot transfer, and our proposed simple baseline can even outperform several models tailored for zero-shot scenarios. The data and code of the benchmark can be found at https://github.com/NineAbyss/GLBench.",,,NeurIPS.csv,,,,,,
J8EMV5YS,journalArticle,,"Wen, Bosi; Ke, Pei; Gu, Xiaotao; Wu, Lindong; Huang, Hao; Zhou, Jinfeng; Li, Wenchuang; Hu, Binxin; Gao, Wendy; Xu, Jiaxin; Liu, Yiming; Tang, Jie; Wang, Hongning; Huang, Minlie",Benchmarking Complex Instruction-Following with Multiple Constraints Composition,,,,"Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose ComplexBench, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition. Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. ComplexBench identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition1.",,,NeurIPS.csv,,,,,,
WC8TCSAT,journalArticle,,"Romero, David; Lyu, Chenyang; Wibowo, Haryo Akbarianto; Lynn, Teresa; Hamed, Injy; Kishore, Aditya Nanda; Mandal, Aishik; Dragonetti, Alina; Abzaliev, Artem; Tonja, Atnafu Lambebo; Balcha, Bontu Fufa; Whitehouse, Chenxi; Salamea, Christian; Velasco, Dan John; Adelani, David Ifeoluwa; Meur, David Le; Villa-Cueva, Emilio; Koto, Fajri; Farooqui, Fauzan; Belcavello, Frederico; Batnasan, Ganzorig; Vallejo, Gisela; Caulfield, Grainne; Ivetta, Guido; Song, Haiyue; Ademtew, Henok Biadglign; Maina, Hernán; Ortiz-Barajas, Jesus-German; Baek, Jinheon; Dunstan, Jocelyn; Alemany, Laura Alonso; Nagasinghe, Kumaranage Ravindu Yasas; Benotti, Luciana; D’Haro, Luis Fernando; Viridiano, Marcelo; Estecha-Garitagoitia, Marcos; Cabrera, Maria Camila Buitrago; Rodríguez-Cantelar, Mario; Jouitteau, Mélanie; Mihaylov, Mihail; Etori, Naome; Imam, Mohamed Fazli Mohamed; Adilazuarda, Muhammad Farid; Gochoo, Munkhjargal; Otgonbold, Munkh-Erdene; Niyomugisha, Olivier; Silva, Paula Mónica; Chitale, Pranjal; Dabre, Raj; Chevi, Rendi; Zhang, Ruochen; Diandaru, Ryandito; Cahyawijaya, Samuel; Góngora, Santiago; Jeong, Soyeong; Purkayastha, Sukannya; Kuribayashi, Tatsuki; Clifford, Teresa; Jayakumar, Thanmay; Torrent, Tiago Timponi; Ehsan, Toqeer; Araujo, Vladimir; Kementchedjhieva, Yova; Burzo, Zara; Lim, Zheng Wei; Yong, Zheng Xin; Ignat, Oana; Nwatu, Joan; Mihalcea, Rada; Solorio, Thamar; Aji, Alham Fikri",CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark,,,,,,,NeurIPS.csv,,,,,,
6EYVI5RH,journalArticle,,"Alberts, Marvin; Schilter, Oliver; Zipoli, Federico; Hartrampf, Nina; Laino, Teodoro",Unraveling Molecular Structure: A Multimodal Spectroscopic Dataset for Chemistry,,,,"Spectroscopic techniques are essential tools for determining the structure of molecules. Different spectroscopic techniques, such as Nuclear magnetic resonance (NMR), Infrared spectroscopy, and Mass Spectrometry, provide insight into the molecular structure, including the presence or absence of functional groups. Chemists leverage the complementary nature of the different methods to their advantage. However, the lack of a comprehensive multimodal dataset, containing spectra from a variety of spectroscopic techniques, has limited machine-learning approaches mostly to single-modality tasks for predicting molecular structures from spectra. Here we introduce a dataset comprising simulated 1H-NMR, 13CNMR, HSQC-NMR, Infrared, and Mass spectra (positive and negative ion modes) for 790k molecules extracted from chemical reactions in patent data. This dataset enables the development of foundation models for integrating information from multiple spectroscopic modalities, emulating the approach employed by human experts. Additionally, we provide benchmarks for evaluating single-modality tasks such as structure elucidation, predicting the spectra for a target molecule, and functional group predictions. This dataset has the potential automate structure elucidation, streamlining the molecular discovery pipeline from synthesis to structure determination. The dataset and code for the benchmarks can be found at https: //rxn4chemistry.github.io/multimodal-spectroscopic-dataset.",,,NeurIPS.csv,,,,,,
E8HEUPJX,journalArticle,,"Fent, Felix; Kuttenreich, Fabian; Ruch, Florian; Rizwin, Farija; Juergens, Stefan; Lechermann, Lorenz; Nissler, Christian; Perl, Andrea; Voll, Ulrich; Yan, Min; Lienkamp, Markus",MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse conditions,,,,"Autonomous trucking is a promising technology that can greatly impact modern logistics and the environment. Ensuring its safety on public roads is one of the main duties that requires an accurate perception of the environment. To achieve this, machine learning methods rely on large datasets, but to this day, no such datasets are available for autonomous trucks. In this work, we present MAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN TruckScenes allows the research community to come into contact with truck-specific challenges, such as trailer occlusions, novel sensor perspectives, and terminal environments for the first time. It comprises more than 740 scenes of 20 s each within a multitude of different environmental conditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2 IMUs, and a high-precision GNSS. The dataset’s 3D bounding boxes were manually annotated and carefully reviewed to achieve a high quality standard. Bounding boxes are available for 27 object classes, 15 attributes, and a range of more than 230 m. The scenes are tagged according to 34 distinct scene tags, and all objects are tracked throughout the scene to promote a wide range of applications. Additionally, MAN TruckScenes is the first dataset to provide 4D radar data with 360° coverage and is thereby the largest radar dataset with annotated 3D bounding boxes. Finally, we provide extensive dataset analysis and baseline results. The dataset, development kit, and more are available online.",,,NeurIPS.csv,,,,,,
PJVIYPQ4,journalArticle,,"Varbella, Anna; Amara, Kenza; Gjorgiev, Blazhe; El-Assady, Mennatallah; Sansavini, Giovanni",PowerGraph: A power grid benchmark dataset for graph neural networks,,,,,,,NeurIPS.csv,,,,,,
AU362KL9,journalArticle,,"Debenedetti, Edoardo; Zhang, Jie; Balunovic, Mislav; Beurer-Kellner, Luca; Fischer, Marc; Tramèr, Florian",AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents,,,,"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.",,,NeurIPS.csv,,,,,,
DRE5R9AI,journalArticle,,"Chen, Pengcheng; Ye, Jin; Wang, Guoan; Li, Yanjun; Deng, Zhongying; Li, Wei; Li, Tianbin; Duan, Haodong; Huang, Ziyan; Su, Yanzhou; Wang, Benyou; Zhang, Shaoting; Fu, Bin; Cai, Jianfei; Zhuang, Bohan; Seibel, Eric J; Qiao, Yu; He, Junjun",GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI,,,,,,,NeurIPS.csv,,,,,,
XPAKHQ4Y,journalArticle,,"Bandara, Nuwan; Kandappu, Thivya; Sen, Argha; Gokarn, Ila; Misra, Archan",EyeGraph: Modularity-aware Spatio Temporal Graph Clustering for Continuous Event-based Eye Tracking,,,,"Continuous tracking of eye movement dynamics plays a significant role in developing a broad spectrum of human-centered applications, such as cognitive skills modeling, biometric user authentication, and foveated rendering. Recently neuromorphic cameras have garnered significant interest in the eye-tracking research community, owing to their sub-microsecond latency in capturing intensity changes resulting from eye movements. Nevertheless, the existing approaches for eventbased eye tracking suffer from several limitations: dependence on RGB frames, label sparsity, and training on datasets collected in controlled lab environments that do not adequately reflect real-world scenarios. To address these limitations, in this paper, we propose a dynamic graph-based approach that uses the event stream for high-fidelity tracking of pupillary movement. We first present EyeGraph, a large-scale, multi-modal near-eye tracking dataset collected using a wearable event camera attached to a head-mounted device from 40 participants – the dataset was curated while mimicking in-the-wild settings, with variations in user movement and ambient lighting conditions. Subsequently, to address the issue of label sparsity, we propose an unsupervised topology-aware spatio-temporal graph clustering approach as a benchmark. We show that our unsupervised approach achieves performance comparable to more onerous supervised approaches while consistently outperforming the conventional clustering-based unsupervised approaches.",,,NeurIPS.csv,,,,,,
THYZLHQB,journalArticle,,"Reuel, Anka; Hardy, Amelia; Smith, Chandler; Lamparth, Max; Hardy, Malcolm; Kochenderfer, Mykel J","BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices",,,,,,,NeurIPS.csv,,,,,,
S5F28W2U,journalArticle,,"Ding, Mucong; Deng, Chenghao; Choo, Jocelyn; Wu, Zichu; Agrawal, Aakriti; Schwarzschild, Avi; Zhou, Tianyi; Goldstein, Tom; Langford, John; Anandkumar, Anima; Huang, Furong",Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization,,,,"While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still missing. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six stateof-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https: //huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",,,NeurIPS.csv,,,,,,
HM2HMXGU,journalArticle,,"Ren, Richard; Basart, Steven; Khoja, Adam; Pan, Alexander; Gatti, Alice; Phan, Long; Yin, Xuwang; Mazeika, Mantas; Mukobi, Gabriel; Kim, Ryan Hwang; Fitz, Stephen; Hendrycks, Dan",Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,,,,"Performance on popular ML benchmarks is highly correlated with model scale, suggesting that most benchmarks tend to measure a similar underlying factor of general model capabilities. However, substantial research effort remains devoted to designing new benchmarks, many of which claim to measure novel phenomena. In the spirit of the Bitter Lesson, we leverage spectral analysis to measure an underlying capabilities component, the direction in benchmark-performance-space which explains most variation in model performance. In an extensive analysis of existing safety benchmarks, we find that variance in model performance on many safety benchmarks is largely explained by the capabilities component. In response, we argue that safety research should prioritize metrics which are not highly correlated with scale. Our work provides a lens to analyze both novel safety benchmarks and novel safety methods, which we hope will enable future work to make differential progress on safety.",,,NeurIPS.csv,,,,,,
73YR5DPZ,journalArticle,,"González-Duque, Miguel; Michael, Richard; Bartels, Simon; Zainchkovskyy, Yevgen; Hauberg, Søren; Boomsma, Wouter",A survey and benchmark of high-dimensional Bayesian optimization of discrete sequences,,,,"Optimizing discrete black box functions is key in several domains, e.g. protein engineering and drug design. Due to the lack of gradient information and the need for sample efficiency, Bayesian optimization is an ideal candidate for these tasks. Several methods for high-dimensional continuous and categorical Bayesian optimization have been proposed recently. However, our survey of the field reveals highly heterogeneous experimental set-ups across methods and technical barriers for the replicability and application of published algorithms to real-world tasks. To address these issues, we develop a unified framework to test a vast array of high-dimensional Bayesian optimization methods and a collection of standardized black box functions representing real-world application domains in chemistry and biology. These two components of the benchmark are each supported by flexible, scalable, and easily extendable software libraries (poli and poli-baselines), allowing practitioners to readily incorporate new optimization objectives or discrete optimizers. Project website: https://machinelearninglifescience.github.io/hdbo_benchmark.",,,NeurIPS.csv,,,,,,
QQU45MB6,journalArticle,,"Song, Jian; Chen, Hongruixuan; Xuan, Weihao; Xia, Junshi; Yokoya, Naoto",SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding from Monocular Remote Sensing Imagery,,,,"Global semantic 3D understanding from single-view high-resolution remote sensing (RS) imagery is crucial for Earth observation (EO). However, this task faces significant challenges due to the high costs of annotations and data collection, as well as geographically restricted data availability. To address these challenges, synthetic data offer a promising solution by being unrestricted and automatically annotatable, thus enabling the provision of large and diverse datasets. We develop a specialized synthetic data generation pipeline for EO and introduce SynRS3D, the largest synthetic RS dataset. SynRS3D comprises 69,667 high-resolution optical images that cover six different city styles worldwide and feature eight land cover types, precise height information, and building change masks. To further enhance its utility, we develop a novel multi-task unsupervised domain adaptation (UDA) method, RS3DAda, coupled with our synthetic dataset, which facilitates the RS-specific transition from synthetic to real scenarios for land cover mapping and height estimation tasks, ultimately enabling global monocular 3D semantic understanding based on synthetic data. Extensive experiments on various real-world datasets demonstrate the adaptability and effectiveness of our synthetic dataset and the proposed RS3DAda method. SynRS3D and related codes are available at https://github.com/JTRNEO/SynRS3D.",,,NeurIPS.csv,,,,,,
6JYUTE5B,journalArticle,,"Zhang, Xueyi; Zhang, Chengwei; Lao, Mingrui; Zhao, Peng; Tang, Jun; Guo, Yanming; Cai, Siqi; Yue, Xianghu; Li, Haizhou",Language Without Borders: A Dataset and Benchmark for Code-Switching Lip Reading,,,,"Lip reading aims at transforming the videos of continuous lip movement into textual contents, and has achieved significant progress over the past decade. It serves as a critical yet practical assistance for speech-impaired individuals, with more practicability than speech recognition in noisy environments. With the increasing interpersonal communications in social media owing to globalization, the existing monolingual datasets for lip reading may not be sufficient to meet the exponential proliferation of bilingual and even multilingual users. However, to our best knowledge, research on code-switching is only explored in speech recognition, while the attempts in lip reading are seriously neglected. To bridge this gap, we have collected a bilingual code-switching lip reading benchmark composed of Chinese and English, dubbed CSLR. As the pioneering work, we recruited 62 speakers with proficient foundations in both spoken Chinese and English to express sentences containing both involved languages. Through rigorous criteria in data selection, CSLR benchmark has accumulated 85,560 video samples with a resolution of 1080x1920, totaling over 71.3 hours of high-quality code-switching lip movement data. To systematically evaluate the technical challenges in CSLR, we implement commonly-used lip reading backbones, as well as competitive solutions in code-switching speech for benchmark testing. Experiments show CSLR to be a challenging and under-explored lip reading task. We hope our proposed benchmark will extend the applicability of code-switching lip reading, and further contribute to the communities of cross-lingual communication and collaboration. Our dataset and benchmark are accessible at GitHub.",,,NeurIPS.csv,,,,,,
EU3A79M5,journalArticle,,"Li, Jiatong; Hu, Renjun; Huang, Kunzhe; Zhuang, Yan; Liu, Qi; Zhu, Mengxiao; Shi, Xing; Lin, Wei",PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations,,,,"Expert-designed close-ended benchmarks are indispensable in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs’ knowledge capacity through knowledge-invariant perturbations. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of response consistency analyses that compare performance on raw vs. perturbed test sets to precisely assess LLMs’ genuine knowledge capacity. Six representative LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs’ uncertainty to specious knowledge, and reveals their potential rote memorization to correct options which leads to overestimated performance. We also find that the detailed response consistency analyses by PertEval could illuminate various weaknesses in existing LLMs’ knowledge mastery and guide the development of refinement. Our findings provide insights for advancing more robust and genuinely knowledgeable LLMs. Our code is available at https://github.com/aigc-apps/PertEval.",,,NeurIPS.csv,,,,,,
7WPEIBAC,journalArticle,,"Yang, Dongjie; Huang, Suyuan; Lu, Chengqiang; Han, Xiaodong; Zhang, Haoxin; Gao, Yan; Hu, Yao; Zhao, Hai",Vript: A Video Is Worth Thousands of Words,,,,,,,NeurIPS.csv,,,,,,
JMYDRE4X,journalArticle,,"Tan, Alvin W M; Yu, Sunny; Long, Bria; Ma, Wanjing Anya; Murray, Tonya; Silverman, Rebecca D; Yeatman, Jason D; Frank, Michael C",DEVBENCH: A multimodal developmental benchmark for language learning,,,,"How (dis)similar are the learning trajectories of vision–language models and children? Recent modeling work has attempted to understand the gap between models’ and humans’ data efficiency by constructing models trained on less data, especially multimodal naturalistic data. However, such models are often evaluated on adultlevel benchmarks, with limited breadth in language abilities tested, and without direct comparison to behavioral data. We introduce DEVBENCH, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. We evaluate a set of vision–language models on these tasks, comparing models and humans on their response patterns, not their absolute performance. Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. We also examine the developmental trajectory of OpenCLIP over training, finding that greater training results in closer approximations to adult response patterns. DEVBENCH thus provides a benchmark for comparing models to human language development. These comparisons highlight ways in which model and human language learning processes diverge, providing insight into entry points for improving language models.",,,NeurIPS.csv,,,,,,
KED34Q4G,journalArticle,,"Kalyan, T Pavan; Pasi, Piyush Singh; Dharod, Sahil Nilesh; Motiwala, Azeem Azaz; Jyothi, Preethi; Chaudhary, Aditi; Srinivasan, Krishna",WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models,,,,"Cross-modal (image-to-text and text-to-image) retrieval is an established task used in evaluation benchmarks to test the performance of vision-language models (VLMs). Several state-of-the-art VLMs (e.g. CLIP, BLIP-2) have achieved near-perfect performance on widely-used image-text retrieval benchmarks such as MSCOCO-Test-5K and Flickr30K-Test-1K. As a measure of out-of-distribution (OOD) generalization, prior works rely on zero-shot performance evaluated on one dataset (Flickr) using a VLM finetuned on another one (MSCOCO). We argue that such comparisons are insufficient to assess the OOD generalization capability of models due to high visual and linguistic similarity between the evaluation and finetuning datasets. To address this gap, we introduce WIKIDO (drawn from Wikipedia Diversity Observatory), a new cross-modal retrieval benchmark to assess the OOD generalization capabilities of pretrained VLMs. This consists of 384K image-text pairs from Wikipedia with domain labels, along with carefully curated, human-verified in-distribution (ID) and OOD test sets of size 3K each. The image-text pairs are very diverse in topics. We evaluate different VLMs of varying capacity on the WIKIDO benchmark; BLIP-2 achieves zero-shot performance of R@1≈ 66% on the OOD test set, compared to ≈ 81% on MSCOCO and ≈ 95% on Flickr. When fine-tuned on WIKIDO, the R@1 improvement is at most ≈ 5% on OOD instances compared to ≈ 12% on ID instances. WIKIDO offers a strong cross-modal retrieval benchmark for current VLMs, especially for evaluating OOD generalization. Our benchmark is hosted as a competition at https://kaggle.com/competitions/wikido24 with public access to dataset and code.",,,NeurIPS.csv,,,,,,
VJNEGL6Y,journalArticle,,"Fawkes, Jake; Fishman, Nic; Andrews, Mel; Lipton, Zachary C",The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning,,,,"Fairness metrics are a core tool in the fair machine learning literature (FairML), used to determine that ML models are, in some sense, “fair.” Real-world data, however, are typically plagued by various measurement biases and other violated assumptions, which can render fairness assessments meaningless. We adapt tools from causal sensitivity analysis to the FairML context, providing a general framework which (1) accommodates effectively any combination of fairness metric and bias that can be posed in the “oblivious setting”; (2) allows researchers to investigate combinations of biases, resulting in non-linear sensitivity; and (3) enables flexible encoding of domain-specific constraints and assumptions. Employing this framework, we analyze the sensitivity of the most common parity metrics under 3 varieties of classifier across 14 canonical fairness datasets. Our analysis reveals the striking fragility of fairness assessments to even minor dataset biases. We show that causal sensitivity analysis provides a powerful and necessary toolkit for gauging the informativeness of parity metric evaluations. Our repository is available here.",,,NeurIPS.csv,,,,,,
5KUFU7CC,journalArticle,,"Obi, Ike; Pant, Rohan; Agrawal, Srishti Shekhar; Ghazanfar, Maham; Basiletti, Aaron",Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets,,,,,,,NeurIPS.csv,,,,,,
2YGUGXND,journalArticle,,"Wang, Aoran; Tong, Tsz Pan; Mizera, Andrzej; Pang, Jun",Benchmarking Structural Inference Methods for Interacting Dynamical Systems with Synthetic Data,,,,"Understanding complex dynamical systems begins with identifying their topological structures, which expose the organization of the systems. This requires robust structural inference methods that can deduce structure from observed behavior. However, existing methods are often domain-specific and lack a standardized, objective comparison framework. We address this gap by benchmarking 13 structural inference methods from various disciplines on simulations representing two types of dynamics and 11 interaction graph models, supplemented by a biological experimental dataset to mirror real-world application. We evaluated the methods for accuracy, scalability, robustness, and sensitivity to graph properties. Our findings indicate that deep learning methods excel with multi-dimensional data, while classical statistics and information theory based approaches are notably accurate and robust. Additionally, performance correlates positively with the graph’s average shortest path length. This benchmark should aid researchers in selecting suitable methods for their specific needs and stimulate further methodological innovation. Project website: https://structinfer.github.io/.",,,NeurIPS.csv,,,,,,
WZCCMYDG,journalArticle,,"Kargaran, Amir Hossein; Yvon, François; Schütze, Hinrich",GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages,,,,"The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it— including the pipeline, language identification model, and filters—available to the research community.",,,NeurIPS.csv,,,,,,
86KSZDPJ,journalArticle,,"Xie, Junlin; Zhang, Ruifei; Chen, Zhihong; Wan, Xiang; Li, Guanbin",WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games,,,,,,,NeurIPS.csv,,,,,,
VSBD3SL6,journalArticle,,"Choi, Yejin; Chung, Jiwan; Shim, Sumin; Oh, Giyeong; Yu, Youngjae",Towards Visual Text Design Transfer Across Languages,,,,"Visual text design plays a critical role in conveying themes, emotions, and atmospheres in multimodal formats such as film posters and album covers. Translating these visual and textual elements across languages extends the concept of translation beyond mere text, requiring the adaptation of aesthetic and stylistic features. To address this, we introduce a novel task of Multimodal Style Translation (MuSTBench), a benchmark designed to evaluate the ability of visual text generation models to perform translation across different writing systems while preserving design intent. Our initial experiments on MuST-Bench reveal that existing visual text generation models struggle with the proposed task due to the inadequacy of textual descriptions in conveying visual design. In response, we introduce SIGIL, a framework for multimodal style translation that eliminates the need for style descriptions. SIGIL enhances image generation models through three innovations: glyph latent for multilingual settings, pre-trained VAEs for stable style guidance, and an OCR model with reinforcement learning feedback for optimizing readable character generation. SIGIL outperforms existing baselines by achieving superior style consistency and legibility while maintaining visual fidelity, setting itself apart from traditional description-based approaches. We release MuST-Bench publicly for broader use and exploration1.",,,NeurIPS.csv,,,,,,
C69GNNI3,journalArticle,,"Ohana, Ruben; McCabe, Michael; Meyer, Lucas; Morel, Rudy; Agocs, Fruzsina J; Beneitez, Miguel; Berger, Marsha; Burkhart, Blakesley; Dalziel, Stuart B; Fielding, Drummond B; Fortunato, Daniel; Goldberg, Jared A; Hirashima, Keiya; Jiang, Yan-Fei; Kerswell, Rich R; Maddu, Suryanarayana; Miller, Jonah; Mukhopadhyay, Payel; Nixon, Stefan S; Shen, Jeff; Watteaux, Romain; Blancard, Bruno Régaldo-Saint; Rozet, François; Parker, Liam H; Cranmer, Miles; Ho, Shirley",The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning,,,,"Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.",,,NeurIPS.csv,,,,,,
HP4ZKJYP,journalArticle,,"Shen, Yongliang; Song, Kaitao; Tan, Xu; Zhang, Wenqi; Ren, Kan; Yuan, Siyu; Lu, Weiming; Li, Dongsheng; Zhuang, Yueting",TaskBench: Benchmarking Large Language Models for Task Automation,,,,"In recent years, the remarkable progress of large language models (LLMs) has sparked interest in task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents. However, there is a lack of systematic and standardized benchmarks to promote the development of LLMs in task automation. To address this, we introduce TASKBENCH, a comprehensive framework to evaluate the capability of LLMs in task automation. Specifically, task automation can be divided into three critical stages: task decomposition, tool selection, and parameter prediction. To tackle the complexities inherent in these stages, we introduce the concept of Tool Graph to represent decomposed tasks and adopt a back-instruct method to generate high-quality user instructions. We propose TASKEVAL, a multi-faceted evaluation methodology that assesses LLM performance across these three stages. Our approach combines automated construction with rigorous human verification, ensuring high consistency with human evaluation. Experimental results demonstrate that TASKBENCH effectively reflects the capabilities of various LLMs in task automation. It provides insights into model performance across different task complexities and domains, pushing the boundaries of what current models can achieve. TASKBENCH offers a scalable, adaptable, and reliable benchmark for advancing LLM-based autonomous agents 1.",,,NeurIPS.csv,,,,,,
P3JCDCNA,journalArticle,,"Koppula, Skanda; Rocco, Ignacio; Yang, Yi; Heyward, Joe; Carreira, João; Zisserman, Andrew; Brostow, Gabriel; Doersch, Carl",TAPVid-3D: A Benchmark for Tracking Any Point in 3D,,,,"We introduce a new benchmark, TAPVid-3D, for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). While point tracking in two dimensions (TAP-2D) has many benchmarks measuring performance on real-world videos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To this end, leveraging existing footage, we build a new benchmark for 3D point tracking featuring 4,000+ real-world videos, composed of three different data sources spanning a variety of object types, motion patterns, and indoor and outdoor environments. To measure performance on the TAP-3D task, we formulate a collection of metrics that extend the Jaccard-based metric used in TAP-2D to handle the complexities of ambiguous depth scales across models, occlusions, and multi-track spatio-temporal smoothness. We manually verify a large sample of trajectories to ensure correct video annotations, and assess the current state of the TAP-3D task by constructing competitive baselines using existing tracking models. We anticipate this benchmark will serve as a guidepost to improve our ability to understand precise 3D motion and surface deformation from monocular video.",,,NeurIPS.csv,,,,,,
2GACHI4L,preprint,2024.0,"Karpowicz, Brianna M.; Ye, Joel; Fan, Chaofei; Tostado-Marcos, Pablo; Rizzoglio, Fabio; Washington, Clay; Scodeler, Thiago; De Lucena, Diogo; Nason-Tomaszewski, Samuel R.; Mender, Matthew J.; Ma, Xuan; Arneodo, Ezequiel Matias; Hochberg, Leigh R.; Chestek, Cynthia A.; Henderson, Jaimie M.; Gentner, Timothy Q.; Gilja, Vikash; Miller, Lee E.; Rouse, Adam G.; Gaunt, Robert A.; Collinger, Jennifer L.; Pandarinath, Chethan",Few-shot Algorithms for Consistent Neural Decoding (FALCON) Benchmark,,10.1101/2024.09.15.613126,http://biorxiv.org/lookup/doi/10.1101/2024.09.15.613126,"Abstract                        Intracortical brain-computer interfaces (iBCIs) can restore movement and communication abilities to individuals with paralysis by decoding their intended behavior from neural activity recorded with an implanted device. While this activity yields high-performance decoding over short timescales, neural data are often nonstationary, which can lead to decoder failure if not accounted for. To maintain performance, users must frequently recalibrate decoders, which requires the arduous collection of new neural and behavioral data. Aiming to reduce this burden, several approaches have been developed that either limit recalibration data requirements (few-shot approaches) or eliminate explicit recalibration entirely (zero-shot approaches). However, progress is limited by a lack of standardized datasets and comparison metrics, causing methods to be compared in an ad hoc manner. Here we introduce the FALCON benchmark suite (Few-shot Algorithms for COnsistent Neural decoding) to standardize evaluation of iBCI robustness. FALCON curates five datasets of neural and behavioral data that span movement and communication tasks to focus on behaviors of interest to modern-day iBCIs. Each dataset includes calibration data, optional few-shot recalibration data, and private evaluation data. We implement a flexible evaluation platform which only requires user-submitted code to return behavioral predictions on unseen data. We also seed the benchmark by applying baseline methods spanning several classes of possible approaches. FALCON aims to provide rigorous selection criteria for robust iBCI decoders, easing their translation to real-world devices.             https://snel-repo.github.io/falcon/",2024-09-16,,NeurIPS.csv,,,,,,
Y3WD8VM5,journalArticle,,"Zheng, Matthew; Simsar, Enis; Yesiltepe, Hidir; Tombari, Federico; Simon, Joel; Yanardag, Pinar",Stylebreeder : Exploring and Democratizing Artistic Styles through Text-to-Image Models,,,,"Text-to-image models are becoming increasingly popular, revolutionizing the landscape of digital art creation by enabling highly detailed and creative visual content generation. These models have been widely employed across various domains, particularly in art generation, where they facilitate a broad spectrum of creative expression and democratize access to artistic creation. In this paper, we introduce STYLEBREEDER, a comprehensive dataset of 6.8M images and 1.8M prompts generated by 95K users on Artbreeder, a platform that has emerged as a significant hub for creative exploration with over 13M users. We introduce a series of tasks with this dataset aimed at identifying diverse artistic styles, generating personalized content, and recommending styles based on user interests. By documenting unique, usergenerated styles that transcend conventional categories like ‘cyberpunk’ or ‘Picasso,’ we explore the potential for unique, crowd-sourced styles that could provide deep insights into the collective creative psyche of users worldwide. We also evaluate different personalization methods to enhance artistic expression and introduce a style atlas, making these models available in LoRA format for public use. Our research demonstrates the potential of text-to-image diffusion models to uncover and promote unique artistic expressions, further democratizing AI in art and fostering a more diverse and inclusive artistic community. The dataset, code, and models are available at https://stylebreeder.github.io under a Public Domain (CC0) license.",,,NeurIPS.csv,,,,,,
45TGPXER,journalArticle,,"Veitch-Michaelis, Josh; Cottam, Andrew; Schweizer, Daniella; Broadbent, Eben N; Dao, David; Zhang, Ce; Zambrano, Angelica Almeyda; Max, Simeon",OAM-TCD: A globally diverse dataset of high-resolution tree cover maps,,,,"Accurately quantifying tree cover is an important metric for ecosystem monitoring and for assessing progress in restored sites. Recent works have shown that deep learning-based segmentation algorithms are capable of accurately mapping trees at country and continental scales using high-resolution aerial and satellite imagery. Mapping at high (ideally sub-meter) resolution is necessary to identify individual trees, however there are few open-access datasets containing instance level annotations and those that exist are small or not geographically diverse. We present a novel open-access dataset for individual tree crown delineation (TCD) in high-resolution aerial imagery sourced from OpenAerialMap (OAM). Our dataset, OAM-TCD, comprises 5072 2048x2048 px images at 10 cm/px resolution with associated human-labeled instance masks for over 280k individual and 56k groups of trees. By sampling imagery from around the world, we are able to better capture the diversity and morphology of trees in different terrestrial biomes and in both urban and natural environments. Using our dataset, we train reference instance and semantic segmentation models that compare favorably to existing state-of-the-art models. We assess performance through k-fold cross-validation and comparison with existing datasets; additionally we demonstrate compelling results on independent aerial imagery captured over Switzerland and compare to municipal tree inventories and LIDAR-derived canopy maps in the city of Zurich. Our dataset, models and training/benchmark code are publicly released under permissive open-source licenses: Creative Commons (majority CC BY 4.0), and Apache 2.0 respectively.",,,NeurIPS.csv,,,,,,
5EPD5ZZA,journalArticle,,"Bitton-Guetta, Nitzan; Slobodkin, Aviv; Maimon, Aviya; Habba, Eliya; Rassin, Royi; Bitton, Yonatan; Szpektor, Idan; Globerson, Amir; Elovici, Yuval",Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models,,,,"Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person’s discomfort, thereby alleviating the need for further information. This example illustrates how subtle visual cues can challenge our cognitive skills and demonstrates the complexity of interpreting visual scenarios. To study these skills, we present Visual Riddles, a benchmark aimed to test vision and language models on visual riddles requiring commonsense and world knowledge. The benchmark comprises 400 visual riddles, each featuring a unique image created by a variety of text-to-image models, question, groundtruth answer, textual hint, and attribution. Human evaluation reveals that existing models lag significantly behind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading with 40% accuracy. Our benchmark comes with automatic evaluation tasks to make assessment scalable. These findings underscore the potential of Visual Riddles as a valuable resource for enhancing vision and language models’ capabilities in interpreting complex visual scenarios.",,,NeurIPS.csv,,,,,,
6ETGMY2D,journalArticle,,"Schneider, David; Reiß, Simon; Kugler, Marco; Jaus, Alexander; Peng, Kunyu; Sutschet, Susanne; Sarfraz, M Saquib; Matthiesen, Sven; Stiefelhagen, Rainer",Muscles in Time: Learning to Understand Human Motion by Simulating Muscle Activations,,,,"Exploring the intricate dynamics between muscular and skeletal structures is pivotal for understanding human motion. This domain presents substantial challenges, primarily attributed to the intensive resources required for acquiring ground truth muscle activation data, resulting in a scarcity of datasets. In this work, we address this issue by establishing Muscles in Time (MinT), a large-scale synthetic muscle activation dataset. For the creation of MinT, we enriched existing motion capture datasets by incorporating muscle activation simulations derived from biomechanical human body models using the OpenSim platform, a common approach in biomechanics and human motion research. Starting from simple pose sequences, our pipeline enables us to extract detailed information about the timing of muscle activations within the human musculoskeletal system. Muscles in Time contains over nine hours of simulation data covering 227 subjects and 402 simulated muscle strands. We demonstrate the utility of this dataset by presenting results on neural network-based muscle activation estimation from human pose sequences with two different sequence-to-sequence architectures.",,,NeurIPS.csv,,,,,,
SKCJC36K,journalArticle,,"Li, Heng; Li, Minghan; Cheng, Zhi-Qi; Dong, Yifei; Zhou, Yuxuan; He, Jun-Yan; Dai, Qi; Mitamura, Teruko; Hauptmann, Alexander G",Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions,,,,"Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Visionand-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and NonExpert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN’s unique challenges, underscores the need for further research to enhance HA-VLN agents’ real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.",,,NeurIPS.csv,,,,,,
CBJ2IX2B,journalArticle,,"Sundar, Anirudh; Xu, Jin; Gay, William; Richardson, Christopher; Heck, Larry",cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers,,,,"An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers. Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists. This work introduces CONVERSATIONAL PAPERS (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv. We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from LATEX source files. Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset.",,,NeurIPS.csv,,,,,,
U3IBVAD3,journalArticle,,"Evans, Talfan; Parthasarathy, Nikhil; Merzic, Hamza; Hénaff, Olivier J",Data curation via joint example selection further accelerates multimodal learning,,,,"Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly prioritizing batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individuallyprioritized data points. As performance improves by selecting from large superbatches, we also leverage recent advances in model approximation to reduce the computational overhead of scoring. As a result, our approach—multimodal contrastive learning with joint example selection (JEST)—surpasses state-of-the-art pretraining methods with up to 13× fewer iterations and 10× less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing data curation as a new dimension for neural scaling laws.",,,NeurIPS.csv,,,,,,
2MF3WSSQ,journalArticle,,"Wang, Zhenzhi; Li, Yixuan; Zeng, Yanhong; Fang, Youqing; Guo, Yuwei; Liu, Wenran; Tan, Jing; Chen, Kai; Xue, Tianfan; Dai, Bo; Lin, Dahua",HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation,,,,"Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.",,,NeurIPS.csv,,,,,,
26LZY7XU,journalArticle,,"Cho, Hoonhee; Kim, Taewoo; Jeong, Yuhwan; Yoon, Kuk-Jin",A Benchmark Dataset for Event-Guided Human Pose Estimation and Tracking in Extreme Conditions,,,,"Multi-person pose estimation and tracking have been actively researched by the computer vision community due to their practical applicability. However, existing human pose estimation and tracking datasets have only been successful in typical scenarios, such as those without motion blur or with well-lit conditions. These RGB-based datasets are limited to learning under extreme motion blur situations or poor lighting conditions, making them inherently vulnerable to such scenarios. As a promising solution, bio-inspired event cameras exhibit robustness in extreme scenarios due to their high dynamic range and micro-second level temporal resolution. Therefore, in this paper, we introduce a new hybrid dataset encompassing both RGB and event data for human pose estimation and tracking in two extreme scenarios: low-light and motion blur environments. The proposed Event-guided Human Pose Estimation and Tracking in eXtreme Conditions (EHPTXC) dataset covers cases of motion blur caused by dynamic objects and low-light conditions individually as well as both simultaneously. With EHPT-XC, we aim to inspire researchers to tackle pose estimation and tracking in extreme conditions by leveraging the advantageous of the event camera. Project pages are available at https://github.com/Chohoonhee/EHPT-XC.",,,NeurIPS.csv,,,,,,
2XIPE3PA,journalArticle,,"Shao, Yijia; Li, Tianshi; Shi, Weiyan; Liu, Yanchen; Yang, Diyi",PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action,,,,"As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents’ actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-ofthe-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.",,,NeurIPS.csv,,,,,,
LXRBCCPB,journalArticle,,"Li, Yinghui; Zhou, Qingyu; Luo, Yuanzhen; Ma, Shirong; Li, Yangning; Zheng, Hai-Tao; Hu, Xuming; Yu, Philip S",When LLMs Meet Cunning Texts:,,,,"Recently, Large Language Models (LLMs) make remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning texts that FLUB focuses on mainly consist of the tricky, humorous, and misleading texts collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs’ ability to understand fallacies. Our data and codes are available at https://github.com/THUKElab/FLUB.",,,NeurIPS.csv,,,,,,
L5JCQE39,journalArticle,,"Wornow, Michael; Narayan, Avanika; Viggiano, Ben; Khare, Ishan S; Verma, Tathagat; Thompson, Tibor; Hernandez, Miguel Angel Fuentes; Sundar, Sudharsan; Trujillo, Chloe; Chawla, Krrish; Lu, Rongfei; Shen, Justin; Nagaraj, Divya; Martinez, Joshua; Agrawal, Vardhan; Hudson, Althea; Shah, Nigam H; Ré, Christopher",WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks,,,,"Existing ML benchmarks lack the depth and diversity of annotations needed for evaluating models on business process management (BPM) tasks. BPM is the practice of documenting, measuring, improving, and automating enterprise workflows. However, research has focused almost exclusively on one task – full end-to-end automation using agents based on multimodal foundation models (FMs) like GPT-4. This focus on automation ignores the reality of how most BPM tools are applied today – simply documenting the relevant workflow takes 60% of the time of the typical process optimization project. To address this gap we present WONDERBREAD, the first benchmark for evaluating multimodal FMs on BPM tasks beyond automation. Our contributions are: (1) a dataset containing 2928 documented workflow demonstrations; (2) 6 novel BPM tasks sourced from real-world applications ranging from workflow documentation to knowledge transfer to process improvement; and (3) an automated evaluation harness. Our benchmark shows that while state-of-the-art FMs can automatically generate documentation (e.g. recalling 88% of the steps taken in a video demonstration of a workflow), they struggle to re-apply that knowledge towards finer-grained validation of workflow completion (F1 < 0.3). We hope WONDERBREAD encourages the development of more “humancentered” AI tooling for enterprise applications and furthers the exploration of multimodal FMs for the broader universe of BPM tasks. We publish our dataset and experiments here: § https://github.com/HazyResearch/wonderbread.",,,NeurIPS.csv,,,,,,
INKTAVDD,journalArticle,,"Li, Xin; Chen, Weize; Chu, Qizhi; Li, Haopeng; Sun, Zhaojun; Li, Ran; Qian, Chen; Wei, Yiwei; Liu, Zhiyuan; Shi, Chuan; Sun, Maosong; Yang, Cheng","Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models",,,,"The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs’ proficiency of graph analysis. The benchmark, datasets and enhanced open-source models are available at https://github.com/BUPT-GAMMA/ProGraph.",,,NeurIPS.csv,,,,,,
2JMWCRKT,journalArticle,,"Lin, Kevin Qinghong; Li, Linjie; Gao, Difei; Wu, Qinchen; Yan, Mingyi; Yang, Zhengyuan; Wang, Lijuan; Shou, Mike Zheng",VideoGUI: A Benchmark for GUI Automation from Instructional Videos,,,,"Graphical User Interface (GUI) automation holds significant promise for enhancing human productivity by assisting with computer tasks. Existing task formulations primarily focus on simple tasks that can be specified by a single, language-only instruction, such as “Insert a new slide.” In this work, we introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, our benchmark focuses on tasks involving professional and novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex activities (e.g., video editing). VideoGUI evaluates GUI assistants through a hierarchical process, allowing for identification of the specific levels at which they may fail: (i) high-level planning: reconstruct procedural subtasks from visual conditions without language descriptions; (ii) middle-level planning: generate sequences of precise action narrations based on visual state (i.e., screenshot) and goals; (iii) atomic action execution: perform specific actions such as accurately clicking designated elements. For each level, we design evaluation metrics across individual dimensions to provide clear signals, such as individual performance in clicking, dragging, typing, and scrolling for atomic action execution. Our evaluation on VideoGUI reveals that even the SoTA large multimodal model GPT4o performs poorly on visual-centric GUI tasks, especially for high-level planning. The data and code are available at https://github.com/showlab/videogui.",,,NeurIPS.csv,,,,,,
T7WL6ZE5,journalArticle,,"Du, Weihua; Lyu, Qiushi; Shan, Jiaming; Qi, Zhenting; Zhang, Hongxin; Chen, Sunli; Peng, Andi; Shu, Tianmin; Lee, Kwonjoon; Dariush, Behzad; Gan, Chuang",Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge,,,,"We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. In CHAIC, the goal is for an embodied agent equipped with egocentric observations to assist a human who may be operating under physical constraints—e.g., unable to reach high places or confined to a wheelchair—in performing common household or outdoor tasks as efficiently as possible. To achieve this, a successful helper must: (1) infer the human’s intents and constraints by following the human and observing their behaviors (social perception), and (2) make a cooperative plan tailored to the human partner to solve the task as quickly as possible, working together as a team (cooperative planning). To benchmark this challenge, we create four new agents with real physical constraints and eight longhorizon tasks featuring both indoor and outdoor scenes with various constraints, emergency events, and potential risks. We benchmark planning- and learningbased baselines on the challenge and introduce a new method that leverages large language models and behavior modeling. Empirical evaluations demonstrate the effectiveness of our benchmark in enabling systematic assessment of key aspects of machine social intelligence. Our benchmark and code are publicly available at https://github.com/UMass-Foundation-Model/CHAIC.",,,NeurIPS.csv,,,,,,
UTNGHKV3,journalArticle,,"Liu, Chang; Wu, Xiwei; Feng, Yuan; Cao, Qinxiang; Yan, Junchi",Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation,,,,"Program verification is vital for ensuring software reliability, especially in the context of increasingly complex systems. Loop invariants, remaining true before and after each iteration of loops, are crucial for this verification process. Traditional provers and machine learning based methods for generating loop invariants often require expert intervention or extensive labeled data, and typically only handle numerical property verification. These methods struggle with programs involving complex data structures and memory manipulations, limiting their applicability and automation capabilities. In this paper, we introduce a new benchmark named LIG-MM, specifically for programs with complex data structures and memory manipulations. We collect 312 programs from various sources, including daily programs from college homework, the international competition (SV-COMP), benchmarks from previous papers (SLING), and programs from real-world software systems (Linux Kernel, GlibC, LiteOS, and Zephyr). Based on LIG-MM, our findings indicate that previous methods, including GPT-4, fail to automate verification for these programs. Consequently, we propose a novel LLM-SE framework that coordinates LLM with symbolic execution, fine-tuned using self-supervised learning, to generate loop invariants. Experimental results on LIG-MM demonstrate that our LLM-SE outperforms state-of-the-art methods, offering a new direction toward automated program verification in real-world scenarios.",,,NeurIPS.csv,,,,,,
F9WI5N7L,journalArticle,,"Witter, R Teal; Musco, Christopher",Benchmarking Estimators for Natural Experiments: A Novel Dataset and a Doubly Robust Algorithm,,,,,,,NeurIPS.csv,,,,,,
ZPHVHPAQ,journalArticle,,"Singh, Pragya; Budhiraja, Ritvik; Gupta, Ankush; Goswami, Anshul; Kumar, Mohan; Singh, Pushpendra",EEVR: A Dataset of Paired Physiological Signals and Textual Descriptions for Joint Emotion Representation Learning,,,,"EEVR (Emotion Elicitation in Virtual Reality) is a novel dataset specifically designed for language supervision-based pre-training of emotion recognition tasks, such as valence and arousal classification. It features high-quality physiological signals, including electrodermal activity (EDA) and photoplethysmography (PPG), acquired through emotion elicitation via 360-degree virtual reality (VR) videos. Additionally, it includes subject-wise textual descriptions of emotions experienced during each stimulus gathered from qualitative interviews. The dataset consists of recordings from 37 participants and is the first dataset to pair raw text with physiological signals, providing additional contextual information that objective labels cannot offer. To leverage this dataset, we introduced the Contrastive Language Signal Pre-training (CLSP) method, which jointly learns representations using pairs of physiological signals and textual descriptions. Our results show that integrating self-reported textual descriptions with physiological signals significantly improves performance on emotion recognition tasks, such as arousal and valence classification. Moreover, our pre-trained CLSP model demonstrates strong zero-shot transferability to existing datasets, outperforming supervised baseline models, suggesting that the representations learned by our method are more contextualized and generalized. The dataset also includes baseline models for arousal, valence, and emotion classification, as well as code for data cleaning and feature extraction. Further details and access to the dataset are available at https://melangelabiiitd.github.io/EEVR/.",,,NeurIPS.csv,,,,,,
WG77W8ZM,journalArticle,,"Cao, Ruisheng; Lei, Fangyu; Wu, Haoyuan; Chen, Jixuan; Fu, Yeqiao; Gao, Hongcheng; Xiong, Xinzhuang; Zhang, Hanchong; Mao, Yuchen; Hu, Wenjing; Xie, Tianbao; Xu, Hongshen; Zhang, Danyang; Sun, Sida Wang Ruoxi; Yin, Pengcheng; Xiong, Caiming; Ni, Ansong; Liu, Qian; Zhong, Victor; Chen, Lu; Yu, Kai; Yu, Tao",Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?,,,,"Data science and engineering workflows often span multiple stages, from warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code generation, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably automate full data workflows (14.0% success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted workspaces (10.6%). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.",,,NeurIPS.csv,,,,,,
LCHQBYGM,journalArticle,,"Wu, Kevin; Wu, Eric; Zou, James",ClashEval: Quantifying the tug-of-war between an LLM’s internal prior and external evidence,,,,"Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model’s initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs – namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.",,,NeurIPS.csv,,,,,,
H6FE2IB5,journalArticle,,"Koehler, Felix; Niedermayr, Simon; Westermann, Rüdiger; Thuerey, Nils",APEBench: A Benchmark for Autoregressive Neural Emulators of PDEs,,,,"We introduce the Autoregressive PDE Emulator Benchmark (APEBench), a comprehensive benchmark suite to evaluate autoregressive neural emulators for solving partial differential equations. APEBench is based on JAX and provides a seamlessly integrated differentiable simulation framework employing efficient pseudo-spectral methods, enabling 46 distinct PDEs across 1D, 2D, and 3D. Facilitating systematic analysis and comparison of learned emulators, we propose a novel taxonomy for unrolled training and introduce a unique identifier for PDE dynamics that directly relates to the stability criteria of classical numerical methods. APEBench enables the evaluation of diverse neural architectures, and unlike existing benchmarks, its tight integration of the solver enables support for differentiable physics training and neural-hybrid emulators. Moreover, APEBench emphasizes rollout metrics to understand temporal generalization, providing insights into the long-term behavior of emulating PDE dynamics. In several experiments, we highlight the similarities between neural emulators and numerical simulators. The code is available at https://github.com/tum-pbs/apebench and APEBench can be installed via pip install apebench.",,,NeurIPS.csv,,,,,,
QTVRLH5I,journalArticle,,"Sivasubramaniam, Sithursan; Osei-Akoto, Cedric; Zhang, Yi; Stockinger, Kurt; Fürst, Jonathan",SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark,,,,"Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far. In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy—a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs. Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last, SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.",,,NeurIPS.csv,,,,,,
ZTYZQPDF,journalArticle,,"Xu, Zhao; Liu, Fan; Liu, Hao",Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs,,,,"Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks. However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced JailTrickBench to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/JailTrickBench.",,,NeurIPS.csv,,,,,,
FNRYG9W8,journalArticle,,"Gröger, Fabian; Lionetti, Simone; Gottfrois, Philippe; Gonzalez-Jimenez, Alvaro; Amruthalingam, Ludovic; Navarini, Alexander A; Pouly, Marc",Intrinsic Self-Supervision for Data Quality Audits,,,,"Benchmark datasets in computer vision often contain off-topic images, near duplicates, and label errors, leading to inaccurate estimates of model performance. In this paper, we revisit the task of data cleaning and formalize it as either a ranking problem, which significantly reduces human inspection effort, or a scoring problem, which allows for automated decisions based on score distributions. We find that a specific combination of context-aware self-supervised representation learning and distance-based indicators is effective in finding issues without annotation biases. This methodology, which we call SELFCLEAN, surpasses state-of-the-art performance in detecting off-topic images, near duplicates, and label errors within widely-used image datasets, such as ImageNet-1k, Food-101N, and STL-10, both for synthetic issues and real contamination. We apply the detailed method to multiple image benchmarks, identify up to 16% of issues, and confirm an improvement in evaluation reliability upon cleaning. The official implementation can be found at: https://github.com/Digital-Dermatology/SelfClean.",,,NeurIPS.csv,,,,,,
34AL7D2Z,journalArticle,,"Enevoldsen, Kenneth; Kardos, Márton; Muennighoff, Niklas; Nielbo, Kristoffer Laigaard",The Scandinavian Embedding Benchmarks: Evaluating Multilingual and Monolingual Text Embedding for Scandinavian languages,,,,"The evaluation of English text embeddings has transitioned from evaluating a handful of datasets to broad coverage across many tasks through benchmarks such as MTEB. However, this is not the case for multilingual text embeddings due to a lack of available benchmarks. To address this problem, we introduce the Scandinavian Embedding Benchmark (SEB). SEB is a framework that enables text embedding evaluation for Scandinavian languages across 24 tasks, 10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26 models, uncovering signiﬁcant performance disparities between public and commercial solutions not previously captured by MTEB. We open-source SEB1 and integrate it with MTEB, thus bridging the text embedding evaluation gap for Scandinavian languages.",,,NeurIPS.csv,,,,,,
SNBEHHT7,journalArticle,,"Perron, Yohann; Sydorov, Vladyslav; Wijker, Adam P; Evans, Damian; Pottier, Christophe; Landrieu, Loic",Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era,,,,"Airborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available at https://archaeoscape.ai/data/2024), a novel large-scale archaeological ALS dataset spanning 888 km2 in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models.",,,NeurIPS.csv,,,,,,
BM7C7REU,journalArticle,,"Khirodkar, Rawal; Song, Jyun-Ting; Cao, Jinkun; Luo, Zhengyi; Kitani, Kris",Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions,,,,"Understanding how humans interact with each other is key to building realistic multi-human virtual reality systems. This area remains relatively unexplored due to the lack of large-scale datasets. Recent datasets focusing on this issue mainly consist of activities captured entirely in controlled indoor environments with choreographed actions, significantly affecting their diversity. To address this, we introduce Harmony4D, a multi-view video dataset for human-human interaction featuring in-the-wild activities such as wrestling, dancing, MMA, and more. We use a flexible multi-view capture system to record these dynamic activities and provide annotations for human detection, tracking, 2D/3D pose estimation, and mesh recovery for closely interacting subjects. We propose a novel markerless algorithm to track 3D human poses in severe occlusion and close interaction to obtain our annotations with minimal manual intervention. Harmony4D consists of 1.66 million images and 3.32 million human instances from more than 20 synchronized cameras with 208 video sequences spanning diverse environments and 24 unique subjects. We rigorously evaluate existing stateof-the-art methods for mesh recovery and highlight their significant limitations in modeling close interaction scenarios. Additionally, we fine-tune a pre-trained HMR2.0 model on Harmony4D and demonstrate an improved performance of 54.8% PVE in scenes with severe occlusion and contact. Code and data are available at https://jyuntins.github.io/harmony4d/.",,,NeurIPS.csv,,,,,,
MJUMCQ7W,journalArticle,,"Lu, Yujie; Jiang, Dongfu; Chen, Wenhu; Wang, William Yang; Choi, Yejin; Lin, Bill Yuchen",WILDVISION: Evaluating Vision-Language Models in the Wild with Human Preferences,,,,"Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WILDVISION-ARENA (WV-ARENA), an online platform that collects human preferences to evaluate VLMs. We curated WVBENCH by selecting 500 high-quality samples from 8,000 user submissions in WV-ARENA. WV-BENCH uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-ARENA Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar. Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.",,,NeurIPS.csv,,,,,,
USGKPNNC,journalArticle,,"Wu, Xueqing; Zheng, Rui; Sha, Jingzhen; Wu, Te-Lin; Zhou, Hanyu; Tang, Mohan; Chang, Kai-Wei; Peng, Nanyun; Huang, Haoran",DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation,,,,"Data analysis is a crucial analytical process essential for deriving insights from realworld databases. As shown in Figure 1, the need for data analysis typically arises from specific application scenarios, and requires diverse reasoning skills including mathematical reasoning, logical reasoning, and strategic reasoning. Existing work often focus on simple factual retrieval or arithmetic resolutions and thus are insufficient for addressing complex real-world queries. This work aims to propose new resources and benchmarks on this crucial yet challenging and under-explored task. Due to the prohibitively high cost of collecting expert annotations, we use large language models (LLMs) enhanced by code generation to automatically generate high-quality data analysis, which will later be refined by human annotators. We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ∼ 2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. Experiments show that while LLMs like GPT-4 exhibit promising data analysis capabilities, they are still evaluated as less helpful than human-written analysis on 58.1% cases. Leveraging our weak supervision data, we experiment with various fine-tuning methods, including supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Our trained model outperforms existing baselines for table question answering, and RLHF further boosts the helpfulness of generated analysis on 58.5% cases. Data and code are released at https://github.com/shirley-wu/daco.",,,NeurIPS.csv,,,,,,
62UMF5LM,journalArticle,,"Jia, Qi; Fan, Baoyu; Xu, Cong; Liu, Lu; Jin, Liang; Du, Guoguang; Guo, Zhenhua; Zhao, Yaqian; Huang, Xuanjing; Li, Rengang","Infer Induced Sentiment of Comment Response to Video: A New Task, Dataset and Baseline",,,,"Existing video multi-modal sentiment analysis mainly focuses on the sentiment expression of people within the video, yet often neglects the induced sentiment of viewers while watching the videos. Induced sentiment of viewers is essential for inferring the public response to videos and has broad application in analyzing public societal sentiment, effectiveness of advertising and other areas. The micro videos and the related comments provide a rich application scenario for viewers’ induced sentiment analysis. In light of this, we introduces a novel research task, Multimodal Sentiment Analysis for Comment Response of Video Induced(MSA-CRVI), aims to infer opinions and emotions according to comments response to micro video. Meanwhile, we manually annotate a dataset named Comment Sentiment toward to Micro Video (CSMV) to support this research. It is the largest video multi-modal sentiment dataset in terms of scale and video duration to our knowledge, containing 107, 267 comments and 8, 210 micro videos with a video duration of 68.83 hours. To infer the induced sentiment of comment should leverage the video content, we propose the Video Content-aware Comment Sentiment Analysis (VC-CSA) method as a baseline to address the challenges inherent in this new task. Extensive experiments demonstrate that our method is showing significant improvements over other established baselines. We make the dataset and source code publicly available at https://github.com/IEIT-AGI/MSA-CRVI.",,,NeurIPS.csv,,,,,,
567XI86S,journalArticle,,"Xin, Yi; Luo, Siqi; Liu, Xuyang; Du, Yuntao; Zhou, Haodi; Cheng, Xinyu; Lee, Christina; Du, Junlong; Wang, Haozhe; Chen, Mingcai; Liu, Ting; Hu, Guimin; Wan, Zhongwei; Zhang, Rongchao; Li, Aoxue; Yi, Mingyang; Liu, Xiaohong",V-PETL Bench: A Uniﬁed Visual Parameter-Efﬁcient Transfer Learning Benchmark,,,,"Parameter-efﬁcient transfer learning (PETL) methods show promise in adapting a pre-trained model to various downstream tasks while training only a few parameters. In the computer vision (CV) domain, numerous PETL algorithms have been proposed, but their direct employment or comparison remains inconvenient. To address this challenge, we construct a Uniﬁed Visual PETL Benchmark (V-PETL Bench) for the CV domain by selecting 30 diverse, challenging, and comprehensive datasets from image recognition, video action recognition, and dense prediction tasks. On these datasets, we systematically evaluate 25 dominant PETL algorithms and open-source a modular and extensible codebase for fair evaluation of these algorithms. V-PETL Bench runs on NVIDIA A800 GPUs and requires approximately 310 GPU days. We release all the benchmark, making it more efﬁcient and friendly to researchers. Additionally, V-PETL Bench will be continuously updated for new PETL algorithms and CV tasks.",,,NeurIPS.csv,,,,,,
A5DBYYPJ,journalArticle,,"Jin, Emily; Huang, Zhuoyi; Fränken, Jan-Philipp; Liu, Weiyu; Cha, Hannah; Brockbank, Erik; Wu, Sarah; Zhang, Ruohan; Wu, Jiajun; Gerstenberg, Tobias",MARPLE: A Benchmark for Long-Horizon Inference,,,,"Reconstructing past events requires reasoning across long time horizons. To figure out what happened, humans draw on prior knowledge about the world and human behavior and integrate insights from various sources of evidence including visual, language, and auditory cues. We introduce MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence. Our benchmark features agents interacting with simulated households, supporting vision, language, and auditory stimuli, as well as procedurally generated environments and agent behaviors. Inspired by classic “whodunit” stories, we ask AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened. The goal is to correctly identify the culprit as early as possible. Our findings show that human participants outperform both traditional Monte Carlo simulation methods and an LLM baseline (GPT-4) on this task. Compared to humans, traditional inference models are less robust and performant, while GPT-4 has difficulty comprehending environmental changes. We analyze factors influencing inference performance and ablate different modes of evidence, finding that all modes are valuable for performance. Overall, our experiments demonstrate that the long-horizon, multimodal inference tasks in our benchmark present a challenge to current models. Project website: https: //marple-benchmark.github.io/.",,,NeurIPS.csv,,,,,,
C5PFK9XS,journalArticle,,"Parmar, Paritosh; Peh, Eric; Chen, Ruirui; Lam, Ting En; Chen, Yuhan; Tan, Elston; Fernando, Basura",CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes,,,,"Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic “Tom and Jerry"" cartoon series. Cartoons use the principles of animation that allow animators to create expressive, unambiguous causal relationships between events to form a coherent storyline. Utilizing these properties, along with thought-provoking questions and multilevel answers (answer and detailed causal explanation), our questions involve causal chains that interconnect multiple dynamic interactions between characters and visual scenes. These factors demand models to solve more challenging, yet well-defined causal relationships. We also introduce hard incorrect answer mining, including a causally confusing version that is even more challenging. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling & joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field.",,,NeurIPS.csv,,,,,,
FQ9C5ZMX,journalArticle,,"Yukhymenko, Hanna; Staab, Robin; Vero, Mark; Vechev, Martin",A Synthetic Dataset for Personal Attribute Inference,,,,"Recently powerful Large Language Models (LLMs) have become easily accessible to hundreds of millions of users world-wide. However, their strong capabilities and vast world knowledge do not come without associated privacy risks. In this work, we focus on the emerging privacy threat LLMs pose – the ability to accurately infer personal information from online texts. Despite the growing importance of LLM-based author profiling, research in this area has been hampered by a lack of suitable public datasets, largely due to ethical and privacy concerns associated with real personal data. We take two steps to address this problem: (i) we construct a simulation framework for the popular social media platform Reddit using LLM agents seeded with synthetic personal profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic dataset of over 7800 comments manually labeled for personal attributes. We validate our dataset with a human study showing that humans barely outperform random guessing on the task of distinguishing our synthetic comments from real ones. Further, we verify that our dataset enables meaningful personal attribute inference research by showing across 18 state-of-theart LLMs that our synthetic comments allow us to draw the same conclusions as real-world data. Combined, our experimental results, dataset and pipeline form a strong basis for future privacy-preserving research geared towards understanding and mitigating inference-based privacy threats that LLMs pose.",,,NeurIPS.csv,,,,,,
SEUYPHI2,journalArticle,,"Wu, Tong; Xu, Yinghao; Po, Ryan; Zhang, Mengchen; Yang, Guandao; Wang, Jiaqi; Liu, Ziwei; Lin, Dahua; Wetzstein, Gordon",FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models,,,,"Recent advances in text-to-image generation have enabled the creation of highquality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable attributes from source images. Current methods attempt to distill identity and style from source images. However, ""style"" is a broad concept that includes texture, color, and artistic elements, but does not cover other important attributes like lighting and dynamics. Additionally, a simplified ""style"" adaptation prevents combining multiple attributes from different sources into one generated image. In this work, we formulate a more effective approach to decompose the aesthetics of a picture into specific visual attributes, letting users apply characteristics like lighting, texture, and dynamics from different images. To achieve this goal, we constructed the first fine-grained visual attributes dataset (FiVA) to the best of our knowledge. This FiVA dataset features a well-organized taxonomy for visual attributes and includes 1 M highquality generated images with visual attribute annotations. Leveraging this dataset, we propose a fine-grained visual attributes adaptation framework (FiVA-Adapter) , which decouples and adapts visual attributes from one or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes to create images that meet their unique preferences and specific content requirements. The data and models will be released at https://huggingface.co/datasets/FiVA/FiVA.",,,NeurIPS.csv,,,,,,
TWVPSICA,journalArticle,,"Du, Mingzhe; Tuan, Luu Anh; Ji, Bin; Liu, Qian; Ng, See-Kiong",Mercury: A Code Efficiency Benchmark for Code Large Language Models,,,,,,,NeurIPS.csv,,,,,,
XLDCKSKR,journalArticle,,"Mucsányi, Bálint; Kirchhof, Michael",Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks,,,,"Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one source of uncertainty. This paper presents the first benchmark of uncertainty disentanglement. We reimplement and evaluate a comprehensive range of uncertainty estimators, from Bayesian over evidential to deterministic ones, across a diverse range of uncertainty tasks on ImageNet. We find that, despite recent theoretical endeavors, no existing approach provides pairs of disentangled uncertainty estimators in practice. We further find that specialized uncertainty tasks are harder than predictive uncertainty tasks, where we observe saturating performance. Our results provide both practical advice for which uncertainty estimators to use for which specific task, and reveal opportunities for future research toward task-centric and disentangled uncertainties. All our reimplementations and Weights & Biases logs are available at https://github.com/bmucsanyi/untangle.",,,NeurIPS.csv,,,,,,
J5R99SM5,journalArticle,,"Liu, Jian; Wu, Jianyu; Xie, Hairun; Zhang, Guoqing; Wang, Jing; Liu, Wei; Ouyang, Wanli; Jiang, Junjun; Liu, Xianming; Tang, Shixiang; Zhang, Miao",AFBench: A Large-scale Benchmark for Airfoil Design,,,,,,,NeurIPS.csv,,,,,,
PPZ3UTZA,journalArticle,,"Li, Yanzhi; Li, Keqiu; Li, Guohui; Wang, Zumin; Ji, Changqing; Wang, Lubo; Zuo, Die; Guo, Qing; Zhang, Feng; Wang, Manyu; Lin, Di",Sim2Real-Fire: A Multi-modal Simulation Dataset for Forecast and Backtracking of Real-world Forest Fire,,,,"The latest research on wildfire forecast and backtracking has adopted AI models, which require a large amount of data from wildfire scenarios to capture fire spread patterns. This paper explores using cost-effective simulated wildfire scenarios to train AI models and apply them to the analysis of real-world wildfire. This solution requires AI models to minimize the Sim2Real gap, a brand-new topic in the fire spread analysis research community. To investigate the possibility of minimizing the Sim2Real gap, we collect the Sim2Real-Fire dataset that contains 1M simulated scenarios with multi-modal environmental information for training AI models. We prepare 1K real-world wildfire scenarios for testing the AI models. We also propose a deep transformer, S2R-FireTr, which excels in considering the multimodal environmental information for forecasting and backtracking the wildfire. S2R-FireTr surpasses state-of-the-art methods in real-world wildfire scenarios.",,,NeurIPS.csv,,,,,,
EZRTXFE8,journalArticle,,"Eppel, Sagi; Li, Jolina Yining; Drehwald, Manuel S; Aspuru-Guzik, Alan",Infusing Synthetic Data with Real-World Patterns for Zero-Shot Material State Segmentation,,,,"Visual recognition of materials and their states is essential for understanding the physical world, from identifying wet regions on surfaces or stains on fabrics to detecting infected areas on plants or minerals in rocks. Collecting data that captures this vast variability is complex due to the scattered and gradual nature of material states. Manually annotating real-world images is constrained by cost and precision, while synthetic data, although accurate and inexpensive, lacks real-world diversity. This work aims to bridge this gap by infusing patterns automatically extracted from real-world images into synthetic data. Hence, patterns collected from natural images are used to generate and map materials into synthetic scenes. This unsupervised approach captures the complexity of the real world while maintaining the precision and scalability of synthetic data. We also present the first comprehensive benchmark for zero-shot material state segmentation, utilizing real-world images across a diverse range of domains, including food, soils, construction, plants, liquids, and more, each appears in various states such as wet, dry, infected, cooked, burned, and many others. The annotation includes partial similarity between regions with similar but not identical materials and hard segmentation of only identical material states. This benchmark eluded top foundation models, exposing the limitations of existing data collection methods. Meanwhile, nets trained on the infused data performed significantly better on this and related tasks. The dataset, code, and trained model are available at these URLs: 1, 2, 3, 4. We also share 300,000 extracted textures and SVBRDF/PBR materials to facilitate future datasets generation at these URLs: 1,2, 3, 4.",,,NeurIPS.csv,,,,,,
3GF3HI2G,journalArticle,,"Ma, Wufei; Zhang, Guofeng; Liu, Qihao; Zeng, Guanning; Kortylewski, Adam; Liu, Yaoyao; Yuille, Alan",ImageNet3D: Towards General-Purpose Object-Level 3D Understanding,,,,"A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (e.g., class name and bounding box) and 3D information (e.g., 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments 200 categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding. Our dataset and project page are available here: https://imagenet3d.github.io.",,,NeurIPS.csv,,,,,,
H2YD2CEH,journalArticle,,"Lee, Tony; Tu, Haoqin; Wong, Chi Heem; Zheng, Wenhao; Zhou, Yiyang; Mai, Yifan; Roberts, Josselin Somerville; Yasunaga, Michihiro; Yao, Huaxiu; Xie, Cihang; Liang, Percy",VHELM: A Holistic Evaluation of Vision Language Models,,,,"Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiencyfocused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.1. VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.",,,NeurIPS.csv,,,,,,
UG9LBU46,journalArticle,,"Bhardwaj, Eshta; Gujral, Harshit; Wu, Siyi; Zogheib, Ciara; Maharaj, Tegan; Becker, Christoph",The State of Data Curation at NeurIPS: An Assessment of Dataset Development Practices in the Datasets and Benchmarks Track,,,,"Data curation is a field with origins in librarianship and archives, whose scholarship and thinking on data issues go back centuries, if not millennia. The field of machine learning is increasingly observing the importance of data curation to the advancement of both applications and fundamental understanding of machine learning models – evidenced not least by the creation of the Datasets and Benchmarks track itself. This work provides an analysis of recent dataset development practices at NeurIPS through the lens of data curation. We present an evaluation framework for dataset documentation, consisting of a rubric and toolkit developed through a thorough literature review of data curation principles. We use the framework to systematically assess the strengths and weaknesses in current dataset development practices of 60 datasets published in the NeurIPS Datasets and Benchmarks track from 2021-2023. We summarize key findings and trends. Results indicate greater need for documentation about environmental footprint, ethical considerations, and data management. We suggest targeted strategies and resources to improve documentation in these areas and provide recommendations for the NeurIPS peer-review process that prioritize rigorous data curation in ML. We also provide guidelines for dataset developers on the use of our rubric as a standalone tool. Finally, we provide results in the format of a dataset that showcases aspects of recommended data curation practices. Our rubric and results are of interest for improving data curation practices broadly in the field of ML as well as to data curation and science and technology studies scholars studying practices in ML. Our aim is to support continued improvement in interdisciplinary research on dataset practices, ultimately improving the reusability and reproducibility of new datasets and benchmarks, enabling standardized and informed human oversight, and strengthening the foundation of rigorous and responsible ML research.",,,NeurIPS.csv,,,,,,
HLKWZ4RU,journalArticle,,"Shen, Xin; Du, Heming; Sheng, Hongwei; Wang, Shuyun; Chen, Hui; Chen, Huiqiang; Wu, Zhuojie; Du, Xiaobiao; Ying, Jiaying; Lu, Ruihan; Xu, Qingzheng; Yu, Xin",MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset,,,,"Isolated Sign Language Recognition (ISLR) focuses on identifying individual sign language signs. Considering the diversity of sign languages across geographical regions, developing region-specific ISLR datasets is crucial for supporting communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale word-level dataset for the ISLR task. To fill this gap, we curate the first large-scale Multi-view Multi-modal Word-Level Australian Sign Language recognition dataset, dubbed MM-WLAuslan. Compared to other publicly available datasets, MM-WLAuslan exhibits three significant advantages: (1) the largest amount of data, (2) the most extensive vocabulary, and (3) the most diverse of multi-modal camera views. Specifically, we record 282K+ sign videos covering 3,215 commonly used Auslan glosses presented by 73 signers in a studio environment. Moreover, our filming system includes two different types of cameras, i.e., three Kinect-V2 cameras and a RealSense camera. We position cameras hemispherically around the front half of the model and simultaneously record videos using all four cameras. Furthermore, we benchmark results with state-of-the-art methods for various multi-modal ISLR settings on MM-WLAuslan, including multi-view, cross-camera, and cross-view. Experiment results indicate that MM-WLAuslan is a challenging ISLR dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide. All datasets and benchmarks are available at  MM-WLAuslan.",,,NeurIPS.csv,,,,,,
JZJNL8RM,journalArticle,,"Tang, Tianqi; Deldari, Shohreh; Xue, Hao",ViLCo-Bench: VIdeo Language COntinual learning Benchmark,,,,"Video language continual learning involves continuously adapting to information from video and text inputs, enhancing a model’s ability to handle new tasks while retaining prior knowledge. This field is a relatively under-explored area, and establishing appropriate datasets is crucial for facilitating communication and research in this field. In this study, we present the first dedicated benchmark, ViLCo-Bench, designed to evaluate continual learning models across a range of video-text tasks. The dataset comprises ten-minute-long videos and corresponding language queries collected from publicly available datasets. Additionally, we introduce a novel memory-efficient framework that incorporates self-supervised learning and mimics long-term and short-term memory effects. This framework addresses challenges including memory complexity from long video clips, natural language complexity from open queries, and text-video misalignment. We posit that ViLCo-Bench, with greater complexity compared to existing continual learning benchmarks, would serve as a critical tool for exploring the video-language domain, extending beyond conventional class-incremental tasks, and addressing complex and limited annotation issues. The curated data, evaluations, and our novel method are available at https://github.com/cruiseresearchgroup/ViLCo.",,,NeurIPS.csv,,,,,,
3SMXH6JQ,journalArticle,,"Chen, Jr-Jen; Liao, Yu-Chien; Lin, Hsi-Che; Yu, Yu-Chu; Chen, Yen-Chun; Wang, Yu-Chiang Frank",REXTIME: A Benchmark Suite for Reasoning-Across-Time in Videos,,,,"We introduce REXTIME, a benchmark designed to rigorously test AI models’ ability to perform temporal reasoning within video events. Speciﬁcally, REXTIME focuses on reasoning across time, i.e. human-like understanding when the question and its corresponding answer occur in different video segments. This form of reasoning, requiring advanced understanding of cause-and-effect relationships across video segments, poses signiﬁcant challenges to even the frontier multimodal large language models. To facilitate this evaluation, we develop an automated pipeline for generating temporal reasoning question-answer pairs, signiﬁcantly reducing the need for labor-intensive manual annotations. Our benchmark includes 921 carefully vetted validation samples and 2,143 test samples, each manually curated for accuracy and relevance. Evaluation results show that while frontier large language models outperform academic models, they still lag behind human performance by a signiﬁcant 14.3% accuracy gap. Additionally, our pipeline creates a training dataset of 9,695 machine generated samples without manual effort, which empirical studies suggest can enhance the across-time reasoning via ﬁne-tuning.",,,NeurIPS.csv,,,,,,
XJKTDMJL,journalArticle,,"Vogel, Liane; Bodensohn, Jan-Micha; Binnig, Carsten",WikiDBs: A Large-Scale Corpus of Relational Databases from Wikidata,,,,"Deep learning on tabular data, and particularly tabular representation learning, has recently gained growing interest. However, representation learning for relational databases with multiple tables is still an underexplored area, which may be attributed to the lack of openly available resources. To support the development of foundation models for tabular data and relational databases, we introduce WikiDBs, a novel open-source corpus of 100,000 relational databases. Each database consists of multiple tables connected by foreign keys. The corpus is based on Wikidata and aims to follow certain characteristics of real-world databases. In this paper, we describe the dataset and our method for creating it. By making our code publicly available, we enable others to create tailored versions of the dataset, for example, by creating databases in different languages. Finally, we conduct a set of initial experiments to showcase how WikiDBs can be used to train for data engineering tasks, such as missing value imputation and column type annotation.",,,NeurIPS.csv,,,,,,
86X4J75B,journalArticle,,"Granqvist, Filip; Song, Congzheng; Cahill, Áine; van Dalen, Rogier; Pelikan, Martin; Chan, Yi Sheng; Feng, Xiaojun; Krishnaswami, Natarajan; Jina, Vojta; Chitnis, Mona",pfl-research: simulation framework for accelerating research in Private Federated Learning,,,,"Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on large and realistic FL datasets. We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that pfl-research is 7-72× faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of benchmarks that evaluates an algorithm’s overall performance on a diverse set of realistic scenarios. The code is available on GitHub at https://github.com/apple/pfl-research.",,,NeurIPS.csv,,,,,,
JDP6NUB2,journalArticle,,"Liu, Yiran; Yang, Ke; Qi, Zehan; Liu, Xiao; Yu, Yang; Zhai, ChengXiang",Bias and Volatility: A Statistical Framework for Evaluating Large Language Model’s Stereotypes and the Associated Generation Inconsistency,,,,"We present a novel statistical framework for analyzing stereotypes in large language models (LLMs) by systematically estimating the bias and variation in their generation. Current evaluation metrics in the alignment literature often overlook the randomness of stereotypes caused by the inconsistent generative behavior of LLMs. For example, this inconsistency can result in LLMs displaying contradictory stereotypes, including those related to gender or race, for identical professions across varied contexts. Neglecting such inconsistency could lead to misleading conclusions in alignment evaluations and hinder the accurate assessment of the risk of LLM applications perpetuating or amplifying social stereotypes and unfairness. This work proposes a Bias-Volatility Framework (BVF) that estimates the probability distribution function of LLM stereotypes. Specifically, since the stereotype distribution fully captures an LLM’s generation variation, BVF enables the assessment of both the likelihood and extent to which its outputs are against vulnerable groups, thereby allowing for the quantification of the LLM’s aggregated discrimination risk. Furthermore, we introduce a mathematical framework to decompose an LLM’s aggregated discrimination risk into two components: bias risk and volatility risk, originating from the mean and variation of LLM’s stereotype distribution, respectively. We apply BVF to assess 12 commonly adopted LLMs and compare their risk levels. Our findings reveal that: i) Bias risk is the primary cause of discrimination risk in LLMs; ii) Most LLMs exhibit significant pro-male stereotypes for nearly all careers; iii) Alignment with reinforcement learning from human feedback lowers discrimination by reducing bias, but increases volatility; iv) Discrimination risk in LLMs correlates with key sociol-economic factors like professional salaries. Finally, we emphasize that BVF can also be used to assess other dimensions of generation inconsistency’s impact on LLM behavior beyond stereotypes, such as knowledge mastery.",,,NeurIPS.csv,,,,,,
5KD5ZF9Y,journalArticle,,"Lyu, Jiafei; Xu, Kang; Xu, Jiacheng; Yan, Mengbei; Yang, Jingwen; Zhang, Zongzhang; Bai, Chenjia; Lu, Zongqing; Li, Xiu",ODRL: A Benchmark for Off-Dynamics Reinforcement Learning,,,,"We consider off-dynamics reinforcement learning (RL) where one needs to transfer policies across different domains with dynamics mismatch. Despite the focus on developing dynamics-aware algorithms, this ﬁeld is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ODRL, the ﬁrst benchmark tailored for evaluating off-dynamics RL methods. ODRL contains four experimental settings where the source and target domains can be either online or ofﬂine, and provides diverse tasks and a broad spectrum of dynamics shifts, making it a reliable platform to comprehensively evaluate the agent’s adaptation ability to the target domain. Furthermore, ODRL includes recent off-dynamics RL algorithms in a uniﬁed framework and introduces some extra baselines for different settings, all implemented in a single-ﬁle manner. To unpack the true adaptation capability of existing methods, we conduct extensive benchmarking experiments, which show that no method has universal advantages across varied dynamics shifts. We hope this benchmark can serve as a cornerstone for future research endeavors. Our code is publicly available at https://github.com/OffDynamicsRL/off-dynamics-rl.",,,NeurIPS.csv,,,,,,
Z3JITVUR,journalArticle,,"Lozano, Alejandro; Nirschl, Jeffrey; Burgess, James; Gupte, Sanket Rajan; Zhang, Yuhui; Unell, Alyssa; Yeung-Levy, Serena",Micro-Bench: A Vision-Language Benchmark for Microscopy Understanding,,,,"Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing researchers’ efficiency, identifying new image biomarkers, and accelerating hypothesis generation and scientific discovery. However, there is a lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs’ perception and cognition capabilities in biological image understanding. To address this gap, we introduce Micro-Bench, an expert-curated benchmark encompassing 24 biomedical tasks across various scientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light), scales (subcellular, cellular, tissue), and organisms in both normal and abnormal states. We evaluate state-ofthe-art biomedical, pathology, and general VLMs on Micro-Bench and find that: i) current models struggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii) current specialist models fine-tuned on biomedical data often perform worse than generalist models; iii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior biomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned and pretrained models offers one solution to forgetting and improves general performance across biomedical tasks. We release Micro-Bench under a permissive license 2 to accelerate the research and development of microscopy foundation models.",,,NeurIPS.csv,,,,,,
FTMFIRR5,journalArticle,,"Gabriel, Wassim; Shouman, Omar; Schroeder, Ayla; Boessl, Florian; Wilhelm, Mathias",PROSPECT PTMs: Rich Labeled Tandem Mass Spectrometry Dataset of Modified Peptides for Machine Learning in Proteomics,,,,"Post-Translational Modifications (PTMs) are changes that occur in proteins after synthesis, influencing their structure, function, and cellular behavior. PTMs are essential in cell biology; they regulate protein function and stability, are involved in various cellular processes, and are linked to numerous diseases. A particularly interesting class of PTMs are chemical modifications such as phosphorylation introduced on amino acid side chains because they can drastically alter the physicochemical properties of the peptides once they are present. One or more PTMs can be attached to each amino acid of the peptide sequence. The most commonly applied technique to detect PTMs on proteins is bottom-up Mass Spectrometrybased proteomics (MS), where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). While an increasing number of machine learning models are published focusing on MS/MS-related property prediction of unmodified peptides, high-quality reference data for modified peptides is missing, impeding model development for this important class of peptides. To enable researchers to train machine learning models that can accurately predict the properties of modified peptides, we introduce four high-quality labeled datasets for applying machine and deep learning to tasks in MS-based proteomics. The four datasets comprise several subgroups of peptides with 1.2 million unique modified peptide sequences and 30 unique pairs of (amino-acid, PTM), covering both experimentally introduced and naturally occurring modifications on various amino acids. We evaluate the utility and importance of the dataset by providing benchmarking results on models trained with and without modifications and highlighting the impact of including modified sequences on downstream tasks. We demonstrate that predicting the properties of modified peptides is more challenging but has a broad impact since they are often the core of protein functionality and its regulation, and they have a potential role as biomarkers in clinical applications. Our datasets contribute to applied machine learning in proteomics by enabling the research community to experiment with methods to encode PTMs as model inputs and to benchmark against reference data for model comparison. With a proper data split for three common tasks in proteomics, we provide a robust way to evaluate model performance and assess generalization on unseen modified sequences.",,,NeurIPS.csv,,,,,,
B7MIY9Z7,journalArticle,,"Cui, Hejie; Mao, Lingjun; Liang, Xin; Zhang, Jieyu; Ren, Hui; Li, Quanzheng; Li, Xiang; Yang, Carl",Biomedical Visual Instruction Tuning with Clinician Preference Alignment,,,,"Recent advancements in multimodal foundation models have showcased impressive capabilities in understanding and reasoning with visual and textual information. Adapting these foundation models trained for general usage to specialized domains like biomedicine requires large-scale domain-specific instruction datasets. While existing works have explored curating such datasets automatically, the resultant datasets are not explicitly aligned with domain expertise. In this work, we propose a data-centric framework, Biomedical Visual Instruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that incorporates clinician preferences into both stages of generating and selecting instruction data for tuning biomedical multimodal foundation models. First, during the generation stage, we prompt the GPT-4V generator with a diverse set of clinician-selected demonstrations for preference-aligned data candidate generation. Then, during the selection phase, we train a separate selection model, which explicitly distills clinician and policy-guided model preferences into a rating function to select high-quality data for medical instruction tuning. Results show that the model tuned with the instruction data from our method demonstrates a significant improvement in open visual chat (18.5% relatively) and medical VQA (win rate up to 81.73%). Our instruction-following data, models, and code are available at https://BioMed-VITAL.github.io.",,,NeurIPS.csv,,,,,,
TIYY8HA5,journalArticle,,"Wu, Jin; Zhou, Haoying; Kazanzides, Peter; Munawar, Adnan; Liu, Anqi",SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy Learning and Benchmarking,,,,,,,NeurIPS.csv,,,,,,
4ELCNUK5,journalArticle,,"Hou, Yufang; Pascale, Alessandra; Carnerero-Cano, Javier; Tchrakian, Tigran; Marinescu, Radu; Daly, Elizabeth; Padhi, Inkit; Sattigeri, Prasanna",WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia,,,,"Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 highquality, human-annotated instances designed to assess the performance of LLMs in providing a complete perspective on conflicts from the retrieved documents, rather than choosing one answer over another, when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances. To facilitate future work, we release WikiContradict at https://ibm.biz/wikicontradict.",,,NeurIPS.csv,,,,,,
5WQYEJXI,journalArticle,,"Brahman, Faeze; Kumar, Sachin; Balachandran, Vidhisha; Dasigi, Pradeep; Pyatkin, Valentina; Ravichander, Abhilasha; Wiegreffe, Sarah; Dziri, Nouha; Chandu, Khyathi; Hessel, Jack; Tsvetkov, Yulia; Smith, Noah A; Choi, Yejin; Hajishirzi, Hannaneh",The Art of Saying No: Contextual Noncompliance in Language Models,,,,"Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of “unsafe” queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a syntheticallygenerated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.",,,NeurIPS.csv,,,,,,
X5DZPYEI,journalArticle,,"Brennan, Connor; Williams, Andrew Robert; Younis, Omar G; Vyas, Vedant; Yasafova, Daria; Rish, Irina",Using Unity to Help Solve Reinforcement Learning,,,,"Leveraging the depth and flexibility of XLand as well as the rapid prototyping features of the Unity engine, we present the United Unity Universe, an open-source toolkit designed to accelerate the creation of innovative reinforcement learning environments. This toolkit includes a robust implementation of OpenXLand, a framework for meta-RL based on XLand 2.0 [23], complemented by a user-friendly interface which allows users to modify the details of procedurally generated terrains and task rules with ease. Along with a ready-to-use implementation of OpenXLand, we provide a curated selection of terrains and rule sets, accompanied by implementations of reinforcement learning baselines to facilitate quick experimentation with novel architectural designs for adaptive agents. Furthermore, we illustrate how the United Unity Universe serves as a high-level language that enables researchers to develop diverse and endlessly variable 3D environments within a unified framework. This functionality establishes the United Unity Universe (U3) as an essential tool for advancing the field of reinforcement learning, especially in the development of adaptive and generalizable learning systems.",,,NeurIPS.csv,,,,,,
M6XBYWLN,journalArticle,,"Yeh, Chen; Chang, You-Ming; Chiu, Wei-Chen; Yu, Ning",T2Vs Meet VLMs: A Scalable Multimodal Dataset for Visual Harmfulness Recognition,,,,,,,NeurIPS.csv,,,,,,
KQNSFBT9,journalArticle,,"Xie, Tianbao; Zhang, Danyang; Chen, Jixuan; Li, Xiaochuan; Zhao, Siheng; Cao, Ruisheng; Hua, Toh Jing; Cheng, Zhoujun; Shin, Dongchan; Lei, Fangyu; Liu, Yitao; Xu, Yiheng; Zhou, Shuyan; Savarese, Silvio; Xiong, Caiming; Zhong, Victor; Yu, Tao",OSWORLD: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments,,,,"Autonomous agents that accomplish complex computer tasks with minimal human interventions can significantly enhance accessibility and productivity of humancomputer interactions. Existing benchmarks either lack interactive environments or are limited to specific applications/domains, failing to reflect the diversity and complexity of real-world computer use and limiting agent scalability. We introduce OSWORLD, the first-of-its-kind scalable real computer environment for multimodal agents, supporting task setup, interactive learning, and execution-based evaluation of open-ended computer tasks across arbitrary applications in Ubuntu, Windows, and macOS. Using OSWORLD, we create a benchmark of 369 tasks involving real web and desktop apps in open domains, OS file I/O, and multi-app workflows. Each example derives from real-world use cases and includes detailed setup and execution-based evaluation for reproducibility. Extensive evaluation of state-of-theart LLM/VLM agents on OSWORLD reveals deficiencies in their ability to serve as computer assistants. While humans accomplish 72.4% of the tasks, the best agents achieve <12.2%, struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWORLD provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Implementation and experiments are at https://os-world.github.io.",,,NeurIPS.csv,,,,,,
BNEBLIEK,journalArticle,,"Zhang, Xiaoyuan; Zhao, Liang; Yu, Yingying; Lin, Xi; Chen, Yifan; Zhao, Han; Zhang, Qingfu",LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch,,,,"Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, fairness, robustness, and more. Unlike single-objective optimization, which aggregates objectives into a scalar through weighted sums, MOPs focus on generating specific or diverse Pareto solutions and learning the entire Pareto set directly. Existing MOP benchmarks primarily focus on evolutionary algorithms, which are zeroth-order or meta-heuristic methods that fail to leverage higher-order objective information and cannot scale to large models. To address these challenges, we introduce LibMOON, the first multiobjective optimization library supporting state-of-the-art gradient-based methods, offering a fair and comprehensive benchmark, and open-sourced for the community.",,,NeurIPS.csv,,,,,,
VNJDI8L3,journalArticle,,"Szałata, Artur; Benz, Andrew; Cannoodt, Robrecht; Cortes, Mauricio; Fong, Jason; Kuppasani, Sunil; Lieberman, Richard; Liu, Tianyu; Mas-Rosario, Javier A; Meinl, Rico; Nourisa, Jalil; Tumiel, Jared; Tunjic, Tin M; Wang, Mengbo; Weber, Noah; Zhao, Hongyu; Anchang, Benedict; Theis, Fabian J; Luecken, Malte D; Burkhardt, Daniel B",A benchmark for prediction of transcriptomic responses to chemical perturbations across cell types,,,,,,,NeurIPS.csv,,,,,,
LAN2R3XZ,journalArticle,,"Han, Tessa; Kumar, Aounon; Agarwal, Chirag; Lakkaraju, Himabindu",MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models,,,,"As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench.",,,NeurIPS.csv,,,,,,
EXZXGEMC,journalArticle,,"Awadalla, Anas; Xue, Le; Lo, Oscar; Shu, Manli; Lee, Hannah; Guha, Etash; Jordan, Matt; Shen, Sheng; Awadalla, Mohamed; Savarese, Silvio; Xiong, Caiming; Xu, Ran; Choi, Yejin; Schmidt, Ludwig",Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens,,,,"Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, open-source multimodal interleaved datasets. In response, we introduce MINT-1T, the most extensive and diverse open-source Multimodal INTerleaved dataset to date. MINT-1T comprises of one trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS. We release our data at https://github.com/mlfoundations/MINT-1T.",,,NeurIPS.csv,,,,,,
EQT5I7LK,journalArticle,,"Liu, Zuxin; Hoang, Thai; Zhang, Jianguo; Zhu, Ming; Lan, Tian; Kokane, Shirley; Tan, Juntao; Yao, Weiran; Liu, Zhiwei; Feng, Yihao; Murthy, Rithesh; Yang, Liangwei; Savarese, Silvio; Niebles, Juan Carlos; Wang, Huan; Heinecke, Shelby; Xiong, Caiming",APIGen: Automated PIpeline for Generating Verifiable and Diverse Function-Calling Datasets,,,,"The advancement of function-calling agent models requires diverse, reliable, and high-quality datasets. This paper presents APIGen, an automated data generation pipeline designed to synthesize high-quality datasets for function-calling applications. We leverage APIGen and collect 3,673 executable APIs across 21 different categories to generate diverse function-calling datasets in a scalable and structured manner. Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, improving its reliability and correctness. We demonstrate that models trained with our curated datasets, even with only 7B parameters, can achieve state-of-the-art performance on the Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models. Moreover, our 1B model achieves exceptional performance, surpassing GPT-3.5Turbo and Claude-3 Haiku. We release a dataset containing 60,000 high-quality entries, aiming to advance the field of function-calling agent domains. The dataset is available on Huggingface 1 and the project homepage 2.",,,NeurIPS.csv,,,,,,
IXKABTNS,journalArticle,,"Ma, Qi; Paudel, Danda Pani; Konukoglu, Ender; Gool, Luc Van",Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes,,,,,,,NeurIPS.csv,,,,,,
VUDH7H9G,journalArticle,,"Roush, Allen; Shabazz, Yusuf; Balaji, Arvind; Zhang, Peter; Mezza, Stefano; Zhang, Markus; Basu, Sanjay; Vishwanath, Sriram; Fatemi, Mehdi; Ziv, Ravid Shwartz",OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset,,,,"We introduce OpenDebateEvidence, a comprehensive dataset for argument mining and summarization sourced from the American Competitive Debate community. This dataset includes over 3.5 million documents with rich metadata, making it one of the most extensive collections of debate evidence. OpenDebateEvidence captures the complexity of arguments in high school and college debates, providing valuable resources for training and evaluation. Our extensive experiments demonstrate the efficacy of fine-tuning state-of-the-art large language models for argumentative abstractive summarization across various methods, models, and datasets. By providing this comprehensive resource, we aim to advance computational argumentation and support practical applications for debaters, educators, and researchers. OpenDebateEvidence is publicly available to support further research and innovation in computational argumentation. Access it here: https://huggingface.co/datasets/Yusuf5/OpenCaselist.",,,NeurIPS.csv,,,,,,
6UU7X73J,journalArticle,,"Caciularu, Avi; Jacovi, Alon; Ben-David, Eyal; Goldshtein, Sasha; Schuster, Tal; Herzig, Jonathan; Elidan, Gal; Globerson, Amir",TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools,,,,"Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT—Text And Calculations through Tables, a dataset crafted to evaluate LLMs’ reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as IE as a tool. Specifically, we propose to add “tools” for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks.",,,NeurIPS.csv,,,,,,
HYJTC287,journalArticle,,"Saparina, Irina; Lapata, Mirella",: A Benchmark for Parsing Ambiguous Questions into Database Queries,,,,"Practical semantic parsers are expected to understand user utterances and map them to executable programs, even when these are ambiguous. We introduce a new benchmark, , which we hope will inform and inspire the development of text-to-SQL parsers capable of recognizing and interpreting ambiguous requests. Our dataset contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries. In each case, the ambiguity persists even when the database context is provided. This is achieved through a novel approach that involves controlled generation of databases from scratch. We benchmark various LLMs on , revealing that even the most advanced models struggle to identify and interpret ambiguity in questions.",,,NeurIPS.csv,,,,,,
V35WWXH2,journalArticle,,"Liu, Chang; Saul, Rebecca; Sun, Yihao; Raff, Edward; Fuchs, Maya; Pantano, Townsend Southard; Holt, James; Micinski, Kristopher",ASSEMBLAGE: Automatic Binary Dataset Construction for Machine Learning,,,,"Binary code is pervasive, and binary analysis is a key task in reverse engineering, malware classification, and vulnerability discovery. Unfortunately, while there exist large corpora of malicious binaries, obtaining high-quality corpora of benign binaries for modern systems has proven challenging (e.g., due to licensing issues). Consequently, machine learning based pipelines for binary analysis utilize either costly commercial corpora (e.g., VirusTotal) or open-source binaries (e.g., coreutils) available in limited quantities. To address these issues, we present ASSEMBLAGE: an extensible distributed system that crawls, configures, and builds Windows PE binaries to obtain high-quality binary corpora suitable for training state-of-the-art models in binary analysis. We have run ASSEMBLAGE on AWS over the past year, producing 890k Windows PE and 428k Linux ELF binaries across 29 configurations. ASSEMBLAGE is designed to be both reproducible and extensible, enabling users to publish “recipes” for their datasets, and facilitating the extraction of a wide array of features. We evaluated ASSEMBLAGE by using its data to train modern learning-based pipelines for compiler provenance and binary function similarity. Our results illustrate the practical need for robust corpora of high-quality Windows PE binaries in training modern learning-based binary analyses. ASSEMBLAGE code is open sourced under the MIT license, and the dataset can be downloaded from https://assemblage-dataset.net/.",,,NeurIPS.csv,,,,,,
FQY6HGTU,journalArticle,,"Victor, Brandon; Letard, Mathilde; Naylor, Peter; Douch, Karim; Longépé, Nicolas; He, Zhen; Ebel, Patrick",Off to new Shores: A Dataset & Benchmark for (near-)coastal Flood Inundation Forecasting,,,,"Floods are among the most common and devastating natural hazards, imposing immense costs on our society and economy due to their disastrous consequences. Recent progress in weather prediction and spaceborne flood mapping demonstrated the feasibility of anticipating extreme events and reliably detecting their catastrophic effects afterwards. However, these efforts are rarely linked to one another and there is a critical lack of datasets and benchmarks to enable the direct forecasting of flood extent. To resolve this issue, we curate a novel dataset enabling a timely prediction of flood extent. Furthermore, we provide a representative evaluation of state-of-the-art methods, structured into two benchmark tracks for forecasting flood inundation maps i) in general and ii) focused on coastal regions. Altogether, our dataset and benchmark provide a comprehensive platform for evaluating flood forecasts, enabling future solutions for this critical challenge. Data, code & models are shared at https://github.com/Multihuntr/GFF under a CC0 license.",,,NeurIPS.csv,,,,,,
XSWTVBC4,journalArticle,,"Wang, Zhecan; Liu, Junzhang; Tang, Chia-Wei; Alomari, Hani; Sivakumar, Anushka; Sun, Rui; Li, Wenhao; Ayyubi, Hammad; You, Haoxuan; Ishmam, Alvi; Chang, Kai-Wei; Chang, Shih-Fu; Thomas, Chris",JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images,,,,,,,NeurIPS.csv,,,,,,
4BQ4JIFR,journalArticle,,"Tian, Minyang; Gao, Luyu; Zhang, Shizhuo Dylan; Chen, Xinan; Fan, Cunwei; Guo, Xuefei; Haas, Roland; Ji, Pan; Krongchon, Kittithat; Li, Yao; Liu, Shengyan; Luo, Di; Ma, Yutao; Tong, Hao; Trinh, Kha; Tian, Chenyu; Wang, Zihan; Wu, Bohao; Xiong, Yanyu; Yin, Shengzhu; Zhu, Minhui; Lieret, Kilian; Lu, Yanxin; Liu, Genglin; Du, Yufeng; Tao, Tianhua; Press, Ofir; Callan, Jamie; Huerta, Eliu; Peng, Hao",SciCode: A Research Coding Benchmark Curated by Scientists,,,,,,,NeurIPS.csv,,,,,,
H4M2DIFZ,journalArticle,,"Gastinger, Julia; Huang, Shenyang; Galkin, Mikhail; Loghmani, Erfan; Parviz, Ali; Poursafaei, Farimah; Danovitch, Jacob; Rossi, Emanuele; Koutis, Ioannis; Stuckenschmidt, Heiner; Rabbany, Reihaneh; Rabusseau, Guillaume",TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous Graphs,,,,"Multi-relational temporal graphs are powerful tools for modeling real-world data, capturing the evolving and interconnected nature of entities over time. Recently, many novel models are proposed for ML on such graphs intensifying the need for robust evaluation and standardized benchmark datasets. However, the availability of such resources remains scarce and evaluation faces added complexity due to reproducibility issues in experimental protocols. To address these challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel benchmarking framework tailored for evaluating methods for predicting future links on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a focus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0 facilitates comprehensive evaluations by presenting eight novel datasets spanning five domains with up to 53 million edges. TGB 2.0 datasets are significantly larger than existing datasets in terms of number of nodes, edges, or timestamps. In addition, TGB 2.0 provides a reproducible and realistic evaluation pipeline for multi-relational temporal graphs. Through extensive experimentation, we observe that 1) leveraging edge-type information is crucial to obtain high performance, 2) simple heuristic baselines are often competitive with more complex methods, 3) most methods fail to run on our largest datasets, highlighting the need for research on more scalable methods.",,,NeurIPS.csv,,,,,,
G23LTUGD,journalArticle,,"Muschalik, Maximilian; Baniecki, Hubert; Fumagalli, Fabian; Kolpaczki, Patrick; Hammer, Barbara; Hüllermeier, Eyke",shapiq: Shapley Interactions for Machine Learning,,,,"Originally rooted in game theory, the Shapley Value (SV) has recently become an important tool in machine learning research. Perhaps most notably, it is used for feature attribution and data valuation in explainable artificial intelligence. Shapley Interactions (SIs) naturally extend the SV and address its limitations by assigning joint contributions to groups of entities, which enhance understanding of black box machine learning models. Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources. In this work, we introduce shapiq, an open-source Python package that unifies state-of-the-art algorithms to efficiently compute SVs and any-order SIs in an application-agnostic framework. Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains. For practitioners, shapiq is able to explain and visualize any-order feature interactions in predictions of models, including vision transformers, language models, as well as XGBoost and LightGBM with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and consolidate the application of SVs and SIs in machine learning that facilitates future research. The source code and documentation are available at https://github.com/mmschlk/shapiq.",,,NeurIPS.csv,,,,,,
THDTMSVY,journalArticle,,"Gu, Tianle; Zhou, Zeyang; Huang, Kexin; Liang, Dandan; Wang, Yixu; Zhao, Haiquan; Yao, Yuanqi; Qiao, Xingge; Wang, Keqing; Yang, Yujiu; Teng, Yan; Qiao, Yu; Wang, Yingchun",MLLMGUARD: A Multi-dimensional Safety Evaluation Suite,,,,,,,NeurIPS.csv,,,,,,
TBDP6JEM,journalArticle,,"Wang, Minjie; Gan, Quan; Wipf, David; Cai, Zhenkun; Li, Ning; Tang, Jianheng; Zhang, Yanlin; Zhang, Zizhao; Mao, Zunyao; Song, Yakun; Wang, Yanbo; Li, Jiahang; Zhang, Han; Yang, Guang; Qin, Xiao; Lei, Chuan; Zhang, Muhan; Zhang, Weinan; Faloutsos, Christos; Zhang, Zheng",4DBInfer: A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on RDBs,,,,"Given a relational database (RDB), how can we predict missing column values in some target table of interest? Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing. This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes. As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics. To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs. Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks. From a delivery standpoint, we operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer; please see https://github.com/awslabs/multi-table-benchmark/.",,,NeurIPS.csv,,,,,,
SLDHIRFS,journalArticle,,"Li, Zhuofeng; Gou, Zixing; Zhang, Xiangnan; Liu, Zhongyuan; Li, Sirui; Hu, Yuntong; Ling, Chen; Zhang, Zheng; Zhao, Liang",TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs,,,,"Text-Attributed Graphs (TAGs) augment graph structures with natural language descriptions, facilitating detailed depictions of data and their interconnections across various real-world settings. However, existing TAG datasets predominantly feature textual information only at the nodes, with edges typically represented by mere binary or categorical attributes. This lack of rich textual edge annotations significantly limits the exploration of contextual relationships between entities, hindering deeper insights into graph-structured data. To address this gap, we introduce Textual-Edge Graphs Datasets and Benchmark (TEG-DB), a comprehensive and diverse collection of benchmark textual-edge datasets featuring rich textual descriptions on nodes and edges. The TEG-DB datasets are large-scale and encompass a wide range of domains, from citation networks to social networks. In addition, we conduct extensive benchmark experiments on TEG-DB to assess the extent to which current techniques, including pre-trained language models (PLMs), graph neural networks (GNNs), proposed novel entangled GNNs and their combinations, can utilize textual node and edge information. Our goal is to elicit advancements in textual-edge graph research, specifically in developing methodologies that exploit rich textual node and edge descriptions to enhance graph analysis and provide deeper insights into complex real-world networks. The entire TEG-DB project is publicly accessible as an open-source repository on Github, accessible at https://github.com/Zhuofeng-Li/TEG-Benchmark.",,,NeurIPS.csv,,,,,,
QL9APSZS,journalArticle,,"Monteiro, João; Noël, Pierre-André; Marcotte, Étienne; Rajeswar, Sai; Zantedeschi, Valentina; Vázquez, David; Chapados, Nicolas; Pal, Christopher; Taslakian, Perouz",REPLIQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content,,,,,,,NeurIPS.csv,,,,,,
WEW4SY6R,journalArticle,,"Shomee, Homaira Huda; Wang, Zhu; Medya, Sourav; Ravi, Sathya N",IMPACT: A Large-scale Integrated Multimodal Patent Analysis and Creation Dataset for Design Patents,,,,"In this paper, we introduce IMPACT (Integrated Multimodal Patent Analysis and CreaTion Dataset for Design Patents), a large-scale multimodal patent dataset with detailed captions for design patent figures. Our dataset includes half a million design patents comprising 3.61 million figures along with captions from patents granted by the United States Patent and Trademark Office (USPTO) over a 16year period from 2007 to 2022. We incorporate the metadata of each patent application with elaborate captions that are coherent with multiple viewpoints of designs. Even though patents themselves contain a variety of design figures, titles, and descriptions of viewpoints, we find that they lack detailed descriptions that are necessary to perform multimodal tasks such as classification and retrieval. IMPACT closes this gap thereby providing researchers with necessary ingredients to instantiate a variety of multimodal tasks. Our dataset has a huge potential for novel design inspiration and can be used with advanced computer vision models in tandem. We perform preliminary evaluations on the dataset on the popular patent analysis tasks such as classification and retrieval. Our results indicate that integrating images with generated captions significantly improves the performance of different models on the corresponding tasks. Given that design patents offer various benefits for modeling novel tasks, we propose two standard computer vision tasks that have not been investigated in analyzing patents as future directions using IMPACT as a benchmark viz., 3D Image Construction and Visual Question Answering (VQA). To facilitate research in these directions, we make our IMPACT dataset and the code/models used in this work publicly available here.",,,NeurIPS.csv,,,,,,
5E7P5EAS,preprint,2024.0,"Salehi, Mohammadreza; Park, Jae Sung; Yadav, Tanush; Kusupati, Aditya; Krishna, Ranjay; Choi, Yejin; Hajishirzi, Hannaneh; Farhadi, Ali",ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition,,10.48550/arXiv.2410.05774,http://arxiv.org/abs/2410.05774,"Our world is full of varied actions and moves across specialized domains that we, as humans, strive to identify and understand. Within any single domain, actions can often appear quite similar, making it challenging for deep models to distinguish them accurately. To evaluate the effectiveness of multimodal foundation models in helping us recognize such actions, we present ActionAtlas v1.0, a multiple-choice video question-answering benchmark featuring short videos across various sports. Each video in the dataset is paired with a question and four or five choices. The question pinpoints specific individuals, asking which choice “best” describes their action within a certain temporal context. Overall, the dataset includes 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices. Unlike most existing video question answering benchmarks that only cover simplistic actions, often identifiable from a single frame, ActionAtlas focuses on intricate movements and rigorously tests the model’s capability to discern subtle differences between moves that look similar within each domain. We evaluate open and proprietary foundation models on this benchmark, finding that the best model, GPT-4o, achieves a maximum accuracy of 45.52%. Meanwhile, Nonexpert crowd workers, provided with action description for each choice, achieve 61.64% accuracy, where random chance is approximately 21%. Our findings with state-of-the-art models indicate that having a high frame sampling rate is important for accurately recognizing actions in ActionAtlas, a feature that some leading proprietary video models, such as Gemini, do not include in their default configurations.",2024-11-11,,NeurIPS.csv,,,,,,
V85SSK78,journalArticle,,"Liu, Sizhe; Xia, Jun; Zhang, Lecheng; Liu, Yuchen; Liu, Yue; Du, Wenjie; Gao, Zhangyang; Hu, Bozhen; Tan, Cheng; Xiang, Hongxin; Li, Stan Z",FlexMol: A Flexible Toolkit for Benchmarking Molecular Relational Learning,,,,"Molecular relational learning (MRL) is crucial for understanding the interaction behaviors between molecular pairs, a critical aspect of drug discovery and development. However, the large feasible model space of MRL poses significant challenges to benchmarking, and existing MRL frameworks face limitations in flexibility and scope. To address these challenges, avoid repetitive coding efforts, and ensure fair comparison of models, we introduce FlexMol, a comprehensive toolkit designed to facilitate the construction and evaluation of diverse model architectures across various datasets and performance metrics. FlexMol offers a robust suite of preset model components, including 16 drug encoders, 13 protein sequence encoders, 9 protein structure encoders, and 7 interaction layers. With its easy-to-use API and flexibility, FlexMol supports the dynamic construction of over 70, 000 distinct combinations of model architectures. Additionally, we provide detailed benchmark results and code examples to demonstrate FlexMol’s effectiveness in simplifying and standardizing MRL model development and comparison. FlexMol is open-sourced and available at https://github.com/Steven51516/FlexMol.",,,NeurIPS.csv,,,,,,
4525YUJ6,journalArticle,,"Ma, Zeyao; Zhang, Bohan; Zhang, Jing; Yu, Jifan; Zhang, Xiaokang; Zhang, Xiaohan; Luo, Sijia; Wang, Xi; Tang, Jie",SPREADSHEETBENCH: Towards Challenging Real World Spreadsheet Manipulation,,,,"We introduce SPREADSHEETBENCH, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SPREADSHEETBENCH is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-ofthe-art (SOTA) models and human performance, highlighting the benchmark’s difficulty.",,,NeurIPS.csv,,,,,,
5CLKKVLP,journalArticle,,"Li, Manling; Zhao, Shiyu; Wang, Qineng; Wang, Kangrui; Zhou, Yu; Srivastava, Sanjana; Gokmen, Cem; Lee, Tony; Li, Li Erran; Zhang, Ruohan; Liu, Weiyu; Liang, Percy; Fei-Fei, Li; Mao, Jiayuan; Wu, Jiajun",Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making,,,,"We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (EMBODIED AGENT INTERFACE) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics that break down evaluation into error types, such as hallucination errors, affordance errors, and various types of planning errors. Overall, our benchmark offers a comprehensive assessment of LLMs’ performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights into the effective and selective use of LLMs in embodied decision making.",,,NeurIPS.csv,,,,,,
3XMFNKZ5,journalArticle,,"Wei, Boyi; Shi, Weijia; Huang, Yangsibo; Smith, Noah A; Zhang, Chiyuan; Zettlemoyer, Luke; Li, Kai; Henderson, Peter",Evaluating Copyright Takedown Methods for Language Models,,,,"Language models (LMs) derive their capabilities from extensive training on diverse data, including potentially copyrighted material. These models can memorize and generate content similar to their training data, posing potential concerns. Therefore, model creators are motivated to develop mitigation methods that prevent generating protected content. We term this procedure as copyright takedowns for LMs, noting the conceptual similarity to (but legal distinction from) the Digital Millennium Copyright Act (DMCA) takedown This paper introduces the first evaluation of the feasibility and side effects of copyright takedowns for LMs. We propose COTAEVAL, an evaluation framework to assess the effectiveness of copyright takedown methods, the impact on the model’s ability to retain uncopyrightable factual knowledge from the training data whose recitation is embargoed, and how well the model maintains its general utility and efficiency. We examine several strategies, including adding system prompts, decoding-time filtering interventions, and unlearning approaches. Our findings indicate that no tested method excels across all metrics, showing significant room for research in this unique problem setting and indicating potential unresolved challenges for live policy proposals.",,,NeurIPS.csv,,,,,,
2Z7FZZFZ,journalArticle,,"Liu, Chu’nan; Denzler, Lilian; Chen, Yihong; Martin, Andrew; Paige, Brooks",AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction,,,,"Epitope identification is vital for antibody design yet challenging due to the inherent variability in antibodies. While many deep learning methods have been developed for general protein binding site prediction tasks, whether they work for epitope prediction remains an understudied research question. The challenge is also heightened by the lack of a consistent evaluation pipeline with sufficient dataset size and epitope diversity. We introduce a filtered antibody-antigen complex structure dataset, AsEP (Antibody-specific Epitope Prediction). AsEP is the largest of its kind and provides clustered epitope groups, allowing the community to develop and test novel epitope prediction methods and evaluate their generalisability. AsEP comes with an easy-to-use interface in Python and pre-built graph representations of each antibody-antigen complex while also supporting customizable embedding methods. Using this new dataset, we benchmark several representative general protein-binding site prediction methods and find that their performances fall short of expectations for epitope prediction. To address this, we propose a novel method, WALLE, which leverages both unstructured modeling from protein language models and structural modeling from graph neural networks. WALLE demonstrate up to 3-10X performance improvement over the baseline methods. Our empirical findings suggest that epitope prediction benefits from combining sequential features provided by language models with geometrical information from graph representations. This provides a guideline for future epitope prediction method design. In addition, we reformulate the task as bipartite link prediction, allowing convenient model performance attribution and interpretability. We open source our data and code at https://github.com/biochunan/AsEP-dataset.",,,NeurIPS.csv,,,,,,
XAJRSLAJ,journalArticle,,"Zi, Chenyi; Zhao, Haihong; Sun, Xiangguo; Lin, Yiqing; Cheng, Hong; Li, Jia",ProG: A Graph Prompt Learning Benchmark,,,,"Artificial general intelligence on graphs has shown significant advancements across various applications, yet the traditional ‘Pre-train & Fine-tune’ paradigm faces inefficiencies and negative transfer issues, particularly in complex and few-shot settings. Graph prompt learning emerges as a promising alternative, leveraging lightweight prompts to manipulate data and fill the task gap by reformulating downstream tasks to the pretext. However, several critical challenges still remain: how to unify diverse graph prompt models, how to evaluate the quality of graph prompts, and to improve their usability for practical comparisons and selection. In response to these challenges, we introduce the first comprehensive benchmark for graph prompt learning. Our benchmark integrates SIX pre-training methods and FIVE state-of-the-art graph prompt techniques, evaluated across FIFTEEN diverse datasets to assess performance, flexibility, and efficiency. We also present ‘ProG’, an easy-to-use open-source library that streamlines the execution of various graph prompt models, facilitating objective evaluations. Additionally, we propose a unified framework that categorizes existing graph prompt methods into two main approaches: prompts as graphs and prompts as tokens. This framework enhances the applicability and comparison of graph prompt techniques. The code is available at: https://github.com/sheldonresearch/ProG.",,,NeurIPS.csv,,,,,,
9Q7RLKGM,journalArticle,,"Chen, Shan; Gallifant, Jack; Gao, Mingye; Moreira, Pedro; Munch, Nikolaj; Muthukkumar, Ajay; Rajan, Arvind; Kolluri, Jaya; Fiske, Amelia; Hastings, Janna; Aerts, Hugo; Anthony, Brian; Celi, Leo Anthony",Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias,,,,"Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data. In this study, we introduce Cross-Care, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups. We systematically evaluate how demographic biases embedded in pre-training corpora like T heP ile influence the outputs of LLMs. We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups. Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs. Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models’ representation of disease prevalence across different languages. For further exploration and analysis, we make all data and a data visualization tool available at: www.crosscare.net.",,,NeurIPS.csv,,,,,,
TAHI6QG5,journalArticle,,"Zhou, Jingbo; Chen, Shaorong; Xia, Jun; Liu, Sizhe; Ling, Tianze; Du, Wenjie; Liu, Yue; Yin, Jianwei; Li, Stan Z",NovoBench: Benchmarking Deep Learning-based De Novo Peptide Sequencing Methods in Proteomics,,,,"Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the high-throughput analysis of protein composition in biological tissues. Many deep learning methods have been developed for de novo peptide sequencing task, i.e., predicting the peptide sequence for the observed mass spectrum. However, two key challenges seriously hinder the further advancement of this important task. Firstly, since there is no consensus for the evaluation datasets, the empirical results in different research papers are often not comparable, leading to unfair comparison. Secondly, the current methods are usually limited to amino acid-level or peptide-level precision and recall metrics. In this work, we present the first unified benchmark NovoBench for de novo peptide sequencing, which comprises diverse mass spectrum data, integrated models, and comprehensive evaluation metrics. Recent impressive methods, including DeepNovo, PointNovo, Casanovo, InstaNovo, AdaNovo and π-HelixNovo are integrated into our framework. In addition to amino acid-level and peptide-level precision and recall, we evaluate the models’ performance in terms of identifying post-tranlational modifications (PTMs), efficiency and robustness to peptide length, noise peaks and missing fragment ratio, which are important influencing factors while seldom be considered. Leveraging this benchmark, we conduct a large-scale study of current methods, report many insightful findings that open up new possibilities for future development. The code is available at https://github.com/Westlake-OmicsAI/NovoBench.",,,NeurIPS.csv,,,,,,
D4I4B7SI,journalArticle,,"Huang, Zhen; Wang, Zengzhi; Xia, Shijie; Li, Xuefeng; Zou, Haoyang; Xu, Ruijie; Fan, Run-Ze; Ye, Lyumanshan; Chern, Ethan; Ye, Yixin; Zhang, Yikai; Yang, Yuqing; Wu, Ting; Wang, Binjie; Sun, Shichao; Xiao, Yang; Li, Yiyuan; Zhou, Fan; Chern, Steffi; Qin, Yiwei; Ma, Yan; Su, Jiadi; Liu, Yixiu; Zheng, Yuxiang; Zhang, Shaoting; Lin, Dahua; Qiao, Yu; Liu, Pengfei",OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI,,,,,,,NeurIPS.csv,,,,,,
5FG2HLT5,journalArticle,,"Wang, Yubo; Ma, Xueguang; Zhang, Ge; Ni, Yuansheng; Chandra, Abhranil; Guo, Shiguang; Ren, Weiming; Arulraj, Aaran; He, Xuan; Jiang, Ziyan; Li, Tianle; Ku, Max; Wang, Kai; Zhuang, Alex; Fan, Rongqi; Yue, Xiang; Chen, Wenhu",MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,,,,"In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.",,,NeurIPS.csv,,,,,,
LRSCBFI6,journalArticle,,"Myung, Junho; Lee, Nayeon; Zhou, Yi; Jin, Jiho; Putri, Rifki Afina; Antypas, Dimosthenis; Borkakoty, Hsuvas; Kim, Eunsu; Perez-Almendros, Carla; Ayele, Abinew Ali; Gutiérrez-Basulto, Víctor; Ibáñez-García, Yazmín; Lee, Hwaran; Muhammad, Shamsuddeen Hassan; Park, Kiwoong; Rzayev, Anar Sabuhi; White, Nina; Yimam, Seid Muhie; Pilehvar, Mohammad Taher; Ousidhoum, Nedjma; Camacho-Collados, Jose; Oh, Alice",BLEND: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages,,,,"Large language models (LLMs) often lack culture-specific knowledge of daily life, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs’ cultural sensitivities are limited to a single language or collected from online sources such as Wikipedia, which do not reflect the mundane everyday lifestyles of diverse regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is common cultural knowledge but uncommon in easily collected online sources, especially for underrepresented cultures. To address this issue, we introduce BLEND, a hand-crafted benchmark designed to evaluate LLMs’ everyday knowledge across diverse cultures and languages. BLEND comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We construct the benchmark to include two formats of questions: short-answer and multiple-choice. We show that LLMs perform better for cultures that are highly represented online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format. For cultures represented by mid-to-high-resource languages, LLMs perform better in their local languages, but for cultures represented by low-resource languages, LLMs perform better in English than the local languages. We make our dataset publicly available at: https://github.com/nlee0212/BLEnD.",,,NeurIPS.csv,,,,,,
YTDXACA2,journalArticle,,"Kwon, Yeonsu; Kim, Jiho; Lee, Gyubok; Bae, Seongsu; Kyung, Daeun; Cha, Wonchul; Pollard, Tom; Johnson, Alistair; Choi, Edward",EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records,,,,"Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (e.g., medications) with detailed clinical notes (e.g., physician notes). These elements are essential for straightforward data retrieval and provide deep, contextual insights into patient care. However, they often suffer from discrepancies due to unintuitive EHR system designs and human errors, posing serious risks to patient safety. To address this, we developed EHRCon, a new dataset and task specifically designed to ensure data consistency between structured tables and unstructured notes in EHRs. EHRCon was crafted in collaboration with healthcare professionals using the MIMIC-III EHR dataset, and includes manual annotations of 4,101 entities across 105 clinical notes checked against database entries for consistency. EHRCon has two versions, one using the original MIMICIII schema, and another using the OMOP CDM schema, in order to increase its applicability and generalizability. Furthermore, leveraging the capabilities of large language models, we introduce CheckEHR, a novel framework for verifying the consistency between clinical notes and database tables. CheckEHR utilizes an eight-stage process and shows promising results in both few-shot and zero-shot settings. The code is available at https://github.com/dustn1259/EHRCon.",,,NeurIPS.csv,,,,,,
VKRVFWUE,journalArticle,,"Hu, Jiamian; Hong, Yuanyuan; Chen, Yihua; Wang, He; Yasuhara, Moriaki","Noisy Ostracods: A Fine-Grained, Imbalanced Real-World Dataset for Benchmarking Robust Machine Learning and Label Correction Methods",,,,"We present the Noisy Ostracods, a noisy dataset for genus and species classification of crustacean ostracods with specialists’ annotations. Over the 71466 specimens collected, 5.58% of them are estimated to be noisy (possibly problematic) at genus level. The dataset is created to addressing a real-world challenge: creating a clean fine-grained taxonomy dataset. The Noisy Ostracods dataset has diverse noises from multiple sources. Firstly, the noise is open-set, including new classes discovered during curation that were not part of the original annotation. The dataset has pseudo-classes, where annotators misclassified samples that should belong to an existing class into a new pseudo-class. The Noisy Ostracods dataset is highly imbalanced with a imbalance factor ρ = 22429. This presents a unique challenge for robust machine learning methods, as existing approaches have not been extensively evaluated on fine-grained classification tasks with such diverse real-world noise. Initial experiments using current robust learning techniques have not yielded significant performance improvements on the Noisy Ostracods dataset compared to cross-entropy training on the raw, noisy data. On the other hand, noise detection methods have underperformed in error hit rate compared to naive cross-validation ensembling for identifying problematic labels. These findings suggest that the fine-grained, imbalanced nature, and complex noise characteristics of the dataset present considerable challenges for existing noiserobust algorithms. By openly releasing the Noisy Ostracods dataset, our goal is to encourage further research into the development of noise-resilient machine learning methods capable of effectively handling diverse, real-world noise in finegrained classification tasks. The dataset, along with its evaluation protocols, can be accessed at https://github.com/H-Jamieu/Noisy_ostracods.",,,NeurIPS.csv,,,,,,
53RT2LG2,journalArticle,,"Udandarao, Vishaal; Roth, Karsten; Dziadzio, Sebastian; Prabhu, Ameya; Cherti, Mehdi; Vinyals, Oriol; Hénaff, Olivier; Albanie, Samuel; Akata, Zeynep; Bethge, Matthias",A Practitioner’s Guide to Continual Multimodal Pretraining,,,,"Multimodal foundation models serve numerous applications at the intersection of vision and language. Still, despite being pretrained on extensive data, they become outdated over time. To keep models updated, research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate updates on large-scale new data, or (2) frequent, sample-level updates. However, practical model deployment often operates in the gap between these two limit cases, as real-world applications demand adaptation to specific subdomains, tasks or concepts — spread over the entire, varying life cycle of a model. In this work, we complement current perspectives on continual pretraining through a research test bed and offer comprehensive guidance for effective continual model updates in such scenarios. We first introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with realistic compute constraints and practical deployment requirements, constructed over 63 datasets with diverse visual and semantic coverage. Using FoMo-in-Flux, we explore the complex landscape of practical continual pretraining through multiple perspectives: (1) data mixtures and stream orderings that emulate real-world deployment settings, (2) methods ranging from simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model merging, (3) meta-learning-rate schedules and mechanistic design choices, and (4) model and compute scaling. Together, our insights provide a practitioner’s guide to continual multimodal pretraining for real-world deployment. Benchmark and code is provided here: github.com/ExplainableML/fomo_in_flux.",,,NeurIPS.csv,,,,,,
F742HUXT,journalArticle,,"Rutherford, Alexander; Ellis, Benjamin; Gallici, Matteo; Cook, Jonathan; Lupu, Andrei; Ingvarsson, Garðar; Willi, Timon; Hammond, Ravi; Khan, Akbir; de Witt, Christian Schroeder; Souly, Alexandra; Bandyopadhyay, Saptarashmi; Samvelyan, Mikayel; Jiang, Minqi; Lange, Robert; Whiteson, Shimon; Lacerda, Bruno; Hawes, Nick; Rocktäschel, Tim; Lu, Chris; Foerster, Jakob",JaxMARL: Multi-Agent RL Environments and Algorithms in JAX,,,,"Benchmarks are crucial in the development of machine learning algorithms, with available environments significantly influencing reinforcement learning (RL) research. Traditionally, RL environments run on the CPU, which limits their scalability with typical academic compute. However, recent advancements in JAX have enabled the wider use of hardware acceleration, enabling massively parallel RL training pipelines and environments. While this has been successfully applied to single-agent RL, it has not yet been widely adopted for multi-agent scenarios. In this paper, we present JaxMARL, the first open-source, Python-based library that combines GPU-enabled efficiency with support for a large number of commonly used MARL environments and popular baseline algorithms. Our experiments show that, in terms of wall clock time, our JAX-based training pipeline is around 14 times faster than existing approaches, and up to 12500x when multiple training runs are vectorized. This enables efficient and thorough evaluations, potentially alleviating the evaluation crisis in the field. We also introduce and benchmark SMAX, a JAXbased approximate reimplementation of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. The code is available at https://github.com/flairox/jaxmarl.",,,NeurIPS.csv,,,,,,
C85Q459D,journalArticle,,"Ma, Yubo; Zang, Yuhang; Chen, Liangyu; Chen, Meiqi; Jiao, Yizhu; Li, Xinze; Lu, Xinyuan; Liu, Ziyu; Ma, Yan; Dong, Xiaoyi; Zhang, Pan; Pan, Liangming; Jiang, Yu-Gang; Wang, Jiaqi; Cao, Yixin; Sun, Aixin",MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations,,,,"Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLONGBENCH-DOC, a long-context, multimodal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e., page number). Moreover, 33.7% of the questions are cross-page questions requiring evidence across multiple pages. 20.6% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9%, while the second-best, GPT-4V, scores 30.5%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.",,,NeurIPS.csv,,,,,,
WPP4Z3IV,journalArticle,,"Boisvert, Léo; Thakkar, Megh; Gasse, Maxime; Caccia, Massimo",WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks,,,,"The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress toward capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena.",,,NeurIPS.csv,,,,,,
ZFRK2BDI,journalArticle,,"Hua, Chenqing; Zhong, Bozitao; Luan, Sitao; Hong, Liang; Wolf, Guy; Precup, Doina; Zheng, Shuangjia",ReactZyme: A Benchmark for Enzyme-Reaction Prediction,,,,"Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation (https://github.com/WillHua127/ReactZyme).",,,NeurIPS.csv,,,,,,
3TLEI6DD,journalArticle,,"Jignasu, Anushrut; Marshall, Kelly O; Mishra, Ankush Kumar; Rillo, Lucas Nerone; Ganapathysubramanian, Baskar; Balu, Aditya; Hegde, Chinmay; Krishnamurthy, Adarsh",Slice-100K: A Multimodal Dataset for Extrusion-based 3D Printing,,,,"G-code (Geometric code) or RS-274 is the most widely used computer numerical control (CNC) and 3D printing programming language. G-code provides machine instructions for the movement of the 3D printer, especially for the nozzle, stage, and extrusion of material for extrusion-based additive manufacturing. Currently, there does not exist a large repository of curated CAD models along with their corresponding G-code files for additive manufacturing. To address this issue, we present Slice-100K, a first-of-its-kind dataset of over 100,000 G-code files, along with their tessellated CAD model, LVIS (Large Vocabulary Instance Segmentation) categories, geometric properties, and renderings. We build our dataset from triangulated meshes derived from Objaverse-XL and Thingi10K datasets. We demonstrate the utility of this dataset by finetuning GPT-2 on a subset of the dataset for G-code translation from a legacy G-code format (Sailfish) to a more modern, widely used format (Marlin). Our dataset can be found here. Slice-100K will be the first step in developing a multimodal foundation model for digital manufacturing.",,,NeurIPS.csv,,,,,,
T49DG9QK,journalArticle,,"Formanek, Claude; Tilbury, Callum Rhys; Beyers, Louise; Shock, Jonathan; Pretorius, Arnu",Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation,,,,"Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications. Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work. In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work. Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve stateof-the-art (SOTA) results across a wide range of tasks. Specifically, we show that on 35 out of 47 datasets used in prior work (almost 75% of cases), we match or surpass the performance of the current purported SOTA. Strikingly, our baselines often substantially outperform these more sophisticated algorithms. Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work. Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward.",,,NeurIPS.csv,,,,,,
N8AHTGIX,journalArticle,,"Martinez, Julieta; Kim, Emily; Romero, Javier; Bagautdinov, Timur; Saito, Shunsuke; Yu, Shoou-I; Anderson, Stuart; Zollhöfer, Michael; Wang, Te-Li; Bai, Shaojie; Li, Chenghui; Wei, Shih-En; Joshi, Rohan; Borsos, Wyatt; Simon, Tomas; Saragih, Jason; Theodosis, Paul; Greene, Alexander; Josyula, Anjani; Maeta, Silvio Mano; Jewett, Andrew I; Venshtain, Simon; Heilman, Christopher; Chen, Yueh-Tung; Fu, Sidi; Elshaer, Mohamed Ezzeldin A; Du, Tingfang; Wu, Longhua; Chen, Shen-Chi; Kang, Kai; Wu, Michael; Emad, Youssef; Longay, Steven; Brewer, Ashley; Shah, Hitesh; Booth, James; Koska, Taylor; Haidle, Kayla; Andromalos, Matt; Hsu, Joanna; Dauer, Thomas; Selednik, Peter; Godisart, Tim; Ardisson, Scott; Cipperly, Matthew; Humberston, Ben; Farr, Lon; Hansen, Bob; Guo, Peihong; Braun, Dave; Krenn, Steven; Wen, He; Evans, Lucas; Fadeeva, Natalia; Stewart, Matthew; Schwartz, Gabriel; Gupta, Divam; Moon, Gyeongsik; Guo, Kaiwen; Dong, Yuan; Xu, Yichen; Shiratori, Takaaki; Prada, Fabian; Pires, Bernardo R; Peng, Bo; Buffalini, Julia; Trimble, Autumn; McPhail, Kevyn; Schoeller, Melissa; Sheikh, Yaser","Codec Avatar Studio: Paired Human Captures for Complete, Driveable, and Generalizable Avatars",,,,"To create photorealistic avatars that users can embody, human modeling must be complete (encompass the full body), driveable (able to reproduce motion of the user from lightweight sensors), and generalizable (i.e., easily adaptable to novel identities). Towards these goals, paired captures, that is, captures of the same subject obtained from systems of diverse quality and availability, are crucial. However, paired captures are rarely available to researchers outside of dedicated industrial labs: Codec Avatar Studio is our proposal to close this gap. Towards generalization and driveability, we introduce a dataset of 256 subjects captured in two modalities: high resolution multi-view scans of their heads, and video from the internal cameras of a headset. Towards completeness, we introduce a dataset of 4 subjects captured in eight modalities: high quality relightable multi-view captures of heads and hands, full body multi-view captures with minimal and regular clothes, and corresponding head, hands and body phone captures. Together with our data, we also provide code and pre-trained models for different state-of-the-art human generation models. Our datasets and code are available at https://github.com/facebookresearch/ava-256 and https://github.com/facebookresearch/goliath.",,,NeurIPS.csv,,,,,,
MZSHRJYL,journalArticle,,"Belharbi, Soufiane; Whitford, Mara KM; Hoang, Phuong; Murtaza, Shakeeb; McCaffrey, Luke; Granger, Eric",SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image Super-Resolution,,,,"Confocal fluorescence microscopy is one of the most accessible and widely used imaging techniques for the study of biological processes at the cellular and subcellular levels. Scanning confocal microscopy allows the capture of high-quality images from thick three-dimensional (3D) samples, yet suffers from well-known limitations such as photobleaching and phototoxicity of specimens caused by intense light exposure, which limits its use in some applications, especially for living cells. Cellular damage can be alleviated by changing imaging parameters to reduce light exposure, often at the expense of image quality. Machine/deep learning methods for single-image super-resolution (SISR) can be applied to restore image quality by upscaling lower-resolution (LR) images to produce high-resolution images (HR). These SISR methods have been successfully applied to photo-realistic images due partly to the abundance of publicly available datasets. In contrast, the lack of publicly available data partly limits their application and success in scanning confocal microscopy. In this paper, we introduce a large scanning confocal microscopy dataset named SR-CACO-2 that is comprised of low- and high-resolution image pairs marked for three different fluorescent markers. It allows to evaluate the performance of SISR methods on three different upscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured with four resolutions and three markers, that have been translated in the form of 9,937 patches for experiments with SISR methods. Given the new SR-CACO-2 dataset, we also provide benchmarking results for 16 state-of-the-art methods that are representative of the main SISR families. Results show that these methods have limited success in producing highresolution textures, indicating that SR-CACO-2 represents a challenging problem. The dataset is released under a Creative Commons license (CC BY-NC-SA 4.0), and it can be accessed freely. Our dataset, code and pretrained weights for SISR methods are publicly available: https://github.com/sbelharbi/sr-caco-2.",,,NeurIPS.csv,,,,,,
UIAZIF7T,journalArticle,,"Boettcher, Wolfgang; Hoyer, Lukas; Unal, Ozan; Lenssen, Jan Eric; Schiele, Bernt",Scribbles for All: Benchmarking Scribble Supervised Segmentation Across Datasets,,,,"In this work, we introduce Scribbles for All, a label and training data generation algorithm for semantic segmentation trained on scribble labels. Training or fine-tuning semantic segmentation models with weak supervision has become an important topic recently and was subject to significant advances in model quality. In this setting, scribbles are a promising label type to achieve high quality segmentation results while requiring a much lower annotation effort than usual pixel-wise dense semantic segmentation annotations. The main limitation of scribbles as source for weak supervision is the lack of challenging datasets for scribble segmentation, which hinders the development of novel methods and conclusive evaluations. To overcome this limitation, Scribbles for All provides scribble labels for several popular segmentation datasets and provides an algorithm to automatically generate scribble labels for any dataset with dense annotations, paving the way for new insights and model advancements in the field of weakly supervised segmentation. In addition to providing datasets and algorithm, we evaluate state-of-the-art segmentation models on our datasets and show that models trained with our synthetic labels perform competitively with respect to models trained on manual labels. Thus, our datasets enable state-of-the-art research into methods for scribble-labeled semantic segmentation. The datasets, scribble generation algorithm, and baselines are publicly available at https://github.com/wbkit/Scribbles4All.",,,NeurIPS.csv,,,,,,
HHUW6FL2,journalArticle,,"Angeloudi, Eirini; Audenaert, Jeroen; Bowles, Micah; Boyd, Benjamin M; Chemaly, David; Cherinka, Brian; Ciucă, Ioana; Cranmer, Miles; Do, Aaron; Grayling, Matthew; Hayes, Erin E; Hehir, Tom; Ho, Shirley; Huertas-Company, Marc; Iyer, Kartheik G; Jablonska, Maja; Lanusse, Francois; Leung, Henry W; Mandel, Kaisey; Martínez-Galarza, Juan Rafael; Melchior, Peter; Meyer, Lucas; Parker, Liam H; Qu, Helen; Shen, Jeff; Smith, Michael J; Stone, Connor; Walmsley, Mike; Wu, John F",The Multimodal Universe: Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientific Data,,,,,,,NeurIPS.csv,,,,,,
7YCGRPA8,journalArticle,,"Alexos, Antonios; Liu, Junze; Galla, Shashank; Hayes, Sean; Bhardwaj, Kshitij; Schwartz, Alexander; Biener, Monika; Baldi, Pierre; Bukkapatnam, Satish; Bhandarkar, Suhas",Nuclear Fusion Diamond Polishing Dataset,,,,"In the Inertial Confinement Fusion (ICF) process, roughly a 2mm spherical shell made of high-density carbon is used as a target for laser beams, which compress and heat it to energy levels needed for high fusion yield in nuclear fusion. These shells are polished meticulously to meet the standards for a fusion shot. However, the polishing of these shells involves multiple stages, with each stage taking several hours. To make sure that the polishing process is advancing in the right direction, we are able to measure the shell surface roughness. This measurement, however, is very labor-intensive, time-consuming, and requires a human operator. To help improve the polishing process we have released the first dataset to the public that consists of raw vibration signals with the corresponding polishing surface roughness changes. We show that this dataset can be used with a variety of neural network based methods for prediction of the change of polishing surface roughness, hence eliminating the need for the time-consuming manual process. This is the first dataset of its kind to be released in public and its use will allow the operator to make any necessary changes to the ICF polishing process for optimal results. This dataset contains the raw vibration data of multiple polishing runs with their extracted statistical features and the corresponding surface roughness values. Additionally, to generalize the prediction models to different polishing conditions, we also apply domain adaptation techniques to improve prediction accuracy for conditions unseen by the trained model. The dataset is available in https://junzeliu.github.io/Diamond-Polishing-Dataset/.",,,NeurIPS.csv,,,,,,
JZJFDV6C,dataset,2024.0,"Wu, Yiwei; Li, Hanlin; Li, Hanlin","Annotations for ""A Systematic Review of NeurIPS Dataset Management Practices""",,10.18738/T8/HLTRQP,https://dataverse.tdl.org/citation?persistentId=doi:10.18738/T8/HLTRQP,"As new machine learning methods demand larger training datasets, researchers and developers face significant challenges in dataset management. Although ethics reviews, documentation, and checklists have been established, it remains uncertain whether consistent dataset management practices exist across the community. This lack of a comprehensive overview hinders our ability to diagnose and address fundamental tensions and ethical issues related to managing large datasets. We present a systematic review of datasets published at the NeurIPS Datasets and Benchmarks track, focusing on four key aspects: provenance, distribution, ethical disclosure, and licensing. Our findings reveal that dataset provenance is often unclear due to ambiguous filtering and curation processes. Additionally, a variety of sites are used for dataset hosting, but only a few offer structured metadata and version control. These inconsistencies underscore the urgent need for standardized data infrastructures for the publication and management of datasets.",2024,,NeurIPS.csv,,,,,,
SH7MGSUG,journalArticle,,"Liu, Ziqiang; Fang, Feiteng; Feng, Xi; Du, Xinrun; Zhang, Chenhao; Wang, Zekun; Bai, Yuelin; Zhao, Qixuan; Fan, Liyang; Gan, Chengguang; Lin, Hongquan; Li, Jiaming; Ni, Yuansheng; Wu, Haihong; Narsupalli, Yaswanth; Zheng, Zhigang; Li, Chengming; Hu, Xiping; Xu, Ruifeng; Chen, Xiaojun; Yang, Min; Liu, Jiaheng; Liu, Ruibo; Huang, Wenhao; Zhang, Ge; Ni, Shiwen",II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models,,,,"The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks. In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs. However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs. To fill this gap, we propose the Image Implication understanding Benchmark, II-Bench, which aims to evaluate the model’s higher-order perception of images. Through extensive experiments on II-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive 98%. Subsequently, MLLMs perform worse on abstract and complex images, suggesting limitations in their ability to understand high-level semantics and capture image details. Finally, it is observed that most models exhibit enhanced accuracy when image sentiment polarity hints are incorporated into the prompts. This observation underscores a notable deficiency in their inherent understanding of image sentiment. We believe that II-Bench will inspire the community to develop the next generation of MLLMs, advancing the journey towards expert artificial general intelligence (AGI). II-Bench is publicly available at https://huggingface.co/datasets/ m-a-p/II-Bench.",,,NeurIPS.csv,,,,,,
XYFSBAP8,journalArticle,,"Pardawala, Huzaifa; Sukhani, Siddhant; Shah, Agam; Kejriwal, Veer; Pillai, Abhishek; Bhasin, Rohan; DiBiasio, Andrew; Mandapati, Tarun; Adha, Dhruv; Chava, Sudheer",SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts’ QA Through Six-Dimensional Feature Analysis,,,,"Fact-checking is extensively studied in the context of misinformation and disinformation, addressing objective inaccuracies. However, a softer form of misinformation involves responses that are factually correct but lack certain features such as clarity and relevance. This challenge is prevalent in formal Question-Answer (QA) settings such as press conferences in ﬁnance, politics, sports, and other domains, where subjective answers can obscure transparency. Despite this, there is a lack of manually annotated datasets for subjective features across multiple dimensions. To address this gap, we introduce SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts’ (ECTs) QA sessions as the answers given by company representatives are often open to subjective interpretations and scrutiny. The dataset includes 49, 446 annotations for long-form QA pairs across six features: Assertive, Cautious, Optimistic, Specific, Clear, and Relevant. These features are carefully selected to encompass the key attributes that reﬂect the tone of the answers provided during QA sessions across different domains. Our ﬁndings are that the best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity, such as Relevant and Clear, with a mean difference of 2.17% in their weighted F1 scores. The models perform signiﬁcantly better on features with higher subjectivity, such as Specific and Assertive, with a mean difference of 10.01% in their weighted F1 scores. Furthermore, testing SubjECTive-QA’s generalizability using QAs from White House Press Brieﬁngs and Gaggles yields an average weighted F1 score of 65.97% using our best models for each feature, demonstrating broader applicability beyond the ﬁnancial domain. SubjECTive-QA is publicly available under the CC BY 4.0 license1.",,,NeurIPS.csv,,,,,,
TTCM6W8P,journalArticle,,"Jin, Ruinan; Xu, Zikang; Zhong, Yuan; Yao, Qingsong; Dou, Qi; Zhou, S Kevin; Li, Xiaoxiao",FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models,,,,"The advent of foundation models (FMs) in healthcare offers unprecedented opportunities to enhance medical diagnostics through automated classification and segmentation tasks. However, these models also raise significant concerns about their fairness, especially when applied to diverse and underrepresented populations in healthcare applications. Currently, there is a lack of comprehensive benchmarks, standardized pipelines, and easily adaptable libraries to evaluate and understand the fairness performance of FMs in medical imaging, leading to considerable challenges in formulating and implementing solutions that ensure equitable outcomes across diverse patient populations. To fill this gap, we introduce FairMedFM, a fairness benchmark for FM research in medical imaging. FairMedFM integrates with 17 popular medical imaging datasets, encompassing different modalities, dimensionalities, and sensitive attributes. It explores 20 widely used FMs, with various usages such as zero-shot learning, linear probing, parameter-efficient fine-tuning, and prompting in various downstream tasks – classification and segmentation. Our exhaustive analysis evaluates the fairness performance over different evaluation metrics from multiple perspectives, revealing the existence of bias, varied utilityfairness trade-offs on different FMs, consistent disparities on the same datasets regardless FMs, and limited effectiveness of existing unfairness mitigation methods. Checkout FairMedFM’s project page and open-sourced codebase, which supports extendible functionalities and applications as well as inclusive for studies on FMs in medical imaging over the long term.",,,NeurIPS.csv,,,,,,
FUINYZU4,journalArticle,,"Yang, Chih-Hsuan; Feuer, Ben; Jubery, Zaki; Deng, Zi K; Nakkab, Andre; Chiranjeevi, Shivani; Marshall, Kelly; Baishnab, Nirmal; Singh, Asheesh K; Sarkar, Soumik; Merchant, Nirav; Hegde, Chinmay",BioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity,,,,"We introduce BioTrove, the largest publicly accessible dataset designed to advance AI applications in biodiversity. Curated from the iNaturalist platform and vetted to include only research-grade data, BioTrove contains 161.9 million images, offering unprecedented scale and diversity from three primary kingdoms: Animalia (""animals""), Fungi (""fungi""), and Plantae (""plants""), spanning approximately 366.6K species. Each image is annotated with scientific names, taxonomic hierarchies, and common names, providing rich metadata to support accurate AI model development across diverse species and ecosystems.",,,NeurIPS.csv,,,,,,
CF4CLFJ4,journalArticle,,"Wang, Siyan; Levy, Bradford","BeanCounter: A low-toxicity, large-scale, and open dataset of business-oriented text",,,,"Many of the recent breakthroughs in language modeling have resulted from scaling effectively the same model architecture to larger datasets. In this vein, recent work has highlighted performance gains from increasing training dataset size and quality, suggesting a need for novel sources of large-scale datasets. In this work, we introduce BeanCounter, a public dataset consisting of more than 159B tokens extracted from businesses’ disclosures. We show that this data is indeed novel: less than 0.1% of BeanCounter appears in Common Crawl-based datasets and the data is an order of magnitude larger than datasets relying on similar sources. Given the data’s provenance, we hypothesize that BeanCounter is comparatively more factual and less toxic than web-based datasets. Exploring this hypothesis, we find that many demographic identities occur with similar prevalence in BeanCounter but with significantly less toxic context relative to other datasets. To demonstrate the utility of BeanCounter, we evaluate and compare two LLMs continually pre-trained on BeanCounter with their base models. We find an 18-33% reduction in toxic generation and improved performance within the finance domain for the continually pretrained models. Collectively, our work suggests that BeanCounter is a novel source of low-toxicity and high-quality domain-specific data with sufficient scale to train multi-billion parameter LLMs.",,,NeurIPS.csv,,,,,,
LTLRWX7D,journalArticle,,"Askari, Farzaneh; Lyu, Lingjuan; Sharma, Vivek",DECO-Bench: Unified Benchmark for Decoupled Task-Agnostic Synthetic Data Release,,,,"In this work, we tackle the question of how to systematically benchmark taskagnostic decoupling methods for privacy-preserving machine learning (ML). Sharing datasets that include sensitive information often triggers privacy concerns, necessitating robust decoupling methods to separate sensitive and non-sensitive attributes. Despite the development of numerous decoupling techniques, a standard benchmark for systematically comparing these methods remains absent. Our framework integrates various decoupling techniques along with synthetic data generation and evaluation protocols within a unified system. Using our framework, we benchmark various decoupling techniques and evaluate their privacy-utility trade-offs. Finally, we release our source code, pre-trained models, datasets of decoupled representations to foster research in this area.",,,NeurIPS.csv,,,,,,
4YLLRQQ8,journalArticle,,"Wang, Jiaqi; Wang, Xiaochen; Lyu, Lingjuan; Chen, Jinghui; Ma, Fenglong",FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection,,,,"This study introduces the Federated Medical Knowledge Injection (FEDMEKI) platform, a new benchmark designed to address the unique challenges of integrating medical knowledge into foundation models under privacy constraints. By leveraging a cross-silo federated learning approach, FEDMEKI circumvents the issues associated with centralized data collection, which is often prohibited under health regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the USA. The platform is meticulously designed to handle multi-site, multi-modal, and multi-task medical data, which includes 7 medical modalities, including images, signals, texts, laboratory test results, vital signs, input variables, and output variables. The curated dataset to validate FEDMEKI covers 8 medical tasks, including 6 classification tasks (lung opacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal detection, mortality prediction, sepsis prediction, and enlarged cardiomediastinum detection) and 2 generation tasks (medical visual question answering (MedVQA) and ECG noise clarification). This comprehensive dataset is partitioned across several clients to facilitate the decentralized training process under 16 benchmark approaches. FEDMEKI not only preserves data privacy but also enhances the capability of medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure, thereby setting a new benchmark in the application of foundation models within the healthcare sector.",,,NeurIPS.csv,,,,,,
7G6TMXKP,journalArticle,,"Deng, Junwei; Li, Ting-Wei; Zhang, Shiyuan; Liu, Shixuan; Pan, Yijun; Huang, Hao; Wang, Xinhe; Hu, Pingbang; Zhang, Xingjian; Ma, Jiaqi W",dattri: A Library for Efficient Data Attribution,,,,"Data attribution methods aim to quantify the influence of individual training samples on the prediction of artificial intelligence (AI) models. As training data plays an increasingly crucial role in the modern development of large-scale AI models, data attribution has found broad applications in improving AI performance and safety. However, despite a surge of new data attribution methods being developed recently, there lacks a comprehensive library that facilitates the development, benchmarking, and deployment of different data attribution methods. In this work, we introduce dattri, an open-source data attribution library that addresses the above needs. Specifically, dattri highlights three novel design features. Firstly, dattri proposes a unified and easy-to-use API, allowing users to integrate different data attribution methods into their PyTorch-based machine learning pipeline with a few lines of code changed. Secondly, dattri modularizes low-level utility functions that are commonly used in data attribution methods, such as Hessianvector product, inverse-Hessian-vector product or random projection, making it easier for researchers to develop new data attribution methods. Thirdly, dattri provides a comprehensive benchmark framework with pre-trained models and ground truth annotations for a variety of benchmark settings, including generative AI settings. We have implemented a variety of state-of-the-art efficient data attribution methods that can be applied to large-scale neural network models, and will continuously update the library in the future. Using the developed dattri library, we are able to perform a comprehensive and fair benchmark analysis across a wide range of data attribution methods. The source code of dattri is available at https://github.com/TRAIS-Lab/dattri.",,,NeurIPS.csv,,,,,,
4AFN7G6D,journalArticle,,"Wu, Cheng-Kuang; Tam, Zhi Rui; Lin, Chieh-Yen; Chen, Yun-Nung; Lee, Hung-yi",StreamBench: Towards Benchmarking Continuous Improvement of Language Agents,,,,"Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios. Source code: https://github.com/stream-bench/stream-bench. Benchmark website: https://stream-bench.github.io.",,,NeurIPS.csv,,,,,,
86RIMHYB,journalArticle,,"Jung, Sangwon; Yu, Sumin; Chun, Sanghyuk; Moon, Taesup",Do Counterfactually Fair Image Classiﬁers Satisfy Group Fairness? – A Theoretical and Empirical Study,,,,"The notion of algorithmic fairness has been actively explored from various aspects of fairness, such as counterfactual fairness (CF) and group fairness (GF). However, the exact relationship between CF and GF remains to be unclear, especially in image classiﬁcation tasks; the reason is because we often cannot collect counterfactual samples regarding a sensitive attribute, essential for evaluating CF, from the existing images (e.g., a photo of the same person but with different secondary sex characteristics). In this paper, we construct new image datasets for evaluating CF by using a high-quality image editing method and carefully labeling with human annotators. Our datasets, CelebA-CF and LFW-CF, build upon the popular image GF benchmarks; hence, we can evaluate CF and GF simultaneously. We empirically observe that CF does not imply GF in image classiﬁcation, whereas previous studies on tabular datasets observed the opposite. We theoretically show that it could be due to the existence of a latent attribute G that is correlated with, but not caused by, the sensitive attribute (e.g., secondary sex characteristics are highly correlated with hair length). From this observation, we propose a simple baseline, Counterfactual Knowledge Distillation (CKD), to mitigate such correlation with the sensitive attributes. Extensive experimental results on CelebA-CF and LFW-CF demonstrate that CF-achieving models satisfy GF if we successfully reduce the reliance on G (e.g., using CKD).",,,NeurIPS.csv,,,,,,
SJYTYDAR,journalArticle,,"Ru, Dongyu; Qiu, Lin; Hu, Xiangkun; Zhang, Tianhang; Shi, Peng; Chang, Shuaichen; Jiayang, Cheng; Wang, Cunxiang; Sun, Shichao; Li, Huanyu; Zhang, Zizhao; Wang, Binjie; Jiang, Jiarong; He, Tong; Wang, Zhiguo; Liu, Pengfei; Zhang, Yue; Zhang, Zheng",RAGCHECKER: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation,,,,"Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGCHECKER, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGCHECKER has significantly better correlations with human judgments than other evaluation metrics. Using RAGCHECKER, we evaluate 8 RAG systems and conduct an indepth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGCHECKER can guide researchers and practitioners in developing more effective RAG systems3.",,,NeurIPS.csv,,,,,,
LVQ39DP8,journalArticle,,"Ye, Fanghua; Yang, Mingming; Pang, Jianhui; Wang, Longyue; Wong, Derek F; Yilmaz, Emine; Shi, Shuming; Tu, Zhaopeng",Benchmarking LLMs via Uncertainty Quantification,,,,"The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect – uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty into the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.",,,NeurIPS.csv,,,,,,
YA46QZFK,journalArticle,,"Longpre, Shayne; Mahari, Robert; Lee, Ariel; Lund, Campbell; Oderinwale, Hamidah; Brannon, William; Saxena, Nayan; Obeng-Marnu, Naana; South, Tobin; Hunter, Cole; Klamm, Christopher; Schoelkopf, Hailey; Singh, Nikhil; Cherep, Manuel; Anis, Mustafa; Dinh, An; Chitongo, Caroline; Yin, Da; Sileo, Damien; Mataciunas, Deividas; Misra, Diganta; Alghamdi, Emad; Shippole, Enrico; Zhang, Jianguo; Materzynska, Joanna; Qian, Kun; Tiwary, Kush; Miranda, Lester; Dey, Manan; Liang, Minnie; Muennighoff, Niklas; Ye, Seonghyeon; Kim, Seungone; Mohanty, Shrestha; Sharma, Vivek; Chien, Vu Minh; Zhou, Xuhui; Li, Yizhi; Xiong, Caiming; Villa, Luis; Biderman, Stella; Li, Hanlin; Ippolito, Daphne; Hooker, Sara; Kabbara, Jad",Consent in Crisis: The Rapid Decline of the AI Data Commons,,,,"General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To our knowledge, we conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains underlying AI training corpora. Our audit of 14, 000 web domains provides an expansive view of crawlable web data and how codified data use preferences are changing over time. We observe a proliferation of AIspecific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites’ expressed intentions in their Terms of Service and their robots.txt. We diagnose these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. Our longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. We hope to illustrate the emerging crises in data consent, for both developers and creators. The foreclosure of much of the open web will impact not only commercial AI, but also non-commercial AI and academic research.",,,NeurIPS.csv,,,,,,
RXUISHL6,journalArticle,,"Peng, Liang; Gao, Junyuan; Liu, Xinran; Li, Weihong; Dong, Shaohua; Zhang, Zhipeng; Fan, Heng; Zhang, Libo",VastTrack: Vast Category Visual Object Tracking,,,,"In this paper, we propose a novel benchmark, named VastTrack, aiming to facilitate the development of general visual tracking via encompassing abundant classes and videos. VastTrack consists of a few attractive properties: (1) Vast Object Category. In particular, it covers targets from 2,115 categories, significantly surpassing object classes of existing popular benchmarks (e.g., GOT-10k with 563 classes and LaSOT with 70 categories). Through providing such vast object classes, we expect to learn more general object tracking. (2) Larger scale. Compared with current benchmarks, VastTrack provides 50,610 videos with 4.2 million frames, which makes it to date the largest dataset in term of the number of videos, and hence could benefit training even more powerful visual trackers in the deep learning era. (3) Rich Annotation. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions with more than 50K sentences for the videos. Such rich annotations of VastTrack enable the development of both vision-only and vision-language tracking. In order to ensure precise annotation, each frame in the videos is manually labeled with multi-stage of careful inspections and refinements. To understand performance of existing trackers and to provide baselines for future comparison, we extensively evaluate 25 representative trackers. The results, not surprisingly, display significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are urgently required to improve general visual tracking. Our VastTrack, the toolkit, and evaluation results are publicly available at https://github.com/HengLan/VastTrack.",,,NeurIPS.csv,,,,,,
AKR9G3D9,journalArticle,,"Monroc, Claire Bizon; Bušic, Ana; Dubuc, Donatien; Zhu, Jiamin",WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control,,,,"The wind farm control problem is challenging, since conventional model-based control strategies require tractable models of complex aerodynamical interactions between the turbines and suffer from the curse of dimension when the number of turbines increases. Recently, model-free and multi-agent reinforcement learning approaches have been used to address this challenge. In this article, we introduce WFCRL (Wind Farm Control with Reinforcement Learning), the first open suite of multi-agent reinforcement learning environments for the wind farm control problem. WFCRL frames a cooperative Multi-Agent Reinforcement Learning (MARL) problem: each turbine is an agent and can learn to adjust its yaw, pitch or torque to maximize the common objective (e.g. the total power production of the farm). WFCRL also offers turbine load observations that will allow to optimize the farm performance while limiting turbine structural damages. Interfaces with two state-of-the-art farm simulators are implemented in WFCRL: a static simulator (FLORIS) and a dynamic simulator (FAST.Farm). For each simulator, 10 wind layouts are provided, including 5 real wind farms. Two state-of-the-art online MARL algorithms are implemented to illustrate the scaling challenges. As learning online on FAST.Farm is highly time-consuming, WFCRL offers the possibility of designing transfer learning strategies from FLORIS to FAST.Farm.",,,NeurIPS.csv,,,,,,
E482C4FD,journalArticle,,"Liu, Jiaheng; Ni, Zehao; Que, Haoran; Sun, Tao; Wang, Zekun; Yang, Jian; Wang, Jiakai; Guo, Hongcheng; Peng, Zhongyuan; Zhang, Ge; Tian, Jiayi; Bu, Xingyuan; Xu, Ke; Rong, Wenge; Peng, Junran; Zhang, Zhaoxiang","RoleAgent: Building, Interacting, and Benchmarking High-quality Role-Playing Agents from Scripts",,,,"Believable agents can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication. Recently, generative agents have been proposed to simulate believable human behavior by using Large Language Models. However, the existing method heavily relies on humanannotated agent profiles (e.g., name, age, personality, relationships with others, and so on) for the initialization of each agent, which cannot be scaled up easily. In this paper, we propose a scalable RoleAgent framework to generate high-quality role-playing agents from raw scripts, which includes building and interacting stages. Specifically, in the building stage, we use a hierarchical memory system to extract and summarize the structure and high-level information of each agent for the raw script. In the interacting stage, we propose a novel innovative mechanism with four steps to achieve a high-quality interaction between agents. Finally, we introduce a systematic and comprehensive evaluation benchmark called RoleAgentBench to evaluate the effectiveness of our RoleAgent, which includes 100 and 28 roles for 20 English and 5 Chinese scripts, respectively. Extensive experimental results on RoleAgentBench demonstrate the effectiveness of RoleAgent.",,,NeurIPS.csv,,,,,,
5EYIE9MV,journalArticle,,"Dauner, Daniel; Hallgarten, Marcel; Li, Tianyu; Weng, Xinshuo; Huang, Zhiyu; Yang, Zetong; Li, Hongyang; Gilitschenski, Igor; Ivanovic, Boris; Pavone, Marco; Geiger, Andreas; Chitta, Kashyap",NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking,,,,"Benchmarking vision-based driving policies is challenging. On one hand, openloop evaluation with real data is easy, but these results do not reflect closedloop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird’s eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.",,,NeurIPS.csv,,,,,,
YSZKJ6UB,journalArticle,,"Wu, Haoning; Li, Dongxu; Chen, Bei; Li, Junnan",LONGVIDEOBENCH: A Benchmark for Long-context Interleaved Video-Language Understanding,,,,"Large multimodal models (LMMs) are processing increasingly longer and richer inputs. Albeit the progress, few public benchmark is available to measure such development. To mitigate this gap, we introduce LONGVIDEOBENCH, a questionanswering benchmark that features video-language interleaved inputs up to an hour long. Our benchmark includes 3,763 varying-length web-collected videos with their subtitles across diverse themes, designed to comprehensively evaluate LMMs on long-term multimodal understanding. To achieve this, we interpret the primary challenge as to accurately retrieve and reason over detailed multimodal information from long inputs. As such, we formulate a novel video question-answering task termed referring reasoning. Specifically, as part of the question, it contains a referring query that references related video contexts, called referred context. The model is then required to reason over relevant video details from the referred context. Following the paradigm of referring reasoning, we curate 6,678 human-annotated multiple-choice questions in 17 fine-grained categories, establishing one of the most comprehensive benchmarks for long-form video understanding. Evaluations suggest that the LONGVIDEOBENCH presents significant challenges even for the most advanced proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their open-source counterparts show an even larger performance gap. In addition, our results indicate that model performance on the benchmark improves only when they are capable of processing more frames, positioning LONGVIDEOBENCH as a valuable benchmark for evaluating future-generation long-context LMMs.",,,NeurIPS.csv,,,,,,
NC88TY6A,journalArticle,,"Zhang, Yuwei; Xia, Tong; Han, Jing; Wu, Yu Yvonne; Rizos, Georgios; Liu, Yang; Mosuily, Mohammed; Chauhan, Jagmohan; Mascolo, Cecilia",Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking,,,,"Respiratory audio, such as coughing and breathing sounds, has predictive power for a wide range of healthcare applications, yet is currently under-explored. The main problem for those applications arises from the difficulty in collecting large labeled task-specific data for model development. Generalizable respiratory acoustic foundation models pretrained with unlabeled data would offer appealing advantages and possibly unlock this impasse. However, given the safety-critical nature of healthcare applications, it is pivotal to also ensure openness and replicability for any proposed foundation model solution. To this end, we introduce OPERA, an OPEn Respiratory Acoustic foundation model pretraining and benchmarking system, as the first approach answering this need. We curate large-scale respiratory audio datasets (∼136K samples, over 400 hours), pretrain three pioneering generalizable acoustic models, and build a benchmark consisting of 19 downstream respiratory health tasks for evaluation. Our pretrained models demonstrate superior performance (against existing acoustic models pretrained with general audio on 16 out of 19 tasks) and generalizability (to unseen datasets and new respiratory audio modalities). This highlights the great promise of respiratory acoustic foundation models and encourages more studies using OPERA as an open resource to accelerate research on respiratory audio for health.",,,NeurIPS.csv,,,,,,
K26PU7Y9,journalArticle,,"Liu, Shuo; Ying, Kaining; Zhang, Hao; Yang, Yue; Lin, Yuqi; Zhang, Tianle; Li, Chuanhao; Qiao, Yu; Luo, Ping; Shao, Wenqi; Zhang, Kaipeng",ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models,,,,"Multi-turn visual conversation is an important ability of real-world AI assistants. However, the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities ablation evaluation for Large Vision-Language Models (LVLMs). ConvBench comprises 577 curated multi-turn conversations, encompassing 215 tasks. These tasks are broad and open-ended, which resemble real-world user behaviors. ConvBench progressively examines the LVLMs’ perception, reasoning, and creativity capabilities in each conversation and can decouple these capabilities in evaluations and thus perform reliable error attribution. Besides, considering the diversity of open-ended questions, we introduce an efficient and reliable automatic evaluation framework. Experimental results reveal that ConvBench is a significant challenge for current LVLMs, even for GPT4V, which achieves only a 39.51% score. Besides, we have some insightful findings, such as the weak perception of LVLMs inhibits authentic strengths in reasoning and creation. We believe our design of hierarchical capabilities, decoupling capabilities evaluation, and multi-turn conversation can blaze a new trail in LVLMs evaluation. Code and benchmark are released at https://github.com/shirlyliu64/ConvBench.",,,NeurIPS.csv,,,,,,
R4CGUV6D,preprint,2024.0,"Ren, Yuchen; Chen, Zhiyuan; Qiao, Lifeng; Jing, Hongtai; Cai, Yuchen; Xu, Sheng; Ye, Peng; Ma, Xinzhu; Sun, Siqi; Yan, Hongliang; Yuan, Dong; Ouyang, Wanli; Liu, Xihui",BEACON: Benchmark for Comprehensive RNA Tasks and Language Models,,10.1101/2024.06.22.600190,http://biorxiv.org/lookup/doi/10.1101/2024.06.22.600190,"RNA plays a pivotal role in translating genetic instructions into functional outcomes, underscoring its importance in biological processes and disease mechanisms. Despite the emergence of numerous deep learning approaches for RNA, particularly universal RNA language models, there remains a significant lack of standardized benchmarks to assess the effectiveness of these methods. In this study, we introduce the first comprehensive RNA benchmark BEACON (BEnchmArk for COmprehensive RNA Task and Language Models). First, BEACON comprises 13 distinct tasks derived from extensive previous work covering structural analysis, functional studies, and engineering applications, enabling a comprehensive assessment of the performance of methods on various RNA understanding tasks. Second, we examine a range of models, including traditional approaches like CNNs, as well as advanced RNA foundation models based on language models, offering valuable insights into the task-specific performances of these models. Third, we investigate the vital RNA language model components from the tokenizer and positional encoding aspects. Notably, our findings emphasize the superiority of single nucleotide tokenization and the effectiveness of Attention with Linear Biases (ALiBi) over traditional positional encoding methods. Based on these insights, a simple yet strong baseline called BEACON-B is proposed, which can achieve outstanding performance with limited data and computational resources. The datasets and source code of our benchmark are available at https://github.com/terry-r123/RNABenchmark.",2024-06-28,,NeurIPS.csv,,,,,,
KRLWVTTX,journalArticle,,"Zhou, Yang; Faith, Tan Li Hui; Xu, Yanyu; Leng, Sicong; Xu, Xinxing; Liu, Yong; Goh, Rick Siow Mong",BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays,,,,"Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.",,,NeurIPS.csv,,,,,,
WKKSDCE4,journalArticle,,"Chen, Zhikai; Mao, Haitao; Liu, Jingzhe; Song, Yu; Li, Bingheng; Jin, Wei; Fatemi, Bahare; Tsitsulin, Anton; Perozzi, Bryan; Liu, Hui; Tang, Jiliang",Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights,,,,"Given the ubiquity of graph data and its applications in diverse domains, building a Graph Foundation Model (GFM) that can work well across different graphs and tasks with a unified backbone has recently garnered significant interests. A major obstacle to achieving this goal stems from the fact that graphs from different domains often exhibit diverse node features. Inspired by multi-modal models that align different modalities with natural language, the text has recently been adopted to provide a unified feature space for diverse graphs. Despite the great potential of these text-space GFMs, current research in this field is hampered by two problems. First, the absence of a comprehensive benchmark with unified problem settings hinders a clear understanding of the comparative effectiveness and practical value of different text-space GFMs. Second, there is a lack of sufficient datasets to thoroughly explore the methods’ full potential and verify their effectiveness across diverse settings. To address these issues, we conduct a comprehensive benchmark providing novel text-space datasets and comprehensive evaluation under unified problem settings. Empirical results provide new insights and inspire future research directions. Our code and data are publicly available from https://github.com/CurryTang/TSGFM.",,,NeurIPS.csv,,,,,,
F4QX38QX,journalArticle,,"Mertens, Laurent; Yargholi, Elahe’; de Beeck, Hans Op",FindingEmo: An Image Dataset for Emotion Recognition in the Wild,,,,"We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.",,,NeurIPS.csv,,,,,,
RJHI9WAD,journalArticle,,"Liu, Yunong; Eyzaguirre, Cristobal; Li, Manling; Khanna, Shubh; Niebles, Juan Carlos; Ravi, Vineeth; Mishra, Saumitra; Liu, Weiyu; Wu, Jiajun",IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos,,,,"Shape assembly is a ubiquitous task in daily life, integral for constructing complex 3D structures like IKEA furniture. While significant progress has been made in developing autonomous agents for shape assembly, existing datasets have not yet tackled the 4D grounding of assembly instructions in videos, essential for a holistic understanding of assembly in 3D space over time. We introduce IKEA Video Manuals, a dataset that features 3D models of furniture parts, instructional manuals, assembly videos from the Internet, and most importantly, annotations of dense spatio-temporal alignments between these data modalities. To demonstrate the utility of IKEA Video Manuals, we present five applications essential for shape assembly: assembly plan generation, part-conditioned segmentation, partconditioned pose estimation, video object segmentation, and furniture assembly based on instructional video manuals. For each application, we provide evaluation metrics and baseline methods. Through experiments on our annotated data, we highlight many challenges in grounding assembly instructions in videos to improve shape assembly, including handling occlusions, varying viewpoints, and extended assembly sequences.",,,NeurIPS.csv,,,,,,
NGZAJ3C7,journalArticle,,"Kajic, Ivana; Wiles, Olivia; Albuquerque, Isabela; Bauer, Matthias; Wang, Su; Pont-Tuset, Jordi; Nematzadeh, Aida",Evaluating Numerical Reasoning in Text-to-Image Models,,,,"Text-to-image generative models are capable of producing high-quality images that often faithfully depict concepts described using natural language. In this work, we comprehensively evaluate a range of text-to-image models on numerical reasoning tasks of varying difficulty, and show that even the most advanced models have only rudimentary numerical skills. Specifically, their ability to correctly generate an exact number of objects in an image is limited to small numbers, it is highly dependent on the context the number term appears in, and it deteriorates quickly with each successive number. We also demonstrate that models have poor understanding of linguistic quantifiers (such as “a few” or “as many as”), the concept of zero, and struggle with more advanced concepts such as partial quantities and fractional representations. We bundle prompts, generated images and human annotations into GECKONUM, a novel benchmark for evaluation of numerical reasoning.",,,NeurIPS.csv,,,,,,
IIFHEXNH,journalArticle,,"Pi, Renjie; Zhang, Jianshu; Zhang, Jipeng; Pan, Rui; Chen, Zhekai; Zhang, Tong",Image Textualization : An Automatic Framework for Creating Accurate and Detailed Image Descriptions,,,,"Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval. Currently, image description datasets primarily originate from two sources. One source is the scraping of image-text pairs from the web. Despite their abundance, these descriptions are often of low quality and noisy. Another is through human labeling. Datasets such as COCO are generally very short and lack details. Although detailed image descriptions can be annotated by humans, the high annotation cost limits the feasibility. These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions. In this paper, we propose an innovative framework termed Image Textualization (IT), which automatically produces high-quality image descriptions by leveraging existing multi-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner, which maximally convert the visual information into text. To address the current lack of benchmarks for detailed descriptions, we propose several benchmarks for comprehensive evaluation, which verifies the quality of image descriptions created by our framework. Furthermore, we show that LLaVA-7B, benefiting from fine-tuning on IT-curated descriptions, acquire improved capability to generate richer image descriptions, substantially increasing the length and detail of their output with less hallucination.",,,NeurIPS.csv,,,,,,
CLZ4ZLE9,journalArticle,,"Ma, Chang; Zhang, Junlei; Zhu, Zhihao; Yang, Cheng; Yang, Yujiu; Jin, Yaohui; Lan, Zhenzhong; Kong, Lingpeng; He, Junxian",AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents,,,,,,,NeurIPS.csv,,,,,,
Q8L7QZBW,journalArticle,,"Estermann, Benjamin; Lanzendörfer, Luca A; Niedermayr, Yannick; Wattenhofer, Roger",PUZZLES: A Benchmark for Neural Algorithmic Reasoning,,,,"Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal role in problem-solving and decision-making processes. Reinforcement Learning (RL) has demonstrated remarkable proficiency in tasks such as motor control, handling perceptual input, and managing stochastic environments. These advancements have been enabled in part by the availability of benchmarks. In this work we introduce PUZZLES, a benchmark based on Simon Tatham’s Portable Puzzle Collection, aimed at fostering progress in algorithmic and logical reasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizes and varying levels of complexity; many puzzles also feature a diverse set of additional configuration parameters. The 40 puzzles provide detailed information on the strengths and generalization capabilities of RL agents. Furthermore, we evaluate various RL algorithms on PUZZLES, providing baseline comparisons and demonstrating the potential for future research. All the software, including the environment, is available at https://github.com/ETH-DISCO/rlp.",,,NeurIPS.csv,,,,,,
DRJ87BZG,journalArticle,,"Hu, Yubin; Wen, Kairui; Zhou, Heng; Guo, Xiaoyang; Liu, Yong-Jin",SS3DM: Benchmarking Street-View Surface Reconstruction with a Synthetic 3D Mesh Dataset,,,,"Reconstructing accurate 3D surfaces for street-view scenarios is crucial for applications such as digital entertainment and autonomous driving simulation. However, existing street-view datasets, including KITTI, Waymo, and nuScenes, only offer noisy LiDAR points as ground-truth data for geometric evaluation of reconstructed surfaces. These geometric ground-truths often lack the necessary precision to evaluate surface positions and do not provide data for assessing surface normals. To overcome these challenges, we introduce the SS3DM dataset, comprising precise Synthetic Street-view 3D Mesh models exported from the CARLA simulator. These mesh models facilitate accurate position evaluation and include normal vectors for evaluating surface normal. To simulate the input data in realistic driving scenarios for 3D reconstruction, we virtually drive a vehicle equipped with six RGB cameras and five LiDAR sensors in diverse outdoor scenes. Leveraging this dataset, we establish a benchmark for state-of-the-art surface reconstruction methods, providing a comprehensive evaluation of the associated challenges. For more information, visit our homepage at https://ss3dm.top.",,,NeurIPS.csv,,,,,,
XTFK5ZGA,journalArticle,,"Lee, Dongwoo; Park, Joonkyu; Lee, Kyoung Mu",GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring,,,,,,,NeurIPS.csv,,,,,,
7N7TVFER,journalArticle,,"Patel, Aman; Singhal, Arpita; Wang, Austin; Pampari, Anusri; Kasowski, Maya; Kundaje, Anshul",DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA,,,,,,,NeurIPS.csv,,,,,,
5LRW9VXK,journalArticle,,"Xu, Xiaoyue; Ye, Qinyuan; Ren, Xiang",Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack,,,,,,,NeurIPS.csv,,,,,,
G2QFLD89,journalArticle,,"Liu, Ye; Ma, Zongyang; Qi, Zhongang; Wu, Yang; Shan, Ying; Chen, Chang Wen",E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding,,,,"Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our finegrained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.",,,NeurIPS.csv,,,,,,
LXJAAARP,dataset,2024.0,"Franzen, Jannik; Winklmayr, Claudia; Guarino, Vanessa Emanuela; Karg, Christoph; Yu, Xiaoyan; Koreuber, Nora; Albrecht, Jan Philipp; Bischoff, Philipp; Kainmueller, Dagmar",Arctique - ARtificial Colon Tissue Images for Qualitative Uncertainty Evaluation,,10.5281/ZENODO.11635056,https://zenodo.org/doi/10.5281/zenodo.11635056,"Uncertainty Quantification (UQ) is crucial for reliable image segmentation. Yet, while the field sees continual development of novel methods, a lack of agreedupon benchmarks limits their systematic comparison and evaluation: Current UQ methods are typically tested either on overly simplistic toy datasets or on complex real-world datasets that do not allow to discern true uncertainty. To unify both controllability and complexity, we introduce Arctique, a procedurally generated dataset modeled after histopathological colon images. We chose histopathological images for two reasons: 1) their complexity in terms of intricate object structures and highly variable appearance, which yields challenging segmentation problems, and 2) their broad prevalence for medical diagnosis and respective relevance of high-quality UQ. To generate Arctique, we established a Blender-based framework for 3D scene creation with intrinsic noise manipulation. Arctique contains up to 50,000 rendered images with precise masks as well as noisy label simulations. We show that by independently controlling the uncertainty in both images and labels, we can effectively study the performance of several commonly used UQ methods. Hence, Arctique serves as a critical resource for benchmarking and advancing UQ techniques and other methodologies in complex, multi-object environments, bridging the gap between realism and controllability. All code is publicly available, allowing re-creation and controlled manipulations of our shipped images as well as creation and rendering of new scenes.",2024-10-31,,NeurIPS.csv,,,,,,
F9PVDNIZ,journalArticle,,"Zhu, Haoyi; Wang, Yating; Huang, Di; Ye, Weicai; Ouyang, Wanli; He, Tong",Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning,,,,"In robot learning, the observation space is crucial due to the distinct characteristics of different modalities, which can potentially become a bottleneck alongside policy design. In this study, we explore the influence of various observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. We introduce OBSBench, a benchmark comprising two simulators and 125 tasks, along with standardized pipelines for various encoders and policy baselines. Extensive experiments on diverse contact-rich manipulation tasks reveal a notable trend: point cloud-based methods, even those with the simplest designs, frequently outperform their RGB and RGB-D counterparts. This trend persists in both scenarios: training from scratch and utilizing pre-training. Furthermore, our findings demonstrate that point cloud observations often yield better policy performance and significantly stronger generalization capabilities across various geometric and visual conditions. These outcomes suggest that the 3D point cloud is a valuable observation modality for intricate robotic tasks. We also suggest that incorporating both appearance and coordinate information can enhance the performance of point cloud methods. We hope our work provides valuable insights and guidance for designing more generalizable and robust robotic models.",,,NeurIPS.csv,,,,,,
R3UB78F4,journalArticle,,"Lyu, Ruiyuan; Lin, Jingli; Wang, Tai; Yang, Shuai; Mao, Xiaohan; Chen, Yilun; Xu, Runsen; Huang, Haifeng; Zhu, Chenming; Lin, Dahua; Pang, Jiangmiao",MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations,,,,"With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress. However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene. To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan. It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding. The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans’ correction in the loop to ensure the annotations are natural, correct, and comprehensive. Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks. We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.",,,NeurIPS.csv,,,,,,
IT9U8Z8B,journalArticle,,"Chen, Lin; Wei, Xilin; Li, Jinsong; Dong, Xiaoyi; Zhang, Pan; Zang, Yuhang; Chen, Zehui; Duan, Haodong; Lin, Bin; Tang, Zhenyu; Yuan, Li; Qiao, Yu; Lin, Dahua; Zhao, Feng; Wang, Jiaqi",ShareGPT4Video: Improving Video Understanding and Generation with Better Captions,,,,,,,NeurIPS.csv,,,,,,
XYIKZGUN,journalArticle,,"Vu, Hien; Prabhune, Omkar; Raskar, Unmesh; Panditharatne, Dimuth; Chung, Hanwook; Choi, Christopher Y; Kim, Younghyun",MMCOWS: A Multimodal Dataset for Dairy Cattle Monitoring,,,,,,,NeurIPS.csv,,,,,,
PG3YYQYN,journalArticle,,"Ying, Jiahao; Cao, Yixin; Bai, Yushi; Sun, Qianru; Wang, Bo; Tang, Wei; Ding, Zhaojun; Yang, Yizhe; Huang, Xuanjing; Yan, Shuicheng",Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models,,,,"Large language models (LLMs) have achieved impressive performance across various natural language benchmarks, prompting a continual need to curate more difficult datasets for larger LLMs, which is costly and time-consuming. In this paper, we propose to automate dataset updating and provide systematical analysis regarding its effectiveness in dealing with benchmark leakage issue, difficulty control, and stability. Thus, once current benchmark has been mastered or leaked, we can update it for timely and reliable evaluation. There are two updating strategies: 1) mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence, and 2) extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom’s taxonomy of educational objectives. Extensive experiments on updated MMLU and BIG-Bench demonstrate the stability of the proposed strategies and find that the mimicking strategy can effectively alleviate issues of overestimation from benchmark leakage. In cases where the efficient mimicking strategy fails, our extending strategy still shows promising results. Additionally, by controlling the difficulty, we can better discern the models’ performance and enable fine-grained analysis — neither too difficult nor too easy an exam can fairly judge students’ learning status. To the best of our knowledge, we are the first to automate updating benchmarks for reliable and timely evaluation. Our demo leaderboard can be found at https://yingjiahao14.github.io/Automating-DatasetUpdates/.",,,NeurIPS.csv,,,,,,
IMSWIT28,journalArticle,,"Zhu, Liyun; Wang, Lei; Raj, Arjun; Gedeon, Tom; Chen, Chen",Advancing Video Anomaly Detection: A Concise Review and a New Dataset,,,,,,,NeurIPS.csv,,,,,,
3ARYZ66Z,journalArticle,,"Vladimirova, Mariia; Diemert, Eustache; Pavone, Federico",FairJob: A Real-World Dataset for Fairness in Online Systems,,,,"We introduce a fairness-aware dataset for job recommendation in advertising, designed to foster research in algorithmic fairness within real-world scenarios. It was collected and prepared to comply with privacy standards and business confidentiality. An additional challenge is the lack of access to protected user attributes such as gender, for which we propose a solution to obtain a proxy estimate. Despite being anonymized and including a proxy for a sensitive attribute, our dataset preserves predictive power and maintains a realistic and challenging benchmark. This dataset addresses a significant gap in the availability of fairnessfocused resources for high-impact domains like advertising – the actual impact being having access or not to precious employment opportunities, where balancing fairness and utility is a common industrial challenge. We also explore various stages in the advertising process where unfairness can occur and introduce a method to compute a fair utility metric for the job recommendations in online systems case from a biased dataset. Experimental evaluations of bias mitigation techniques on the released dataset demonstrate potential improvements in fairness and the associated trade-offs with utility.",,,NeurIPS.csv,,,,,,
D2ZZLUQL,journalArticle,,"Junczyk, Michał",BIGOS V2 Benchmark for Polish ASR: Curated Datasets and Tools for Reproducible Evaluation,,,,,,,NeurIPS.csv,,,,,,
DYBSMENP,journalArticle,,"Kuratov, Yuri; Bulatov, Aydar; Anokhin, Petr; Rodkin, Ivan; Sorokin, Dmitry; Sorokin, Artyom; Burtsev, Mikhail",BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack,,,,"In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models’ ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to incontext reasoning, Retrieval-Augmented Generation methods achieve a modest 60% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.",,,NeurIPS.csv,,,,,,
W2PBE9XP,journalArticle,,"Wang, Xihuai; Zhang, Shao; Zhang, Wenhao; Dong, Wentao; Chen, Jingxiao; Wen, Ying; Zhang, Weinan",ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination,,,,"Zero-shot coordination (ZSC) is a new cooperative multi-agent reinforcement learning (MARL) challenge that aims to train an ego agent to work with diverse, unseen partners during deployment. The significant difference between the deploymenttime partners’ distribution and the training partners’ distribution determined by the training algorithm makes ZSC a unique out-of-distribution (OOD) generalization challenge. The potential distribution gap between evaluation and deploymenttime partners leads to inadequate evaluation, which is exacerbated by the lack of appropriate evaluation metrics. In this paper, we present ZSC-Eval, the first evaluation toolkit and benchmark for ZSC algorithms. ZSC-Eval consists of: 1) Generation of evaluation partner candidates through behavior-preferring rewards to approximate deployment-time partners’ distribution; 2) Selection of evaluation partners by Best-Response Diversity (BR-Div); 3) Measurement of generalization performance with various evaluation partners via the Best-Response Proximity (BR-Prox) metric. We use ZSC-Eval to benchmark ZSC algorithms in Overcooked and Google Research Football environments and get novel empirical findings. We also conduct a human experiment of current ZSC algorithms to verify the ZSC-Eval’s consistency with human evaluation. ZSC-Eval is now available at https://github.com/sjtu-marl/ZSC-Eval.",,,NeurIPS.csv,,,,,,
ABLUNMWT,journalArticle,,"Wang, Wenhao; Yang, Yi",: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models,,,,,,,NeurIPS.csv,,,,,,
WC3M3QMN,journalArticle,,"Jiang, Yifan; Zhang, Jiarui; Sun, Kexuan; Sourati, Zhivar; Ahrabian, Kian; Ma, Kaixin; Ilievski, Filip; Pujara, Jay",MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning,,,,"While multi-modal large language models (MLLMs) have shown significant progress across popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints on numbers) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix). However, existing AVR benchmarks only consider a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 × 3 matrices). And they fail to capture all abstract reasoning patterns in human cognition necessary for addressing real-world tasks, such as geometric properties and object boundary understanding in realworld navigation. To evaluate MLLMs’ AVR abilities systematically, we introduce MARVEL founded on the core knowledge system in human cognition, a multidimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model performance is grounded in perception or reasoning, MARVEL complements the standard AVR question with perception questions in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with ten representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all MLLMs show near-random performance on MARVEL, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance). Although closed-source MLLMs, such as GPT-4V, show a promising understanding of reasoning patterns (on par with humans) after adding textual descriptions, this advantage is hindered by their weak perception abilities. We release our entire code and dataset at https://github.com/1171-jpg/MARVEL_AVR.",,,NeurIPS.csv,,,,,,
ZK6KY64S,journalArticle,,"Zhang, Hugh; Da, Jeff; Lee, Dean; Robinson, Vaughn; Wu, Catherine; Song, Will; Zhao, Tiffany; Raja, Pranav; Zhuang, Charlotte; Slack, Dylan; Lyu, Qin; Hendryx, Sean; Kaplan, Russell; Yue, Summer",A Careful Examination of Large Language Model Performance on Grade School Arithmetic,,,,,,,NeurIPS.csv,,,,,,
G9EEVG3W,journalArticle,,"Wei, Fangyun; Zhao, Jinjing; Yan, Kun; Zhang, Hongyang; Xu, Chang",A Large-Scale Human-Centric Benchmark for Referring Expression Comprehension in the LMM Era,,,,"Prior research in human-centric AI has primarily addressed single-modality tasks like pedestrian detection, action recognition, and pose estimation. However, the emergence of large multimodal models (LMMs) such as GPT-4V has redirected attention towards integrating language with visual content. Referring expression comprehension (REC) represents a prime example of this multimodal approach. Current human-centric REC benchmarks, typically sourced from general datasets, fall short in the LMM era due to their limitations, such as insufficient testing samples, overly concise referring expressions, and limited vocabulary, making them inadequate for evaluating the full capabilities of modern REC models. In response, we present HC-RefLoCo (Human-Centric Referring Expression Comprehension with Long Context), a benchmark that includes 13,452 images, 24,129 instances, and 44,738 detailed annotations, encompassing a vocabulary of 18,681 words. Each annotation, meticulously reviewed for accuracy, averages 93.2 words and includes topics such as appearance, human-object interaction, location, action, celebrity, and OCR. HC-RefLoCo provides a wider range of instance scales and diverse evaluation protocols, encompassing accuracy with various IoU criteria, scale-aware evaluation, and subject-specific assessments. Our experiments, which assess 24 models, highlight HC-RefLoCo’s potential to advance human-centric AI by challenging contemporary REC models with comprehensive and varied data. Our benchmark, along with the evaluation code, are available at https: //github.com/ZhaoJingjing713/HC-RefLoCo.",,,NeurIPS.csv,,,,,,
4ASC94KU,journalArticle,,"Pa, Victor-Alexandru; Singla, Adish",Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming,,,,"Generative models have demonstrated human-level proficiency in various benchmarks across domains like programming, natural sciences, and general knowledge. Despite these promising results on competitive benchmarks, they still struggle with seemingly simple problem-solving tasks typically carried out by elementary-level students. How do state-of-the-art models perform on standardized programmingrelated tests designed to assess computational thinking and problem-solving skills at schools? In this paper, we curate a novel benchmark involving computational thinking tests grounded in elementary visual programming domains. Our initial results show that state-of-the-art models like GPT-4o and Llama3 barely match the performance of an average school student. To further boost the performance of these models, we fine-tune them using a novel synthetic data generation methodology. The key idea is to develop a comprehensive dataset using symbolic methods that capture different skill levels, ranging from recognition of visual elements to multi-choice quizzes to synthesis-style tasks. We showcase how various aspects of symbolic information in synthetic data help improve fine-tuned models’ performance. We will release the full implementation and datasets to facilitate further research on enhancing computational thinking in generative models.",,,NeurIPS.csv,,,,,,
7JTD9BD7,journalArticle,,"Zhang, Jifan; Jain, Lalit; Guo, Yang; Chen, Jiayi; Zhou, Kuan Lok; Suresh, Siddharth; Wagenmaker, Andrew; Sievert, Scott; Rogers, Timothy; Jamieson, Kevin; Mankoff, Robert; Nowak, Robert",Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning,,,,"We present a novel multimodal preference dataset for creative tasks, consisting of over 250 million human ratings on more than 2.2 million captions, collected through crowdsourcing rating data for The New Yorker’s weekly cartoon caption contest over the past eight years. This unique dataset supports the development and evaluation of multimodal large language models and preference-based fine-tuning algorithms for humorous caption generation. We propose novel benchmarks for judging the quality of model-generated captions, utilizing both GPT4 and human judgments to establish ranking-based evaluation strategies. Our experimental results highlight the limitations of current fine-tuning methods, such as RLHF and DPO, when applied to creative tasks. Furthermore, we demonstrate that even stateof-the-art models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. As we conclude this extensive data collection effort, we release the entire preference dataset to the research community, fostering further advancements in AI humor generation and evaluation.",,,NeurIPS.csv,,,,,,
5JQ8WMAM,journalArticle,,"Su, Zhaochen; Zhang, Jun; Qu, Xiaoye; Zhu, Tong; Li, Yanshu; Sun, Jiashuo; Li, Juntao; Zhang, Min; Cheng, Yu",CONFLICTBANK: A Benchmark for Evaluating Knowledge Conﬂicts in Large Language Models,,,,"Large language models (LLMs) have achieved impressive advancements across numerous disciplines, yet the critical issue of knowledge conﬂicts, a major source of hallucinations, has rarely been studied. While a few research explored the conﬂicts between the inherent knowledge of LLMs and the retrieved contextual knowledge, a comprehensive assessment of knowledge conﬂict in LLMs is still missing. Motivated by this research gap, we ﬁrstly propose CONFLICTBANK, the largest benchmark with 7.45M claim-evidence pairs and 553k QA pairs, addressing conﬂicts from misinformation, temporal discrepancies, and semantic divergences. Using CONFLICTBANK, we conduct the thorough and controlled experiments for a comprehensive understanding of LLM behavior in knowledge conﬂicts, focusing on three key aspects: (i) conﬂicts encountered in retrieved knowledge, (ii) conﬂicts within the models’ encoded knowledge, and (iii) the interplay between these conﬂict forms. Our investigation delves into four model families and twelve LLM instances and provides insights into conﬂict types, model sizes, and the impact at different stages. We believe that knowledge conﬂicts represent a critical bottleneck to achieving trustworthy artiﬁcial intelligence and hope our work will offer valuable guidance for future model training and development. Resources are available at https://github.com/zhaochen0110/conflictbank.",,,NeurIPS.csv,,,,,,
D45CII3Z,journalArticle,,"Naik, Hemal; Yang, Junran; Das, Dipin; Crofoot, Margaret C; Rathore, Akanksha; Sridhar, Vivek Hari",BuckTales: A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes,,,,"Understanding animal behaviour is central to predicting, understanding, and mitigating impacts of natural and anthropogenic changes on animal populations and ecosystems. However, the challenges of acquiring and processing long-term, ecologically relevant data in wild settings have constrained the scope of behavioural research. The increasing availability of Unmanned Aerial Vehicles (UAVs), coupled with advances in machine learning, has opened new opportunities for wildlife monitoring using aerial tracking. However, limited availability of datasets with wild animals in natural habitats has hindered progress in automated computer vision solutions for long-term animal tracking. Here we introduce BuckTales, the first large-scale UAV dataset designed to solve multi-object tracking (MOT) and re-identification (Re-ID) problem in wild animals, specifically the mating behaviour (or lekking) of blackbuck antelopes. Collected in collaboration with biologists, the MOT dataset includes over 1.2 million annotations including 680 tracks across 12 high-resolution (5.4K) videos, each averaging 66 seconds and featuring 30 to 130 individuals. The Re-ID dataset includes 730 individuals captured with two UAVs simultaneously. The dataset is designed to drive scalable, long-term animal behaviour tracking using multiple camera sensors. By providing baseline performance with two detectors, and benchmarking several state-of-the-art tracking methods, our dataset reflects the real-world challenges of tracking wild animals in socially and ecologically relevant contexts. In making these data widely available, we hope to catalyze progress in MOT and Re-ID for wild animals, fostering insights into animal behaviour, conservation efforts, and ecosystem dynamics through automated, long-term monitoring.",,,NeurIPS.csv,,,,,,
4MKMH5VR,journalArticle,,"Haider, Momin; Yin, Ming; Zhang, Menglei; Gupta, Arpit; Zhu, Jing; Wang, Yu-Xiang",NetworkGym: Reinforcement Learning Environments for Multi-Access Trafﬁc Management in Network Simulation,,,,"Mobile devices such as smartphones, laptops, and tablets can often connect to multiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously. Recent advancements facilitate seamless integration of these connections below the transport layer, enhancing the experience for apps that lack inherent multi-path support. This optimization hinges on dynamically determining the trafﬁc distribution across networks for each device, a process referred to as multi-access trafﬁc splitting. This paper introduces NetworkGym, a high-ﬁdelity network environment simulator that facilitates generating multiple network trafﬁc ﬂows and multiaccess trafﬁc splitting. This simulator facilitates training and evaluating different RL-based solutions for the multi-access trafﬁc splitting problem. Our initial explorations demonstrate that the majority of existing state-of-the-art ofﬂine RL algorithms (e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average. This illustrates the urgent need to evaluate ofﬂine RL algorithms against a broader range of benchmarks, rather than relying solely on popular ones such as D4RL. We also propose an extension to the TD3+BC algorithm, named Pessimistic TD3 (PTD3), and demonstrate that it outperforms many state-of-the-art ofﬂine RL algorithms. PTD3’s behavioral constraint mechanism, which relies on value-function pessimism, is theoretically motivated and relatively simple to implement.",,,NeurIPS.csv,,,,,,
ZLZNADUV,journalArticle,,"Krojer, Benno; Vattikonda, Dheeraj; Lara, Luis; Jampani, Varun; Portelance, Eva; Pal, Christopher; Reddy, Siva",Learning Action and Reasoning-Centric Image Editing from Videos and Simulations,,,,"An image editing model should be able to perform diverse edits, ranging from object replacement, changing attributes or style, to performing actions or movement, which require many forms of reasoning. Current general instruction-guided editing models have significant shortcomings with action and reasoning-centric edits. Object, attribute or stylistic changes can be learned from visually static datasets. On the other hand, high-quality data for action and reasoning-centric edits is scarce and has to come from entirely different sources that cover e.g. physical dynamics, temporality and spatial reasoning. To this end, we meticulously curate the AURORA Dataset (Action-Reasoning-Object-Attribute), a collection of highquality training data, human-annotated and curated from videos and simulation engines. We focus on a key aspect of quality training data: triplets (source image, prompt, target image) contain a single meaningful visual change described by the prompt, i.e., truly minimal changes between source and target images. To demonstrate the value of our dataset, we evaluate an AURORA-finetuned model on a new expert-curated benchmark (AURORA-BENCH) covering 8 diverse editing tasks. Our model significantly outperforms previous editing models as judged by human raters. For automatic evaluations, we find important flaws in previous metrics and caution their use for semantically hard editing tasks. Instead, we propose a new automatic metric that focuses on discriminative understanding. We hope that our efforts : (1) curating a quality training dataset and an evaluation benchmark, (2) developing critical evaluations, and (3) releasing a state-of-the-art model1, will fuel further progress on general image editing.",,,NeurIPS.csv,,,,,,
6SHK2GYP,journalArticle,,"Jaume, Guillaume; Doucet, Paul; Song, Andrew H; Lu, Ming Y; Almagro-Pérez, Cristina; Wagner, Sophia J; Vaidya, Anurag J; Chen, Richard J; Williamson, Drew F K; Kim, Ahrong; Mahmood, Faisal",HEST-1k: A Dataset for Spatial Transcriptomics and Histology Image Analysis,,,,"Spatial transcriptomics enables interrogating the molecular composition of tissue with ever-increasing resolution and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods in ST to narrow tasks and small cohorts. In addition, the underlying tissue morphology, as reflected by H&E-stained whole slide images (WSIs), encodes rich information often overlooked in ST studies. Here, we introduce HEST-1k, a collection of 1,229 spatial transcriptomic profiles, each linked to a WSI and extensive metadata. HEST-1k was assembled from 153 public and internal cohorts encompassing 26 organs, two species (Homo Sapiens and Mus Musculus), and 367 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 2.1 million expression–morphology pairs and over 76 million nuclei. To support its development, we additionally introduce the HEST-Library, a Python package designed to perform a range of actions with HEST samples. We test HEST-1k and Library on three use cases: (1) benchmarking foundation models for pathology (HEST-Benchmark), (2) biomarker exploration, and (3) multimodal representation learning. HEST-1k, HEST-Library, and HEST-Benchmark can be freely accessed at https://github.com/mahmoodlab/hest.",,,NeurIPS.csv,,,,,,
SMDYYVAU,journalArticle,,"Wang, Bowen; Chang, Jiuyang; Qian, Yiming; Chen, Guoxin; Chen, Junhao; Jiang, Zhouqiang; Zhang, Jiahao; Nakashima, Yuta; Nagahara, Hajime",DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models,,,,"Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 511 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability and that of human doctors, highlighting the critical need for models that can reason effectively in real-world clinical scenarios ‡.",,,NeurIPS.csv,,,,,,
EY2IKJZS,journalArticle,,"Wu, Junchao; Zhan, Runzhe; Wong, Derek F; Yang, Shu; Yang, Xinyi; Yuan, Yulin; Chao, Lidia S",DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios,,,,"Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating various prompts usages, human revisions like word substitutions, and writing noises like spelling mistakes. Our development of DetectRL reveals the strengths and limitations of current SOTA detectors. More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe DetectRL could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors2.",,,NeurIPS.csv,,,,,,
EYHD7LUZ,journalArticle,,"Guo, Kehan; Nan, Bozhao; Zhou, Yujun; Guo, Taicheng; Guo, Zhichun; Surve, Mihir; Liang, Zhenwen; Chawla, Nitesh V; Wiest, Olaf; Zhang, Xiangliang",Can LLMs Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation,,,,,,,NeurIPS.csv,,,,,,
HFYQ2Z7E,journalArticle,,"Xie, Qianqian; Han, Weiguang; Chen, Zhengyu; Xiang, Ruoyu; Zhang, Xiao; He, Yueru; Xiao, Mengxi; Li, Dong; Dai, Yongfu; Feng, Duanyu; Xu, Yijing; Kang, Haoqiang; Kuang, Ziyan; Yuan, Chenhan; Yang, Kailai; Luo, Zheheng; Zhang, Tianlin; Liu, Zhiwei; Xiong, Guojun; Deng, Zhiyang; Jiang, Yuechen; Yao, Zhiyuan; Li, Haohang; Yu, Yangyang; Hu, Gang; Huang, Jiajia; Liu, Xiao-Yang; Lopez-Lira, Alejandro; Wang, Benyou; Lai, Yanzhao; Wang, Hao; Peng, Min; Ananiadou, Sophia; Huang, Jimin",FinBen: A Holistic Financial Benchmark for Large Language Models,,,,"LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 42 datasets spanning 24 financial tasks, covering eight critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, decision-making, and bilingual (English and Spanish). FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and two novel datasets for regulations and stock trading. Our evaluation of 21 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen’s potential to drive innovations in financial LLMs. All datasets and code are publicly available for the research community2, with results shared and updated regularly on the Open Financial LLM Leaderboard3.",,,NeurIPS.csv,,,,,,
UJIRXRWH,journalArticle,,"Jeon, Minkyu; Raghu, Rishwanth; Astore, Miro; Woollard, Geoffrey; Feathers, Ryan; Kaz, Alkin; Hanson, Sonya M; Cossio, Pilar; Zhong, Ellen D",CryoBench: Diverse and challenging datasets for the heterogeneity problem in cryo-EM,,,,"Cryo-electron microscopy (cryo-EM) is a powerful technique for determining high-resolution 3D biomolecular structures from imaging data. Its unique ability to capture structural variability has spurred the development of heterogeneous reconstruction algorithms that can infer distributions of 3D structures from noisy, unlabeled imaging data. Despite the growing number of advanced methods, progress in the field is hindered by the lack of standardized benchmarks with ground truth information and reliable validation metrics. Here, we introduce CryoBench, a suite of datasets, metrics, and benchmarks for heterogeneous reconstruction in cryo-EM. CryoBench includes five datasets representing different sources of heterogeneity and degrees of difficulty. These include conformational heterogeneity generated from designed motions of antibody complexes or sampled from a molecular dynamics simulation, as well as compositional heterogeneity from mixtures of ribosome assembly states or 100 common complexes present in cells. We then analyze stateof-the-art heterogeneous reconstruction tools, including neural and non-neural methods, assess their sensitivity to noise, and propose new metrics for quantitative evaluation. We hope that CryoBench will be a foundational resource for accelerating algorithmic development and evaluation in the cryo-EM and machine learning communities. Project page: https://cryobench.cs.princeton.edu.",,,NeurIPS.csv,,,,,,
9R4SF5L9,journalArticle,,"Melistas, Thomas; Spyrou, Nikos; Gkouti, Nefeli; Sanchez, Pedro; Panagakis, Yannis; Papanastasiou, Giorgos; Tsaftaris, Sotirios A",Benchmarking Counterfactual Image Generation,,,,"Generative AI has revolutionised visual content editing, empowering users to effortlessly modify images and videos. However, not all edits are equal. To perform realistic edits in domains such as natural image or medical imaging, modifications must respect causal relationships inherent to the data generation process. Such image editing falls into the counterfactual image generation regime. Evaluating counterfactual image generation is substantially complex: not only it lacks observable ground truths, but also requires adherence to causal constraints. Although several counterfactual image generation methods and evaluation metrics exist, a comprehensive comparison within a unified setting is lacking. We present a comparison framework to thoroughly benchmark counterfactual image generation methods. We evaluate the performance of three conditional image generation model families developed within the Structural Causal Model (SCM) framework. We incorporate several metrics that assess diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We integrate all models that have been used for the task at hand and expand them to novel datasets and causal graphs, demonstrating the superiority of Hierarchical VAEs across most datasets and metrics. Our framework is implemented in a userfriendly Python package that can be extended to incorporate additional SCMs, causal methods, generative models, and datasets for the community to build on. Code: https://github.com/gulnazaki/counterfactual-benchmark.",,,NeurIPS.csv,,,,,,
SCT7UJYA,journalArticle,,"Remonda, Adrian; Hansen, Nicklas; Raji, Ayoub; Musiu, Nicola; Bertogna, Marko; Veas, Eduardo E; Wang, Xiaolong",A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data,,,,"Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: https://assetto-corsa-gym.github.io.",,,NeurIPS.csv,,,,,,
4ZXIT6JT,journalArticle,,"Weber, Maurice; Fu, Daniel Y; Anthony, Quentin; Oren, Yonatan; Adams, Shane; Alexandrov, Anton; Lyu, Xiaozhong; Nguyen, Huu; Yao, Xiaozhe; Adams, Virginia; Athiwaratkun, Ben; Chalamala, Rahul; Chen, Kezhen; Ryabinin, Max; Dao, Tri; Liang, Percy; Ré, Christopher; Rish, Irina; Zhang, Ce",RedPajama: an Open Dataset for Training Large Language Models,,,,"Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce’s XGen and AI2’s OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.",,,NeurIPS.csv,,,,,,
ADXMQKYJ,journalArticle,,"Kotalwar, Nachiket; Gotovos, Alkis; Singla, Adish",Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation,,,,"Generative AI and large language models hold great promise in enhancing programming education by generating individualized feedback and hints for learners. Recent works have primarily focused on improving the quality of generated feedback to achieve human tutors’ quality. While quality is an important performance criterion, it is not the only criterion to optimize for real-world educational deployments. In this paper, we benchmark language models for programming feedback generation across several performance criteria, including quality, cost, time, and data privacy. The key idea is to leverage recent advances in the new paradigm of in-browser inference that allow running these models directly in the browser, thereby providing direct benefits across cost and data privacy. To boost the feedback quality of small models compatible with in-browser inference engines, we develop a fine-tuning pipeline based on GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM’s in-browser inference engine on three different Python programming datasets. We also release the full implementation along with a web app and datasets to facilitate further research on in-browser language models.",,,NeurIPS.csv,,,,,,
T2RZDH46,journalArticle,,"Blacher, Mark; Staudt, Christoph; Klaus, Julien; Wenig, Maurice; Merk, Niklas; Breuer, Alexander; Engel, Max; Laue, Sören; Giesen, Joachim",Einsum Benchmark: Enabling the Development of Next-Generation Tensor Execution Engines,,,,"Modern artiﬁcial intelligence and machine learning workﬂows rely on efﬁcient tensor libraries. However, tuning tensor libraries without considering the actual problems they are meant to execute can lead to a mismatch between expected performance and the actual performance. Einsum libraries are tuned to efﬁciently execute tensor expressions with only a few, relatively large, dense, ﬂoating-point tensors. But, practical applications of einsum cover a much broader range of tensor expressions than those that can currently be executed efﬁciently. For this reason, we have created a benchmark dataset that encompasses this broad range of tensor expressions, allowing future implementations of einsum to build upon and be evaluated against. In addition, we also provide generators for einsum expressions and converters to einsum expressions in our repository, so that additional data can be generated as needed. The benchmark dataset, the generators and converters are released openly and are publicly available at https://benchmark.einsum.org.",,,NeurIPS.csv,,,,,,
MFUF8JZG,journalArticle,,"Klein, Lukas; Lüth, Carsten; Schlegel, Udo; Bungert, Till; El-Assady, Mennatallah; Jäger, Paul",Navigating the Maze of Explainable AI: A Systematic Approach to Evaluating Methods and Metrics,,,,"Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed methods as well as metrics aiming to evaluate their efficacy. However, current studies are often of limited scope, examining only a handful of XAI methods and ignoring underlying design parameters for performance, such as the model architecture or the nature of input data. Moreover, they often rely on one or a few metrics and neglect thorough validation, increasing the risk of selection bias and ignoring discrepancies among metrics. These shortcomings leave practitioners confused about which method to choose for their problem. In response, we introduce LATEC, a large-scale benchmark that critically evaluates 17 prominent XAI methods using 20 distinct metrics. We systematically incorporate vital design parameters like varied architectures and diverse input modalities, resulting in 7,560 examined combinations. Through LATEC, we showcase the high risk of conflicting metrics leading to unreliable rankings and consequently propose a more robust evaluation scheme. Further, we comprehensively evaluate various XAI methods to assist practitioners in selecting appropriate methods aligning with their needs. Curiously, the emerging top-performing method, Expected Gradients, is not examined in any relevant related study. LATEC reinforces its role in future XAI research by publicly releasing all 326k saliency maps and 378k metric scores as a (meta-)evaluation dataset. The benchmark is hosted at: https://github.com/IML-DKFZ/latec.",,,NeurIPS.csv,,,,,,
FCKKWWZK,journalArticle,,"Meier, Manuel; Demirel, Berken Utku; Holz, Christian",WildPPG: A Real-World PPG Dataset of Long Continuous Recordings,,,,"Reflective photoplethysmography (PPG) has become the default sensing technique in wearable devices to monitor cardiac activity via a person’s heart rate (HR). However, PPG-based HR estimates can be substantially impacted by factors such as the wearer’s activities, sensor placement and resulting motion artifacts, as well as environmental characteristics such as temperature and ambient light. These and other factors can significantly impact and decrease HR prediction reliability.",,,NeurIPS.csv,,,,,,
KWXPUU73,journalArticle,,"Li, Baiqi; Lin, Zhiqiu; Peng, Wenxuan; Nyandwi, Jean de Dieu; Jiang, Daniel; Ma, Zixian; Khanuja, Simran; Krishna, Ranjay; Neubig, Graham; Ramanan, Deva",NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples,,,,"Vision-language models (VLMs) have made significant progress in recent visualquestion-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using offthe-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a vision-centric design by pairing each question with two images that yield different answers, preventing “blind” solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can largely be solved with language priors like commonsense knowledge. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like BLIP-3, LLaVA-OneVision, Cambrian-1, InternLM-XC2, Llama3.2-Vision, Molmo, Qwen2-VL, and even the (closed-source) GPT-4o lag 50%-70% behind human performance (which is above 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. We show that debiasing can be crucial for VLM performance. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.",,,NeurIPS.csv,,,,,,
PA92XWJB,journalArticle,,"Luo, Bingqiao; Zhang, Zhen; Wang, Qian; He, Bingsheng",Multi-Chain Graphs of Graphs: A New Approach to Analyzing Blockchain Datasets,,,,"Machine learning applied to blockchain graphs offers significant opportunities for enhanced data analysis and applications. However, the potential of this field is constrained by the lack of a large-scale, cross-chain dataset that includes hierarchical graph-level data. To address this issue, we present novel datasets that provide detailed label information at the token level and integrate interactions between tokens across multiple blockchain platforms. We model transactions within each token as local graphs and the relationships between tokens as global graphs, collectively forming a ""Graphs of Graphs"" (GoG) approach. This innovative approach facilitates a deeper understanding of systemic structures and hierarchical interactions, which are essential for applications such as link prediction, anomaly detection, and token classification. We conduct a series of experiments demonstrating that this dataset delivers new insights and challenges for exploring GoG within the blockchain domain. Our work promotes advancements and opens new avenues for research in both the blockchain and graph communities. Source code and datasets are available at https: //github.com/Xtra-Computing/Cryptocurrency-Graphs-of-graphs.",,,NeurIPS.csv,,,,,,
N33YU32G,journalArticle,1954.0,,"University of the Witwatersrand, Johannesburg, South Africa",Public Health,10.1016/S0033-3506(54)80136-3,https://linkinghub.elsevier.com/retrieve/pii/S0033350654801363,"Large language models (LLMs) have recently demonstrated great success in generating and understanding natural language. While they have also shown potential beyond the domain of natural language, it remains an open question as to what extent and in which way these LLMs can plan. We investigate their planning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark consisting of diverse 2D grid-based game maps. An LLM succeeds if it can traverse through given objectives, with a minimum number of steps and a minimum number of generation errors. We evaluate a number of LLMs on GTB and found that GPT-4-Turbo achieved the highest score of 44.97% on GTB_Score (GTBS), a composite score that combines the three above criteria. Furthermore, we preliminarily test large reasoning models, namely o1, which scores 67.84% on GTBS, indicating that the benchmark remains challenging for current models. Code, data, and documentation are available at https://github.com/umair-nasir14/Game-Traversal-Benchmark.",1954-10,,NeurIPS.csv,,,,,,
I5WSDYAN,journalArticle,,"Naug, Avisek; Guillen, Antonio; Luna, Ricardo; Gundecha, Vineet; Bash, Cullen; Ghorbanpour, Sahand; Mousavi, Sajad; Babu, Ashwin Ramesh; Markovikj, Dejan; Kashyap, Lekhapriya D; Rengarajan, Desik; Sarkar, Soumyendu",SustainDC: Benchmarking for Sustainable Data Center Control,,,,"Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multiagent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities to improve data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for developing and benchmarking advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.",,,NeurIPS.csv,,,,,,
U3D3F2H7,journalArticle,,"Zhou, Hangyu; Kao, Chia-Hsiang; Phoo, Cheng Perng; Mall, Utkarsh; Hariharan, Bharath; Bala, Kavita",AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in Satellite Imagery,,,,"Clouds in satellite imagery pose a significant challenge for downstream applications. A major challenge in current cloud removal research is the absence of a comprehensive benchmark and a sufficiently large and diverse training dataset. To address this problem, we introduce the largest public dataset — AllClear for cloud removal, featuring 23,742 globally distributed regions of interest (ROIs) with diverse land-use patterns, comprising 4 million images in total. Each ROI includes complete temporal captures from the year 2022, with (1) multi-spectral optical imagery from Sentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from Sentinel-1, and (3) auxiliary remote sensing products such as cloud masks and land cover maps. We validate the effectiveness of our dataset by benchmarking performance, demonstrating the scaling law — the PSNR rises from 28.47 to 33.87 with 30× more data, and conducting ablation studies on the temporal length and the importance of individual modalities. This dataset aims to provide comprehensive coverage of the Earth’s surface and promote better cloud removal results.",,,NeurIPS.csv,,,,,,
XH3Z8EEK,journalArticle,,"Cherian, Anoop; Peng, Kuan-Chuan; Lohit, Suhas; Matthiesen, Joanna; Smith, Kevin; Tenenbaum, Joshua B",Evaluating Large Vision-and-Language Models on Children’s Mathematical Olympiads,,,,"Recent years have seen a significant progress in the general-purpose problem solving abilities of large vision and language models (LVLMs), such as ChatGPT, Gemini, etc.; some of these breakthroughs even seem to enable AI models to outperform human abilities in varied tasks that demand higher-order cognitive skills. Are the current large AI models indeed capable of generalized problem solving as humans do? A systematic analysis of AI capabilities for joint vision and text reasoning, however, is missing in the current scientific literature. In this paper, we make an effort towards filling this gap, by evaluating state-of-the-art LVLMs on their mathematical and algorithmic reasoning abilities using visuo-linguistic problems from children’s Olympiads. Specifically, we consider problems from the Mathematical Kangaroo (MK) Olympiad, which is a popular international competition targeted at children from grades 1-12, that tests children’s deeper mathematical abilities using puzzles that are appropriately gauged to their age and skills. Using the puzzles from MK, we created a dataset, dubbed SMART-840, consisting of 840 problems from years 2020-2024. With our dataset, we analyze LVLMs power on mathematical reasoning; their responses on our puzzles offer a direct way to compare against that of children. Our results show that modern LVLMs do demonstrate increasingly powerful reasoning skills in solving problems for higher grades, but lack the foundations to correctly answer problems designed for younger children. Further analysis shows that there is no significant correlation between the reasoning capabilities of AI models and that of young children, and their capabilities appear to be based on a different type of reasoning than the cumulative knowledge that underlies children’s mathematics and logic skills.",,,NeurIPS.csv,,,,,,
SA9JE3YR,journalArticle,,"Lee, Kyungeun; Rhee, Wonjong",A Benchmark Suite for Evaluating Neural Mutual Information Estimators on Unstructured Datasets,,,,"Mutual Information (MI) is a fundamental metric for quantifying dependency between two random variables. When we can access only the samples, but not the underlying distribution functions, we can evaluate MI using sample-based estimators. Assessment of such MI estimators, however, has almost always relied on analytical datasets including Gaussian multivariates. Such datasets allow analytical calculations of the true MI values, but they are limited in that they do not reflect the complexities of real-world datasets. This study introduces a comprehensive benchmark suite for evaluating neural MI estimators on unstructured datasets, specifically focusing on images and texts. By leveraging same-class sampling for positive pairing and introducing a binary symmetric channel trick, we show that we can accurately manipulate true MI values of real-world datasets. Using the benchmark suite, we investigate seven challenging scenarios, shedding light on the reliability of neural MI estimators for unstructured datasets.",,,NeurIPS.csv,,,,,,
6IBQVQ4V,journalArticle,,"Li, Zehui; Subasri, Vallijah; Stan, Guy-Bart; Zhao, Yiren; Wang, Bo",GV-Rep: A Large-Scale Dataset for Genetic Variant Representation Learning,,,,"Genetic variants (GVs) are defined as differences in the DNA sequences among individuals and play a crucial role in diagnosing and treating genetic diseases. The rapid decrease in next generation sequencing cost, analogous to Moore’s Law, has led to an exponential increase in the availability of patient-level GV data. This growth poses a challenge for clinicians who must efficiently prioritize patientspecific GVs and integrate them with existing genomic databases to inform patient management. To addressing the interpretation of GVs, genomic foundation models (GFMs) have emerged. However, these models lack standardized performance assessments, leading to considerable variability in model evaluations. This poses the question: How effectively do deep learning methods classify unknown GVs and align them with clinically-verified GVs? We argue that representation learning, which transforms raw data into meaningful feature spaces, is an effective approach for addressing both indexing and classification challenges. We introduce a large-scale genetic variant dataset, named GV-Rep, featuring variable-length contexts and detailed annotations, designed for deep learning models to learn GV representations across various traits, diseases, tissue types, and experimental contexts. Our contributions are three-fold: (i) Construction of a comprehensive dataset with 7 million records, each labeled with characteristics of the corresponding variants, alongside additional data from 17,548 gene knockout tests across 1,107 cell types, 1,808 variant combinations, and 156 unique clinically-verified GVs from real-world patients. (ii) Analysis of the structure and properties of the dataset. (iii) Experimentation of the dataset with pre-trained genomic foundation models (GFMs). The results highlight a significant disparity between the current capabilities of GFMs and the accurate representation of GVs. We hope this dataset will advance genomic deep learning to bridge this gap.",,,NeurIPS.csv,,,,,,
6CD6HJSI,journalArticle,,"Werner, Thorben; Burchert, Johannes; Stubbemann, Maximilian; Schmidt-Thieme, Lars",A Cross-Domain Benchmark for Active Learning,,,,"Active Learning (AL) deals with identifying the most informative samples for labeling to reduce data annotation costs for supervised learning tasks. AL research suffers from the fact that lifts from literature generalize poorly and that only a small number of repetitions of experiments are conducted. To overcome these obstacles, we propose CDALBench, the first active learning benchmark which includes tasks in computer vision, natural language processing and tabular learning. Furthermore, by providing an efficient, greedy oracle, CDALBench can be evaluated with 50 runs for each experiment. We show, that both the cross-domain character and a large amount of repetitions are crucial for sophisticated evaluation of AL research. Concretely, we show that the superiority of specific methods varies over the different domains, making it important to evaluate Active Learning with a cross-domain benchmark. Additionally, we show that having a large amount of runs is crucial. With only conducting three runs as often done in the literature, the superiority of specific methods can strongly vary with the specific runs. This effect is so strong, that, depending on the seed, even a well-established method’s performance can be significantly better and significantly worse than random for the same dataset.",,,NeurIPS.csv,,,,,,
UGB54RWM,journalArticle,,"Shao, Hao; Qian, Shengju; Xiao, Han; Song, Guanglu; Zong, Zhuofan; Wang, Letian; Liu, Yu; Li, Hongsheng",Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning,,,,"Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges, we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally, about 98k pairs of them are annotated with detailed reasoning steps. Importantly, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available on this webpage to support further research in this area.",,,NeurIPS.csv,,,,,,
SFS3ST54,journalArticle,,"Sukthanker, Rhea Sanjay; Zela, Arber; Staffler, Benedikt; Klein, Aaron; Purucker, Lennart; Franke, Jörg K H; Hutter, Frank",HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models,,,,"The increasing size of language models necessitates a thorough analysis across multiple dimensions to assess trade-offs among crucial hardware metrics such as latency, energy consumption, GPU memory usage, and performance. Identifying optimal model configurations under specific hardware constraints is becoming essential but remains challenging due to the computational load of exhaustive training and evaluation on multiple devices. To address this, we introduce HWGPT-Bench, a hardware-aware benchmark that utilizes surrogate predictions to approximate various hardware metrics across 13 devices of architectures in the GPT2 family, with architectures containing up to 1.55B parameters. Our surrogates, via calibrated predictions and reliable uncertainty estimates, faithfully model the heteroscedastic noise inherent in the energy and latency measurements. To estimate perplexity, we employ weight-sharing techniques from Neural Architecture Search (NAS), inheriting pretrained weights from the largest GPT-2 model. Finally, we demonstrate the utility of HW-GPT-Bench by simulating optimization trajectories of various multi-objective optimization algorithms in just a few seconds.",,,NeurIPS.csv,,,,,,
8HGTXYTQ,journalArticle,,"Sun, Qingyun; Chen, Ziying; Yang, Beining; Ji, Cheng; Fu, Xingcheng; Zhou, Sheng; Peng, Hao; Li, Jianxin; Yu, Philip S",GC-Bench: An Open and Unified Benchmark for Graph Condensation,,,,"Graph condensation (GC) has recently garnered considerable attention due to its ability to reduce large-scale graph datasets while preserving their essential properties. The core concept of GC is to create a smaller, more manageable graph that retains the characteristics of the original graph. Despite the proliferation of graph condensation methods developed in recent years, there is no comprehensive evaluation and in-depth analysis, which creates a great obstacle to understanding the progress in this field. To fill this gap, we develop a comprehensive Graph Condensation Benchmark (GC-Bench) to analyze the performance of graph condensation in different scenarios systematically. Specifically, GC-Bench systematically investigates the characteristics of graph condensation in terms of the following dimensions: effectiveness, transferability, and complexity. We comprehensively evaluate 12 state-of-the-art graph condensation algorithms in node-level and graphlevel tasks and analyze their performance in 12 diverse graph datasets. Further, we have developed an easy-to-use library for training and evaluating different GC methods to facilitate reproducible research. The GC-Bench library is available at https://github.com/RingBDStack/GC-Bench.",,,NeurIPS.csv,,,,,,
VLPYXRXB,journalArticle,,"Zhao, Haozhe; Ma, Xiaojian; Chen, Liang; Si, Shuzheng; Wu, Rujie; An, Kaikai; Yu, Peiyu; Zhang, Minjia; Li, Qing; Chang, Baobao",UltraEdit: Instruction-based Fine-Grained Image Editing at Scale,,,,"This paper presents ULTRAEDIT, a large-scale (~4M editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix [10] and MagicBrush [71], and provide a systematic approach to producing massive and highquality image editing samples. ULTRAEDIT offers several distinct advantages: 1) It features a broader range of editing instructions by leveraging the creativity of large language models (LLMs) alongside in-context editing examples from human raters; 2) Its data sources are based on real images, including photographs and artworks, which provide greater diversity and reduced bias compared to datasets solely generated by text-to-image models; 3) It also supports region-based editing, enhanced by high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on ULTRAEDIT set new records on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models are available in github.com/pkunlp-icler/UltraEdit.",,,NeurIPS.csv,,,,,,
M95HSALJ,journalArticle,,"Zhang, Dan; Hu, Ziniu; Zhoubian, Sining; Du, Zhengxiao; Yang, Kaiyu; Wang, Zihan; Yue, Yisong; Dong, Yuxiao; Tang, Jie",SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models,,,,"Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs’ deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.",,,NeurIPS.csv,,,,,,
FLL6HW5L,journalArticle,,"Allen, Matt; Dorr, Francisco; Gallego-Mejia, Joseph A; Martínez-Ferrer, Laura; Jungbluth, Anna; Kalaitzis, Freddie; Ramos-Pollán, Raúl","M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and Multispectral Data",,,,"Satellite-based remote sensing has revolutionised the way we address global challenges in a rapidly evolving world. Huge quantities of Earth Observation (EO) data are generated by satellite sensors daily, but processing these large datasets for use in ML pipelines is technically and computationally challenging. Specifically, different types of EO data are often hosted on a variety of platforms, with differing degrees of availability for Python preprocessing tools. In addition, spatial alignment across data sources and data tiling for easier handling can present significant technical hurdles for novice users. While some preprocessed Earth observation datasets exist, their content is often limited to optical or near-optical wavelength data, which is ineffective at night or in adverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing technique based on microwave length radiation, offers a viable alternative. However, the application of machine learning to SAR has been limited due to a lack of ML-ready data and pipelines, particularly for the full diversity of SAR data, including polarimetry, coherence and interferometry. In this work, we introduce M3LEO, a multi-modal, multi-label Earth observation dataset that includes polarimetric, interferometric, and coherence SAR data derived from Sentinel-1, alongside multispectral Sentinel-2 imagery and a suite of auxiliary data describing terrain properties such as land use. M3LEO spans approximately 17M data chips, each measuring 4x4 km, across six diverse geographic regions. The dataset is complemented by a flexible PyTorch Lightning framework, with configuration management using Hydra, to accommodate its use across diverse ML applications in Earth observation. Additionally, we provide tools to process any dataset available on popular platforms such as Google Earth Engine for seamless integration with our framework. We show that the distribution shift in self-supervised embeddings is substantial across geographic regions, even when controlling for terrain properties. Data is available at huggingface.co/M3LEO, and code at github.com/spaceml-org/M3LEO.",,,NeurIPS.csv,,,,,,
YVHG9VDL,journalArticle,,"Zhang, Jiasheng; Chen, Jialin; Yang, Menglin; Feng, Aosong; Liang, Shuang; Shao, Jie; Ying, Rex",DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs,,,,"Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DyTAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at https://github.com/zjs123/DTGB.",,,NeurIPS.csv,,,,,,
ASV6G9S7,journalArticle,,"Chen, Houlun; Wang, Xin; Chen, Hong; Zhang, Zeyang; Feng, Wei; Huang, Bin; Jia, Jia; Zhu, Wenwu",VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding,,,,"Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding, which hinders precise video moment localization when given finegrained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic VidEo-text annotation pipeline to generate captions with RelIable FInE-grained statics and Dynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR. Code and Datasets are in https://github.com/hlchen23/VERIFIED.",,,NeurIPS.csv,,,,,,
IF62GV8J,journalArticle,,"Pal, Anisha; Kruk, Julia; Phute, Mansi; Bhattaram, Manognya; Yang, Diyi; Chau, Duen Horng; Hoffman, Judy",Semi-Truths: A Large-Scale Dataset of AI-Augmented Images for Evaluating Robustness of AI-Generated Image detectors,,,,"Text-to-image diffusion models have impactful applications in art, design, and entertainment, yet these technologies also pose significant risks by enabling the creation and dissemination of misinformation. Although recent advancements have produced AI-generated image detectors that claim robustness against various augmentations, their true effectiveness remains uncertain. Do these detectors reliably identify images with different levels of augmentation? Are they biased toward specific scenes or data distributions? To investigate, we introduce SEMI-TRUTHS, featuring 27, 600 real images, 223, 400 masks, and 1, 329, 155 AI-augmented images that feature targeted and localized perturbations produced using diverse augmentation techniques, diffusion models, and data distributions. Each augmented image is accompanied by metadata for standardized and targeted evaluation of detector robustness. Our findings suggest that state-of-the-art detectors exhibit varying sensitivities to the types and degrees of perturbations, data distributions, and augmentation methods used, offering new insights into their performance and limitations. The code for the augmentation and evaluation pipeline is available at https://github.com/J-Kruk/SemiTruths.",,,NeurIPS.csv,,,,,,
SZIUQ4HV,journalArticle,,"Arodi, Akshatha; Luck, Margaux; Bedwani, Jean-Luc; Zaimi, Aldo; Li, Ge; Pouliot, Nicolas; Beaudry, Julien; Caron, Gaétan Marceau",CableInspect-AD: An Expert-Annotated Anomaly Detection Dataset,,,,"Machine learning models are increasingly being deployed in real-world contexts. However, systematic studies on their transferability to speciﬁc and critical applications are underrepresented in the research literature. An important example is visual anomaly detection (VAD) for robotic power line inspection. While existing VAD methods perform well in controlled environments, real-world scenarios present diverse and unexpected anomalies that current datasets fail to capture. To address this gap, we introduce CableInspect-AD, a high-quality, publicly available dataset created and annotated by domain experts from Hydro-Québec, a Canadian public utility. This dataset includes high-resolution images with challenging real-world anomalies, covering defects with varying severity levels. To address the challenges of collecting diverse anomalous and nominal examples for setting a detection threshold, we propose an enhancement to the celebrated PatchCore algorithm. This enhancement enables its use in scenarios with limited labeled data. We also present a comprehensive evaluation protocol based on cross-validation to assess models’ performances. We evaluate our Enhanced-PatchCore for few-shot and many-shot detection, and Vision-Language Models for zero-shot detection. While promising, these models struggle to detect all anomalies, highlighting the dataset’s value as a challenging benchmark for the broader research community. Project page: https://mila-iqia.github.io/cableinspect-ad/.",,,NeurIPS.csv,,,,,,
BA8EN5SW,journalArticle,,"Defrance, MaryBeth; Buyl, Maarten; Bie, Tijl De",ABCFair: an Adaptable Benchmark approach for Comparing Fairness methods,,,,"Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, these subtle differences make it highly complicated to benchmark fairness methods, as their performance can strongly depend on exactly how the bias mitigation problem was originally framed.",,,NeurIPS.csv,,,,,,
J88CX3VQ,journalArticle,,"Leong, Jovin; Koa, Ming Di","SHDocs: A dataset, benchmark, and method to efficiently generate high-quality, real-world specular highlight data with near-perfect alignment",,,,,,,NeurIPS.csv,,,,,,
C385T6KG,journalArticle,,"Saeed, Mehreen; Chan, Adrian; Mijar, Anupam; Moukarzel, Joseph; Habchi, Georges; Younes, Carlos; Elias, Amin; Wong, Chau-Wai; Khater, Akram",Muharaf: Manuscripts of Handwritten Arabic Dataset for Cursive Text Recognition,,,,"We present the Manuscripts of Handwritten Arabic (Muharaf) dataset, which is a machine learning dataset consisting of more than 1,600 historic handwritten page images transcribed by experts in archival Arabic. Each document image is accompanied by spatial polygonal coordinates of its text lines as well as basic page elements. This dataset was compiled to advance the state of the art in handwritten text recognition (HTR), not only for Arabic manuscripts but also for cursive text in general. The Muharaf dataset includes diverse handwriting styles and a wide range of document types, including personal letters, diaries, notes, poems, church records, and legal correspondences. In this paper, we describe the data acquisition pipeline, notable dataset features, and statistics. We also provide a preliminary baseline result achieved by training convolutional neural networks using this data.",,,NeurIPS.csv,,,,,,
RIYWYZM5,journalArticle,,"Kirk, Hannah Rose; Whitefield, Alexander; Röttger, Paul; Bean, Andrew; Margatina, Katerina; Ciro, Juan; Mosquera, Rafael; Bartolo, Max; Williams, Adina; He, He; Vidgen, Bertie; Hale, Scott A","The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models",,,,,,,NeurIPS.csv,,,,,,
EJHCXCWN,journalArticle,,"Abdelnabi, Sahar; Gomaa, Amr; Sivaprasad, Sarath; Schönherr, Lea; Fritz, Mario","Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation",,,,"There is a growing interest in using Large Language Models (LLMs) in multi-agent systems to tackle interactive real-world tasks that require effective collaboration and assessment of complex situations. Yet, we have a limited understanding of LLMs’ communication and decision-making abilities in multi-agent setups. The fundamental task of negotiation spans many key features of communication, such as cooperation, competition, and manipulation potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We create a testbed of complex multi-agent, multi-issue, and semantically rich negotiation games. To reach an agreement, agents must have strong arithmetic, inference, exploration, and planning capabilities while integrating them in a dynamic and multi-turn setup. We propose metrics to rigorously quantify agents’ performance and alignment with the assigned role. We provide procedures to create new games and increase the difﬁculty of games to have an evolving benchmark. Importantly, we evaluate critical safety aspects such as the interaction dynamics between agents inﬂuenced by greedy and adversarial players. Our benchmark is highly challenging; GPT-3.5 and small models mostly fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform in reaching agreement in non-cooperative and more difﬁcult games1.",,,NeurIPS.csv,,,,,,
VJ4CNPQS,journalArticle,,"Pramanick, Shraman; Chellappa, Rama; Venugopalan, Subhashini",SPIQA: A Dataset for Multimodal Question Answering on Scientiﬁc Papers,,,,"Seeking answers to questions within long scientiﬁc research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientiﬁc papers are limited in scale and focus solely on textual content. We introduce SPIQA (Scientiﬁc Paper Image Question Answering), the ﬁrst large-scale QA dataset speciﬁcally designed to interpret complex ﬁgures and tables within the context of scientiﬁc research articles across various domains of computer science. Leveraging the breadth of expertise and ability of multimodal large language models (MLLMs) to understand ﬁgures, we employ automatic and manual curation to create the dataset. We craft an information-seeking task on interleaved images and text that involves multiple images covering plots, charts, tables, schematic diagrams, and result visualizations. SPIQA comprises 270K questions divided into training, validation, and three different evaluation splits. Through extensive experiments with 12 prominent foundational models, we evaluate the ability of current multimodal systems to comprehend the nuanced aspects of research articles. Additionally, we propose a Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that allows ﬁne-grained, step-by-step assessment and improves model performance. We further explore the upper bounds of performance enhancement with additional textual information, highlighting its promising potential for future research and the dataset’s impact on revolutionizing how we interact with scientiﬁc literature.",,,NeurIPS.csv,,,,,,
BA3ZHWMA,journalArticle,,"Wang, Yuqi; Cheng, Ke; He, Jiawei; Wang, Qitai; Dai, Hengchen; Chen, Yuntao; Xia, Fei; Zhang, Zhaoxiang",DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model,,,,"Driving world models have gained increasing attention due to their ability to model complex physical dynamics. However, their superb modeling capability is yet to be fully unleashed due to the limited video diversity in current driving datasets. We introduce DrivingDojo, the ﬁrst dataset tailor-made for training interactive world models with complex driving dynamics. Our dataset features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. We further deﬁne an action instruction following (AIF) benchmark for world models and demonstrate the superiority of the proposed dataset for generating action-controlled future predictions.",,,NeurIPS.csv,,,,,,
PXAN75Y4,preprint,2024.0,"Bean, Andrew M.; Hellsten, Simi; Mayne, Harry; Magomere, Jabez; Chi, Ethan A.; Chi, Ryan; Hale, Scott A.; Kirk, Hannah Rose",LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages,,10.48550/arXiv.2406.06196,http://arxiv.org/abs/2406.06196,"In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.",2024-10-31,,NeurIPS.csv,,,,,,
LA6KYGKV,preprint,2024.0,"Jin, Zhuoran; Cao, Pengfei; Wang, Chenhao; He, Zhitao; Yuan, Hongbang; Li, Jiachun; Chen, Yubo; Liu, Kang; Zhao, Jun",RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models,,10.48550/arXiv.2406.10890,http://arxiv.org/abs/2406.10890,"Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",2024-06-16,,NeurIPS.csv,,,,,,
QAY2NUD7,preprint,2025.0,"Zhang, Jieyu; Huang, Weikai; Ma, Zixian; Michel, Oscar; He, Dong; Gupta, Tanmay; Ma, Wei-Chiu; Farhadi, Ali; Kembhavi, Aniruddha; Krishna, Ranjay",Task Me Anything,,10.48550/arXiv.2406.11775,http://arxiv.org/abs/2406.11775,"Benchmarks for large multimodal language models (MLMs) now serve to simultaneously assess the general capabilities of models instead of evaluating for a specific capability. As a result, when a developer wants to identify which models to use for their application, they are overwhelmed by the number of benchmarks and remain uncertain about which benchmark's results are most reflective of their specific use case. This paper introduces Task-Me-Anything, a benchmark generation engine which produces a benchmark tailored to a user's needs. Task-Me-Anything maintains an extendable taxonomy of visual assets and can programmatically generate a vast number of task instances. Additionally, it algorithmically addresses user queries regarding MLM performance efficiently within a computational budget. It contains 113K images, 10K videos, 2K 3D object assets, over 365 object categories, 655 attributes, and 335 relationships. It can generate 750M image/video question-answering pairs, which focus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals critical insights: open-source MLMs excel in object and attribute recognition but lack spatial and temporal understanding; each model exhibits unique strengths and weaknesses; larger models generally perform better, though exceptions exist; and GPT4o demonstrates challenges in recognizing rotating/moving objects and distinguishing colors.",2025-01-27,,NeurIPS.csv,,,,,,
JAU7B4GH,preprint,2024.0,"Kim, Hee Jae; Sengupta, Kathakoli; Kuribayashi, Masaki; Kacorri, Hernisa; Ohn-Bar, Eshed",Text to Blind Motion,,10.48550/arXiv.2412.05277,http://arxiv.org/abs/2412.05277,"People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io.",2024-12-06,,NeurIPS.csv,,,,,,
C29X6MZN,preprint,2025.0,"Bassi, Pedro R. A. S.; Li, Wenxuan; Tang, Yucheng; Isensee, Fabian; Wang, Zifu; Chen, Jieneng; Chou, Yu-Cheng; Kirchhoff, Yannick; Rokuss, Maximilian; Huang, Ziyan; Ye, Jin; He, Junjun; Wald, Tassilo; Ulrich, Constantin; Baumgartner, Michael; Roy, Saikat; Maier-Hein, Klaus H.; Jaeger, Paul; Ye, Yiwen; Xie, Yutong; Zhang, Jianpeng; Chen, Ziyang; Xia, Yong; Xing, Zhaohu; Zhu, Lei; Sadegheih, Yousef; Bozorgpour, Afshin; Kumari, Pratibha; Azad, Reza; Merhof, Dorit; Shi, Pengcheng; Ma, Ting; Du, Yuxin; Bai, Fan; Huang, Tiejun; Zhao, Bo; Wang, Haonan; Li, Xiaomeng; Gu, Hanxue; Dong, Haoyu; Yang, Jichen; Mazurowski, Maciej A.; Gupta, Saumya; Wu, Linshan; Zhuang, Jiaxin; Chen, Hao; Roth, Holger; Xu, Daguang; Blaschko, Matthew B.; Decherchi, Sergio; Cavalli, Andrea; Yuille, Alan L.; Zhou, Zongwei",Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?,,10.48550/arXiv.2411.03670,http://arxiv.org/abs/2411.03670,"How can we test AI performance? This question seems trivial, but it isn't. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone, a large-scale collaborative segmentation benchmark of 9 types of abdominal organs. This benchmark is based on 5,195 training CT scans from 76 hospitals around the world and 5,903 testing CT scans from 11 additional hospitals. This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across various out-of-distribution scenarios. We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms on three test sets. In addition, we also evaluated pre-existing AI frameworks--which, differing from algorithms, are more flexible and can support different algorithms--including MONAI from NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are committed to expanding this benchmark to encourage more innovation of AI algorithms for the medical domain.",2025-01-20,,NeurIPS.csv,,,,,,
UWSSZJTM,preprint,2024.0,"Zhang, Yihua; Fan, Chongyu; Zhang, Yimeng; Yao, Yuguang; Jia, Jinghan; Liu, Jiancheng; Zhang, Gaoyuan; Liu, Gaowen; Kompella, Ramana Rao; Liu, Xiaoming; Liu, Sijia",UnlearnCanvas: Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models,,10.48550/arXiv.2402.11846,http://arxiv.org/abs/2402.11846,"The technological advancements in diffusion models (DMs) have demonstrated unprecedented capabilities in text-to-image generation and are widely used in diverse applications. However, they have also raised significant societal concerns, such as the generation of harmful content and copyright disputes. Machine unlearning (MU) has emerged as a promising solution, capable of removing undesired generative capabilities from DMs. However, existing MU evaluation systems present several key challenges that can result in incomplete and inaccurate assessments. To address these issues, we propose UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates the evaluation of the unlearning of artistic styles and associated objects. This dataset enables the establishment of a standardized, automated evaluation framework with 7 quantitative metrics assessing various aspects of the unlearning performance for DMs. Through extensive experiments, we benchmark 9 state-of-the-art MU methods for DMs, revealing novel insights into their strengths, weaknesses, and underlying mechanisms. Additionally, we explore challenging unlearning scenarios for DMs to evaluate worst-case performance against adversarial prompts, the unlearning of finer-scale concepts, and sequential unlearning. We hope that this study can pave the way for developing more effective, accurate, and robust DM unlearning methods, ensuring safer and more ethical applications of DMs in the future. The dataset, benchmark, and codes are publicly available at https://unlearn-canvas.netlify.app/.",2024-10-29,,NeurIPS.csv,,,,,,
WLIXSJPZ,conferencePaper,2024.0,"Xiong, Zhangyang; Li, Chenghong; Liu, Kenkun; Liao, Hongjie; Hu, Jianqiao; Zhu, Junyi; Ning, Shuliang; Qiu, Lingteng; Wang, Chongjie; Wang, Shijie; Cui, Shuguang; Han, Xiaoguang",MVHumanNet: A Large-Scale Dataset of Multi-View Daily Dressing Human Captures,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01872,https://ieeexplore.ieee.org/document/10656111/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
6K8EAF8P,conferencePaper,2024.0,"Liu, Haolin; Ye, Chongjie; Nie, Yinyu; He, Yingfan; Han, Xiaoguang",LASA: Instance Reconstruction from Real Scans using A Large-scale Aligned Shape Annotation Dataset,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01933,https://ieeexplore.ieee.org/document/10657169/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
9C8KUSIR,conferencePaper,2024.0,"Sun, Peng; Shi, Bei; Yu, Daiwei; Lin, Tao",On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00897,https://ieeexplore.ieee.org/document/10657238/,"Contemporary machine learning, which involves training large neural networks on massive datasets, faces significant computational challenges. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggles with large-scale and high-resolution datasets, hindering its practicality and feasibility. Thus, we re-examine existing methods and identify three properties essential for real-world applications: realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationallyefficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various model architectures and datasets demonstrate the advancement of RDED: we can distill a dataset to 10 images per class from full ImageNet1K [6] within 7 minutes, achieving a notable 42% accuracy with ResNet-18 [14] on a single RTX-4090 GPU (while the SOTA only achieves 21% but requires 6 hours). Code: https://github.com/LINs-lab/RDED.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
NUMF9YYQ,conferencePaper,2024.0,"Zhang, Zhanwei; Chen, Minghao; Xiao, Shuai; Peng, Liang; Li, Hengjia; Lin, Binbin; Li, Ping; Wang, Wenxiao; Wu, Boxi; Cai, Deng",Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-Dataset 3D Object Detection,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01448,https://ieeexplore.ieee.org/document/10655619/,"Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA). These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain. However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background. Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process. To resolve this problem, in this paper, we propose a novel pseudo label refinery framework. Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy. This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box. Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process. We alleviate this issue by generating additional proposals and aligning RoI features across different domains. Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks. Code will be available at https://github.com/Zhanwei-Z/PERE.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
EPA2424Y,journalArticle,,"Verma, Aayush Atul; Chakravarthi, Bharatesh; Vaghela, Arpitsinh; Wei, Hua; Yang, Yezhou",eTraM: Event-based Traffic Monitoring Dataset,,,,"Event cameras, with their high temporal and dynamic range and minimal memory usage, have found applications in various fields. However, their potential in static traffic monitoring remains largely unexplored. To facilitate this exploration, we present eTraM - a first-of-itskind, fully event-based traffic monitoring dataset. eTraM offers 10 hr of data from different traffic scenarios in various lighting and weather conditions, providing a comprehensive overview of real-world situations. Providing 2M bounding box annotations, it covers eight distinct classes of traffic participants, ranging from vehicles to pedestrians and micro-mobility. eTraM’s utility has been assessed using state-of-the-art methods for traffic participant detection, including RVT, RED, and YOLOv8. We quantitatively evaluate the ability of event-based models to generalize on nighttime and unseen scenes. Our findings substantiate the compelling potential of leveraging event cameras for traffic monitoring, opening new avenues for research and application. eTraM is available at https: //eventbasedvision.github.io/eTraM .",,,CVPR.csv,,,,,,
KCXVKPD6,journalArticle,,"Gu, Jianyang; Vahidian, Saeed; Kungurtsev, Vyacheslav; Wang, Haonan; Jiang, Wei; You, Yang; Chen, Yiran",Efficient Dataset Distillation via Minimax Diffusion,,,,"Dataset distillation reduces the storage and computational consumption of training a network by generating a small surrogate dataset that encapsulates rich information of the original large-scale one. However, previous distillation methods heavily rely on the sample-wise iterative optimization scheme. As the images-per-class (IPC) setting or image resolution grows larger, the necessary computation will demand overwhelming time and resources. In this work, we intend to incorporate generative diffusion techniques for computing the surrogate dataset. Observing that key factors for constructing an effective surrogate dataset are representativeness and diversity, we design additional minimax criteria in the generative training to enhance these facets for the generated images of diffusion models. We present a theoretical model of the process as hierarchical diffusion control demonstrating the flexibility of the diffusion process to target these criteria without jeopardizing the faithfulness of the sample to the desired distribution. The proposed method achieves state-of-the-art validation performance while demanding much less computational resources. Under the 100-IPC setting on ImageWoof, our method requires less than one-twentieth the distillation time of previous methods, yet yields even better performance. Source code and generated data are available in https://github.com/vimar-gu/MinimaxDiffusion.",,,CVPR.csv,,,,,,
VQZEQAT3,preprint,2024.0,"Wu, Xiaoyang; Tian, Zhuotao; Wen, Xin; Peng, Bohao; Liu, Xihui; Yu, Kaicheng; Zhao, Hengshuang",Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training,,10.48550/arXiv.2308.09718,http://arxiv.org/abs/2308.09718,"The rapid advancement of deep learning models often attributes to their ability to leverage massive training data. In contrast, such privilege has not yet fully benefited 3D deep learning, mainly due to the limited availability of large-scale 3D datasets. Merging multiple available data sources and letting them collaboratively train a single model is a potential solution. However, due to the large domain gap between 3D point cloud datasets, such mixed supervision could adversely affect the model's performance and lead to degenerated performance (i.e., negative transfer) compared to single-dataset training. In view of this challenge, we introduce Point Prompt Training (PPT), a novel framework for multi-dataset synergistic learning in the context of 3D representation learning that supports multiple pre-training paradigms. Based on this framework, we propose Prompt-driven Normalization, which adapts the model to different datasets with domain-specific prompts and Language-guided Categorical Alignment that decently unifies the multiple-dataset label spaces by leveraging the relationship between label text. Extensive experiments verify that PPT can overcome the negative transfer associated with synergistic learning and produce generalizable representations. Notably, it achieves state-of-the-art performance on each dataset using a single weight-shared model with supervised multi-dataset training. Moreover, when served as a pre-training framework, it outperforms other pre-training approaches regarding representation quality and attains remarkable state-of-the-art performance across over ten diverse downstream tasks spanning both indoor and outdoor 3D scenarios.",2024-07-21,,CVPR.csv,,,,,,
JSR7GZ77,conferencePaper,2024.0,"Geada, Rob; Towers, David; Forshaw, Matthew; Atapour-Abarghouei, Amir; McGough, A. Stephen",Insights from the Use of Previously Unseen Neural Architecture Search Datasets,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02127,https://ieeexplore.ieee.org/document/10657037/,"The boundless possibility of neural networks which can be used to solve a problem – each with different performance – leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning methods as well as the best results from challenge participants.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
EAS85N62,conferencePaper,2024.0,"Khanna, Mukul; Mao, Yongsen; Jiang, Hanxiao; Haresh, Sanjay; Shacklett, Brennan; Batra, Dhruv; Clegg, Alexander; Undersander, Eric; Chang, Angel X.; Savva, Manolis",Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01550,https://ieeexplore.ieee.org/document/10657917/,"We contribute the Habitat Synthetic Scenes Dataset (HSSD-200), a dataset of 211 high-quality 3D scenes, and use it to test navigation agent generalization to realistic 3D environments. Our dataset represents real interiors and contains a diverse set of 18,656 models of real-world objects. We investigate the impact of synthetic 3D scene dataset scale and realism on the task of training embodied agents to find and navigate to objects (ObjectGoal navigation). By comparing to synthetic 3D scene datasets from prior work, we find that scale helps in generalization, but the benefits quickly saturate, making visual fidelity and correlation to real-world scenes more important. Our experiments show that agents trained on our smaller-scale dataset can outperform agents trained on much larger datasets. Surprisingly, we observe that agents trained on just 122 scenes from our dataset outperform agents trained on 10,000 scenes from the ProcTHOR-10K dataset in terms of zero-shot generalization in real-world scanned environments.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
LNR73Q6S,conferencePaper,2024.0,"Zhang, Xin; Du, Jiawei; Li, Yunsong; Xie, Weiying; Zhou, Joey Tianyi",Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02477,https://ieeexplore.ieee.org/document/10656153/,"Dataset pruning aims to construct a coreset capable of achieving performance comparable to the original, full dataset. Most existing dataset pruning methods rely on snapshot-based criteria to identify representative samples, often resulting in poor generalization across various pruning and cross-architecture scenarios. Recent studies have addressed this issue by expanding the scope of training dynamics considered, including factors such as forgetting event and probability change, typically using an averaging approach. However, these works struggle to integrate a broader range of training dynamics without overlooking well-generalized samples, which may not be sufficiently highlighted in an averaging manner. In this study, we propose a novel dataset pruning method termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS utilizes a dual-depth strategy to achieve a balance between incorporating extensive training dynamics and identifying representative samples for dataset pruning. In the first depth, we estimate the series of each sample’s individual contributions spanning the training progress, ensuring comprehensive integration of training dynamics. In the second depth, we focus on the variability of the sample-wise contributions identified in the first depth to highlight wellgeneralized samples. Extensive experiments conducted on CIFAR and ImageNet datasets verify the superiority of TDDS over previous SOTA methods. Specifically on CIFAR100, our method achieves 54.51% accuracy with only 10% training data, surpassing baselines methods by more than 12.69%. Our codes are available at https://github. com/zhangxin-xd/Dataset-Pruning-TDDS.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
IKX6QP43,conferencePaper,2024.0,"Le, Duy Tho; Gou, Chenhui; Datta, Stavya; Shi, Hengcan; Reid, Ian; Cai, Jianfei; Rezatofighi, Hamid",JRDB-PanoTrack: An Open-World Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02107,https://ieeexplore.ieee.org/document/10655429/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
BUXSFHWX,conferencePaper,2024.0,"Zhan, Xinyu; Yang, Lixin; Zhao, Yifei; Mao, Kangrui; Xu, Hanlin; Lin, Zenan; Li, Kailin; Lu, Cewu",OakInk2 : A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00050,https://ieeexplore.ieee.org/document/10657079/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
L4288P6F,conferencePaper,2024.0,"Feng, Chengjian; Zhong, Yujie; Jie, Zequn; Xie, Weidi; Ma, Lin",InstaGen: Enhancing Object Detection by Training on Synthetic Dataset,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01339,https://ieeexplore.ieee.org/document/10657847/,"In this paper, we present a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pretrained, generative diffusion model, to augment it with the ability of localising instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. We conduct thorough experiments to show that, this enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer, to enhance object detectors by training on its generated samples, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 ∼ 5.2 AP) scenarios.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
W7UI4ZIJ,preprint,2024.0,"Zimmer, Walter; Wardana, Gerhard Arya; Sritharan, Suren; Zhou, Xingcheng; Song, Rui; Knoll, Alois C.",TUMTraf V2X Cooperative Perception Dataset,,10.48550/arXiv.2403.01316,http://arxiv.org/abs/2403.01316,"Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, nearmiss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website.",2024-03-02,,CVPR.csv,,,,,,
XPGXZWMQ,preprint,2025.0,"Huang, Yifei; Chen, Guo; Xu, Jilan; Zhang, Mingfang; Yang, Lijin; Pei, Baoqi; Zhang, Hongjie; Dong, Lu; Wang, Yali; Wang, Limin; Qiao, Yu",EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World,,10.48550/arXiv.2403.16182,http://arxiv.org/abs/2403.16182,"Being able to map the activities of others into one’s own point of view is a fundamental human skill even from a very early age. Taking a step toward understanding this human ability, we introduce EgoExoLearn, a large-scale dataset that emulates the human demonstration following process, in which individuals record egocentric videos as they execute tasks guided by exocentric-view demonstration videos. Focusing on the potential applications in daily assistance and professional support, EgoExoLearn contains egocentric and demonstration video data spanning 120 hours captured in daily life scenarios and specialized laboratories. Along with the videos we record high-quality gaze data and provide detailed multimodal annotations, formulating a playground for modeling the human ability to bridge asynchronous procedural actions from different viewpoints. To this end, we present benchmarks such as crossview association, cross-view action planning, and crossview referenced skill assessment, along with detailed analysis. We expect EgoExoLearn can serve as an important resource for bridging the actions across views, thus paving the way for creating AI agents capable of seamlessly learning by observing humans in the real world. The dataset and benchmark codes are available at https: //github.com/OpenGVLab/EgoExoLearn.",2025-03-06,,CVPR.csv,,,,,,
WLH426M5,conferencePaper,2024.0,"Flepp, Roman; Ignatov, Andrey; Timofte, Radu; Van Gool, Luc",Real-World Mobile Image Denoising Dataset with Efficient Baselines,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02111,https://ieeexplore.ieee.org/document/10658253/,"The recently increased role of mobile photography has raised the standards of on-device photo processing tremendously. Despite the latest advancements in camera hardware, the mobile camera sensor area cannot be increased significantly due to physical constraints, leading to a pixel size of 0.6–2.0 µm, which results in strong image noise even in moderate lighting conditions. In the era of deep learning, one can train a CNN model to perform robust image denoising. However, there is still a lack of a substantially diverse dataset for this task. To address this problem, we introduce a novel Mobile Image Denoising Dataset (MIDD) comprising over 400,000 noisy / noise-free image pairs captured under various conditions by 20 different mobile camera sensors. Additionally, we propose a new DPreview test set consisting of data from 294 different cameras for precise model evaluation. Furthermore, we present the efficient baseline model SplitterNet for the considered mobile image denoising task that achieves high numerical and visual results, while being able to process 8MP photos directly on smartphone GPUs in under one second. Thereby outperforming models with similar runtimes. This model is also compatible with recent mobile NPUs, demonstrating an even higher speed when deployed on them. The conducted experiments demonstrate high robustness of the proposed solution when applied to images from previously unseen sensors, showing its high generalizability. The datasets, code and models can be found on the official project website1,2.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
87TVMP6C,journalArticle,,"Ling, Lu; Sheng, Yichen; Tu, Zhi; Zhao, Wentian; Xin, Cheng; Wan, Kun; Yu, Lantao; Guo, Qianyu; Yu, Zixun; Lu, Yawen; Li, Xuanmao; Sun, Xingpeng; Ashok, Rohan; Mukherjee, Aniruddha; Kang, Hao; Kong, Xiangrui; Hua, Gang; Zhang, Tianyi; Benes, Bedrich; Bera, Aniket",DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision,,,,,,,CVPR.csv,,,,,,
CGVPSQ9G,conferencePaper,2024.0,"Chen, Xiaoyang; Zheng, Hao; Li, Yuemeng; Ma, Yuncong; Ma, Liang; Li, Hongming; Fan, Yong",Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguation,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01116,https://ieeexplore.ieee.org/document/10656515/,"A versatile medical image segmentation model applicable to images acquired with diverse equipment and protocols can facilitate model deployment and maintenance. However, building such a model typically demands a large, diverse, and fully annotated dataset, which is challenging to obtain due to the labor-intensive nature of data curation. To address this challenge, we propose a cost-effective alternative that harnesses multi-source data with only partial or sparse segmentation labels for training, substantially reducing the cost of developing a versatile model. We devise strategies for model self-disambiguation, prior knowledge incorporation, and imbalance mitigation to tackle challenges associated with inconsistently labeled multi-source data, including label ambiguity and modality, dataset, and class imbalances. Experimental results on a multi-modal dataset compiled from eight different sources for abdominal structure segmentation have demonstrated the effectiveness and superior performance of our method compared to state-of-the-art alternative approaches. We anticipate that its cost-saving features, which optimize the utilization of existing annotated data and reduce annotation efforts for new data, will have a signiﬁcant impact in the ﬁeld.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
BXWUPI3L,conferencePaper,2024.0,"Qu, Chenfan; Zhong, Yiwu; Liu, Chongyu; Xu, Guitao; Peng, Dezhi; Guo, Fengjun; Jin, Lianwen",Towards Modern Image Manipulation Localization: A Large-Scale Dataset and Novel Methods,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01025,https://ieeexplore.ieee.org/document/10656008/,"In recent years, image manipulation localization has attracted increasing attention due to its pivotal role in guaranteeing social media security. However, how to accurately identify the forged regions remains an open challenge. One of the main bottlenecks lies in the severe scarcity of highquality data, due to its costly creation process. To address this limitation, we propose a novel paradigm, termed as CAAA, to automatically and precisely annotate the numerous manually forged images from the web at the pixel level. We further propose a novel metric QES to facilitate the automatic filtering of unreliable annotations. With CAAA and QES, we construct a large-scale, diverse, and high-quality dataset comprising 123,150 manually forged images with mask annotations. Besides, we develop a new model APSCNet for accurate image manipulation localization. According to extensive experiments, our dataset significantly improves the performance of various models on the widelyused benchmarks and such improvements are attributed to our proposed effective methods. The dataset and code are publicly available at https://github.com/qcf-568/MIML.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
6B9NMZML,conferencePaper,2024.0,"Xie, Yiming; Wei, Henglu; Liu, Zhenyi; Wang, Xiaoyu; Ji, Xiangyang",SynFog: A Photorealistic Synthetic Fog Dataset Based on End-to-End Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02056,https://ieeexplore.ieee.org/document/10656511/,"To advance research in learning-based defogging algorithms, various synthetic fog datasets have been developed. However, existing datasets created using the Atmospheric Scattering Model (ASM) or real-time rendering engines often struggle to produce photo-realistic foggy images that accurately mimic the actual imaging process. This limitation hinders the effective generalization of models from synthetic to real data. In this paper, we introduce an end-to-end simulation pipeline designed to generate photorealistic foggy images. This pipeline comprehensively considers the entire physically-based foggy scene imaging process, closely aligning with real-world image capture methods. Based on this pipeline, we present a new synthetic fog dataset named SynFog, which features both sky light and active lighting conditions, as well as three levels of fog density. Experimental results demonstrate that models trained on SynFog exhibit superior performance in visual perception and detection accuracy compared to others when applied to real-world foggy images.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
UWAVR42Z,preprint,2024.0,"Liang, Guoqiang; Chen, Kanghao; Li, Hangyu; Lu, Yunfan; Wang, Lin",Towards Robust Event-guided Low-Light Image Enhancement: A Large-Scale Real-World Event-Image Dataset and Novel Approach,,10.48550/arXiv.2404.00834,http://arxiv.org/abs/2404.00834,"Event camera has recently received much attention for low-light image enhancement (LIE) thanks to their distinct advantages, such as high dynamic range. However, current research is prohibitively restricted by the lack of largescale, real-world, and spatial-temporally aligned eventimage datasets. To this end, we propose a real-world (indoor and outdoor) dataset comprising over 30K pairs of images and events under both low and normal illumination conditions. To achieve this, we utilize a robotic arm that traces a consistent non-linear trajectory to curate the dataset with spatial alignment precision under 0.03mm. We then introduce a matching alignment strategy, rendering 90% of our dataset with errors less than 0.01s. Based on the dataset, we propose a novel event-guided LIE approach, called EvLight, towards robust performance in real-world low-light scenes. Specifically, we first design the multiscale holistic fusion branch to extract holistic structural and textural information from both events and images. To ensure robustness against variations in the regional illumination and noise, we then introduce a Signal-to-Noise-Ratio (SNR)-guided regional feature selection to selectively fuse features of images from regions with high SNR and enhance those with low SNR by extracting regional structure information from events. Extensive experiments on our dataset and the synthetic SDSD dataset demonstrate our EvLight significantly surpasses the frame-based methods, e.g., [4] by 1.14 dB and 2.62 dB, respectively.",2024-04-01,,CVPR.csv,,,,,,
T8KTWLYP,preprint,2024.0,"Wang, Wenbo; Ho, Hsuan-I.; Guo, Chen; Rong, Boxiang; Grigorev, Artur; Song, Jie; Zarate, Juan Jose; Hilliges, Otmar",4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations,,10.48550/arXiv.2404.18630,http://arxiv.org/abs/2404.18630,"The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.",2024-04-29,,CVPR.csv,,,,,,
CTZL8FIW,preprint,2024.0,"Ge, Yunhao; Tang, Yihe; Xu, Jiashu; Gokmen, Cem; Li, Chengshu; Ai, Wensi; Martinez, Benjamin Jose; Aydin, Arman; Anvari, Mona; Chakravarthy, Ayush K.; Yu, Hong-Xing; Wong, Josiah; Srivastava, Sanjana; Lee, Sharon; Zha, Shengxin; Itti, Laurent; Li, Yunzhu; Martín-Martín, Roberto; Liu, Miao; Zhang, Pengchuan; Zhang, Ruohan; Fei-Fei, Li; Wu, Jiajun",BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation,,10.48550/arXiv.2405.09546,http://arxiv.org/abs/2405.09546,"The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as ""filled"" and ""folded""), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction. Project website: https://behavior-vision-suite.github.io/",2024-05-15,,CVPR.csv,,,,,,
QXXHIN4Y,conferencePaper,2024.0,"Hao, Ruiyang; Fan, Siqi; Dai, Yingru; Zhang, Zhenlin; Li, Chenxi; Wang, Yuntian; Yu, Haibao; Yang, Wenxian; Yuan, Jirui; Nie, Zaiqing",RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02109,https://ieeexplore.ieee.org/document/10656910/,"The value of roadside perception, which could extend the boundaries of autonomous driving and trafﬁc management, has gradually become more prominent and acknowledged in recent years. However, existing roadside perception approaches only focus on the single-infrastructure sensor system, which cannot realize a comprehensive understanding of a trafﬁc area because of the limited sensing range and blind spots. Orienting high-quality roadside perception, we need Roadside Cooperative Perception (RCooper) to achieve practical area-coverage roadside perception for restricted trafﬁc areas. Rcooper has its own domain-speciﬁc challenges, but further exploration is hindered due to the lack of datasets. We hence release the ﬁrst real-world, large-scale RCooper dataset to bloom the research on practical roadside cooperative perception, including detection and tracking. The manually annotated dataset comprises 50k images and 30k point clouds, including two representative trafﬁc scenes (i.e., intersection and corridor). The constructed benchmarks prove the effectiveness of roadside cooperation perception and demonstrate the direction of further research. Codes and dataset can be accessed at: https://github.com/AIR-THU/DAIR-RCooper.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
FSL3VID7,conferencePaper,2024.0,"Jiang, Peng-Tao; Yang, Yuqi; Cao, Yang; Hou, Qibin; Cheng, Ming-Ming; Shen, Chunhua",Traffic Scene Parsing Through the TSP6K Dataset,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02066,https://ieeexplore.ieee.org/document/10657907/,"Traffic scene perception in computer vision is a critically important task to achieve intelligent cities. To date, most existing datasets focus on autonomous driving scenes. We observe that the models trained on those driving datasets often yield unsatisfactory results on traffic monitoring scenes. However, little effort has been put into improving the traffic monitoring scene understanding, mainly due to the lack of specific datasets. To fill this gap, we introduce a specialized traffic monitoring dataset, termed TSP6K, containing images from the traffic monitoring scenario, with high-quality pixellevel and instance-level annotations. The TSP6K dataset captures more crowded traffic scenes with several times more traffic participants than the existing driving scenes. We perform a detailed analysis of the dataset and comprehensively evaluate previous popular scene parsing methods, instance segmentation methods and unsupervised domain adaption methods. Furthermore, considering the vast difference in instance sizes, we propose a detail refining decoder for scene parsing, which recovers the details of different semantic regions in traffic scenes owing to the proposed TSP6K dataset. Experiments show its effectiveness in parsing the traffic monitoring scenes. Code and dataset are available at https://github.com/PengtaoJiang/TSP6K .",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
8SEJBQC4,conferencePaper,2024.0,"Li, Yiming; Li, Zhiheng; Chen, Nuo; Gong, Moonjun; Lyu, Zonglin; Wang, Zehong; Jiang, Peili; Feng, Chen",Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02081,https://ieeexplore.ieee.org/document/10658087/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
95GZ66HS,conferencePaper,2024.0,"Duan, Yuxing",LED: A Large-scale Real-world Paired Dataset for Event Camera Denoising,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02422,https://ieeexplore.ieee.org/document/10655931/,"Event camera has significant advantages in capturing dynamic scene information while being prone to noise interference, particularly in challenging conditions like low threshold and low illumination. However, most existing research focuses on gentle situations, hindering event camera applications in realistic complex scenarios. To tackle this limitation and advance the field, we construct a new paired real-world event denoising dataset (LED), including 3K sequences with 18K seconds of high-resolution (1200*680) event streams and showing three notable distinctions compared to others: diverse noise levels and scenes, largerscale with high-resolution, and high-quality GT. Specifically, it contains stepped parameters and varying illumination with diverse scenarios. Moreover, based on the property of noise events inconsistency and signal events consistency, we propose a novel effective denoising framework(DED) using homogeneous dual events to generate the GT with better separating noise from the raw. Furthermore, we design a bio-inspired baseline leveraging LeakyIntegrate-and-Fire (LIF) neurons with dynamic thresholds to realize accurate denoising. The experimental results demonstrate that the remarkable performance of the proposed approach on different datasets.The dataset and code are at https://github.com/Yee-Sing/led.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
QR72N4F7,conferencePaper,2024.0,"Vecchio, Giuseppe; Deschaintre, Valentin",MatSynth: A Modern PBR Materials Dataset,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02087,http://arxiv.org/abs/2401.06056,"We introduce MatSynth, a dataset of 4, 000+ CC0 ultrahigh resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation, and acquisition. However, in the past 6 years, most research in material acquisition or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset, we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset for material acquisition and generation applications. The complete data further contains metadata with each material’s origin, license, category, tags, creation method, and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www. gvecchio.com/matsynth.",2024-06-16,,CVPR.csv,,,,,,
6VY3PM9C,preprint,2024.0,"Lilja, Adam; Fu, Junsheng; Stenborg, Erik; Hammarstrand, Lars",Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It,,10.48550/arXiv.2312.06420,http://arxiv.org/abs/2312.06420,"The task of online mapping is to predict a local map using current sensor observations, e.g. from lidar and camera, without relying on a pre-built map. State-of-the-art methods are based on supervised learning and are trained predominantly using two datasets: nuScenes and Argoverse 2. However, these datasets revisit the same geographic locations across training, validation, and test sets. Specifically, over $80$% of nuScenes and $40$% of Argoverse 2 validation and test samples are less than $5$ m from a training sample. At test time, the methods are thus evaluated more on how well they localize within a memorized implicit map built from the training data than on extrapolating to unseen locations. Naturally, this data leakage causes inflated performance numbers and we propose geographically disjoint data splits to reveal the true performance in unseen environments. Experimental results show that methods perform considerably worse, some dropping more than $45$ mAP, when trained and evaluated on proper data splits. Additionally, a reassessment of prior design choices reveals diverging conclusions from those based on the original split. Notably, the impact of lifting methods and the support from auxiliary tasks (e.g., depth supervision) on performance appears less substantial or follows a different trajectory than previously perceived. Splits can be found at https://github.com/LiljaAdam/geographical-splits",2024-04-05,,CVPR.csv,,,,,,
IM6GKE7Z,conferencePaper,2024.0,"Burgert, Ryan D.; Price, Brian L.; Kuen, Jason; Li, Yijun; Ryoo, Michael S.",MAGICK: A Large-Scale Captioned Dataset from Matting Generated Images Using Chroma Keying,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02132,https://ieeexplore.ieee.org/document/10656389/,"We introduce MAGICK, a large-scale dataset of generated objects with high-quality alpha mattes. While image generation methods have produced segmentations, they cannot generate alpha mattes with accurate details in hair, fur, and transparencies. This is likely due to the small size of current alpha matting datasets and the difficulty in obtaining ground-truth alpha. We propose a scalable method for synthesizing images of objects with high-quality alpha that can be used as a ground-truth dataset. A key idea is to generate objects on a single-colored background so chroma keying approaches can be used to extract the alpha. However, this faces several challenges, including that current text-to-image generation methods cannot create images that can be easily chroma keyed and that chroma keying is an underconstrained problem that generally requires manual intervention for high-quality results. We address this using a combination of generation and alpha extraction methods. Using our method, we generate a dataset of 150,000 objects with alpha. We show the utility of our dataset by training an alpha-to-rgb generation method that outperforms baselines. Please see our project website at https://ryanndagreat.github.io/MAGICK/.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
P6SGFNE6,conferencePaper,2024.0,"Yan, Ming; Zhang, Yan; Cai, Shuqiang; Fan, Shuqi; Lin, Xincheng; Dai, Yudi; Shen, Siqi; Wen, Chenglu; Xu, Lan; Ma, Yuexin; Wang, Cheng",RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00219,https://ieeexplore.ieee.org/document/10655546/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
WMYZPHVK,preprint,2024.0,"Slyman, Eric; Lee, Stefan; Cohen, Scott; Kafle, Kushal",FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication,,10.48550/arXiv.2404.16123,http://arxiv.org/abs/2404.16123,"Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used imagecaption datasets collected from the web – datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-toimplement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",2024-04-24,,CVPR.csv,,,,,,
863Q7SJB,conferencePaper,2024.0,"Wu, Tao; He, Runyu; Wu, Gangshan; Wang, Limin",SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01754,https://ieeexplore.ieee.org/document/10656134/,"Video-based visual relation detection tasks, such as video scene graph generation, play important roles in finegrained video understanding. However, current video visual relation detection datasets have two main limitations that hinder the progress of research in this area. First, they do not explore complex human-human interactions in multi-person scenarios. Second, the relation types of existing datasets have relatively low-level semantics and can be often recognized by appearance or simple prior information, without the need for detailed spatio-temporal context reasoning. Nevertheless, comprehending high-level interactions between humans is crucial for understanding complex multi-person videos, such as sports and surveillance videos. To address this issue, we propose a new video visual relation detection task: video human-human interaction detection, and build a dataset named SportsHHI for it. SportsHHI contains 34 high-level interaction classes from basketball and volleyball sports. 118,075 human bounding boxes and 50,649 interaction instances are annotated on 11,398 keyframes. To benchmark this, we propose a twostage baseline method and conduct extensive experiments to reveal the key factors for a successful human-human interaction detector. We hope that SportsHHI can stimulate research on human interaction understanding in videos and promote the development of spatio-temporal context modeling techniques in video visual relation detection.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
KNJIGCQN,conferencePaper,2024.0,"Yuan, Tongtong; Zhang, Xuange; Liu, Kun; Liu, Bo; Chen, Chen; Jin, Jian; Jiao, Zhenzhen","Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02082,https://ieeexplore.ieee.org/document/10656129/,"Surveillance videos are important for public security. However, current surveillance video tasks mainly focus on classifying and localizing anomalous events. Existing methods are limited to detecting and classifying the predefined events with unsatisfactory semantic understanding, although they have obtained considerable performance. To address this issue, we propose a new research direction of surveillance video-and-language understanding (VALU), and construct the first multimodal surveillance video dataset. We manually annotate the real-world surveillance dataset UCFCrime with fine-grained event content and timing. Our newly annotated dataset, UCA (UCF-Crime Annotation)1, contains 23,542 sentences, with an average length of 20 words, and its annotated videos are as long as 110.7 hours. Furthermore, we benchmark SOTA models for four multimodal tasks on this newly created dataset, which serve as new baselines for surveillance VALU. Through experiments, we find that mainstream models used in previously public datasets perform poorly on surveillance video, demonstrating new challenges in surveillance VALU. We also conducted experiments on multimodal anomaly detection. These results demonstrate that our multimodal surveillance learning can improve the performance of anomaly detection. All the experiments highlight the necessity of constructing this dataset to advance surveillance AI.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
EEGSQ3HM,conferencePaper,2024.0,"Deng, Wenxiao; Li, Wenbin; Ding, Tianyu; Wang, Lei; Zhang, Hongguang; Huang, Kuihua; Huo, Jing; Gao, Yang",Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01614,https://ieeexplore.ieee.org/document/10656653/,"Dataset distillation has emerged as a promising approach in deep learning, enabling efﬁcient training with small synthetic datasets derived from larger real ones. Particularly, distribution matching-based distillation methods attract attention thanks to its effectiveness and low computational cost. However, these methods face two primary limitations: the dispersed feature distribution within the same class in synthetic datasets, reducing class discrimination, and an exclusive focus on mean feature consistency, lacking precision and comprehensiveness. To address these challenges, we introduce two novel constraints: a class centralization constraint and a covariance matching constraint. The class centralization constraint aims to enhance class discrimination by more closely clustering samples within classes. The covariance matching constraint seeks to achieve more accurate feature distribution matching between real and synthetic datasets through local feature covariance matrices, particularly beneﬁcial when sample sizes are much smaller than the number of features. Experiments demonstrate notable improvements with these constraints, yielding performance boosts of up to 6.6% on CIFAR10, 2.9% on SVHN, 2.5% on CIFAR100, and 2.5% on TinyImageNet, compared to the state-of-the-art relevant methods. In addition, our method maintains robust performance in cross-architecture settings, with a maximum performance drop of 1.7% on four architectures. Code is available at https://github.com/VincenDen/IID.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
62YNJGNW,conferencePaper,2024.0,"Xu, Jinglin; Zhao, Guohao; Yin, Sibo; Zhou, Wenhao; Peng, Yuxin",FineSports: A Multi-Person Hierarchical Sports Video Dataset for Fine-Grained Action Understanding,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02057,https://ieeexplore.ieee.org/document/10657871/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
HUHYD3IE,conferencePaper,2024.0,"Guo, Heng; Ren, Jieji; Wang, Feishi; Shi, Boxin; Ren, Mingjun; Matsushita, Yasuyuki",DiLiGenRT: A Photometric Stereo Dataset with Quantified Roughness and Translucency,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01122,https://ieeexplore.ieee.org/document/10655452/,"Photometric stereo faces challenges from non-Lambertian reflectance in real-world scenarios. Systematically measuring the reliability of photometric stereo methods in handling such complex reflectance necessitates a real-world dataset with quantitatively controlled reflectances. This paper introduces DiLiGenRT, the first real-world dataset for evaluating photometric stereo methods under quantified reflectances by manufacturing 54 hemispheres with varying degrees of two reflectance properties: Roughness and Transluency. Unlike qualitative and semantic labels, such as “diffuse” and “specular,” that have been used in previous datasets, our quantified dataset allows comprehensive and systematic benchmark evaluations. In addition, it facilitates selecting best-fit photometric stereo methods based on the quantitative reflectance properties. Our dataset and benchmark results are available at https://photometricstereo. github.io/diligentrt.html.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
J2TQ2TVC,conferencePaper,2024.0,"Nguyen, Thien-Minh; Yuan, Shenghai; Nguyen, Thien Hoang; Yin, Pengyu; Cao, Haozhi; Xie, Lihua; Wozniak, Maciej; Jensfelt, Patric; Thiel, Marko; Ziegenbein, Justin; Blunder, Noel",MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02105,https://ieeexplore.ieee.org/document/10656126/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
K4ISHFHM,conferencePaper,2024.0,"Shu, Yong; Shen, Liquan; Hu, Xiangyu; Li, Mengyao; Zhou, Zihao",Towards Real-World HDR Video Reconstruction: A Large-Scale Benchmark Dataset and A Two-Stage Alignment Network,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00278,https://ieeexplore.ieee.org/document/10658278/,"As an important and practical way to obtain high dynamic range (HDR) video, HDR video reconstruction from sequences with alternating exposures is still less explored, mainly due to the lack of large-scale real-world datasets. Existing methods are mostly trained on synthetic datasets, which perform poorly in real scenes. In this work, to facilitate the development of real-world HDR video reconstruction, we present Real-HDRV, a large-scale real-world benchmark dataset for HDR video reconstruction, featuring various scenes, diverse motion patterns, and high-quality labels. Specifically, our dataset contains 500 LDRs-HDRs video pairs, comprising about 28,000 LDR frames and 4,000 HDR labels, covering daytime, nighttime, indoor, and outdoor scenes. To our best knowledge, our dataset is the largest real-world HDR video reconstruction dataset. Correspondingly, we propose an end-to-end network for HDR video reconstruction, where a novel two-stage strategy is designed to perform alignment sequentially. Specifically, the first stage performs global alignment with the adaptively estimated global offsets, reducing the difficulty of subsequent alignment. The second stage implicitly performs local alignment in a coarse-to-fine manner at the feature level using the adaptive separable convolution. Extensive experiments demonstrate that: (1) models trained on our dataset can achieve better performance on real scenes than those trained on synthetic datasets; (2) our method outperforms previous state-of-the-art methods. Our dataset is available at https://github.com/yungsyu99/Real-HDRV.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
MVITSKC5,conferencePaper,2024.0,"Kent, Daniel; Alyaqoub, Mohammed; Lu, Xiaohu; Khatounabadi, Hamed; Sung, Kookjin; Scheller, Cole; Dalat, Alexander; Guo, Xinwei; Bin Thabit, Asma; Whitley, Roberto; Radha, Hayder",MSU-4S - The Michigan State University Four Seasons Dataset,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02138,https://ieeexplore.ieee.org/document/10656488/,"Public datasets, such as KITTI, nuScenes, and Waymo, have played a key role in the research and development of autonomous vehicles and advanced driver assistance systems. However, many of these datasets fail to incorporate a full range of driving conditions; some datasets only contain clear-weather conditions, underrepresenting or entirely missing colder weather conditions such as snow or autumn scenes with bright colorful foliage. In this paper, we present the Michigan State University Four Seasons (MSU4S) Dataset, which contains real-world collections of autonomous vehicle data from varied types of driving scenarios. These scenarios were recorded throughout a full range of seasons, and capture clear, rainy, snowy, and fall weather conditions, at varying times of day. MSU-4S contains more than 100,000 two- and three-dimensional frames for camera, lidar, and radar data, as well as Global Navigation Satellite System (GNSS), wheel speed, and steering data, all annotated with weather, time-of-day, and time-of-year. Our data includes cluttered scenes that have large numbers of vehicles and pedestrians; and it also captures industrial scenes, busy traffic thoroughfare with traffic lights and numerous signs, and scenes with dense foliage. While providing a diverse set of scenes, our data incorporate an important feature: virtually every scene and its corresponding lidar, camera, and radar frames were captured in four different seasons, enabling unparalleled object detection analysis and testing of the domain shift problem across weather conditions. In that context, we present detailed analyses for 3D and 2D object detection showing a strong domain shift effect among MSU-4S data segments collected across different conditions. MSU-4S will also enable advanced multimodal fusion research including different combinations of camera-lidar-radar fusion, which continues to be of strong interest for the computer vision, autonomous driving and ADAS development communities. The MSU-4S dataset is available online at https://egr.msu.edu/waves/msu4s.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
5U3P8TBK,preprint,2023.0,"Jung, HyunJun; Zhai, Guangyao; Wu, Shun-Cheng; Ruhkamp, Patrick; Schieber, Hannah; Rizzoli, Giulia; Wang, Pengyuan; Zhao, Hongcheng; Garattoni, Lorenzo; Meier, Sven; Roth, Daniel; Navab, Nassir; Busam, Benjamin",HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Perception Dataset with Household Objects in Realistic Scenarios,,10.48550/arXiv.2212.10428,http://arxiv.org/abs/2212.10428,"Estimating 6D object poses is a major challenge in 3D computer vision. Building on successful instance-level approaches, research is shifting towards category-level pose estimation for practical applications. Current category-level datasets, however, fall short in annotation quality and pose variety. Addressing this, we introduce HouseCat6D, a new category-level 6D pose dataset. It features 1) multi-modality with Polarimetric RGB and Depth (RGBD+P), 2) encompasses 194 diverse objects across 10 household categories, including two photometrically challenging ones, and 3) provides high-quality pose annotations with an error range of only 1.35 mm to 1.74 mm. The dataset also includes 4) 41 large-scale scenes with comprehensive viewpoint and occlusion coverage, 5) a checkerboard-free environment, and 6) dense 6D parallel-jaw robotic grasp annotations. Additionally, we present benchmark results for leading category-level pose estimation networks.",2023-12-01,,CVPR.csv,,,,,,
W3V8YX3G,journalArticle,,"Tudosiu, Petru-Daniel; Yang, Yongxin; Zhang, Shifeng; Chen, Fei; McDonagh, Steven; Lampouras, Gerasimos; Iacobacci, Ignacio; Parisot, Sarah",MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation,,,,,,,CVPR.csv,,,,,,
T8YUT66N,conferencePaper,2024.0,"Sun, Yanjun; Qiu, Yue; Khan, Mariia; Matsuzawa, Fumiya; Iwata, Kenji",The STVchrono Dataset: Towards Continuous Change Recognition in Time,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01338,https://ieeexplore.ieee.org/document/10657948/,"Recognizing continuous changes offers valuable insights into past historical events, supports current trend analysis, and facilitates future planning. This knowledge is crucial for a variety of fields, such as meteorology and agriculture, environmental science, urban planning and construction, tourism, and cultural preservation. Currently available datasets in the field of scene change understanding primarily concentrate on two main tasks: the detection of changed regions within a scene and the linguistic description of the change content. Existing datasets focus on recognizing discrete changes, such as adding or deleting an object from two images, and largely rely on artificially generated images. Consequently, the existing change understanding methods primarily focus on identifying distinct object differences, overlooking the importance of continuous, gradual changes occurring over extended time intervals.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
HKCD7W58,journalArticle,,"Jahangard, Simindokht; Cai, Zhixi; Wen, Shiki; Rezatofighi, Hamid",JRDB-Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Interactions Within Social Groups,,,,"Understanding human social behaviour is crucial in computer vision and robotics. Micro-level observations like individual actions fall short, necessitating a comprehensive approach that considers individual behaviour, intra-group dynamics, and social group levels for a thorough understanding. To address dataset limitations, this paper introduces JRDB-Social, an extension of JRDB [2]. Designed to fill gaps in human understanding across diverse indoor and outdoor social contexts, JRDB-Social provides annotations at three levels: individual attributes, intra-group interactions, and social group context. This dataset aims to enhance our grasp of human social dynamics for robotic applications. Utilizing the recent cutting-edge multi-modal large language models, we evaluated our benchmark to explore their capacity to decipher social human behaviour.",,,CVPR.csv,,,,,,
2C8LHBZW,conferencePaper,2024.0,"Su, Duo; Hou, Junjie; Gao, Weizhi; Tian, Yingjie; Tang, Bowen",D<sup>4</sup> M: Dataset Distillation via Disentangled Diffusion Model,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00555,https://ieeexplore.ieee.org/document/10657869/,"Dataset distillation offers a lightweight synthetic dataset for fast network training with promising test accuracy. To imitate the performance of the original dataset, most approaches employ bi-level optimization and the distillation space relies on the matching architecture. Nevertheless, these approaches either suffer significant computational costs on large-scale datasets or experience performance decline on cross-architectures. We advocate for designing an economical dataset distillation framework that is independent of the matching architectures. With empirical observations, we argue that constraining the consistency of the real and synthetic image spaces will enhance the cross-architecture generalization. Motivated by this, we introduce Dataset Distillation via Disentangled Diffusion Model (D4M), an efficient framework for dataset distillation. Compared to architecture-dependent methods, D4M employs latent diffusion model to guarantee consistency and incorporates label information into category prototypes. The distilled datasets are versatile, eliminating the need for repeated generation of distinct datasets for various architectures. Through comprehensive experiments, D4M demonstrates superior performance and robust generalization, surpassing the SOTA methods across most aspects.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
36F9K7SF,conferencePaper,2024.0,"Lu, Cheng-You; Zhou, Peisen; Xing, Angela; Pokhariya, Chandradeep; Dey, Arnab; Shah, Ishaan Nikhil; Mavidipalli, Rugved; Hu, Dylan; Comport, Andrew I.; Chen, Kefan; Sridhar, Srinath",DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02120,https://ieeexplore.ieee.org/document/10657232/,"Advances in neural fields are enabling high-fidelity capture of the shape and appearance of dynamic 3D scenes. However, their capabilities lag behind those offered by conventional representations such as 2D videos because of algorithmic challenges and the lack of large-scale multi-view real-world datasets. We address the dataset limitation with DiVa-360, a real-world 360◦ dynamic visual dataset that contains synchronized high-resolution and long-duration multi-view video sequences of table-scale scenes captured using a customized low-cost system with 53 cameras. It contains 21 object-centric sequences categorized by different motion types, 25 intricate hand-object interaction sequences, and 8 long-duration sequences for a total of 17.4 M image frames. In addition, we provide foreground-background segmentation masks, synchronized audio, and text descriptions. We benchmark the state-of-the-art dynamic neural field methods on DiVa-360 and provide insights about existing methods and future challenges on long-duration neural field capture.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
2HC7VPMH,conferencePaper,2024.0,"Rangwani, Harsh; Mondal, Pradipto; Mondal, Pradipto; Mishra, Mayank; Asokan, Ashish Ramayee; Babu, R. Venkatesh",DeiT-LT: Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02208,https://ieeexplore.ieee.org/document/10658499/,"Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self-attention blocks. However, unlike Convolutional Neural Network (CNN), ViT’s simple architecture has no informative inductive bias (e.g., locality, etc.). Due to this, ViT requires a large amount of data for pre-training. Various data-efﬁcient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on longtailed datasets. In DeiT-LT, we introduce an efﬁcient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. This leads to the learning of local CNN-like features in early ViT blocks, improving generalization for tail classes. Further, to mitigate overﬁtting, we propose distilling from a ﬂat CNN teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks. With the proposed DeiT-LT scheme, the distillation DIST token becomes an expert on the tail classes, and the classiﬁer CLS token becomes an expert on the head classes. The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture. We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from smallscale CIFAR-10 LT to large-scale iNaturalist-2018. Project Page: https://rangwani-harsh.github.io/DeiT-LT.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
LFA9BEYJ,conferencePaper,2024.0,"Shum, Ka Chun; Kim, Jaeyeon; Hua, Binh-Son; Nguyen, Duc Thanh; Yeung, Sai-Kit",Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00495,https://ieeexplore.ieee.org/document/10656426/,"Neural radiance field (NeRF) is an emerging technique for 3D scene reconstruction and modeling. However, current NeRF-based methods are limited in the capabilities of adding or removing objects. This paper fills the aforementioned gap by proposing a new language-driven method for object manipulation in NeRFs through dataset updates. Specifically, to insert an object represented by a set of multi-view images into a background NeRF, we use a text-to-image diffusion model to blend the object into the given background across views. The generated images are then used to update the NeRF so that we can render view-consistent images of the object within the background. To ensure view consistency, we propose a dataset update strategy that prioritizes the radiance field training based on camera poses in a poseordered manner. We validate our method in two case studies: object insertion and object removal. Experimental results show that our method can generate photo-realistic results and achieves state-of-the-art performance in NeRF editing.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
6LHBYJ9W,preprint,2024.0,"Dao, Trung Tuan; Vu, Duc Hong; Pham, Cuong; Tran, Anh",EFHQ: Multi-purpose ExtremePose-Face-HQ dataset,,10.48550/arXiv.2312.17205,http://arxiv.org/abs/2312.17205,"The existing facial datasets, while having plentiful images at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with profile or pitched faces. This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k high-quality images of faces at extreme poses. To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings. Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation, and face reenactment. Specifically, training with EFHQ helps models generalize well across diverse poses, significantly improving performance in scenarios involving extreme views, confirmed by extensive experiments. Additionally, we utilize EFHQ to define a challenging cross-view face verification benchmark, in which the performance of SOTA face recognition models drops 5-37% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild.",2024-04-11,,CVPR.csv,,,,,,
6MXVPKXP,conferencePaper,2024.0,"Nguyen, Hoang-Quan; Truong, Thanh-Dat; Nguyen, Xuan Bac; Dowling, Ashley; Li, Xin; Luu, Khoa",Insect-Foundation: A Foundation Model and Large-Scale 1M Dataset for Visual Insect Understanding,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02072,https://ieeexplore.ieee.org/document/10655889/,"In precision agriculture, the detection and recognition of insects play an essential role in the ability of crops to grow healthy and produce a high-quality yield. The current machine vision model requires a large volume of data to achieve high performance. However, there are approximately 5.5 million different insect species in the world. None of the existing insect datasets can cover even a fraction of them due to varying geographic locations and acquisition costs. In this paper, we introduce a novel “Insect1M” dataset, a game-changing resource poised to revolutionize insect-related foundation model training. Covering a vast spectrum of insect species, our dataset, including 1 million images with dense identiﬁcation labels of taxonomy hierarchy and insect descriptions, offers a panoramic view of entomology, enabling foundation models to comprehend visual and semantic information about insects like never before. Then, to efﬁciently establish an Insect Foundation Model, we develop a micro-feature self-supervised learning method with a Patch-wise Relevant Attention mechanism capable of discerning the subtle differences among insect images. In addition, we introduce Description Consistency loss to improve micro-feature modeling via insect descriptions. Through our experiments, we illustrate the effectiveness of our proposed approach in insect modeling and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks. Our Insect Foundation Model and Dataset promise to empower the next generation of insect-related vision models, bringing them closer to the ultimate goal of precision agriculture.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
W6Q2E3RU,conferencePaper,2024.0,"Liao, Jiaqi; Luo, Chuanchen; Du, Yinuo; Wang, Yuxi; Yin, Xucheng; Zhang, Man; Zhang, Zhaoxiang; Peng, Junran",HardMo: A Large-Scale Hardcase Dataset for Motion Capture,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00161,https://ieeexplore.ieee.org/document/10657414/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
EBDY4X3Y,conferencePaper,2024.0,"Zhang, Jing; Fang, Irving; Wu, Hao; Kaushik, Akshat; Rodriguez, Alice; Zhao, Hanwen; Zhang, Juexiao; Zheng, Zhuo; Iovita, Radu; Feng, Chen",LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02129,https://ieeexplore.ieee.org/document/10657240/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
9EXZP5US,conferencePaper,2024.0,"Guo, Yanwen; Li, Yuanqi; Ren, Dayong; Zhang, Xiaohong; Li, Jiawei; Pu, Liang; Ma, Changfeng; Zhan, Xiaoyu; Guo, Jie; Wei, Mingqiang; Zhang, Yan; Yu, Piaopiao; Yang, Shuangyu; Ji, Donghao; Ye, Huisheng; Sun, Hao; Liu, Yansong; Chen, Yinuo; Zhu, Jiaqi; Liu, Hongyu",LiDAR-Net: A Real-Scanned 3D Point Cloud Dataset for Indoor Scenes,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02076,https://ieeexplore.ieee.org/document/10655599/,"In this paper, we present LiDAR-Net, a new real-scanned indoor point cloud dataset, containing nearly 3.6 billion precisely point-level annotated points, covering an expansive area of 30,000m2. It encompasses three prevalent daily environments, including learning scenes, working scenes, and living scenes. LiDAR-Net is characterized by its nonuniform point distribution, e.g., scanning holes and scanning lines. Additionally, it meticulously records and annotates scanning anomalies, including reflection noise and ghost. These anomalies stem from specular reflections on glass or metal, as well as distortions due to moving persons. LiDAR-Net’s realistic representation of non-uniform distribution and anomalies significantly enhances the training of deep learning models, leading to improved generalization in practical applications. We thoroughly evaluate the performance of state-of-the-art algorithms on LiDARNet and provide a detailed analysis of the results. Crucially, our research identifies several fundamental challenges in understanding indoor point clouds, contributing essential insights to future explorations in this field. Our dataset can be found online: http://lidar-net.njumeta.com.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
U5J5VKSW,journalArticle,,"Qiu, Jielin; Zhu, Jiacheng; Han, William; Kumar, Aditesh; Mittal, Karthik; Jin, Claire; Yang, Zhengyuan; Li, Linjie; Wang, Jianfeng; Zhao, Ding; Li, Bo; Wang, Lijuan",MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos,,,,"Multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction. Nonetheless, numerous limitations exist within existing public MSMO datasets, including insufficient maintenance, data inaccessibility, limited size, and the absence of proper categorization, which pose significant challenges. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated the MMSum dataset. Our new dataset features (1) Humanvalidated summaries for both video and textual content, providing superior human instruction and labels for multimodal learning. (2) Comprehensively and meticulously arranged categorization, spanning 17 principal categories and 170 subcategories to encapsulate a diverse array of real-world scenarios. (3) Benchmark tests performed on the proposed dataset to assess various tasks and methods, including video summarization, text summarization, and multimodal summarization. To champion accessibility and collaboration, we released the MMSum dataset and the data collection tool as fully open-source resources, fostering transparency and accelerating future developments, at https://mmsum-dataset.github.io/.",,,CVPR.csv,,,,,,
4YZEX2CG,conferencePaper,2024.0,"Mahmoud, Anas; Elhoushi, Mostafa; Abbas, Amro; Yang, Yu; Ardalani, Newsha; Leather, Hugh; Morcos, Ari S.",Sieve: Multimodal Dataset Pruning Using Image Captioning Models,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02116,https://ieeexplore.ieee.org/document/10655641/,"Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy web-crawled datasets. This underscores the critical need for dataset pruning, as the quality of these datasets is strongly correlated with the performance of VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train models using highlyaligned samples is one of the most successful methods for pruning. We argue that this approach suffers from multiple limitations including: false positives and negatives due to CLIP’s pretraining on noisy labels. We propose a pruning signal, Sieve, that employs synthetic captions generated by image-captioning models pretrained on small, diverse, and well-aligned image-text pairs to evaluate the alignment of noisy image-text pairs. To bridge the gap between the limited diversity of generated captions and the high diversity of alternative text (alt-text), we estimate the semantic textual similarity in the embedding space of a language model pretrained on unlabeled text corpus. Using DataComp, a multimodal dataset ﬁltering benchmark, when evaluating on 38 downstream tasks, our pruning approach, surpasses CLIPScore by 2.6% and 1.7% on medium and large scale respectively. In addition, on retrieval tasks, Sieve leads to a signiﬁcant improvement of 2.7% and 4.5% on medium and large scale respectively.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
PZXYBNEF,conferencePaper,2024.0,"Jeon, Yujin; Choi, Eunsue; Kim, Youngchan; Moon, Yunseong; Omer, Khalid; Heide, Felix; Baek, Seung-Hwan",Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02086,https://ieeexplore.ieee.org/document/10656737/,"Image datasets are essential not only in validating existing methods in computer vision but also in developing new methods. Many image datasets exist, consisting of trichromatic intensity images taken with RGB cameras, which are designed to replicate human vision. However, polarization and spectrum, the wave properties of light that animals in harsh environments and with limited brain capacity often rely on, remain underrepresented in existing datasets. Although there are previous spectro-polarimetric datasets, they have insufﬁcient object diversity, limited illumination conditions, linear-only polarization data, and inadequate image count. Here, we introduce two spectro-polarimetric datasets, consisting of trichromatic Stokes images and hyperspectral Stokes images. These datasets encompass both linear and circular polarization; they introduce multiple spectral channels; and they feature a broad selection of real-world scenes. With our dataset in hand, we analyze the spectro-polarimetric image statistics, develop efﬁcient representations of such high-dimensional data, and evaluate spectral dependency of shape-from-polarization methods. As such, the proposed dataset promises a foundation for data-driven spectro-polarimetric imaging and vision research.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
VXE54RTQ,conferencePaper,2024.0,"Wang, Yinong Oliver; Chung, Younjoon; Wu, Chen Henry; De La Torre, Fernando",Domain Gap Embeddings for Generative Dataset Augmentation,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02710,https://ieeexplore.ieee.org/document/10657264/,"The performance of deep learning models is intrinsically tied to the quality, volume, and relevance of their training data. Gathering ample data for production scenarios often demands signiﬁcant time and resources. Among various strategies, data augmentation circumvents exhaustive data collection by generating new data points from existing ones. However, traditional augmentation techniques can be less effective amidst a shift in training and testing distributions. This paper explores the potential of synthetic data by leveraging large pre-trained models for data augmentation, especially when confronted with distribution shifts. Although recent advancements in generative models have enabled several prior works in cross-distribution data generation, they require model ﬁne-tuning and a complex setup. To bypass these shortcomings, we introduce Domain Gap Embeddings (DoGE), a plug-and-play semantic data augmentation framework in a cross-distribution few-shot setting. Our method extracts disparities between source and desired data distributions in a latent form, and subsequently steers a generative process to supplement the training set with endless diverse synthetic samples. Our evaluations, conducted on a subpopulation shift and three domain adaptation scenarios under a few-shot paradigm, reveal that our versatile method improves performance across tasks without needing hands-on intervention or intricate ﬁne-tuning. DoGE paves the way to effortlessly generate realistic, controllable synthetic datasets following the test distributions, bolstering real-world efﬁcacy for downstream task models.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
6FZI9XKV,preprint,2024.0,"Zhao, Shibo; Gao, Yuanjun; Wu, Tianhao; Singh, Damanpreet; Jiang, Rushan; Sun, Haoxiang; Sarawata, Mansi; Qiu, Yuheng; Whittaker, Warren; Higgins, Ian; Du, Yi; Su, Shaoshu; Xu, Can; Keller, John; Karhade, Jay; Nogueira, Lucas; Saha, Sourojit; Zhang, Ji; Wang, Wenshan; Wang, Chen; Scherer, Sebastian",SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments,,10.48550/arXiv.2307.07607,http://arxiv.org/abs/2307.07607,"Simultaneous localization and mapping (SLAM) is a fundamental task for numerous applications such as autonomous navigation and exploration. Despite many SLAM datasets have been released, current SLAM solutions still struggle to have sustained and resilient performance. One major issue is the absence of high-quality datasets including diverse all-weather conditions and a reliable metric for assessing robustness. This limitation significantly restricts the scalability and generalizability of SLAM technologies, impacting their development, validation, and deployment. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push SLAM towards all-weather environments to pursue the most robust SLAM performance. It contains multi-degraded environments including over 30 diverse scenes such as structureless corridors, varying lighting conditions, and perceptual obscurants like smoke and dust; multimodal sensors such as LiDAR, fisheye camera, IMU, and thermal camera; and multiple locomotions like aerial, legged, and wheeled robots. We developed accuracy and robustness evaluation tracks for SLAM and introduced novel robustness metrics. Comprehensive studies are performed, revealing new observations, challenges, and opportunities for future research.",2024-05-30,,CVPR.csv,,,,,,
HH5WVXHI,preprint,2024.0,"Zhang, He; Ren, Shenghao; Yuan, Haolei; Zhao, Jianhui; Li, Fan; Sun, Shuangpeng; Liang, Zhenghao; Yu, Tao; Shen, Qiu; Cao, Xun",MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors,,10.48550/arXiv.2403.17610,http://arxiv.org/abs/2403.17610,"Foot contact is an important cue for human motion capture, understanding, and generation. Existing datasets tend to annotate dense foot contact using visual matching with thresholding or incorporating pressure signals. However, these approaches either suffer from low accuracy or are only designed for small-range and slow motion. There is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/.",2024-03-30,,CVPR.csv,,,,,,
K3LMU29Y,conferencePaper,2024.0,"Chen, Hao; Hou, Yuqi; Qu, Chenyuan; Testini, Irene; Hong, Xiaohan; Jiao, Jianbo",$360+x$: A Panoptic Multi-modal Scene Understanding Dataset,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01833,https://ieeexplore.ieee.org/document/10657996/,"Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 different scene understanding tasks on the proposed 360+x dataset to evaluate the impact and benefit of each data modality and perspective in panoptic scene understanding. We hope this unique dataset could broaden the scope of comprehensive scene understanding and encourage the community to approach these problems from more diverse perspectives.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
TVSHGSED,conferencePaper,2024.0,"Wang, Xiao; Wang, Shiao; Tang, Chuanming; Zhu, Lin; Jiang, Bo; Tian, Yonghong; Tang, Jin",Event Stream-Based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01821,https://ieeexplore.ieee.org/document/10655615/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
PWZS32K7,preprint,2023.0,"Chen, Jiaben; Jiang, Huaizu",SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation,,10.48550/arXiv.2308.16876,http://arxiv.org/abs/2308.16876,"Human-centric video frame interpolation has great potential for enhancing entertainment experiences and finding commercial applications in the sports analysis industry, e.g., synthesizing slow-motion videos. Although there are multiple benchmark datasets available for video frame interpolation in the community, none of them is dedicated to human-centric scenarios. To bridge this gap, we introduce SportsSloMo, a benchmark featuring over 130K highresolution (≥720p) slow-motion sports video clips, totaling over 1M video frames, sourced from YouTube. We retrain several state-of-the-art methods on our benchmark, and we observed a noticeable decrease in their accuracy compared to other datasets. This highlights the difficulty of our benchmark and suggests that it poses significant challenges even for the best-performing methods, as human bodies are highly deformable and occlusions are frequent in sports videos. To tackle these challenges, we propose human-aware loss terms, where we add auxiliary supervision for human segmentation in panoptic settings and keypoints detection. These loss terms are model-agnostic and can be easily plugged into any video frame interpolation approach. Experimental results validate the effectiveness of our proposed human-aware loss terms, leading to consistent performance improvement over existing models. The dataset and code can be found at: https://neuvi.github.io/SportsSlomo/.",2023-12-12,,CVPR.csv,,,,,,
799BHHHL,conferencePaper,2024.0,"Huang, Huajian; Liu, Changkun; Zhu, Yipeng; Cheng, Hui; Braud, Tristan; Yeung, Sai-Kit",360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-Device Queries,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02106,https://ieeexplore.ieee.org/document/10655932/,"Portable 360◦ cameras are becoming a cheap and efficient tool to establish large visual databases. By capturing omnidirectional views of a scene, these cameras could expedite building environment models that are essential for visual localization. However, such an advantage is often overlooked due to the lack of valuable datasets. This paper introduces a new benchmark dataset, 360Loc, composed of 360◦ images with ground truth poses for visual localization. We present a practical implementation of 360◦ mapping combining 360◦ images with lidar data to generate the ground truth 6DoF poses. 360Loc is the first dataset and benchmark that explores the challenge of crossdevice visual positioning, involving 360◦ reference frames, and query frames from pinhole, ultra-wide FoV fisheye, and 360◦ cameras. We propose a virtual camera approach to generate lower-FoV query frames from 360◦ images, which ensures a fair comparison of performance among different query types in visual localization tasks. We also extend this virtual camera approach to feature matchingbased and pose regression-based methods to alleviate the performance loss caused by the cross-device domain gap, and evaluate its effectiveness against state-of-the-art baselines. We demonstrate that omnidirectional visual localization is more robust in challenging large-scale scenes with symmetries and repetitive structures. These results provide new insights into 360-camera mapping and omnidirectional visual localization with cross-device queries. Project Page and dataset: https://huajianup.github. io/research/360Loc/.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
7MEC7S4F,conferencePaper,2024.0,"Wang, Jiong; Yang, Fengyu; Li, Bingliang; Gou, Wenbo; Yan, Danqi; Zeng, Ailing; Gao, Yijun; Wang, Junle; Jing, Yanqing; Zhang, Ruimao",FreeMan: Towards Benchmarking 3D Human Pose Estimation Under Real-World Conditions,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02075,https://ieeexplore.ieee.org/document/10655984/,"Estimating the 3D structure of the human body from natural scenes is a fundamental aspect of visual perception. 3D human pose estimation is a vital step in advancing fields like AIGC and human-robot interaction, serving as a crucial technique for understanding and interacting with human actions in real-world settings. However, the current datasets, often collected under single laboratory conditions using complex motion capture equipment and unvarying backgrounds, are insufficient. The absence of datasets on variable conditions is stalling the progress of this crucial task. To facilitate the development of 3D pose estimation, we present FreeMan, the first large-scale, multi-view dataset collected under the real-world conditions. FreeMan was captured by synchronizing 8 smartphones across diverse scenarios. It comprises 11M frames from 8000 sequences, viewed from different perspectives. These sequences cover 40 subjects across 10 different scenarios, each with varying lighting conditions.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
RAHZR8CB,conferencePaper,2024.0,"Rosasco, Andrea; Berti, Stefano; Pasquale, Giulia; Malafronte, Damiano; Sato, Shogo; Segawa, Hiroyuki; Inada, Tetsugo; Natale, Lorenzo",ConCon-Chi: Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02099,https://ieeexplore.ieee.org/document/10655903/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
LAPTQBE9,conferencePaper,2024.0,"Khanna, Mukul; Ramrakhya, Ram; Chhablani, Gunjan; Yenamandra, Sriram; Gervet, Theophile; Chang, Matthew; Kira, Zsolt; Chaplot, Devendra Singh; Batra, Dhruv; Mottaghi, Roozbeh",GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01549,https://ieeexplore.ieee.org/document/10655426/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
PE5MZ8XG,conferencePaper,2024.0,"Ma, Cong; Qiao, Lei; Zhu, Chengkai; Liu, Kai; Kong, Zelong; Li, Qing; Zhou, Xueqi; Kan, Yuheng; Wu, Wei",HoloVic:Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02089,https://ieeexplore.ieee.org/document/10658281/,"Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicleinfrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
XXV7UPJS,conferencePaper,2024.0,"Zhao, Xiaoqi; Pang, Youwei; Chen, Zhenyu; Yu, Qian; Zhang, Lihe; Liu, Hanqi; Zuo, Jiaming; Lu, Huchuan","Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02079,https://ieeexplore.ieee.org/document/10655788/,"We conduct a comprehensive study on a new task named power battery detection (PBD), which aims to localize the dense cathode and anode plates endpoints from X-ray images to evaluate the quality of power batteries. Existing manufacturers usually rely on human eye observation to complete PBD, which makes it difficult to balance the accuracy and efficiency of detection. To address this issue and drive more attention into this meaningful task, we first elaborately collect a dataset, called X-ray PBD, which has 1, 500 diverse X-ray images selected from thousands of power batteries of 5 manufacturers, with 7 different visual interference. Then, we propose a novel segmentation-based solution for PBD, termed multi-dimensional collaborative network (MDCNet). With the help of line and counting predictors, the representation of the point segmentation branch can be improved at both semantic and detail aspects. Besides, we design an effective distance-adaptive mask generation strategy, which can alleviate the visual challenge caused by the inconsistent distribution density of plates to provide MDCNet with stable supervision. Without any bells and whistles, our segmentation-based MDCNet consistently outperforms various other corner detection, crowd counting and general/tiny object detection-based solutions, making it a strong baseline that can help facilitate future research in PBD. Finally, we share some potential difficulties and works for future researches. The source code and datasets will be publicly available at X-ray PBD.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
LTJDHBUP,conferencePaper,2024.0,"Liu, Yaofang; Cun, Xiaodong; Liu, Xuebo; Wang, Xintao; Zhang, Yong; Chen, Haoxin; Liu, Yang; Zeng, Tieyong; Chan, Raymond; Shan, Ying",EvalCrafter: Benchmarking and Evaluating Large Video Generation Models,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02090,https://ieeexplore.ieee.org/document/10657882/,"The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services have been developed to generate high-quality videos. However, these methods often use a few metrics, e.g., FVD [56] or IS [45], to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a novel framework and pipeline for exhaustively evaluating the performance of the generated videos. Our approach involves generating a diverse and comprehensive list of 700 prompts for text-to-video generation, which is based on an analysis of real-world user data and generated with the assistance of a large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmark, in terms of visual qualities, content qualities, motion qualities, and text-video alignment with 17 well-selected objective metrics. To obtain the final leaderboard of the models, we further fit a series of coefficients to align the objective metrics to the users’ opinions. Based on the proposed human alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
2EIM2GB7,conferencePaper,2024.0,"Yang, Karren D.; Ranjan, Anurag; Chang, Jen-Hao Rick; Vemulapalli, Raviteja; Tuzel, Oncel","Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02577,https://ieeexplore.ieee.org/document/10657034/,"We consider the task of animating 3D facial geometry from speech signal. Existing works are primarily deterministic, focusing on learning a one-to-one mapping from speech signal to 3D face meshes on small datasets with limited speakers. While these models can achieve high-quality lip articulation for speakers in the training set, they are unable to capture the full and diverse distribution of 3D facial motions that accompany speech in the real world. Importantly, the relationship between speech and facial motion is one-to-many, containing both inter-speaker and intraspeaker variations and necessitating a probabilistic approach. In this paper, we identify and address key challenges that have so far limited the development of probabilistic models: lack of datasets and metrics that are suitable for training and evaluating them, as well as the difficulty of designing a model that generates diverse results while remaining faithful to a strong conditioning signal as speech. We first propose large-scale benchmark datasets and metrics suitable for probabilistic modeling. Then, we demonstrate a probabilistic model that achieves both diversity and fidelity to speech, outperforming other methods across the proposed benchmarks. Finally, we showcase useful applications of probabilistic models trained on these large-scale datasets: we can generate diverse speechdriven 3D facial motion that matches unseen speaker styles extracted from reference clips; and our synthetic meshes can be used to improve the performance of downstream audio-visual models.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
AZFVUPKP,conferencePaper,2024.0,"Paplhám, Jakub; Franc, Vojtêch",A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00120,https://ieeexplore.ieee.org/document/10656298/,"Comparing different age estimation methods poses a challenge due to the unreliability of published results stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. This paper identifies two trivial, yet persistent issues with the currently used evaluation protocol and describes how to resolve them. We offer an extensive comparative analysis for state-of-the-art facial age estimation methods. Surprisingly, we find that the performance differences between the methods are negligible compared to the effect of other factors, such as facial alignment, facial coverage, image resolution, model architecture, or the amount of data used for pretraining. We use the gained insights to propose using FaRL as the backbone model and demonstrate its effectiveness on all public datasets. We make the source code and exact data splits public on GitHub and in the supplementary material.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
A9NE2VBL,conferencePaper,2024.0,"Hua, Tonayan; Wang, Lin",Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02017,https://ieeexplore.ieee.org/document/10656090/,"Implicit neural representation (INR), in combination with geometric rendering, has recently been employed in real-time dense RGB-D SLAM. Despite active research endeavors being made, there lacks a unified protocol for fair evaluation, impeding the evolution of this area. In this work, we establish, to our knowledge, the first open-source benchmark framework to evaluate the performance of a wide spectrum of commonly used INRs and rendering functions for mapping and localization. The goal of our benchmark is to 1) gain an intuition of how different INRs and rendering functions impact mapping and localization and 2) establish a unified evaluation protocol w.r.t. the design choices that may impact the mapping and localization. With the framework, we conduct a large suite of experiments, offering various insights in choosing the INRs and geometric rendering functions: for example, the dense feature grid outperforms other INRs (e.g. tri-plane and hash grid), even when geometric and color features are jointly encoded for memory efficiency. To extend the findings into the practical scenario, a hybrid encoding strategy is proposed to bring the best of the accuracy and completion from the gridbased and decomposition-based INRs. We further propose explicit hybrid encoding for high-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts the premise on robustness and computation efficiency.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
GXW5I4YC,preprint,2024.0,"Chen, Yufan; Zhang, Jiaming; Peng, Kunyu; Zheng, Junwei; Liu, Ruiping; Torr, Philip; Stiefelhagen, Rainer",RoDLA: Benchmarking the Robustness of Document Layout Analysis Models,,10.48550/arXiv.2403.14442,http://arxiv.org/abs/2403.14442,"Before developing a Document Layout Analysis (DLA) model in real-world applications, conducting comprehensive robustness testing is essential. However, the robustness of DLA models remains underexplored in the literature. To address this, we are the first to introduce a robustness benchmark for DLA models, which includes 450K document images of three datasets. To cover realistic corruptions, we propose a perturbation taxonomy with 36 common document perturbations inspired by real-world document processing. Additionally, to better understand document perturbation impacts, we propose two metrics, Mean Perturbation Effect (mPE) for perturbation assessment and Mean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we introduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA), which improves attention mechanisms to boost extraction of robust features. Experiments on the proposed benchmarks (PubLayNet-P, DocLayNet-P, and M$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of 115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA achieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.",2024-03-21,,CVPR.csv,,,,,,
H3RCDR5R,preprint,2023.0,"Huang, Ziqi; He, Yinan; Yu, Jiashuo; Zhang, Fan; Si, Chenyang; Jiang, Yuming; Zhang, Yuanhan; Wu, Tianxing; Jin, Qingyang; Chanpaisit, Nattapol; Wang, Yaohui; Chen, Xinyuan; Wang, Limin; Lin, Dahua; Qiao, Yu; Liu, Ziwei",VBench: Comprehensive Benchmark Suite for Video Generative Models,,10.48550/arXiv.2311.17982,http://arxiv.org/abs/2311.17982,"Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects ""video generation quality"" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has three appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). The evaluation metrics with fine-grained levels reveal individual models' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models' ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. We will open-source VBench, including all prompts, evaluation methods, generated videos, and human preference annotations, and also include more video generation models in VBench to drive forward the field of video generation.",2023-11-29,,CVPR.csv,,,,,,
P7IVMYAH,conferencePaper,2024.0,"Chen, Ziyang; Gebru, Israel D.; Richardt, Christian; Kumar, Anurag; Laney, William; Owens, Andrew; Richard, Alexander",Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02067,https://ieeexplore.ieee.org/document/10656013/,"We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities. The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms. We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data. In our evaluation, we thoroughly assessed existing audio and audio-visual models against multiple criteria and proposed settings to enhance their performance on real-world data. We also conducted experiments to investigate the impact of incorporating visual data (i.e., images and depth) into neural acoustic field models. Additionally, we demonstrated the effectiveness of a simple sim2real approach, where a model is pre-trained with simulated data and fine-tuned with sparse real-world data, resulting in significant improvements in the few-shot learning approach. RAF is the first dataset to provide densely captured room acoustic data, making it an ideal resource for researchers working on audio and audiovisual neural acoustic field modeling techniques. Demos and datasets are available on our project page.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
AW99UX2Y,conferencePaper,2024.0,"Li, Wenqiao; Xu, Xiaohao; Gu, Yao; Zheng, Bozhong; Gaol, Shenghua; Wu, Yingna",Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02096,https://ieeexplore.ieee.org/document/10658462/,"Recently, 3D anomaly detection, a crucial problem involving fine-grained geometry discrimination, is getting more attention. However, the lack of abundant real 3D anomaly data limits the scalability of current models. To enable scalable anomaly data collection, we propose a 3D anomaly synthesis pipeline to adapt existing large-scale 3D models for 3D anomaly detection. Specifically, we construct a synthetic dataset, i.e., Anomaly-ShapeNet, based on ShapeNet. Anomaly-ShapeNet consists of 1600 point cloud samples under 40 categories, which provides a rich and varied collection of data, enabling efficient training and enhancing adaptability to industrial scenarios. Meanwhile, to enable scalable representation learning for 3D anomaly localization, we propose a self-supervised method, i.e., Iterative Mask Reconstruction Network (IMRNet). During training, we propose a geometry-aware sample module to preserve potentially anomalous local regions during point cloud down-sampling. Then, we randomly mask out point patches and sent the visible patches to a transformer for reconstruction-based self-supervision. During testing, the point cloud repeatedly goes through the Mask Reconstruction Network, with each iteration’s output becoming the next input. By merging and contrasting the final reconstructed point cloud with the initial input, our method successfully locates anomalies. Experiments show that IMRNet outperforms previous state-of-the-art methods, achieving 66.1% in I-AUC on our Anomaly-ShapeNet dataset and 72.5% in I-AUC on Real3D-AD dataset. Our benchmark will be released at https://github.com/Chopper233/Anomaly-ShapeNet.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
3YR82PLT,journalArticle,,"Liu, Yun; Yang, Haolin; Si, Xu; Liu, Ling; Li, Zipeng; Zhang, Yuxiang; Liu, Yebin; Yi, Li",TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding,,,,,,,CVPR.csv,,,,,,
UE33QNUG,conferencePaper,2024.0,"Wang, Chengjie; Zhu, Wenbing; Gao, Bin-Bin; Gan, Zhenye; Zhang, Jiangning; Gu, Zhihao; Qian, Shuguang; Chen, Mingang; Ma, Lizhuang",Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02159,https://ieeexplore.ieee.org/document/10658200/,"Industrial anomaly detection (IAD) has garnered significant attention and experienced rapid development. However, the recent development of IAD approach has encountered certain difficulties due to dataset limitations. On the one hand, most of the state-of-the-art methods have achieved saturation (over 99% in AUROC) on mainstream datasets such as MVTec, and the differences of methods cannot be well distinguished, leading to a significant gap between public datasets and actual application scenarios. On the other hand, the research on various new practical anomaly detection settings is limited by the scale of the dataset, posing a risk of overfitting in evaluation results. Therefore, we propose a large-scale, Real-world, and multi-view Industrial Anomaly Detection dataset, named Real-IAD, which contains 150K high-resolution images of 30 different objects, an order of magnitude larger than existing datasets. It has a larger range of defect area and ratio proportions, making it more challenging than previous datasets. To make the dataset closer to real application scenarios, we adopted a multi-view shooting method and proposed sample-level evaluation metrics. In addition, beyond the general unsupervised anomaly detection setting, we propose a new setting for Fully Unsupervised Industrial Anomaly Detection (FUIAD) based on the observation that the yield rate in industrial production is usually greater than 60%, which has more practical application value. Finally, we report the results of popular IAD methods on the Real-IAD dataset, providing a highly challenging benchmark to promote the development of the IAD field.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
4TRBIH4V,conferencePaper,2024.0,"Pu, Bin; Wang, Liwen; Yang, Jiewen; He, Guannan; Dong, Xingbo; Li, Shengli; Tan, Ying; Chen, Ming; Jin, Zhe; Li, Kenli; Li, Xiaomeng",M<sup>3</sup> -UDA: A New Benchmark for Unsupervised Domain Adaptive Fetal Cardiac Structure Detection,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01104,https://ieeexplore.ieee.org/document/10656881/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
LAVHNET7,conferencePaper,2024.0,"Liu, Chen; Li, Peike Patrick; Yu, Qingtao; Sheng, Hongwei; Wang, Dadong; Li, Lincheng; Yu, Xin",Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02143,https://ieeexplore.ieee.org/document/10655596/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
JCZ9PEWJ,journalArticle,,"Hu, Yutao; Li, Tianbin; Lu, Quanfeng; Shao, Wenqi; He, Junjun; Qiao, Yu; Luo, Ping",OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM,,,,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks. However, their potential in the medical domain remains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications. To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical Visual Question Answering (VQA) benchmark. This benchmark is collected from 73 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions. Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs. Through our extensive experiments, we have found that existing LVLMs struggle to address these medical VQA problems effectively. Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field. The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset’s significance. Our code with dataset are available at https://github.com/OpenGVLab/ Multi-Modality-Arena.",,,CVPR.csv,,,,,,
PNFGPH84,conferencePaper,2024.0,"Papa, Samuele; Valperga, Riccardo; Knigge, David; Kofinas, Miltiadis; Lippe, Phillip; Sonke, Jan-Jakob; Gavves, Efstratios",How to Train Neural Field Representations: A Comprehensive Study and Benchmark,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02134,https://ieeexplore.ieee.org/document/10657405/,"Neural fields (NeFs) have recently emerged as a versatile method for modeling signals of various modalities, including images, shapes, and scenes. Subsequently, a number of works have explored the use of NeFs as representations for downstream tasks, e.g. classifying an image based on the parameters of a NeF that has been fit to it. However, the impact of the NeF hyperparameters on their quality as downstream representation is scarcely understood and remains largely unexplored. This is in part caused by the large amount of time required to fit datasets of neural fields.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
JS6HNCJH,preprint,2024.0,"Zhang, Chenshuang; Pan, Fei; Kim, Junmo; Kweon, In So; Mao, Chengzhi",ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object,,10.48550/arXiv.2403.18775,http://arxiv.org/abs/2403.18775,"We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models’ robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at https://github.com/chenshuangzhang/imagenet_d.",2024-03-27,,CVPR.csv,,,,,,
9FNQGQPT,preprint,2024.0,"Li, Kunchang; Wang, Yali; He, Yinan; Li, Yizhuo; Wang, Yi; Liu, Yi; Wang, Zun; Xu, Jilan; Chen, Guo; Luo, Ping; Wang, Limin; Qiao, Yu",MVBench: A Comprehensive Multi-modal Video Understanding Benchmark,,10.48550/arXiv.2311.17005,http://arxiv.org/abs/2311.17005,"With the rapid development of Multi-modal Large Language Models (MLLMs), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models. However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks. To alleviate this issue, we introduce a comprehensive Multi-modal Video understanding Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot be effectively solved with a single frame. Specifically, we first introduce a novel staticto-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition. Then, guided by the task definition, we automatically convert public video annotations into multiplechoice QA to evaluate each task. On one hand, such a distinct paradigm allows us to build MVBench efficiently, without much manual intervention. On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of LLMs. Moreover, we further develop a robust video MLLM baseline, i.e., VideoChat2, by progressive multi-modal training with diverse instruction-tuning data. The extensive results on our MVBench reveal that, the existing MLLMs are far from satisfactory in temporal understanding, while our VideoChat2 largely surpasses these leading models by over 15% on MVBench. All models and data are available at https: //github.com/OpenGVLab/Ask-Anything.",2024-05-23,,CVPR.csv,,,,,,
JI5FHBYN,conferencePaper,2024.0,"Cao, Xu; Zhou, Tong; Ma, Yunsheng; Ye, Wenqian; Cui, Can; Tang, Kun; Cao, Zhipeng; Liang, Kaizhao; Wang, Ziran; Rehg, James M.; Zheng, Chao",MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02061,https://ieeexplore.ieee.org/document/10657418/,"Vision-language generative AI has demonstrated remarkable promise for empowering cross-modal scene understanding of autonomous driving and high-definition (HD) map systems. However, current benchmark datasets lack multi-modal point cloud, image, and language data pairs. Recent approaches utilize visual instruction learning and cross-modal prompt engineering to expand visionlanguage models into this domain. In this paper, we propose a new vision-language benchmark that can be used to finetune traffic and HD map domain-specific foundation models. Specifically, we annotate and leverage large-scale, broad-coverage traffic and map data extracted from huge HD map annotations, and use CLIP and LLaMA-2 / Vicuna to finetune a baseline model with instruction-following data. Our experimental results across various algorithms reveal that while visual instruction-tuning large language models (LLMs) can effectively learn meaningful representations from MAPLM-QA, there remains significant room for further advancements. To facilitate applying LLMs and multi-modal data into self-driving research, we will release our visual-language QA data, and the baseline models at GitHub.com/LLVM-AD/MAPLM.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
434HJQGC,conferencePaper,2024.0,"Li, Bohao; Ge, Yuying; Ge, Yixiao; Wang, Guangzhi; Wang, Rui; Zhang, Ruimao; Shan, Ying",SEED-Bench: Benchmarking Multimodal Large Language Models,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01263,https://ieeexplore.ieee.org/document/10658180/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
YVMW8CMR,conferencePaper,2024.0,"Yin, Zijin; Liang, Kongming; Li, Bing; Ma, Zhanyu; Guo, Jun",Benchmarking Segmentation Models with Mask-Preserved Attribute Editing,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02124,https://ieeexplore.ieee.org/document/10657997/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
WN5QRLW9,conferencePaper,2024.0,"Zeng, Runhao; Chen, Xiaoyong; Liang, Jiaming; Wu, Huisi; Cao, Guangzhong; Guo, Yong",Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01729,https://ieeexplore.ieee.org/document/10655650/,"Temporal action detection (TAD) aims to locate action positions and recognize action categories in long-term untrimmed videos. Although many methods have achieved promising results, their robustness has not been thoroughly studied. In practice, we observe that temporal information in videos can be occasionally corrupted, such as missing or blurred frames. Interestingly, existing methods often incur a significant performance drop even if only one frame is affected. To formally evaluate the robustness, we establish two temporal corruption robustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper, we extensively analyze the robustness of seven leading TAD methods and obtain some interesting findings: 1) Existing methods are particularly vulnerable to temporal corruptions, and end-to-end methods are often more susceptible than those with a pre-trained feature extractor; 2) Vulnerability mainly comes from localization error rather than classification error; 3) When corruptions occur in the middle of an action instance, TAD models tend to yield the largest performance drop. Besides building a benchmark, we further develop a simple but effective robust training method to defend against temporal corruptions, through the FrameDrop augmentation and Temporal-Robust Consistency loss. Remarkably, our approach not only improves robustness but also yields promising improvements on clean data. We believe that this study will serve as a benchmark for future research in robust video analysis. Source code and models are available at https://github.com/AlvinZeng/temporal-robustness-benchmark.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
YWPN9CJM,conferencePaper,2024.0,"Deng, Bowen; Song, Siyang; French, Andrew P.; Schluppeck, Denis; Pound, Michael P.","Advancing Saliency Ranking with Human Fixations: Dataset, Models and Benchmarks",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02678,https://ieeexplore.ieee.org/document/10656059/,"Saliency ranking detection (SRD) has emerged as a challenging task in computer vision, aiming not only to identify salient objects within images but also to rank them based on their degree of saliency. Existing SRD datasets have been created primarily using mouse-trajectory data, which inadequately captures the intricacies of human visual perception. Addressing this gap, this paper introduces the first large-scale SRD dataset, SIFR, constructed using genuine human fixation data, thereby aligning more closely with real visual perceptual processes. To establish a baseline for this dataset, we propose QAGNet, a novel model that leverages salient instance query features from a transformer detector within a tri-tiered nested graph. Through extensive experiments, we demonstrate that our approach outperforms existing state-of-the-art methods across two widely used SRD datasets and our newly proposed dataset. Code and dataset are available at https://github.com/ EricDengbowen/QAGNet.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
UM6U2I9H,conferencePaper,2024.0,"Tao, M.; Bai, Bing; Lin, Haozhe; Wang, Heyuan; Wang, Yu; Luo, Lin; Fang, Lu",When Visual Grounding Meets Gigapixel-Level Large-Scale Scenes: Benchmark and Approach,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02088,https://ieeexplore.ieee.org/document/10656407/,"Visual grounding refers to the process of associating natural language expressions with corresponding regions within an image. Existing benchmarks for visual grounding primarily operate within small-scale scenes with a few objects. Nevertheless, recent advances in imaging technology have enabled the acquisition of gigapixel-level images, providing high-resolution details in large-scale scenes containing numerous objects. To bridge this gap between imaging and computer vision benchmarks and make grounding more practically valuable, we introduce a novel dataset, named GigaGrounding, designed to challenge visual grounding models in gigapixel-level large-scale scenes. We extensively analyze and compare the dataset with existing benchmarks, demonstrating that GigaGrounding presents unique challenges such as large-scale scene understanding, gigapixellevel resolution, significant variations in object scales, and the “multi-hop expressions”. Furthermore, we introduced a simple yet effective grounding approach, which employs a “glance-to-zoom-in” paradigm and exhibits enhanced capabilities for addressing the GigaGrounding task. The dataset is available at www.gigavision.ai.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
WMKTCMNP,preprint,2024.0,"Yuan, Haocheng; Xu, Jing; Pan, Hao; Bousseau, Adrien; Mitra, Niloy J.; Li, Changjian",CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs,,10.48550/arXiv.2311.16703,http://arxiv.org/abs/2311.16703,"CAD programs are a popular way to compactly encode shapes as a sequence of operations that are easy to parametrically modify. However, without sufficient semantic comments and structure, such programs can be challenging to understand, let alone modify. We introduce the problem of semantic commenting CAD programs, wherein the goal is to segment the input program into code blocks corresponding to semantically meaningful shape parts and assign a semantic label to each block. We solve the problem by combining program parsing with visual-semantic analysis afforded by recent advances in foundational language and vision models. Specifically, by executing the input programs, we create shapes, which we use to generate conditional photorealistic images to make use of semantic annotators for such images. We then distill the information across the images and link back to the original programs to semantically comment on them. Additionally, we collected and annotated a benchmark dataset, CADTalk, consisting of 5,288 machine-made programs and 45 human-made programs with ground truth semantic comments. We extensively evaluated our approach, compared it to a GPT-based baseline, and an open-set shape segmentation baseline, and reported an 83.24% accuracy on the new CADTalk dataset. Code and data: https://enigma-li.github.io/CADTalk/.",2024-03-25,,CVPR.csv,,,,,,
DZDGB5ZF,preprint,2024.0,"Zheng, Xiaoyun; Liao, Liwei; Li, Xufeng; Jiao, Jianbo; Wang, Rongjie; Gao, Feng; Wang, Shiqi; Wang, Ronggang",PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling,,10.48550/arXiv.2403.16080,http://arxiv.org/abs/2403.16080,"High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data.",2024-04-02,,CVPR.csv,,,,,,
7CM99RLG,preprint,2023.0,"Balasingam, Arjun; Chandler, Joseph; Li, Chenning; Zhang, Zhoutong; Balakrishnan, Hari",DriveTrack: A Benchmark for Long-Range Point Tracking in Real-World Videos,,10.48550/arXiv.2312.09523,http://arxiv.org/abs/2312.09523,"This paper presents DriveTrack, a new benchmark and data generation framework for long-range keypoint tracking in real-world videos. DriveTrack is motivated by the observation that the accuracy of state-of-the-art trackers depends strongly on visual attributes around the selected keypoints, such as texture and lighting. The problem is that these artifacts are especially pronounced in real-world videos, but these trackers are unable to train on such scenes due to a dearth of annotations. DriveTrack bridges this gap by building a framework to automatically annotate point tracks on autonomous driving datasets. We release a dataset consisting of 1 billion point tracks across 24 hours of video, which is seven orders of magnitude greater than prior real-world benchmarks and on par with the scale of synthetic benchmarks. DriveTrack unlocks new use cases for point tracking in real-world videos. First, we show that finetuning keypoint trackers on DriveTrack improves accuracy on real-world scenes by up to 7%. Second, we analyze the sensitivity of trackers to visual artifacts in real scenes and motivate the idea of running assistive keypoint selectors alongside trackers.",2023-12-15,,CVPR.csv,,,,,,
JWGAGQCG,conferencePaper,2024.0,"Xie, Yaofeng; Kong, Lingwei; Chen, Kai; Zheng, Ziqiang; Yu, Xiao; Yu, Zhibin; Zheng, Bing",UVEB: A Large-scale Benchmark and Baseline Towards Real-World Underwater Video Enhancement,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02110,https://ieeexplore.ieee.org/document/10656519/,"Learning-based underwater image enhancement (UIE) methods have made great progress. However, the lack of large-scale and high-quality paired training samples has become the main bottleneck hindering the development of UIE. The inter-frame information in underwater videos can accelerate or optimize the UIE process. Thus, we constructed the first large-scale high-resolution underwater video enhancement benchmark (UVEB) to promote the development of underwater vision. It contains 1,308 pairs of video sequences and more than 453,000 high-resolution with 38% Ultra-High-Definition (UHD) 4K frame pairs. UVEB comes from multiple countries, containing various scenes and video degradation types to adapt to diverse and complex underwater environments. We also propose the first supervised underwater video enhancement method, UVENet. UVE-Net converts the current frame information into convolutional kernels and passes them to adjacent frames for efficient inter-frame information exchange. By fully utilizing the redundant degraded information of underwater videos, UVE-Net completes video enhancement better. Experiments show the effective network design and good performance of UVE-Net.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
2KVTGARI,conferencePaper,2024.0,"Woo, Sanghyun; Park, Kwanyong; Shin, Inkyu; Kim, Myungchul; Kweon, In So",MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02108,https://ieeexplore.ieee.org/document/10655925/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
HX9XTY8Z,conferencePaper,2024.0,"Fu, Huiyuan; Peng, Fei; Li, Xianwei; Li, Yejun; Wang, Xin; Ma, Huadong",Continuous Optical Zooming: A Benchmark for Arbitrary-Scale Image Super-Resolution in Real World,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.00293,https://ieeexplore.ieee.org/document/10655459/,"Most current arbitrary-scale image super-resolution (SR) methods has commonly relied on simulated data generated by simple synthetic degradation models (e.g., bicubic downsampling) at continuous various scales, thereby falling short in capturing the complex degradation of real-world images. This limitation hinders the visual quality of these methods when applied to real-world images. To address this issue, we propose the Continuous Optical Zooming dataset (COZ), by constructing an automatic imaging system to collect images at fine-grained various focal lengths within a specific range and providing strict image pair alignment. The COZ dataset serves as a benchmark to provide real-world data for training and testing arbitrary-scale SR models. To enhance the model’s robustness against real-world image degradation, we propose a Local Mix Implicit network (LMI) based on the MLP-mixer architecture and meta-learning, which directly learns the local texture information by simultaneously mixing features and coordinates of multiple independent points. The extensive experiments demonstrate the superior performance of the arbitrary-scale SR models trained on the COZ dataset compared to models trained on simulated data. Our LMI model exhibits the superior effectiveness compared to other models. This study is of great significance in developing more efficient algorithms and improving the performance of arbitrary-scale image SR methods in practical applications. Our dataset and codes are available at https://github.com/pf0607/COZ.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
CST8WHRX,conferencePaper,2024.0,"Kaul, Prannay; Li, Zhizhong; Yang, Hao; Dukler, Yonatan; Swaminathan, Ashwin; Taylor, C. J.; Soatto, Stefano",THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02571,https://ieeexplore.ieee.org/document/10657848/,"Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term “Type I hallucinations”. Instead, they focus on hallucinations responding to very specific question formats—typically a multiple-choice response regarding a particular object or attribute—which we term “Type II hallucinations”. Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
2LQQKSLV,conferencePaper,2024.0,"Mais, Lisa; Hirsch, Peter; Managan, Claire; Kandarpa, Ramya; Rumberger, Josef Lorenz; Reinke, Annika; Maier-Hein, Lena; Ihrke, Gudrun; Kainmueller, Dagmar",FISBe: A Real-World Benchmark Dataset for Instance Segmentation of Long-Range thin Filamentous Structures,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.02100,https://ieeexplore.ieee.org/document/10656899/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
CNAGWNS5,conferencePaper,2024.0,"Ma, Yunsheng; Cui, Can; Cao, Xu; Ye, Wenqian; Liu, Peiran; Lu, Juanwu; Abdelraouf, Amr; Gupta, Rohit; Han, Kyungtae; Bera, Aniket; Rehg, James M.; Wang, Ziran",LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01434,https://ieeexplore.ieee.org/document/10657936/,,2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
2UKF2U35,journalArticle,,"Du, Hang; Zhang, Sicheng; Xie, Binzhu; Nan, Guoshun; Zhang, Jiayang; Xu, Junrui; Liu, Hangyu; Leng, Sicong; Liu, Jiangming; Fan, Hehe; Huang, Dajiu; Feng, Jing; Chen, Linli; Zhang, Can; Li, Xuhuan; Zhang, Hao; Chen, Jianhang; Cui, Qimei; Tao, Xiaofeng",Uncovering What Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly,,,,,,,CVPR.csv,,,,,,
JKD6587F,preprint,2024.0,"Wang, Andong; Wu, Bo; Chen, Sunli; Chen, Zhenfang; Guan, Haotian; Lee, Wei-Ning; Li, Li Erran; Gan, Chuang",SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge,,10.48550/arXiv.2405.09713,http://arxiv.org/abs/2405.09713,"Learning commonsense reasoning from visual contexts and scenes in real-world is a crucial step toward advanced artificial intelligence. However, existing video reasoning benchmarks are still inadequate since they were mainly designed for factual or situated reasoning and rarely involve broader knowledge in the real world. Our work aims to delve deeper into reasoning evaluations, specifically within dynamic, open-world, and structured context knowledge. We propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K situations with instance-level annotations depicted in the videos. The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving. To create such a dataset, we propose an automatic and scalable generation method to generate question-answer pairs, knowledge graphs, and rationales by instructing the combinations of LLMs and MLLMs. Concretely, we first extract observable situated entities, relations, and processes from videos for situated knowledge and then extend to openworld knowledge beyond the visible content. The task generation is facilitated through multiple dialogues as iterations and subsequently corrected and refined by our designed self-promptings and demonstrations. With a corpus of both explicit situated facts and implicit commonsense, we generate associated question-answer pairs and reasoning processes, finally followed by manual reviews for quality assurance. We evaluated recent mainstream large visionlanguage models on the benchmark and found several insightful conclusions. For more information, please refer to our benchmark at www.bobbywu.com/SOKBench.",2024-05-17,,CVPR.csv,,,,,,
MSNU5R64,preprint,2023.0,"Ma, Junyi; Chen, Xieyuanli; Huang, Jiawei; Xu, Jingyi; Luo, Zhen; Xu, Jintao; Gu, Weihao; Ai, Rui; Wang, Hesheng",Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications,,10.48550/arXiv.2311.17663,http://arxiv.org/abs/2311.17663,"Understanding how the surrounding environment changes is crucial for performing downstream tasks safely and reliably in autonomous driving applications. Recent occupancy estimation techniques using only camera images as input can provide dense occupancy representations of large-scale scenes based on the current observation. However, they are mostly limited to representing the current 3D space and do not consider the future state of surrounding objects along the time axis. To extend camera-only occupancy estimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark for camera-only 4D occupancy forecasting, evaluating the surrounding scene changes in a near future. We build our benchmark based on multiple publicly available datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5, which provides sequential occupancy states of general movable and static objects, as well as their 3D backward centripetal flow. To establish this benchmark for future research with comprehensive comparisons, we introduce four baseline types from diverse camera-based perception and prediction implementations, including a staticworld occupancy model, voxelization of point cloud prediction, 2D-3D instance-based prediction, and our proposed novel endto-end 4D occupancy forecasting network. Furthermore, the standardized evaluation protocol for preset multiple tasks is also provided to compare the performance of all the proposed baselines on present and future occupancy estimation with respect to objects of interest in autonomous driving scenarios. The dataset and our implementation of all four baselines in the proposed Cam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.",2023-12-07,,CVPR.csv,,,,,,
ZNQSM5LP,preprint,2024.0,"Yue, Xiang; Ni, Yuansheng; Zhang, Kai; Zheng, Tianyu; Liu, Ruoqi; Zhang, Ge; Stevens, Samuel; Jiang, Dongfu; Ren, Weiming; Sun, Yuxuan; Wei, Cong; Yu, Botao; Yuan, Ruibin; Sun, Renliang; Yin, Ming; Zheng, Boyuan; Yang, Zhenzhu; Liu, Yibo; Huang, Wenhao; Sun, Huan; Su, Yu; Chen, Wenhu",MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,,10.48550/arXiv.2311.16502,http://arxiv.org/abs/2311.16502,"We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.",2024-06-13,,CVPR.csv,,,,,,
NXQF9STX,conferencePaper,2024.0,"Zhang, Junyuan; Zeng, Shuang; Zhang, Miao; Wang, Runxi; Wang, Feifei; Zhou, Yuyin; Liang, Paul Pu; Qu, Liangqiong",FLHetBench: Benchmarking Device and State Heterogeneity in Federated Learning,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),10.1109/CVPR52733.2024.01150,https://ieeexplore.ieee.org/document/10657899/,"Federated learning (FL) is a powerful technology that enables collaborative training of machine learning models without sharing private data among clients. The fundamental challenge in FL lies in learning over extremely heterogeneous data distributions, device capacities, and device state availabilities, all of which adversely impact performance and communication efficiency. While data heterogeneity has been well-studied in the literature, this paper introduces FLHetBench, the first FL benchmark targeted toward understanding device and state heterogeneity. FLHetBench comprises two new sampling methods to generate real-world device and state databases with varying heterogeneity and new metrics for quantifying the success of FL methods under these real-world constraints. Using FLHetBench, we conduct a comprehensive evaluation of existing methods and find that they struggle under these settings, which inspires us to propose BiasPrompt+, a new method employing staleness-aware aggregation and fast weights to tackle these new heterogeneity challenges. Experiments on various FL tasks and datasets validate the effectiveness of our BiasPrompt+ method and highlight the value of FLHetBench in fostering the development of more efficient and robust FL solutions under real-world device and state constraints.",2024-06-16,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR.csv,,,,,,
X3QJSSHG,preprint,2023.0,"Chen, Huancheng; Vikalo, Haris",Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices,,10.48550/arXiv.2311.18129,http://arxiv.org/abs/2311.18129,"While federated learning (FL) systems often utilize quantization to battle communication and computational bottlenecks, they have heretofore been limited to deploying fixedprecision quantization schemes. Meanwhile, the concept of mixed-precision quantization (MPQ), where different layers of a deep learning model are assigned varying bit-width, remains unexplored in the FL settings. We present a novel FL algorithm, FedMPQ, which introduces mixed-precision quantization to resource-heterogeneous FL systems. Specifically, local models, quantized so as to satisfy bit-width constraint, are trained by optimizing an objective function that includes a regularization term which promotes reduction of precision in some of the layers without significant performance degradation. The server collects local model updates, de-quantizes them into full-precision models, and then aggregates them into a global model. To initialize the next round of local training, the server relies on the information learned in the previous training round to customize bit-width assignments of the models delivered to different clients. In extensive benchmarking experiments on several model architectures and different datasets in both iid and non-iid settings, FedMPQ outperformed the baseline FL schemes that utilize fixed-precision quantization while incurring only a minor computational overhead on the participating devices.",2023-11-29,,CVPR.csv,,,,,,
TMEBI2NQ,preprint,2023.0,"Patel, Maitreya; Kim, Changhoon; Cheng, Sheng; Baral, Chitta; Yang, Yezhou",ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations,,10.48550/arXiv.2312.04655,http://arxiv.org/abs/2312.04655,"Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g., DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional T2I benchmarks, at the cost of significant computational resources. The unCLIP stack comprises T2I prior and diffusion image decoder. The T2I prior model alone adds a billion parameters compared to the Latent Diffusion Models, which increases the computational and high-quality data requirements. We introduce ECLIPSE, a novel contrastive learning method that is both parameter and data-efficient. ECLIPSE leverages pre-trained vision-language models (e.g., CLIP) to distill the knowledge into the prior model. We demonstrate that the ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere 2.8% of the data, surpasses the baseline T2I priors with an average of 71.6% preference score under resource-limited setting. It also attains performance on par with SOTA big models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions. Extensive experiments on two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE priors consistently deliver high performance while significantly reducing resource dependency.",2023-12-07,,CVPR.csv,,,,,,
QEWGFYA9,journalArticle,,"Ilhan, Fatih; Su, Gong; Tekin, Selim Furkan; Huang, Tiansheng; Hu, Sihao; Liu, Ling",Resource-Efficient Transformer Pruning for Finetuning of Large Models,,,,"With the recent advances in vision transformers and large language models (LLMs), ﬁnetuning costly large models on downstream learning tasks poses signiﬁcant challenges under limited computational resources. This paper presents a REsource and ComputAtion-efﬁcient Pruning framework (RECAP) for the ﬁnetuning of transformerbased large models. RECAP by design bridges the gap between efﬁciency and performance through an iterative process cycling between pruning, ﬁnetuning, and updating stages to explore different chunks of the given largescale model. At each iteration, we ﬁrst prune the model with Taylor-approximation-based importance estimation and then only update a subset of the pruned model weights based on the Fisher-information criterion. In this way, RECAP achieves two synergistic and yet conﬂicting goals: reducing the GPU memory footprint while maintaining model performance, unlike most existing pruning methods that require the model to be ﬁnetuned beforehand for better preservation of model performance. We perform extensive experiments with a wide range of large transformer-based architectures on various computer vision and natural language understanding tasks. Compared to recent pruning techniques, we demonstrate that RECAP offers signiﬁcant improvements in GPU memory efﬁciency, capable of reducing the footprint by up to 65%.",,,CVPR.csv,,,,,,
EP3NEYEB,preprint,2024.0,"Zhang, Yunhua; Doughty, Hazel; Snoek, Cees G. M.",Low-Resource Vision Challenges for Foundation Models,,10.48550/arXiv.2401.04716,http://arxiv.org/abs/2401.04716,"Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for deep learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we address this gap and explore the challenges of low-resource image tasks with vision foundation models. We first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share three challenges: data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on our three low-resource tasks demonstrate our proposals already provide a better baseline than transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project page: https://xiaobai1217.github.io/ Low-Resource-Vision/.",2024-04-11,,CVPR.csv,,,,,,
